{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 781.513, "latencies_ms": [781.513], "images_per_second": 1.28, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A woman is standing at the dining table, preparing food in a cozy living room with yellow walls, a wooden floor, and a green accent wall.", "error": null, "sys_before": {"cpu_percent": 5.8, "ram_used_mb": 25482.4, "ram_available_mb": 100289.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25475.3, "ram_available_mb": 100296.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4418.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4418.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.76, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.05, "peak": 32.7, "min": 21.67}, "VIN": {"avg": 60.77, "peak": 89.21, "min": 51.03}, "VDD_CPU_SOC_MSS": {"avg": 13.52, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 27.05, "energy_joules_est": 21.16, "sample_count": 6, "duration_seconds": 0.782}, "timestamp": "2026-01-17T17:38:02.349115"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1751.822, "latencies_ms": [1751.822], "images_per_second": 0.571, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 1, "output_text": "TV: 1\nCabinet: 2\nRadiator: 1\nChair: 4\nDining table: 2\nWoman: 1\nFlowers: 2\nVase: 1\nFireplace: 1\nRug: 1\nTable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.3, "ram_available_mb": 100296.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25460.1, "ram_available_mb": 100312.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4418.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4418.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 22.74, "peak": 33.5, "min": 17.73}, "VIN": {"avg": 57.38, "peak": 76.98, "min": 51.35}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 22.74, "energy_joules_est": 39.85, "sample_count": 13, "duration_seconds": 1.752}, "timestamp": "2026-01-17T17:38:04.112151"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 917.562, "latencies_ms": [917.562], "images_per_second": 1.09, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The living room is positioned to the left of the dining area. The dining area is located in the background, separated by a narrow opening. The living room and dining area are separated by a small gap.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25460.1, "ram_available_mb": 100312.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25452.7, "ram_available_mb": 100319.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4418.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4418.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.66, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 25.82, "peak": 30.34, "min": 20.89}, "VIN": {"avg": 61.77, "peak": 90.48, "min": 55.68}, "VDD_CPU_SOC_MSS": {"avg": 13.45, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 25.82, "energy_joules_est": 23.7, "sample_count": 6, "duration_seconds": 0.918}, "timestamp": "2026-01-17T17:38:05.036148"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1587.137, "latencies_ms": [1587.137], "images_per_second": 0.63, "prompt_tokens": 19, "response_tokens_est": 80, "n_tiles": 1, "output_text": "The scene depicts a modern, open-plan living space with a dining area, kitchen, and living room. The living area features a flat-screen TV, wooden furniture, and warm lighting. A woman is standing at the dining table, seemingly engaged in conversation or preparing to eat. The room is decorated with potted plants, vases, and a fireplace, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25452.7, "ram_available_mb": 100319.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25436.9, "ram_available_mb": 100335.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4418.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4418.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 22.95, "peak": 31.92, "min": 18.13}, "VIN": {"avg": 57.06, "peak": 79.1, "min": 48.87}, "VDD_CPU_SOC_MSS": {"avg": 13.75, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 22.95, "energy_joules_est": 36.44, "sample_count": 12, "duration_seconds": 1.588}, "timestamp": "2026-01-17T17:38:06.630406"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 820.655, "latencies_ms": [820.655], "images_per_second": 1.219, "prompt_tokens": 18, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The room features warm yellow walls and rich hardwood flooring. Natural light streams in through multiple windows, creating a bright and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25436.9, "ram_available_mb": 100335.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25428.9, "ram_available_mb": 100343.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4418.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4418.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 26.01, "peak": 30.74, "min": 21.28}, "VIN": {"avg": 60.57, "peak": 79.94, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 13.59, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.01, "energy_joules_est": 21.36, "sample_count": 6, "duration_seconds": 0.821}, "timestamp": "2026-01-17T17:38:07.461689"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 670.474, "latencies_ms": [670.474], "images_per_second": 1.491, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A brown bear sits in a grassy area, gazing directly at the camera with a calm and curious expression.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 25428.9, "ram_available_mb": 100343.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25429.4, "ram_available_mb": 100342.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.61, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 29.17, "peak": 33.89, "min": 24.05}, "VIN": {"avg": 65.89, "peak": 109.74, "min": 45.98}, "VDD_CPU_SOC_MSS": {"avg": 13.54, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 29.17, "energy_joules_est": 19.57, "sample_count": 5, "duration_seconds": 0.671}, "timestamp": "2026-01-17T17:38:08.147527"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1095.567, "latencies_ms": [1095.567], "images_per_second": 0.913, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "bear: 1\ngrass: 2\nfur: 2\nface: 2\nears: 2\nnose: 1\nmouth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25429.4, "ram_available_mb": 100342.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25413.7, "ram_available_mb": 100358.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.77, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 26.95, "peak": 35.09, "min": 20.1}, "VIN": {"avg": 62.29, "peak": 115.81, "min": 50.3}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 26.95, "energy_joules_est": 29.54, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T17:38:09.251068"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 692.8, "latencies_ms": [692.8], "images_per_second": 1.443, "prompt_tokens": 25, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The bear is positioned in the foreground, close to the camera. The grassy background extends behind it, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25413.7, "ram_available_mb": 100358.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25412.8, "ram_available_mb": 100359.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 28.69, "peak": 33.1, "min": 23.64}, "VIN": {"avg": 67.09, "peak": 116.28, "min": 53.69}, "VDD_CPU_SOC_MSS": {"avg": 13.62, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 28.69, "energy_joules_est": 19.89, "sample_count": 5, "duration_seconds": 0.693}, "timestamp": "2026-01-17T17:38:09.950132"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 757.148, "latencies_ms": [757.148], "images_per_second": 1.321, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A large brown bear is sitting on green grass, gazing directly at the camera. The scene appears to be outdoors in a natural environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25412.8, "ram_available_mb": 100359.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25403.7, "ram_available_mb": 100368.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.65, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 30.34, "peak": 35.07, "min": 24.82}, "VIN": {"avg": 65.59, "peak": 108.81, "min": 50.08}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 30.34, "energy_joules_est": 22.98, "sample_count": 5, "duration_seconds": 0.757}, "timestamp": "2026-01-17T17:38:10.717594"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1099.413, "latencies_ms": [1099.413], "images_per_second": 0.91, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The bear's fur is a light brown color. The lighting appears to be natural, possibly sunlight, giving the bear a warm and alert appearance. The bear is situated in a grassy area, suggesting it might be in a natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.7, "ram_available_mb": 100368.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25396.0, "ram_available_mb": 100376.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 26.75, "peak": 34.66, "min": 20.11}, "VIN": {"avg": 61.75, "peak": 110.75, "min": 49.82}, "VDD_CPU_SOC_MSS": {"avg": 13.64, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.75, "energy_joules_est": 29.42, "sample_count": 8, "duration_seconds": 1.1}, "timestamp": "2026-01-17T17:38:11.825513"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 844.589, "latencies_ms": [844.589], "images_per_second": 1.184, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The room features a comfortable blue-comforter bed, wooden dresser with mirror, bookshelf, and potted plants, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25396.0, "ram_available_mb": 100376.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25388.7, "ram_available_mb": 100383.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 26.8, "peak": 31.92, "min": 21.67}, "VIN": {"avg": 60.95, "peak": 83.69, "min": 51.6}, "VDD_CPU_SOC_MSS": {"avg": 13.66, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.8, "energy_joules_est": 22.64, "sample_count": 6, "duration_seconds": 0.845}, "timestamp": "2026-01-17T17:38:12.682279"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1384.473, "latencies_ms": [1384.473], "images_per_second": 0.722, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "bed: 1\ndresser: 2\nmirror: 1\nchair: 1\nbookshelf: 5\nplant: 2\nwicker basket: 1\ncarpet: 1\nwindow: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25388.7, "ram_available_mb": 100383.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25389.2, "ram_available_mb": 100383.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 24.12, "peak": 32.3, "min": 18.52}, "VIN": {"avg": 57.32, "peak": 77.39, "min": 50.26}, "VDD_CPU_SOC_MSS": {"avg": 13.82, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.12, "energy_joules_est": 33.41, "sample_count": 10, "duration_seconds": 1.385}, "timestamp": "2026-01-17T17:38:14.074635"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1009.917, "latencies_ms": [1009.917], "images_per_second": 0.99, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The bed occupies the foreground, positioned to the left of the image. The bookshelf occupies the background, extending from right to left. The window is situated in the background, offering a view of the greenery outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.2, "ram_available_mb": 100383.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25382.8, "ram_available_mb": 100389.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 25.27, "peak": 30.73, "min": 20.1}, "VIN": {"avg": 58.78, "peak": 79.19, "min": 50.32}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 25.27, "energy_joules_est": 25.53, "sample_count": 7, "duration_seconds": 1.01}, "timestamp": "2026-01-17T17:38:15.090856"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1157.443, "latencies_ms": [1157.443], "images_per_second": 0.864, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The scene depicts a cozy bedroom with a large window offering a view of lush greenery outside. The room features a comfortable bed, a wooden dresser, a bookshelf filled with books, and a chair. The room is bathed in natural light, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25382.8, "ram_available_mb": 100389.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.96, "peak": 31.92, "min": 19.69}, "VIN": {"avg": 59.94, "peak": 88.91, "min": 52.49}, "VDD_CPU_SOC_MSS": {"avg": 13.74, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 24.96, "energy_joules_est": 28.9, "sample_count": 8, "duration_seconds": 1.158}, "timestamp": "2026-01-17T17:38:16.258806"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1059.011, "latencies_ms": [1059.011], "images_per_second": 0.944, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The room features a blue comforter, a wooden dresser with a mirror, and a large window that lets in natural light. The walls are covered in floral wallpaper, and the carpet is light-colored.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25371.3, "ram_available_mb": 100400.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.76, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 24.83, "peak": 31.13, "min": 19.72}, "VIN": {"avg": 59.22, "peak": 83.86, "min": 50.58}, "VDD_CPU_SOC_MSS": {"avg": 13.63, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 24.83, "energy_joules_est": 26.31, "sample_count": 8, "duration_seconds": 1.059}, "timestamp": "2026-01-17T17:38:17.324560"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 560.465, "latencies_ms": [560.465], "images_per_second": 1.784, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A red stop sign is mounted on a metal pole at the corner of a street, with a building and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.3, "ram_available_mb": 100400.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 25368.3, "ram_available_mb": 100403.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.41, "peak": 13.79, "min": 12.98}, "VDD_GPU": {"avg": 22.26, "peak": 24.83, "min": 20.1}, "VIN": {"avg": 54.55, "peak": 59.59, "min": 50.28}, "VDD_CPU_SOC_MSS": {"avg": 13.29, "peak": 13.39, "min": 12.99}}, "power_watts_avg": 22.26, "energy_joules_est": 12.49, "sample_count": 4, "duration_seconds": 0.561}, "timestamp": "2026-01-17T17:38:17.895647"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1218.519, "latencies_ms": [1218.519], "images_per_second": 0.821, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Stop sign: 1\nPole: 1\nTrees: 4\nBuildings: 2\nTractor: 1\nShrubs: 2\nAsphalt: 2\nStreet: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.3, "ram_available_mb": 100403.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25365.6, "ram_available_mb": 100406.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.66, "peak": 14.2, "min": 12.88}, "VDD_GPU": {"avg": 20.49, "peak": 25.61, "min": 17.73}, "VIN": {"avg": 55.43, "peak": 58.6, "min": 51.9}, "VDD_CPU_SOC_MSS": {"avg": 13.61, "peak": 13.79, "min": 12.99}}, "power_watts_avg": 20.49, "energy_joules_est": 24.97, "sample_count": 9, "duration_seconds": 1.219}, "timestamp": "2026-01-17T17:38:19.121033"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1015.107, "latencies_ms": [1015.107], "images_per_second": 0.985, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The stop sign is positioned in the foreground, slightly to the right of the image. The background features a street scene with trees, buildings, and a parked vehicle. The stop sign is situated near the center of the image, slightly to the right of the street scene.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25365.6, "ram_available_mb": 100406.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25362.2, "ram_available_mb": 100409.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.62, "peak": 13.9, "min": 13.09}, "VDD_GPU": {"avg": 20.55, "peak": 24.03, "min": 18.53}, "VIN": {"avg": 55.44, "peak": 57.53, "min": 52.62}, "VDD_CPU_SOC_MSS": {"avg": 13.56, "peak": 13.78, "min": 13.0}}, "power_watts_avg": 20.55, "energy_joules_est": 20.87, "sample_count": 7, "duration_seconds": 1.015}, "timestamp": "2026-01-17T17:38:20.142460"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 858.166, "latencies_ms": [858.166], "images_per_second": 1.165, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The scene depicts a red stop sign on a metal pole in a suburban area with trees, bushes, and a paved road. A construction vehicle is visible in the background, indicating ongoing work in the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25362.2, "ram_available_mb": 100409.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25359.3, "ram_available_mb": 100412.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 13.9, "min": 12.98}, "VDD_GPU": {"avg": 21.08, "peak": 24.44, "min": 18.91}, "VIN": {"avg": 54.1, "peak": 57.48, "min": 50.14}, "VDD_CPU_SOC_MSS": {"avg": 13.51, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 21.08, "energy_joules_est": 18.1, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T17:38:21.007003"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 564.112, "latencies_ms": [564.112], "images_per_second": 1.773, "prompt_tokens": 18, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The stop sign is red and white. The lighting appears to be natural daylight. The stop sign appears to be made of metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25359.3, "ram_available_mb": 100412.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25359.6, "ram_available_mb": 100412.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.36, "peak": 13.79, "min": 12.88}, "VDD_GPU": {"avg": 22.08, "peak": 24.43, "min": 20.11}, "VIN": {"avg": 55.3, "peak": 58.15, "min": 52.75}, "VDD_CPU_SOC_MSS": {"avg": 13.19, "peak": 13.38, "min": 12.99}}, "power_watts_avg": 22.08, "energy_joules_est": 12.47, "sample_count": 4, "duration_seconds": 0.565}, "timestamp": "2026-01-17T17:38:21.577883"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1011.142, "latencies_ms": [1011.142], "images_per_second": 0.989, "prompt_tokens": 8, "response_tokens_est": 43, "n_tiles": 1, "output_text": "Three teddy bears are nestled together, with one brown bear resting its head on the shoulder of another brown bear, while a third brown bear lies down with its head resting on the shoulder of the third bear.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25359.6, "ram_available_mb": 100412.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25356.7, "ram_available_mb": 100415.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 25.84, "peak": 31.93, "min": 20.5}, "VIN": {"avg": 59.96, "peak": 89.48, "min": 47.94}, "VDD_CPU_SOC_MSS": {"avg": 13.44, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 25.84, "energy_joules_est": 26.14, "sample_count": 7, "duration_seconds": 1.012}, "timestamp": "2026-01-17T17:38:22.601628"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1691.348, "latencies_ms": [1691.348], "images_per_second": 0.591, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 1, "output_text": "Teddy bear: 3\nTeddy bear: 2\nTeddy bear: 1\nTeddy bear: 1\nTeddy bear: 1\nTeddy bear: 1\nTeddy bear: 1\nTeddy bear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.7, "ram_available_mb": 100415.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25355.0, "ram_available_mb": 100417.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 22.44, "peak": 31.92, "min": 17.73}, "VIN": {"avg": 57.99, "peak": 84.66, "min": 50.75}, "VDD_CPU_SOC_MSS": {"avg": 13.87, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 22.44, "energy_joules_est": 37.96, "sample_count": 13, "duration_seconds": 1.692}, "timestamp": "2026-01-17T17:38:24.299404"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1058.558, "latencies_ms": [1058.558], "images_per_second": 0.945, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, creating a sense of proximity and closeness. The teddy bears are situated in a somewhat haphazard arrangement, suggesting an informal arrangement rather than a carefully planned composition.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25355.0, "ram_available_mb": 100417.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25353.5, "ram_available_mb": 100418.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.76, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 24.37, "peak": 30.33, "min": 19.29}, "VIN": {"avg": 59.06, "peak": 80.67, "min": 51.88}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.37, "energy_joules_est": 25.81, "sample_count": 8, "duration_seconds": 1.059}, "timestamp": "2026-01-17T17:38:25.364336"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 783.156, "latencies_ms": [783.156], "images_per_second": 1.277, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Three teddy bears are nestled together on a surface, possibly a bed or couch. The scene suggests a cozy and warm environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.5, "ram_available_mb": 100418.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25354.0, "ram_available_mb": 100418.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.55, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 27.65, "peak": 31.5, "min": 23.25}, "VIN": {"avg": 59.86, "peak": 75.02, "min": 54.07}, "VDD_CPU_SOC_MSS": {"avg": 13.63, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 27.65, "energy_joules_est": 21.66, "sample_count": 5, "duration_seconds": 0.783}, "timestamp": "2026-01-17T17:38:26.153704"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 917.194, "latencies_ms": [917.194], "images_per_second": 1.09, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The teddy bears are brown and appear to be made of a soft, plush material. The lighting in the image is soft and warm, enhancing the cozy atmosphere of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25354.0, "ram_available_mb": 100418.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25353.0, "ram_available_mb": 100419.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 26.29, "peak": 33.1, "min": 20.48}, "VIN": {"avg": 58.41, "peak": 81.24, "min": 51.83}, "VDD_CPU_SOC_MSS": {"avg": 13.61, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 26.29, "energy_joules_est": 24.12, "sample_count": 7, "duration_seconds": 0.917}, "timestamp": "2026-01-17T17:38:27.077103"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 734.079, "latencies_ms": [734.079], "images_per_second": 1.362, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A woman in a red ski jacket and black pants is skiing down a snowy slope, leaning forward and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.0, "ram_available_mb": 100419.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.63, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 28.05, "peak": 32.3, "min": 23.25}, "VIN": {"avg": 63.82, "peak": 89.79, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 13.62, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 28.05, "energy_joules_est": 20.6, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T17:38:27.822450"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1433.616, "latencies_ms": [1433.616], "images_per_second": 0.698, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "woman: 1\nskis: 2\nsnow: 2\ngloves: 2\nhat: 1\nsunglasses: 1\nski poles: 2\nskis: 2\nbib: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25351.1, "ram_available_mb": 100421.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 23.96, "peak": 33.5, "min": 18.51}, "VIN": {"avg": 57.62, "peak": 82.13, "min": 48.83}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 23.96, "energy_joules_est": 34.36, "sample_count": 11, "duration_seconds": 1.434}, "timestamp": "2026-01-17T17:38:29.262707"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1144.473, "latencies_ms": [1144.473], "images_per_second": 0.874, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The skier is positioned in the foreground of the image, facing the camera. The snowy slope and ski poles are in the background, extending into the distance. The skier is relatively close to the viewer, suggesting they are actively skiing down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25351.1, "ram_available_mb": 100421.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25351.1, "ram_available_mb": 100421.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.47, "peak": 30.73, "min": 19.31}, "VIN": {"avg": 58.6, "peak": 87.31, "min": 49.08}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.47, "energy_joules_est": 28.01, "sample_count": 8, "duration_seconds": 1.145}, "timestamp": "2026-01-17T17:38:30.414381"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 942.744, "latencies_ms": [942.744], "images_per_second": 1.061, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A woman is skiing down a snowy slope, wearing a red jacket, black pants, and a striped hat. Red poles and markers are visible in the background, indicating a designated ski area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25351.1, "ram_available_mb": 100421.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25350.1, "ram_available_mb": 100422.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.64, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 25.67, "peak": 31.13, "min": 20.49}, "VIN": {"avg": 58.94, "peak": 77.25, "min": 52.04}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.8, "min": 13.38}}, "power_watts_avg": 25.67, "energy_joules_est": 24.21, "sample_count": 7, "duration_seconds": 0.943}, "timestamp": "2026-01-17T17:38:31.365590"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 981.89, "latencies_ms": [981.89], "images_per_second": 1.018, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The skier is wearing a red and white jacket, blue ski boots, and sunglasses. The lighting is bright and clear, indicating a sunny day. The snow appears to be well-maintained and undisturbed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25350.1, "ram_available_mb": 100422.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25350.9, "ram_available_mb": 100421.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 25.84, "peak": 31.91, "min": 20.5}, "VIN": {"avg": 58.15, "peak": 77.94, "min": 51.87}, "VDD_CPU_SOC_MSS": {"avg": 13.61, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 25.84, "energy_joules_est": 25.38, "sample_count": 7, "duration_seconds": 0.982}, "timestamp": "2026-01-17T17:38:32.354120"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 648.331, "latencies_ms": [648.331], "images_per_second": 1.542, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A white refrigerator stands next to a wooden cabinet in a kitchen with a white stove and beige tiled floor.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25350.9, "ram_available_mb": 100421.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25351.4, "ram_available_mb": 100420.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.52, "peak": 13.79, "min": 13.19}, "VDD_GPU": {"avg": 29.06, "peak": 31.53, "min": 25.22}, "VIN": {"avg": 59.76, "peak": 68.49, "min": 56.26}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 29.06, "energy_joules_est": 18.85, "sample_count": 4, "duration_seconds": 0.649}, "timestamp": "2026-01-17T17:38:33.016682"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1348.057, "latencies_ms": [1348.057], "images_per_second": 0.742, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "oven: 2\nstove: 1\nrange hood: 1\ncabinets: 4\nrefrigerator: 1\ncountertop: 1\ndrawers: 3\ntile floor: 8", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25351.4, "ram_available_mb": 100420.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25351.8, "ram_available_mb": 100420.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.79, "peak": 14.1, "min": 13.09}, "VDD_GPU": {"avg": 24.58, "peak": 33.88, "min": 18.51}, "VIN": {"avg": 59.1, "peak": 83.58, "min": 52.91}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 24.58, "energy_joules_est": 33.15, "sample_count": 10, "duration_seconds": 1.349}, "timestamp": "2026-01-17T17:38:34.372080"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 945.53, "latencies_ms": [945.53], "images_per_second": 1.058, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main objects are positioned in a relatively close proximity to each other, with the stove and refrigerator situated closer to the viewer. The tiled floor extends from the foreground towards the background, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25351.8, "ram_available_mb": 100420.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25351.8, "ram_available_mb": 100420.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.77, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 25.32, "peak": 30.71, "min": 20.09}, "VIN": {"avg": 59.1, "peak": 78.57, "min": 52.28}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 25.32, "energy_joules_est": 23.95, "sample_count": 7, "duration_seconds": 0.946}, "timestamp": "2026-01-17T17:38:35.324326"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 907.746, "latencies_ms": [907.746], "images_per_second": 1.102, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The kitchen features a white stove, oven, and refrigerator, complemented by light brown wooden cabinets. The scene suggests a clean, well-maintained space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25351.8, "ram_available_mb": 100420.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25351.6, "ram_available_mb": 100420.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.52, "peak": 13.79, "min": 13.09}, "VDD_GPU": {"avg": 26.6, "peak": 31.51, "min": 21.68}, "VIN": {"avg": 57.96, "peak": 67.14, "min": 52.74}, "VDD_CPU_SOC_MSS": {"avg": 13.65, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.6, "energy_joules_est": 24.15, "sample_count": 6, "duration_seconds": 0.908}, "timestamp": "2026-01-17T17:38:36.242629"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 828.524, "latencies_ms": [828.524], "images_per_second": 1.207, "prompt_tokens": 18, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The kitchen features light brown wooden cabinets and light beige tile flooring. The lighting is soft and evenly distributed, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25351.6, "ram_available_mb": 100420.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25351.6, "ram_available_mb": 100420.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.54, "peak": 13.79, "min": 13.19}, "VDD_GPU": {"avg": 26.86, "peak": 32.72, "min": 21.66}, "VIN": {"avg": 61.45, "peak": 87.17, "min": 49.07}, "VDD_CPU_SOC_MSS": {"avg": 13.66, "peak": 13.8, "min": 13.39}}, "power_watts_avg": 26.86, "energy_joules_est": 22.27, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T17:38:37.078294"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 723.843, "latencies_ms": [723.843], "images_per_second": 1.382, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "Two baseball players are running on a field, one in a white uniform and the other in a green uniform, attempting to catch the ball.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 25351.6, "ram_available_mb": 100420.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25352.1, "ram_available_mb": 100420.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.51, "peak": 13.79, "min": 13.09}, "VDD_GPU": {"avg": 29.23, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 64.12, "peak": 101.16, "min": 46.95}, "VDD_CPU_SOC_MSS": {"avg": 13.63, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 29.23, "energy_joules_est": 21.17, "sample_count": 5, "duration_seconds": 0.724}, "timestamp": "2026-01-17T17:38:37.817766"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1814.415, "latencies_ms": [1814.415], "images_per_second": 0.551, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 1, "output_text": "baseball glove: 1\nbaseball helmet: 1\nbaseball uniform: 2\nbaseball pants: 2\nbaseball cleats: 2\nbaseball field: 2\nbaseball bat: 1\nbaseball mitt: 1\ngrass: 2\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25352.1, "ram_available_mb": 100420.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.2, "min": 13.09}, "VDD_GPU": {"avg": 23.28, "peak": 35.06, "min": 17.73}, "VIN": {"avg": 59.79, "peak": 106.83, "min": 49.71}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 23.28, "energy_joules_est": 42.25, "sample_count": 14, "duration_seconds": 1.815}, "timestamp": "2026-01-17T17:38:39.638647"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 863.063, "latencies_ms": [863.063], "images_per_second": 1.159, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the player running towards the right. The background features the grassy field and trees, creating a natural setting for the baseball game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 26.99, "peak": 31.91, "min": 21.67}, "VIN": {"avg": 62.76, "peak": 106.38, "min": 48.06}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.99, "energy_joules_est": 23.31, "sample_count": 6, "duration_seconds": 0.864}, "timestamp": "2026-01-17T17:38:40.509104"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1109.44, "latencies_ms": [1109.44], "images_per_second": 0.901, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "Two baseball players are running on a baseball field. One player is wearing a white uniform and helmet, while the other player is wearing a green uniform and cap. The scene takes place outdoors, with green grass and trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.77, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 26.3, "peak": 33.89, "min": 20.09}, "VIN": {"avg": 59.93, "peak": 103.49, "min": 41.62}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 26.3, "energy_joules_est": 29.19, "sample_count": 8, "duration_seconds": 1.11}, "timestamp": "2026-01-17T17:38:41.625587"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 999.709, "latencies_ms": [999.709], "images_per_second": 1.0, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The players are wearing light gray uniforms. The lighting appears to be natural daylight. The uniforms appear to be made of a durable material, likely cotton or polyester. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.61, "peak": 33.09, "min": 20.88}, "VIN": {"avg": 60.84, "peak": 104.32, "min": 48.05}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.61, "energy_joules_est": 26.61, "sample_count": 7, "duration_seconds": 1.0}, "timestamp": "2026-01-17T17:38:42.631662"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 922.285, "latencies_ms": [922.285], "images_per_second": 1.084, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A tennis player, dressed in white and blue, is poised to hit a forehand shot on a blue tennis court, with a red and white tennis racket in hand.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25352.6, "ram_available_mb": 100419.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25353.0, "ram_available_mb": 100419.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 25.84, "peak": 32.31, "min": 20.48}, "VIN": {"avg": 60.8, "peak": 91.52, "min": 48.27}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 25.84, "energy_joules_est": 23.84, "sample_count": 7, "duration_seconds": 0.923}, "timestamp": "2026-01-17T17:38:43.567168"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2195.291, "latencies_ms": [2195.291], "images_per_second": 0.456, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis player: 1\nTennis ball: 1\nTennis court: 1\nTennis ball umpire: 1\nTennis ball umpire: 1\nTennis ball umpire: 1\nTennis ball umpire: 1\nTennis ball umpire: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.0, "ram_available_mb": 100419.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25352.8, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 21.32, "peak": 31.91, "min": 17.34}, "VIN": {"avg": 57.32, "peak": 73.75, "min": 45.83}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 21.32, "energy_joules_est": 46.81, "sample_count": 17, "duration_seconds": 2.196}, "timestamp": "2026-01-17T17:38:45.768650"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 943.869, "latencies_ms": [943.869], "images_per_second": 1.059, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The tennis player is positioned in the foreground, preparing to hit the ball. The ball is in the background, slightly out of focus. The tennis court is visible in the background, extending beyond the player.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25352.8, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25352.8, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 25.04, "peak": 30.33, "min": 20.1}, "VIN": {"avg": 58.69, "peak": 76.8, "min": 51.26}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 25.04, "energy_joules_est": 23.64, "sample_count": 7, "duration_seconds": 0.944}, "timestamp": "2026-01-17T17:38:46.718939"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1285.61, "latencies_ms": [1285.61], "images_per_second": 0.778, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A tennis player is executing a forehand shot on a blue tennis court, leaning forward and holding a red and white tennis racket. A line judge stands behind the player, observing the match. Spectators are visible in the background, watching the ongoing game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25352.8, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25353.0, "ram_available_mb": 100419.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 23.92, "peak": 31.91, "min": 18.51}, "VIN": {"avg": 58.41, "peak": 77.58, "min": 49.9}, "VDD_CPU_SOC_MSS": {"avg": 13.66, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 23.92, "energy_joules_est": 30.76, "sample_count": 10, "duration_seconds": 1.286}, "timestamp": "2026-01-17T17:38:48.011369"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1094.373, "latencies_ms": [1094.373], "images_per_second": 0.914, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The tennis court is green and appears to be well-lit, likely by sunlight. The tennis racket is orange and appears to be made of metal. The background features a blue wall with the \"J.P.Morgan\" logo prominently displayed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.0, "ram_available_mb": 100419.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25352.5, "ram_available_mb": 100419.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 24.77, "peak": 30.73, "min": 19.69}, "VIN": {"avg": 59.13, "peak": 77.05, "min": 54.23}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 24.77, "energy_joules_est": 27.12, "sample_count": 8, "duration_seconds": 1.095}, "timestamp": "2026-01-17T17:38:49.112219"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 756.733, "latencies_ms": [756.733], "images_per_second": 1.321, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A group of young tennis players, including both boys and girls, stand together holding their tennis rackets and celebrating a victory.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 25352.5, "ram_available_mb": 100419.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25352.7, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.63, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 27.66, "peak": 31.12, "min": 23.24}, "VIN": {"avg": 61.32, "peak": 82.86, "min": 50.63}, "VDD_CPU_SOC_MSS": {"avg": 13.63, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 27.66, "energy_joules_est": 20.95, "sample_count": 5, "duration_seconds": 0.757}, "timestamp": "2026-01-17T17:38:49.888074"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1681.195, "latencies_ms": [1681.195], "images_per_second": 0.595, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 1, "output_text": "Tennis racket: 2\nTennis ball: 1\nTennis court: 1\nTennis players: 8\nTrophy: 1\nTennis bag: 1\nTennis shoes: 4\nTennis visors: 4\nTennis caps: 4", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25352.7, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25352.7, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.3, "min": 13.19}, "VDD_GPU": {"avg": 23.11, "peak": 33.09, "min": 18.13}, "VIN": {"avg": 59.6, "peak": 93.26, "min": 49.6}, "VDD_CPU_SOC_MSS": {"avg": 13.74, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 23.11, "energy_joules_est": 38.87, "sample_count": 13, "duration_seconds": 1.682}, "timestamp": "2026-01-17T17:38:51.576332"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1028.772, "latencies_ms": [1028.772], "images_per_second": 0.972, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main objects are positioned in a relatively close-knit group, with the children standing close together in the foreground and the adults standing further back. The tennis court is situated in the background, providing a backdrop to the group.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25352.7, "ram_available_mb": 100419.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25353.7, "ram_available_mb": 100418.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 25.33, "peak": 30.34, "min": 20.49}, "VIN": {"avg": 57.13, "peak": 69.03, "min": 51.03}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 25.33, "energy_joules_est": 26.07, "sample_count": 7, "duration_seconds": 1.029}, "timestamp": "2026-01-17T17:38:52.611564"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 960.642, "latencies_ms": [960.642], "images_per_second": 1.041, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A group of young tennis players, including adults and children, pose for a photo on a blue tennis court, celebrating their victory with a trophy. The setting appears to be a sunny outdoor area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25353.7, "ram_available_mb": 100418.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25353.7, "ram_available_mb": 100418.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 25.9, "peak": 31.53, "min": 20.48}, "VIN": {"avg": 61.05, "peak": 79.03, "min": 57.02}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.9, "energy_joules_est": 24.89, "sample_count": 7, "duration_seconds": 0.961}, "timestamp": "2026-01-17T17:38:53.579460"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 941.026, "latencies_ms": [941.026], "images_per_second": 1.063, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The group is wearing bright, colorful clothing. The lighting appears to be natural sunlight, creating a vibrant atmosphere. The tennis court surface is blue, and the overall setting suggests a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.7, "ram_available_mb": 100418.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25353.4, "ram_available_mb": 100418.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 26.01, "peak": 31.92, "min": 20.48}, "VIN": {"avg": 59.77, "peak": 76.15, "min": 54.67}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.01, "energy_joules_est": 24.49, "sample_count": 7, "duration_seconds": 0.941}, "timestamp": "2026-01-17T17:38:54.527544"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 781.683, "latencies_ms": [781.683], "images_per_second": 1.279, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A couple sits under a bridge, watching a swan glide across the water, while another person captures the moment with a camera.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25353.4, "ram_available_mb": 100418.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25353.3, "ram_available_mb": 100418.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.61, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 26.99, "peak": 32.31, "min": 21.66}, "VIN": {"avg": 59.8, "peak": 74.84, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 13.65, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.99, "energy_joules_est": 21.11, "sample_count": 6, "duration_seconds": 0.782}, "timestamp": "2026-01-17T17:38:55.321392"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1160.142, "latencies_ms": [1160.142], "images_per_second": 0.862, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "bridge: 4\nswan: 1\nperson: 2\nperson: 1\nperson: 1\nperson: 1\nperson: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.3, "ram_available_mb": 100418.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25353.5, "ram_available_mb": 100418.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 25.09, "peak": 33.09, "min": 19.31}, "VIN": {"avg": 58.3, "peak": 75.61, "min": 53.63}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 25.09, "energy_joules_est": 29.12, "sample_count": 9, "duration_seconds": 1.161}, "timestamp": "2026-01-17T17:38:56.488952"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 870.982, "latencies_ms": [870.982], "images_per_second": 1.148, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The swan is positioned near the foreground, close to the water's edge. The bridge spans the background, providing a vantage point for observing the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.5, "ram_available_mb": 100418.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25353.5, "ram_available_mb": 100418.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 26.59, "peak": 31.51, "min": 21.67}, "VIN": {"avg": 59.47, "peak": 69.58, "min": 56.21}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.59, "energy_joules_est": 23.17, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T17:38:57.366431"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1005.838, "latencies_ms": [1005.838], "images_per_second": 0.994, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene is set under a bridge over a river, where people are relaxing and observing a swan on the water's edge. Boats and buildings are visible in the background, contributing to the urban atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25353.5, "ram_available_mb": 100418.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25354.8, "ram_available_mb": 100417.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 26.35, "peak": 32.31, "min": 20.89}, "VIN": {"avg": 57.22, "peak": 74.67, "min": 49.69}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.35, "energy_joules_est": 26.52, "sample_count": 7, "duration_seconds": 1.006}, "timestamp": "2026-01-17T17:38:58.380078"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1398.375, "latencies_ms": [1398.375], "images_per_second": 0.715, "prompt_tokens": 18, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The scene is bathed in warm sunlight, creating a golden glow on the water and the bridge. The colors are predominantly earthy tones, with the water reflecting the sunlight and the bridge providing a contrasting dark shade. The lighting suggests a sunny day, and the materials appear to be natural elements like stone and metal.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25354.8, "ram_available_mb": 100417.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25354.7, "ram_available_mb": 100417.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.3, "min": 13.19}, "VDD_GPU": {"avg": 23.43, "peak": 31.92, "min": 18.12}, "VIN": {"avg": 58.02, "peak": 68.75, "min": 53.43}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 23.43, "energy_joules_est": 32.77, "sample_count": 11, "duration_seconds": 1.399}, "timestamp": "2026-01-17T17:38:59.784990"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 601.918, "latencies_ms": [601.918], "images_per_second": 1.661, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A young woman is holding a Hello Kitty phone and appears to be taking a picture or selfie.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25354.7, "ram_available_mb": 100417.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25355.0, "ram_available_mb": 100417.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 13.9, "min": 13.39}, "VDD_GPU": {"avg": 28.18, "peak": 30.74, "min": 24.44}, "VIN": {"avg": 59.64, "peak": 74.54, "min": 49.77}, "VDD_CPU_SOC_MSS": {"avg": 13.58, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 28.18, "energy_joules_est": 16.98, "sample_count": 4, "duration_seconds": 0.602}, "timestamp": "2026-01-17T17:39:00.399080"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1277.721, "latencies_ms": [1277.721], "images_per_second": 0.783, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "phone: 1\nphone case: 1\nphone: 1\nphone: 1\nphone: 1\nphone: 1\nphone: 1\nphone: 1\nphone: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25355.0, "ram_available_mb": 100417.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25355.0, "ram_available_mb": 100417.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.3, "min": 13.09}, "VDD_GPU": {"avg": 24.82, "peak": 34.28, "min": 18.91}, "VIN": {"avg": 59.63, "peak": 93.28, "min": 50.86}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 24.82, "energy_joules_est": 31.72, "sample_count": 10, "duration_seconds": 1.278}, "timestamp": "2026-01-17T17:39:01.683616"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 917.456, "latencies_ms": [917.456], "images_per_second": 1.09, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The Hello Kitty phone is held in the foreground, while the woman is positioned slightly behind and to the right of the phone. The background is blurred, indicating a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25355.0, "ram_available_mb": 100417.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 25.6, "peak": 31.12, "min": 20.48}, "VIN": {"avg": 61.52, "peak": 87.88, "min": 55.57}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.6, "energy_joules_est": 23.5, "sample_count": 7, "duration_seconds": 0.918}, "timestamp": "2026-01-17T17:39:02.606820"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1276.355, "latencies_ms": [1276.355], "images_per_second": 0.783, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "A woman is holding a Hello Kitty phone and appears to be taking a picture or selfie. She is wearing a white top with a Hello Kitty design and a green beaded bracelet. The setting appears to be an outdoor event or gathering, with other people visible in the blurred background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25357.2, "ram_available_mb": 100415.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 24.78, "peak": 31.91, "min": 19.31}, "VIN": {"avg": 57.7, "peak": 75.47, "min": 49.44}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.78, "energy_joules_est": 31.64, "sample_count": 9, "duration_seconds": 1.277}, "timestamp": "2026-01-17T17:39:03.894155"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1052.536, "latencies_ms": [1052.536], "images_per_second": 0.95, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The woman's phone has a Hello Kitty case. The phone appears to be white or light-colored. The lighting is bright, suggesting an outdoor setting. The phone appears to be made of a smooth, plastic-like material.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25357.2, "ram_available_mb": 100415.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25356.9, "ram_available_mb": 100415.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 24.82, "peak": 31.12, "min": 19.71}, "VIN": {"avg": 57.88, "peak": 80.1, "min": 50.12}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 24.82, "energy_joules_est": 26.14, "sample_count": 8, "duration_seconds": 1.053}, "timestamp": "2026-01-17T17:39:04.955621"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 533.697, "latencies_ms": [533.697], "images_per_second": 1.874, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A group of children are seated in a small, red train-like ride, enjoying a fun and exciting experience.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25356.9, "ram_available_mb": 100415.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25357.0, "ram_available_mb": 100415.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.32, "peak": 13.59, "min": 12.98}, "VDD_GPU": {"avg": 23.38, "peak": 25.22, "min": 21.67}, "VIN": {"avg": 56.39, "peak": 57.41, "min": 55.88}, "VDD_CPU_SOC_MSS": {"avg": 13.25, "peak": 13.38, "min": 12.99}}, "power_watts_avg": 23.38, "energy_joules_est": 12.49, "sample_count": 3, "duration_seconds": 0.534}, "timestamp": "2026-01-17T17:39:05.499062"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 939.381, "latencies_ms": [939.381], "images_per_second": 1.065, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "train: 5\nroller coaster: 4\nchildren: 5\nleather jacket: 2\nhoodie: 1\nblack vest: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25357.0, "ram_available_mb": 100415.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25357.0, "ram_available_mb": 100415.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.53, "peak": 14.0, "min": 12.88}, "VDD_GPU": {"avg": 21.67, "peak": 26.01, "min": 18.91}, "VIN": {"avg": 56.45, "peak": 58.69, "min": 54.2}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 21.67, "energy_joules_est": 20.37, "sample_count": 7, "duration_seconds": 0.94}, "timestamp": "2026-01-17T17:39:06.445461"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 759.848, "latencies_ms": [759.848], "images_per_second": 1.316, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The main object appears to be a red train car or ride, positioned in the foreground of the image. The background features the interior of a building with wooden walls and flooring.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25357.0, "ram_available_mb": 100415.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25356.6, "ram_available_mb": 100415.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.57, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 21.82, "peak": 24.82, "min": 19.7}, "VIN": {"avg": 54.5, "peak": 57.54, "min": 50.29}, "VDD_CPU_SOC_MSS": {"avg": 13.62, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 21.82, "energy_joules_est": 16.59, "sample_count": 5, "duration_seconds": 0.76}, "timestamp": "2026-01-17T17:39:07.211804"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 714.359, "latencies_ms": [714.359], "images_per_second": 1.4, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A group of children are riding on a small, red train-like ride in a room with wooden walls. The setting appears to be a children's amusement park or similar recreational venue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.6, "ram_available_mb": 100415.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25356.9, "ram_available_mb": 100415.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.61, "peak": 14.0, "min": 12.98}, "VDD_GPU": {"avg": 22.06, "peak": 25.21, "min": 19.69}, "VIN": {"avg": 56.59, "peak": 57.19, "min": 54.62}, "VDD_CPU_SOC_MSS": {"avg": 13.63, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 22.06, "energy_joules_est": 15.77, "sample_count": 5, "duration_seconds": 0.715}, "timestamp": "2026-01-17T17:39:07.932648"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 835.95, "latencies_ms": [835.95], "images_per_second": 1.196, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The train cars are red and appear to be made of plastic or metal. The lighting is warm and creates a cozy atmosphere. The floor appears to be made of a smooth, light-colored material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.9, "ram_available_mb": 100415.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25356.7, "ram_available_mb": 100415.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.58, "peak": 14.0, "min": 12.98}, "VDD_GPU": {"avg": 21.6, "peak": 25.2, "min": 19.32}, "VIN": {"avg": 56.0, "peak": 59.49, "min": 50.93}, "VDD_CPU_SOC_MSS": {"avg": 13.52, "peak": 13.78, "min": 13.0}}, "power_watts_avg": 21.6, "energy_joules_est": 18.06, "sample_count": 6, "duration_seconds": 0.836}, "timestamp": "2026-01-17T17:39:08.774756"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 718.241, "latencies_ms": [718.241], "images_per_second": 1.392, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A black and white close-up shot captures a half-eaten sandwich on a white plate, accompanied by a small white dish containing a sauce.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25356.7, "ram_available_mb": 100415.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 28.2, "peak": 32.3, "min": 23.24}, "VIN": {"avg": 66.7, "peak": 117.9, "min": 50.0}, "VDD_CPU_SOC_MSS": {"avg": 13.7, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 28.2, "energy_joules_est": 20.27, "sample_count": 5, "duration_seconds": 0.719}, "timestamp": "2026-01-17T17:39:09.503730"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1309.877, "latencies_ms": [1309.877], "images_per_second": 0.763, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "sandwich: 2\nbun: 1\ncheese: 1\ncheese spread: 1\nbacon: 1\nmayonnaise: 1\nplate: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25356.7, "ram_available_mb": 100415.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 25.36, "peak": 34.66, "min": 18.92}, "VIN": {"avg": 62.4, "peak": 115.46, "min": 52.32}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.36, "energy_joules_est": 33.23, "sample_count": 10, "duration_seconds": 1.31}, "timestamp": "2026-01-17T17:39:10.819988"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 806.615, "latencies_ms": [806.615], "images_per_second": 1.24, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The sandwich is positioned in the foreground, partially obscuring the background. The small bowl of sauce is situated near the sandwich, closer to the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.7, "ram_available_mb": 100415.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 28.3, "peak": 32.31, "min": 23.27}, "VIN": {"avg": 59.91, "peak": 77.26, "min": 50.98}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 28.3, "energy_joules_est": 22.84, "sample_count": 5, "duration_seconds": 0.807}, "timestamp": "2026-01-17T17:39:11.633215"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 964.414, "latencies_ms": [964.414], "images_per_second": 1.037, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The scene depicts a close-up view of a partially eaten sandwich on a plate, accompanied by a small dish of sauce. The setting appears to be a casual dining environment, possibly a restaurant or cafe.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.84, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 27.7, "peak": 34.68, "min": 21.28}, "VIN": {"avg": 62.41, "peak": 103.08, "min": 49.61}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.7, "energy_joules_est": 26.73, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T17:39:12.604834"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1301.296, "latencies_ms": [1301.296], "images_per_second": 0.768, "prompt_tokens": 18, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The sandwich appears to be made with dark-colored bread, possibly chocolate. The lighting in the image is soft and diffused, creating a gentle contrast with the darker tones of the food. The plate is white and appears to be made of ceramic or porcelain. The overall atmosphere is calm and intimate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.5, "ram_available_mb": 100415.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25356.4, "ram_available_mb": 100415.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 25.32, "peak": 33.5, "min": 19.31}, "VIN": {"avg": 59.9, "peak": 91.99, "min": 53.19}, "VDD_CPU_SOC_MSS": {"avg": 13.82, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.32, "energy_joules_est": 32.96, "sample_count": 9, "duration_seconds": 1.302}, "timestamp": "2026-01-17T17:39:13.917436"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 535.045, "latencies_ms": [535.045], "images_per_second": 1.869, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A person in a wetsuit stands on a paddleboard, holding a paddle and gazing out at the calm ocean.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25356.4, "ram_available_mb": 100415.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25356.6, "ram_available_mb": 100415.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.39, "peak": 13.69, "min": 12.98}, "VDD_GPU": {"avg": 22.72, "peak": 24.42, "min": 21.27}, "VIN": {"avg": 55.4, "peak": 56.78, "min": 53.69}, "VDD_CPU_SOC_MSS": {"avg": 13.26, "peak": 13.39, "min": 13.0}}, "power_watts_avg": 22.72, "energy_joules_est": 12.17, "sample_count": 3, "duration_seconds": 0.536}, "timestamp": "2026-01-17T17:39:14.467196"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 964.85, "latencies_ms": [964.85], "images_per_second": 1.036, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "person: 1\nboard: 1\npaddle: 1\nwater: 1\nshoreline: 1\nsky: 1\nbuildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.6, "ram_available_mb": 100415.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25356.6, "ram_available_mb": 100415.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 14.2, "min": 12.88}, "VDD_GPU": {"avg": 21.39, "peak": 25.59, "min": 18.53}, "VIN": {"avg": 56.0, "peak": 58.49, "min": 50.49}, "VDD_CPU_SOC_MSS": {"avg": 13.44, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 21.39, "energy_joules_est": 20.65, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T17:39:15.442865"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 783.417, "latencies_ms": [783.417], "images_per_second": 1.276, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The paddleboarder is positioned in the foreground of the image, moving towards the left side of the frame. The calm water extends to the background, creating a serene and expansive setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.6, "ram_available_mb": 100415.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25356.6, "ram_available_mb": 100415.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 21.52, "peak": 24.43, "min": 19.31}, "VIN": {"avg": 57.46, "peak": 58.25, "min": 55.61}, "VDD_CPU_SOC_MSS": {"avg": 13.54, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 21.52, "energy_joules_est": 16.87, "sample_count": 5, "duration_seconds": 0.784}, "timestamp": "2026-01-17T17:39:16.233033"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 803.178, "latencies_ms": [803.178], "images_per_second": 1.245, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The scene depicts a person paddleboarding on a calm body of water, likely enjoying a peaceful day outdoors. The person is wearing a wetsuit and using a paddle to navigate the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25356.6, "ram_available_mb": 100415.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25357.1, "ram_available_mb": 100415.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 21.41, "peak": 24.82, "min": 18.92}, "VIN": {"avg": 56.31, "peak": 57.6, "min": 55.13}, "VDD_CPU_SOC_MSS": {"avg": 13.58, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 21.41, "energy_joules_est": 17.2, "sample_count": 6, "duration_seconds": 0.803}, "timestamp": "2026-01-17T17:39:17.045011"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1052.096, "latencies_ms": [1052.096], "images_per_second": 0.95, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The water appears relatively calm and grayish-blue. The lighting suggests a daytime scene, possibly overcast. The paddleboard is made of a light-colored material, possibly fiberglass or plastic. The overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25357.1, "ram_available_mb": 100415.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25357.6, "ram_available_mb": 100414.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.77, "peak": 14.2, "min": 12.98}, "VDD_GPU": {"avg": 20.69, "peak": 24.83, "min": 18.13}, "VIN": {"avg": 55.99, "peak": 59.28, "min": 52.07}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 20.69, "energy_joules_est": 21.78, "sample_count": 8, "duration_seconds": 1.053}, "timestamp": "2026-01-17T17:39:18.104482"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 618.593, "latencies_ms": [618.593], "images_per_second": 1.617, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A white desk holds a laptop, desktop computer, keyboard, mouse, and external hard drive, creating a functional workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25357.6, "ram_available_mb": 100414.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25357.4, "ram_available_mb": 100414.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.64, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 22.06, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 56.62, "peak": 59.8, "min": 54.18}, "VDD_CPU_SOC_MSS": {"avg": 13.48, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 22.06, "energy_joules_est": 13.66, "sample_count": 4, "duration_seconds": 0.619}, "timestamp": "2026-01-17T17:39:18.730821"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1211.451, "latencies_ms": [1211.451], "images_per_second": 0.825, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "laptop: 2\nkeyboard: 1\nmouse: 1\nprinter: 1\nspeaker: 2\nmonitor: 1\nwindow blinds: 1\ndesk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25357.4, "ram_available_mb": 100414.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25358.3, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.3, "min": 13.09}, "VDD_GPU": {"avg": 20.44, "peak": 25.21, "min": 18.12}, "VIN": {"avg": 55.23, "peak": 58.11, "min": 51.05}, "VDD_CPU_SOC_MSS": {"avg": 13.7, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 20.44, "energy_joules_est": 24.77, "sample_count": 9, "duration_seconds": 1.212}, "timestamp": "2026-01-17T17:39:19.948747"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 692.3, "latencies_ms": [692.3], "images_per_second": 1.444, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The laptop is positioned to the left of the monitor and keyboard, situated in the foreground. The printer and external device are located in the background, near the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.3, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25358.6, "ram_available_mb": 100413.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 21.51, "peak": 24.42, "min": 19.3}, "VIN": {"avg": 55.6, "peak": 58.16, "min": 49.99}, "VDD_CPU_SOC_MSS": {"avg": 13.62, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 21.51, "energy_joules_est": 14.9, "sample_count": 5, "duration_seconds": 0.693}, "timestamp": "2026-01-17T17:39:20.647306"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1063.719, "latencies_ms": [1063.719], "images_per_second": 0.94, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The scene depicts a well-organized workspace featuring a desktop computer with a monitor, keyboard, and mouse, along with two speakers. A laptop is also present on the desk. The setting appears to be a home office or a workspace with a window providing natural light.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25358.6, "ram_available_mb": 100413.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25358.6, "ram_available_mb": 100413.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.2, "min": 12.98}, "VDD_GPU": {"avg": 20.74, "peak": 25.21, "min": 18.12}, "VIN": {"avg": 56.04, "peak": 58.77, "min": 48.92}, "VDD_CPU_SOC_MSS": {"avg": 13.53, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 20.74, "energy_joules_est": 22.07, "sample_count": 8, "duration_seconds": 1.064}, "timestamp": "2026-01-17T17:39:21.718013"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1120.596, "latencies_ms": [1120.596], "images_per_second": 0.892, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The desk is white and appears to be made of a smooth material. The lighting in the room is soft and diffused, suggesting natural light from a window with blinds. The materials include metal for the laptop, plastic for the speakers, and possibly other computer peripherals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.6, "ram_available_mb": 100413.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25358.3, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 20.44, "peak": 24.42, "min": 18.12}, "VIN": {"avg": 56.67, "peak": 57.89, "min": 54.78}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 20.44, "energy_joules_est": 22.91, "sample_count": 8, "duration_seconds": 1.121}, "timestamp": "2026-01-17T17:39:22.845790"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 915.687, "latencies_ms": [915.687], "images_per_second": 1.092, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Vehicles travel under a green highway overpass with directional signs for Ventura, North 101, Hollywood Blvd, and Sunset Blvd.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25358.3, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 25.49, "peak": 30.73, "min": 20.48}, "VIN": {"avg": 60.1, "peak": 77.54, "min": 55.57}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.49, "energy_joules_est": 23.35, "sample_count": 7, "duration_seconds": 0.916}, "timestamp": "2026-01-17T17:39:23.773887"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1325.046, "latencies_ms": [1325.046], "images_per_second": 0.755, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "highway overpass: 4\ngreen highway signs: 4\ntaxi: 2\nvan: 2\nSUV: 2\ncar: 2\ntrees: 2\nbuildings: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25358.6, "ram_available_mb": 100413.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 23.99, "peak": 31.91, "min": 18.51}, "VIN": {"avg": 58.72, "peak": 79.34, "min": 53.01}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 23.99, "energy_joules_est": 31.8, "sample_count": 10, "duration_seconds": 1.326}, "timestamp": "2026-01-17T17:39:25.105972"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 895.31, "latencies_ms": [895.31], "images_per_second": 1.117, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The left foreground features the vehicles traveling under the overpass. The background includes more vehicles and distant buildings. The foreground is dominated by the vehicles traveling on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.6, "ram_available_mb": 100413.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25359.1, "ram_available_mb": 100413.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.33, "peak": 30.73, "min": 21.28}, "VIN": {"avg": 59.0, "peak": 76.9, "min": 51.19}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.33, "energy_joules_est": 23.59, "sample_count": 6, "duration_seconds": 0.896}, "timestamp": "2026-01-17T17:39:26.011226"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1210.132, "latencies_ms": [1210.132], "images_per_second": 0.826, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a busy highway with multiple vehicles traveling under a bridge. Green overhead signs provide directions to various locations, including Ventura, Hollywood, and Sunset Blvd. The setting appears to be an urban area with trees lining the road.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25359.1, "ram_available_mb": 100413.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.68, "peak": 32.31, "min": 19.3}, "VIN": {"avg": 58.67, "peak": 74.4, "min": 53.27}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 24.68, "energy_joules_est": 29.88, "sample_count": 9, "duration_seconds": 1.211}, "timestamp": "2026-01-17T17:39:27.228773"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 738.102, "latencies_ms": [738.102], "images_per_second": 1.355, "prompt_tokens": 18, "response_tokens_est": 24, "n_tiles": 1, "output_text": "The highway signs are green and white. The sky is clear and blue. The highway is busy with cars and taxis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.49, "peak": 31.12, "min": 22.84}, "VIN": {"avg": 60.46, "peak": 74.41, "min": 56.39}, "VDD_CPU_SOC_MSS": {"avg": 13.94, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.49, "energy_joules_est": 20.3, "sample_count": 5, "duration_seconds": 0.739}, "timestamp": "2026-01-17T17:39:27.973718"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1016.654, "latencies_ms": [1016.654], "images_per_second": 0.984, "prompt_tokens": 8, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A red double-decker bus, numbered 15 and labeled \"ALDWYCH,\" is prominently featured driving down a city street, with other vehicles and pedestrians visible in the background.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25359.0, "ram_available_mb": 100413.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 27.45, "peak": 34.27, "min": 21.25}, "VIN": {"avg": 63.61, "peak": 116.44, "min": 47.56}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.45, "energy_joules_est": 27.92, "sample_count": 7, "duration_seconds": 1.017}, "timestamp": "2026-01-17T17:39:29.001959"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1154.365, "latencies_ms": [1154.365], "images_per_second": 0.866, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "double-decker bus: 2\nbuilding: 2\ntrees: 2\npeople: 2\nbus: 2\nroad: 2\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25359.0, "ram_available_mb": 100413.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 25.43, "peak": 33.88, "min": 19.29}, "VIN": {"avg": 62.88, "peak": 117.04, "min": 48.92}, "VDD_CPU_SOC_MSS": {"avg": 13.82, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.43, "energy_joules_est": 29.37, "sample_count": 9, "duration_seconds": 1.155}, "timestamp": "2026-01-17T17:39:30.163206"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1067.794, "latencies_ms": [1067.794], "images_per_second": 0.937, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main object is a red double-decker bus driving on the right side of the road. The background includes other vehicles and buildings, suggesting an urban setting. The foreground is dominated by the bus, with people walking nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25359.0, "ram_available_mb": 100413.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 26.22, "peak": 32.7, "min": 20.49}, "VIN": {"avg": 64.82, "peak": 117.86, "min": 51.15}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.22, "energy_joules_est": 28.01, "sample_count": 7, "duration_seconds": 1.068}, "timestamp": "2026-01-17T17:39:31.237558"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 926.454, "latencies_ms": [926.454], "images_per_second": 1.079, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A red double-decker bus is driving down a city street, passing a green park with trees and people walking nearby. Other buses and buildings are visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25359.0, "ram_available_mb": 100413.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 26.69, "peak": 32.71, "min": 20.89}, "VIN": {"avg": 63.28, "peak": 110.23, "min": 47.85}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.69, "energy_joules_est": 24.74, "sample_count": 7, "duration_seconds": 0.927}, "timestamp": "2026-01-17T17:39:32.170430"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1238.541, "latencies_ms": [1238.541], "images_per_second": 0.807, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The red double-decker bus is brightly colored, contrasting with the gray sky. The bus has prominent headlights and windows, giving it a classic and iconic appearance. The overall condition of the bus suggests it is well-maintained and possibly vintage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.8, "ram_available_mb": 100413.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25358.3, "ram_available_mb": 100413.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 25.7, "peak": 33.89, "min": 19.72}, "VIN": {"avg": 62.72, "peak": 114.92, "min": 51.83}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.7, "energy_joules_est": 31.85, "sample_count": 9, "duration_seconds": 1.239}, "timestamp": "2026-01-17T17:39:33.415776"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 648.795, "latencies_ms": [648.795], "images_per_second": 1.541, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A black and white cat with yellow eyes is lying on top of an open silver laptop, gazing at the camera.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25358.3, "ram_available_mb": 100413.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25358.5, "ram_available_mb": 100413.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 13.79, "min": 13.29}, "VDD_GPU": {"avg": 28.58, "peak": 31.14, "min": 24.83}, "VIN": {"avg": 59.69, "peak": 71.12, "min": 53.64}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.77, "min": 13.38}}, "power_watts_avg": 28.58, "energy_joules_est": 18.56, "sample_count": 4, "duration_seconds": 0.649}, "timestamp": "2026-01-17T17:39:34.080572"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1144.95, "latencies_ms": [1144.95], "images_per_second": 0.873, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "cat: 2\nlaptop: 1\nkeyboard: 1\ncord: 1\npower adapter: 1\nwall: 1\npaper: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25358.5, "ram_available_mb": 100413.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25358.1, "ram_available_mb": 100414.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.1, "min": 13.09}, "VDD_GPU": {"avg": 25.48, "peak": 33.89, "min": 19.31}, "VIN": {"avg": 60.05, "peak": 83.36, "min": 54.75}, "VDD_CPU_SOC_MSS": {"avg": 13.74, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 25.48, "energy_joules_est": 29.19, "sample_count": 9, "duration_seconds": 1.146}, "timestamp": "2026-01-17T17:39:35.232798"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 854.763, "latencies_ms": [854.763], "images_per_second": 1.17, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The black and white cat is positioned in the foreground, partially obscuring the silver laptop. The laptop is situated near the cat, occupying the majority of the foreground space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.1, "ram_available_mb": 100414.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25358.1, "ram_available_mb": 100414.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.68, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 26.6, "peak": 31.51, "min": 21.67}, "VIN": {"avg": 59.36, "peak": 83.04, "min": 49.75}, "VDD_CPU_SOC_MSS": {"avg": 13.65, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 26.6, "energy_joules_est": 22.75, "sample_count": 6, "duration_seconds": 0.855}, "timestamp": "2026-01-17T17:39:36.093955"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 970.26, "latencies_ms": [970.26], "images_per_second": 1.031, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A black and white cat is lying on top of an open laptop computer. The laptop is situated on a white surface, possibly a table or desk. The cat appears to be resting or relaxing in the laptop's space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.1, "ram_available_mb": 100414.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25358.1, "ram_available_mb": 100414.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 26.23, "peak": 33.1, "min": 20.48}, "VIN": {"avg": 60.16, "peak": 79.71, "min": 53.78}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.23, "energy_joules_est": 25.46, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-17T17:39:37.070835"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 831.002, "latencies_ms": [831.002], "images_per_second": 1.203, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The black and white cat has striking yellow eyes. The laptop is silver and appears to be open. The lighting is soft and diffused, suggesting indoor lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.1, "ram_available_mb": 100414.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25358.4, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 26.98, "peak": 32.3, "min": 21.66}, "VIN": {"avg": 62.07, "peak": 84.02, "min": 54.36}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.98, "energy_joules_est": 22.43, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T17:39:37.908239"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 741.697, "latencies_ms": [741.697], "images_per_second": 1.348, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "Two large commercial airplanes fly in formation over the iconic Sydney Harbour Bridge, passing over the iconic Sydney Opera House.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25358.4, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25358.4, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.51, "peak": 13.69, "min": 13.19}, "VDD_GPU": {"avg": 28.37, "peak": 32.7, "min": 23.25}, "VIN": {"avg": 61.47, "peak": 82.86, "min": 53.42}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 28.37, "energy_joules_est": 21.06, "sample_count": 5, "duration_seconds": 0.742}, "timestamp": "2026-01-17T17:39:38.661665"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1083.341, "latencies_ms": [1083.341], "images_per_second": 0.923, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "airplanes: 2\nbridge: 1\nsky: 1\ncity: 1\nsydney: 1\nsydney opera house: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.4, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25358.4, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.66, "peak": 14.1, "min": 13.09}, "VDD_GPU": {"avg": 25.95, "peak": 33.48, "min": 20.09}, "VIN": {"avg": 57.26, "peak": 84.14, "min": 49.08}, "VDD_CPU_SOC_MSS": {"avg": 13.69, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 25.95, "energy_joules_est": 28.12, "sample_count": 8, "duration_seconds": 1.084}, "timestamp": "2026-01-17T17:39:39.751699"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1063.586, "latencies_ms": [1063.586], "images_per_second": 0.94, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The large airplane is positioned above the bridge, flying towards the right side of the image. The bridge spans across the foreground, connecting the two objects. The Sydney Opera House is visible in the background, near the right edge of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25358.4, "ram_available_mb": 100413.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25358.6, "ram_available_mb": 100413.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.76, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 24.97, "peak": 31.91, "min": 19.71}, "VIN": {"avg": 58.6, "peak": 75.18, "min": 49.17}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 24.97, "energy_joules_est": 26.57, "sample_count": 8, "duration_seconds": 1.064}, "timestamp": "2026-01-17T17:39:40.821889"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 926.092, "latencies_ms": [926.092], "images_per_second": 1.08, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Two large airplanes are flying over a bridge, likely Sydney Harbour Bridge, amidst a cloudy sky. The bridge spans the water, with the iconic Sydney Opera House visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25358.6, "ram_available_mb": 100413.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25359.4, "ram_available_mb": 100412.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 25.72, "peak": 31.92, "min": 20.48}, "VIN": {"avg": 59.8, "peak": 77.63, "min": 52.25}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 25.72, "energy_joules_est": 23.83, "sample_count": 7, "duration_seconds": 0.926}, "timestamp": "2026-01-17T17:39:41.754411"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1035.32, "latencies_ms": [1035.32], "images_per_second": 0.966, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The sky is mostly cloudy with patches of blue. The lighting suggests an overcast day. The planes appear to be flying in formation, highlighting the impressive scale of the bridge and the Sydney Opera House.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25359.4, "ram_available_mb": 100412.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25360.4, "ram_available_mb": 100411.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.05, "peak": 32.29, "min": 20.49}, "VIN": {"avg": 59.06, "peak": 73.72, "min": 53.21}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.05, "energy_joules_est": 26.98, "sample_count": 7, "duration_seconds": 1.036}, "timestamp": "2026-01-17T17:39:42.796139"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 693.52, "latencies_ms": [693.52], "images_per_second": 1.442, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A zebra stands tall and proud, displaying its striking black and white stripes as it grazes in a grassy field.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25360.4, "ram_available_mb": 100411.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25360.0, "ram_available_mb": 100412.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.61, "peak": 13.79, "min": 13.29}, "VDD_GPU": {"avg": 27.74, "peak": 31.51, "min": 23.26}, "VIN": {"avg": 61.36, "peak": 82.88, "min": 52.12}, "VDD_CPU_SOC_MSS": {"avg": 13.7, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 27.74, "energy_joules_est": 19.25, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:39:43.502028"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 892.439, "latencies_ms": [892.439], "images_per_second": 1.121, "prompt_tokens": 21, "response_tokens_est": 24, "n_tiles": 1, "output_text": "zebra: 2\ngrass: 2\ntree: 0\nanimal: 1\nnursing zebra: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25360.0, "ram_available_mb": 100412.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25360.5, "ram_available_mb": 100411.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.61, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 27.85, "peak": 34.28, "min": 22.08}, "VIN": {"avg": 61.29, "peak": 77.15, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.85, "energy_joules_est": 24.87, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T17:39:44.401397"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1022.596, "latencies_ms": [1022.596], "images_per_second": 0.978, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The zebra is positioned in the foreground, with its body partially obscuring the cow behind it. The cow is situated in the background, slightly out of focus. The zebra and cow are close together, creating a sense of proximity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25360.5, "ram_available_mb": 100411.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25360.5, "ram_available_mb": 100411.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 25.61, "peak": 32.31, "min": 20.09}, "VIN": {"avg": 59.47, "peak": 73.88, "min": 55.83}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.19, "min": 13.38}}, "power_watts_avg": 25.61, "energy_joules_est": 26.2, "sample_count": 8, "duration_seconds": 1.023}, "timestamp": "2026-01-17T17:39:45.430388"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 856.276, "latencies_ms": [856.276], "images_per_second": 1.168, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The scene depicts a zebra nursing from its mother in a grassy field. The black and white image highlights the zebra's distinctive stripes and the soft, natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25360.5, "ram_available_mb": 100411.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25360.5, "ram_available_mb": 100411.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.66, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 26.92, "peak": 31.91, "min": 21.66}, "VIN": {"avg": 60.57, "peak": 83.36, "min": 50.16}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 26.92, "energy_joules_est": 23.06, "sample_count": 6, "duration_seconds": 0.857}, "timestamp": "2026-01-17T17:39:46.293221"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 946.34, "latencies_ms": [946.34], "images_per_second": 1.057, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The zebra's coat is predominantly black and white. The lighting in the image appears to be natural daylight, creating a contrast between the zebra's stripes and the surrounding grass. The zebra appears to be nursing from its mother.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25360.4, "ram_available_mb": 100411.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25360.2, "ram_available_mb": 100412.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 26.17, "peak": 32.7, "min": 20.48}, "VIN": {"avg": 60.21, "peak": 79.3, "min": 54.22}, "VDD_CPU_SOC_MSS": {"avg": 13.66, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 26.17, "energy_joules_est": 24.78, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T17:39:47.246064"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 742.101, "latencies_ms": [742.101], "images_per_second": 1.348, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The room features a bed with a colorful, patterned comforter, a small round table with a chair, and a stone wall.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25360.2, "ram_available_mb": 100412.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25361.4, "ram_available_mb": 100410.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.63, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 27.97, "peak": 32.3, "min": 22.84}, "VIN": {"avg": 62.23, "peak": 87.95, "min": 51.02}, "VDD_CPU_SOC_MSS": {"avg": 13.7, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 27.97, "energy_joules_est": 20.78, "sample_count": 5, "duration_seconds": 0.743}, "timestamp": "2026-01-17T17:39:47.999318"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1207.745, "latencies_ms": [1207.745], "images_per_second": 0.828, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "bed: 1\ntable: 1\nchair: 1\nwindow: 2\nstone wall: 2\ndoor: 2\nlight fixture: 1\ncar: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25361.4, "ram_available_mb": 100410.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25361.7, "ram_available_mb": 100410.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 25.03, "peak": 33.08, "min": 19.3}, "VIN": {"avg": 59.16, "peak": 83.79, "min": 48.7}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.03, "energy_joules_est": 30.24, "sample_count": 9, "duration_seconds": 1.208}, "timestamp": "2026-01-17T17:39:49.213498"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1032.138, "latencies_ms": [1032.138], "images_per_second": 0.969, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The bed occupies the foreground, positioned slightly to the right of the image. The small table and chair are situated to the left of the bed. The window and door are located in the background, offering a view outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25361.7, "ram_available_mb": 100410.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25361.4, "ram_available_mb": 100410.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 25.06, "peak": 31.51, "min": 19.7}, "VIN": {"avg": 61.33, "peak": 89.81, "min": 55.77}, "VDD_CPU_SOC_MSS": {"avg": 13.88, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.06, "energy_joules_est": 25.88, "sample_count": 8, "duration_seconds": 1.033}, "timestamp": "2026-01-17T17:39:50.251362"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1019.363, "latencies_ms": [1019.363], "images_per_second": 0.981, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The room features a bed with a colorful quilt, a small round table with a chair, and a stone wall. The setting appears to be a bedroom or hotel room with a view of an adjacent building through a large window.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25361.4, "ram_available_mb": 100410.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25361.4, "ram_available_mb": 100410.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 25.55, "peak": 31.51, "min": 20.49}, "VIN": {"avg": 58.56, "peak": 74.33, "min": 49.81}, "VDD_CPU_SOC_MSS": {"avg": 13.66, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 25.55, "energy_joules_est": 26.05, "sample_count": 7, "duration_seconds": 1.02}, "timestamp": "2026-01-17T17:39:51.278036"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1172.398, "latencies_ms": [1172.398], "images_per_second": 0.853, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The room features a purple bedspread with a geometric pattern. The walls are painted a dark purple, and the floor is covered in dark purple carpeting. A small round table with a light brown top and black metal legs sits near the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25361.4, "ram_available_mb": 100410.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25361.4, "ram_available_mb": 100410.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.51, "peak": 31.92, "min": 18.9}, "VIN": {"avg": 59.04, "peak": 71.41, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 24.51, "energy_joules_est": 28.75, "sample_count": 9, "duration_seconds": 1.173}, "timestamp": "2026-01-17T17:39:52.456993"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 784.268, "latencies_ms": [784.268], "images_per_second": 1.275, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A purple bus numbered 96, labeled \"South Tyne,\" is stopped at a bus stop, waiting for passengers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25361.4, "ram_available_mb": 100410.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25361.6, "ram_available_mb": 100410.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 13.9, "min": 13.39}, "VDD_GPU": {"avg": 27.33, "peak": 31.51, "min": 22.85}, "VIN": {"avg": 61.26, "peak": 79.84, "min": 51.38}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.33, "energy_joules_est": 21.46, "sample_count": 5, "duration_seconds": 0.785}, "timestamp": "2026-01-17T17:39:53.258220"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1998.747, "latencies_ms": [1998.747], "images_per_second": 0.5, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 1, "output_text": "bus: 5\nperson: 1\nwindshield wipers: 2\nbus number: 96\nbus branding: South Tyne\nbus license plate: NA52 BUU\nbus windows: 10\nbus doors: 2\nbus front bumper: 1\nbus body: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25361.6, "ram_available_mb": 100410.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25362.6, "ram_available_mb": 100409.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.3, "min": 13.09}, "VDD_GPU": {"avg": 22.01, "peak": 32.7, "min": 17.73}, "VIN": {"avg": 56.38, "peak": 69.48, "min": 49.16}, "VDD_CPU_SOC_MSS": {"avg": 13.97, "peak": 14.19, "min": 13.38}}, "power_watts_avg": 22.01, "energy_joules_est": 44.0, "sample_count": 15, "duration_seconds": 1.999}, "timestamp": "2026-01-17T17:39:55.264353"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 988.844, "latencies_ms": [988.844], "images_per_second": 1.011, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The bus is positioned in the foreground, moving towards the viewer. The person walking nearby is positioned in the background, slightly further away. The bus is situated on a street, further back in the scene.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25362.6, "ram_available_mb": 100409.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25362.4, "ram_available_mb": 100409.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 25.33, "peak": 30.33, "min": 20.49}, "VIN": {"avg": 61.51, "peak": 91.36, "min": 49.7}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 25.33, "energy_joules_est": 25.05, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T17:39:56.259591"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 970.162, "latencies_ms": [970.162], "images_per_second": 1.031, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A purple bus labeled \"South Tyne\" is driving down a city street, passing a pedestrian. A man is visible inside the bus, seemingly waiting or preparing to board.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25362.4, "ram_available_mb": 100409.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25362.6, "ram_available_mb": 100409.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 25.83, "peak": 31.92, "min": 20.48}, "VIN": {"avg": 59.99, "peak": 75.66, "min": 55.99}, "VDD_CPU_SOC_MSS": {"avg": 13.67, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 25.83, "energy_joules_est": 25.08, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-17T17:39:57.236824"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 901.625, "latencies_ms": [901.625], "images_per_second": 1.109, "prompt_tokens": 18, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The bus is purple and features bright orange numbers. The bus has a windshield and side windows. The bus is driving on a road with a sidewalk nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25362.6, "ram_available_mb": 100409.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25363.1, "ram_available_mb": 100409.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 26.92, "peak": 31.91, "min": 21.66}, "VIN": {"avg": 58.16, "peak": 68.11, "min": 53.83}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.92, "energy_joules_est": 24.28, "sample_count": 6, "duration_seconds": 0.902}, "timestamp": "2026-01-17T17:39:58.144945"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 650.872, "latencies_ms": [650.872], "images_per_second": 1.536, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A white bowl filled with shiny green apples sits on a table, displaying their natural shape and vibrant color.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25363.1, "ram_available_mb": 100409.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25362.9, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.54, "peak": 13.79, "min": 13.19}, "VDD_GPU": {"avg": 29.25, "peak": 32.3, "min": 25.21}, "VIN": {"avg": 62.3, "peak": 79.67, "min": 56.2}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 29.25, "energy_joules_est": 19.05, "sample_count": 4, "duration_seconds": 0.651}, "timestamp": "2026-01-17T17:39:58.805762"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1308.915, "latencies_ms": [1308.915], "images_per_second": 0.764, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "apple: 8\ngreen apple: 8\ngreen apple: 8\ngreen apple: 8\ngreen apple: 8\ngreen apple: 8\ngreen apple: 8\ngreen apple: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25362.9, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25362.9, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.3, "min": 13.19}, "VDD_GPU": {"avg": 24.7, "peak": 34.26, "min": 18.91}, "VIN": {"avg": 58.14, "peak": 77.55, "min": 50.48}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.7, "energy_joules_est": 32.34, "sample_count": 10, "duration_seconds": 1.309}, "timestamp": "2026-01-17T17:40:00.122119"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 831.024, "latencies_ms": [831.024], "images_per_second": 1.203, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned close together, creating a sense of proximity and shared space. The apples are situated in the foreground, while the background is dark and out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25362.9, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25362.8, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.39, "peak": 31.12, "min": 21.67}, "VIN": {"avg": 60.17, "peak": 78.27, "min": 54.13}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 26.39, "energy_joules_est": 21.94, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T17:40:00.959661"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 904.487, "latencies_ms": [904.487], "images_per_second": 1.106, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A white bowl filled with green apples sits on a dark surface, possibly a table. The apples are piled together, showcasing their vibrant green color and smooth skin.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25362.8, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25362.8, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.68, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 27.19, "peak": 32.71, "min": 21.66}, "VIN": {"avg": 59.8, "peak": 85.66, "min": 52.08}, "VDD_CPU_SOC_MSS": {"avg": 13.65, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 27.19, "energy_joules_est": 24.61, "sample_count": 6, "duration_seconds": 0.905}, "timestamp": "2026-01-17T17:40:01.874884"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 850.184, "latencies_ms": [850.184], "images_per_second": 1.176, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The apples are bright green, indicating they are likely freshly picked. The lighting in the image is soft and warm, enhancing the vibrant green color of the apples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25362.8, "ram_available_mb": 100409.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25363.8, "ram_available_mb": 100408.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 27.18, "peak": 32.29, "min": 21.67}, "VIN": {"avg": 59.3, "peak": 69.29, "min": 52.7}, "VDD_CPU_SOC_MSS": {"avg": 13.64, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 27.18, "energy_joules_est": 23.12, "sample_count": 6, "duration_seconds": 0.851}, "timestamp": "2026-01-17T17:40:02.731856"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 809.69, "latencies_ms": [809.69], "images_per_second": 1.235, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A baseball game is in progress, with a batter mid-swing, a pitcher preparing to throw, and a catcher crouched behind home plate.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25363.8, "ram_available_mb": 100408.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25364.1, "ram_available_mb": 100408.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.66, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 27.38, "peak": 32.71, "min": 22.06}, "VIN": {"avg": 60.82, "peak": 81.05, "min": 54.69}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 27.38, "energy_joules_est": 22.18, "sample_count": 6, "duration_seconds": 0.81}, "timestamp": "2026-01-17T17:40:03.553295"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1712.257, "latencies_ms": [1712.257], "images_per_second": 0.584, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 1, "output_text": "baseball bat: 1\nbaseball: 1\nbaseball glove: 1\nbaseball: 1\nbaseball field: 1\nbaseball diamond: 1\nbaseball pitcher: 1\nbaseball umpire: 1\nbaseball catcher: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.1, "ram_available_mb": 100408.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25363.8, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.09}, "VDD_GPU": {"avg": 23.03, "peak": 33.09, "min": 18.12}, "VIN": {"avg": 58.5, "peak": 80.52, "min": 52.63}, "VDD_CPU_SOC_MSS": {"avg": 13.99, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 23.03, "energy_joules_est": 39.44, "sample_count": 13, "duration_seconds": 1.713}, "timestamp": "2026-01-17T17:40:05.272251"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 834.558, "latencies_ms": [834.558], "images_per_second": 1.198, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The pitcher is on the left side of the image, while the batter is in the foreground. The background is filled with green grass, creating a contrast with the brown dirt of the infield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.8, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25364.1, "ram_available_mb": 100408.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.06, "peak": 30.33, "min": 21.27}, "VIN": {"avg": 59.31, "peak": 73.19, "min": 50.87}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 26.06, "energy_joules_est": 21.76, "sample_count": 6, "duration_seconds": 0.835}, "timestamp": "2026-01-17T17:40:06.113623"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1192.827, "latencies_ms": [1192.827], "images_per_second": 0.838, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A baseball game is in progress, with a batter mid-swing, a pitcher preparing to throw, and a catcher crouched behind home plate. The scene takes place on a well-maintained baseball field with green grass and a brown dirt infield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.1, "ram_available_mb": 100408.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25363.6, "ram_available_mb": 100408.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 24.95, "peak": 32.3, "min": 19.31}, "VIN": {"avg": 59.5, "peak": 82.86, "min": 53.26}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 24.95, "energy_joules_est": 29.77, "sample_count": 9, "duration_seconds": 1.193}, "timestamp": "2026-01-17T17:40:07.312910"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1052.232, "latencies_ms": [1052.232], "images_per_second": 0.95, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The baseball game is played under bright, sunny lighting. The field is well-maintained and appears to be made of natural dirt. The batter is wearing a red and white uniform, and the pitcher is wearing a gray uniform.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.6, "ram_available_mb": 100408.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25363.6, "ram_available_mb": 100408.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 25.07, "peak": 31.53, "min": 19.7}, "VIN": {"avg": 59.62, "peak": 81.23, "min": 51.28}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 25.07, "energy_joules_est": 26.39, "sample_count": 8, "duration_seconds": 1.053}, "timestamp": "2026-01-17T17:40:08.371684"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 958.81, "latencies_ms": [958.81], "images_per_second": 1.043, "prompt_tokens": 8, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A table is set with an assortment of desserts, meats, and fruits, including a large fruit-topped cake, cheese and crackers, grapes, bread, and wine glasses.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25363.6, "ram_available_mb": 100408.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25363.2, "ram_available_mb": 100408.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 25.99, "peak": 31.51, "min": 20.89}, "VIN": {"avg": 61.77, "peak": 87.53, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 13.84, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.99, "energy_joules_est": 24.93, "sample_count": 7, "duration_seconds": 0.959}, "timestamp": "2026-01-17T17:40:09.341497"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1655.406, "latencies_ms": [1655.406], "images_per_second": 0.604, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "cake: 1\nraspberry: 2\nblueberry: 2\nwine glass: 4\nwater glass: 1\nbread: 2\ncheese: 2\ngrapes: 2\nbutter knife: 1\nserving spatula: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.2, "ram_available_mb": 100408.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 22.88, "peak": 32.3, "min": 18.13}, "VIN": {"avg": 58.06, "peak": 84.18, "min": 51.12}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 22.88, "energy_joules_est": 37.89, "sample_count": 13, "duration_seconds": 1.656}, "timestamp": "2026-01-17T17:40:11.003496"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 896.387, "latencies_ms": [896.387], "images_per_second": 1.116, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The main objects are positioned close together, with the cake and fruit platter in the foreground and the wine glasses and plates in the background. The arrangement suggests a casual, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 25.61, "peak": 30.74, "min": 20.48}, "VIN": {"avg": 59.4, "peak": 74.73, "min": 55.64}, "VDD_CPU_SOC_MSS": {"avg": 13.96, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 25.61, "energy_joules_est": 22.97, "sample_count": 7, "duration_seconds": 0.897}, "timestamp": "2026-01-17T17:40:11.906613"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1369.275, "latencies_ms": [1369.275], "images_per_second": 0.73, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The scene depicts an outdoor gathering with a table laden with food and drinks. A large cake with fresh berries is prominently displayed, while a platter of cheese, crackers, grapes, and bread is arranged nearby. The setting suggests a casual, festive atmosphere, possibly a celebration or party.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25363.2, "ram_available_mb": 100408.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 24.26, "peak": 31.91, "min": 18.91}, "VIN": {"avg": 58.2, "peak": 79.98, "min": 50.23}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 24.26, "energy_joules_est": 33.23, "sample_count": 10, "duration_seconds": 1.37}, "timestamp": "2026-01-17T17:40:13.286429"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1030.585, "latencies_ms": [1030.585], "images_per_second": 0.97, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The table is covered with a red tablecloth. The food items are arranged on plates, platters, and serving dishes. The lighting appears to be natural and bright, enhancing the visual appeal of the food.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25363.2, "ram_available_mb": 100408.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25363.2, "ram_available_mb": 100409.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 24.97, "peak": 31.12, "min": 19.7}, "VIN": {"avg": 58.79, "peak": 72.86, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.97, "energy_joules_est": 25.74, "sample_count": 8, "duration_seconds": 1.031}, "timestamp": "2026-01-17T17:40:14.323735"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 624.341, "latencies_ms": [624.341], "images_per_second": 1.602, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A man wearing a black shirt is riding a wave on a blue boogie board in the ocean.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25363.2, "ram_available_mb": 100409.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25363.2, "ram_available_mb": 100409.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.64, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 28.85, "peak": 31.91, "min": 24.82}, "VIN": {"avg": 63.0, "peak": 81.21, "min": 53.64}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 28.85, "energy_joules_est": 18.02, "sample_count": 4, "duration_seconds": 0.625}, "timestamp": "2026-01-17T17:40:14.960144"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 939.849, "latencies_ms": [939.849], "images_per_second": 1.064, "prompt_tokens": 21, "response_tokens_est": 24, "n_tiles": 1, "output_text": "person: 1\nbodyboard: 1\nwave: 1\nwater: 1\nshore: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25363.2, "ram_available_mb": 100409.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 27.41, "peak": 34.66, "min": 21.28}, "VIN": {"avg": 60.25, "peak": 75.96, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 27.41, "energy_joules_est": 25.77, "sample_count": 7, "duration_seconds": 0.94}, "timestamp": "2026-01-17T17:40:15.906488"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 870.995, "latencies_ms": [870.995], "images_per_second": 1.148, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The man is positioned near the center of the image, riding a wave in the ocean. The wave is breaking towards the right side of the image, creating a dynamic and energetic scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 27.38, "peak": 32.7, "min": 22.06}, "VIN": {"avg": 62.35, "peak": 88.33, "min": 53.48}, "VDD_CPU_SOC_MSS": {"avg": 13.92, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 27.38, "energy_joules_est": 23.85, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T17:40:16.784137"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 899.187, "latencies_ms": [899.187], "images_per_second": 1.112, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A man is riding a wave on a blue boogie board in the ocean. He is surrounded by green water and white foam. The scene suggests a sunny day at the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.0, "ram_available_mb": 100409.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25363.7, "ram_available_mb": 100408.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 27.31, "peak": 32.3, "min": 22.06}, "VIN": {"avg": 59.78, "peak": 81.59, "min": 51.01}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.31, "energy_joules_est": 24.57, "sample_count": 6, "duration_seconds": 0.9}, "timestamp": "2026-01-17T17:40:17.689932"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1196.548, "latencies_ms": [1196.548], "images_per_second": 0.836, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The water is a vibrant green color, creating a striking contrast with the man's black shirt. The lighting suggests a sunny day, as evidenced by the bright reflections on the water's surface. The man appears to be enjoying the wave, riding it on his bodyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.7, "ram_available_mb": 100408.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25363.3, "ram_available_mb": 100408.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.69, "peak": 32.7, "min": 19.32}, "VIN": {"avg": 58.85, "peak": 85.06, "min": 49.38}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.69, "energy_joules_est": 29.55, "sample_count": 9, "duration_seconds": 1.197}, "timestamp": "2026-01-17T17:40:18.893231"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 508.138, "latencies_ms": [508.138], "images_per_second": 1.968, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "The black and white photograph shows a group of children gathered in front of a brick building, posing for a group photograph.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25363.3, "ram_available_mb": 100408.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25363.3, "ram_available_mb": 100408.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.46, "peak": 13.69, "min": 13.19}, "VDD_GPU": {"avg": 23.12, "peak": 24.83, "min": 21.67}, "VIN": {"avg": 56.52, "peak": 57.28, "min": 55.77}, "VDD_CPU_SOC_MSS": {"avg": 13.51, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 23.12, "energy_joules_est": 11.76, "sample_count": 3, "duration_seconds": 0.509}, "timestamp": "2026-01-17T17:40:19.411151"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1163.968, "latencies_ms": [1163.968], "images_per_second": 0.859, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "child: 12\nboy: 8\ngirl: 8\nman: 2\nwoman: 1\nman: 1\nwoman: 1\nman: 1\nwoman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.3, "ram_available_mb": 100408.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25363.0, "ram_available_mb": 100409.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.2, "min": 12.98}, "VDD_GPU": {"avg": 20.92, "peak": 26.0, "min": 18.13}, "VIN": {"avg": 56.45, "peak": 58.67, "min": 55.09}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 20.92, "energy_joules_est": 24.36, "sample_count": 9, "duration_seconds": 1.164}, "timestamp": "2026-01-17T17:40:20.581696"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 735.886, "latencies_ms": [735.886], "images_per_second": 1.359, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The main objects are arranged in a symmetrical and organized manner, creating a sense of balance and order. The children are positioned both in the foreground and background, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.0, "ram_available_mb": 100409.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25363.0, "ram_available_mb": 100409.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.61, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 21.83, "peak": 24.43, "min": 19.7}, "VIN": {"avg": 56.43, "peak": 58.1, "min": 54.32}, "VDD_CPU_SOC_MSS": {"avg": 13.62, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 21.83, "energy_joules_est": 16.07, "sample_count": 5, "duration_seconds": 0.736}, "timestamp": "2026-01-17T17:40:21.324904"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 826.537, "latencies_ms": [826.537], "images_per_second": 1.21, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The black and white photo depicts a group of children gathered together, possibly for a school photo session. They are standing and sitting in front of a brick wall, dressed in formal attire typical of a school setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25362.8, "ram_available_mb": 100409.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25362.8, "ram_available_mb": 100409.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.66, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 21.6, "peak": 25.21, "min": 19.3}, "VIN": {"avg": 55.66, "peak": 57.86, "min": 51.52}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 21.6, "energy_joules_est": 17.86, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T17:40:22.157770"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1575.201, "latencies_ms": [1575.201], "images_per_second": 0.635, "prompt_tokens": 18, "response_tokens_est": 80, "n_tiles": 1, "output_text": "The black and white photograph captures a lively scene of children gathered together, likely in a school setting. The lighting is soft and diffused, creating a gentle atmosphere. The children appear to be dressed in formal attire, suggesting the photo was taken during a school event or class photo session. The materials used include clothing, paper, and possibly a backdrop, contributing to the overall vintage feel of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25362.8, "ram_available_mb": 100409.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25363.8, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.4, "min": 12.98}, "VDD_GPU": {"avg": 19.69, "peak": 24.82, "min": 17.72}, "VIN": {"avg": 57.09, "peak": 59.07, "min": 55.44}, "VDD_CPU_SOC_MSS": {"avg": 14.05, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 19.69, "energy_joules_est": 31.03, "sample_count": 12, "duration_seconds": 1.576}, "timestamp": "2026-01-17T17:40:23.739516"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 784.745, "latencies_ms": [784.745], "images_per_second": 1.274, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A white rectangular platter holds a selection of grilled bread pieces, accompanied by a small white bowl and a wrapped piece of bread.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.8, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25363.8, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 26.87, "peak": 30.33, "min": 22.44}, "VIN": {"avg": 62.74, "peak": 86.95, "min": 55.57}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.87, "energy_joules_est": 21.1, "sample_count": 5, "duration_seconds": 0.785}, "timestamp": "2026-01-17T17:40:24.535672"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1368.066, "latencies_ms": [1368.066], "images_per_second": 0.731, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "toast: 6\nbread: 6\nbutter: 1\nvinegar: 1\nspoon: 1\ngrater: 1\nglass: 1\nplate: 1\ncup: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.5, "ram_available_mb": 100408.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25364.0, "ram_available_mb": 100408.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 24.38, "peak": 32.68, "min": 18.91}, "VIN": {"avg": 58.39, "peak": 73.39, "min": 54.43}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.17, "min": 13.39}}, "power_watts_avg": 24.38, "energy_joules_est": 33.36, "sample_count": 10, "duration_seconds": 1.368}, "timestamp": "2026-01-17T17:40:25.910213"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1141.908, "latencies_ms": [1141.908], "images_per_second": 0.876, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The foreground features the bread, butter, and sauce arranged on a white platter. The background includes a wooden table, a wine glass, and a knife. The bread and sauce are placed closer to the viewer, while the butter and sauce are further away.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25364.0, "ram_available_mb": 100408.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25364.3, "ram_available_mb": 100407.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.2, "min": 13.49}, "VDD_GPU": {"avg": 24.87, "peak": 30.73, "min": 19.71}, "VIN": {"avg": 58.9, "peak": 76.99, "min": 50.2}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 24.87, "energy_joules_est": 28.41, "sample_count": 8, "duration_seconds": 1.142}, "timestamp": "2026-01-17T17:40:27.063837"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 896.775, "latencies_ms": [896.775], "images_per_second": 1.115, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene is set on a wooden table with a white serving tray containing bread, cheese, and a small bowl. A wine glass and a knife are also present, suggesting a casual dining setting.", "error": null, "sys_before": {"cpu_percent": 28.6, "ram_used_mb": 25364.3, "ram_available_mb": 100407.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25364.0, "ram_available_mb": 100408.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.66, "peak": 31.13, "min": 21.66}, "VIN": {"avg": 62.44, "peak": 89.55, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 26.66, "energy_joules_est": 23.92, "sample_count": 6, "duration_seconds": 0.897}, "timestamp": "2026-01-17T17:40:27.967068"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 736.792, "latencies_ms": [736.792], "images_per_second": 1.357, "prompt_tokens": 18, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The bread appears golden-brown, indicating it has been toasted. The table is wooden, and the overall lighting is warm and soft.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25364.0, "ram_available_mb": 100408.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25364.0, "ram_available_mb": 100408.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 28.44, "peak": 32.7, "min": 23.64}, "VIN": {"avg": 62.73, "peak": 90.4, "min": 49.67}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 28.44, "energy_joules_est": 20.97, "sample_count": 5, "duration_seconds": 0.737}, "timestamp": "2026-01-17T17:40:28.710467"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 916.267, "latencies_ms": [916.267], "images_per_second": 1.091, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A skier wearing a colorful jacket and pants is captured mid-air, performing a jump with ski poles in hand, against a clear blue sky and snow-covered trees.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25364.0, "ram_available_mb": 100408.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25364.2, "ram_available_mb": 100407.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 27.58, "peak": 33.88, "min": 21.67}, "VIN": {"avg": 61.47, "peak": 81.25, "min": 54.21}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.18, "min": 13.4}}, "power_watts_avg": 27.58, "energy_joules_est": 25.28, "sample_count": 6, "duration_seconds": 0.917}, "timestamp": "2026-01-17T17:40:29.637591"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1140.523, "latencies_ms": [1140.523], "images_per_second": 0.877, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "person: 1\nskis: 2\npoles: 2\nsnowboard: 1\ntree: 5\nsnow: 2\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.2, "ram_available_mb": 100407.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25363.7, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 25.42, "peak": 32.71, "min": 19.7}, "VIN": {"avg": 56.78, "peak": 72.67, "min": 49.52}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 25.42, "energy_joules_est": 29.0, "sample_count": 8, "duration_seconds": 1.141}, "timestamp": "2026-01-17T17:40:30.785035"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1321.004, "latencies_ms": [1321.004], "images_per_second": 0.757, "prompt_tokens": 25, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The skier is positioned in the foreground, mid-jump, against a backdrop of snow-covered trees and a clear blue sky. The skier is relatively close to the viewer, emphasizing the dynamic nature of the scene. The background features more trees and a clear sky, further emphasizing the snowy environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.7, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25363.7, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 23.75, "peak": 31.91, "min": 18.51}, "VIN": {"avg": 58.46, "peak": 83.13, "min": 50.09}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 23.75, "energy_joules_est": 31.38, "sample_count": 10, "duration_seconds": 1.321}, "timestamp": "2026-01-17T17:40:32.112950"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 925.778, "latencies_ms": [925.778], "images_per_second": 1.08, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A skier is performing a jump off a snow ramp in a snowy mountain area with evergreen trees in the background. Another skier is watching the action from the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25363.7, "ram_available_mb": 100408.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25365.5, "ram_available_mb": 100406.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 25.61, "peak": 31.12, "min": 20.49}, "VIN": {"avg": 62.69, "peak": 94.68, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.61, "energy_joules_est": 23.72, "sample_count": 7, "duration_seconds": 0.926}, "timestamp": "2026-01-17T17:40:33.047328"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 744.568, "latencies_ms": [744.568], "images_per_second": 1.343, "prompt_tokens": 18, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The skier is wearing a multicolored jacket and beige pants. The scene is brightly lit by the clear blue sky.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25365.5, "ram_available_mb": 100406.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25364.9, "ram_available_mb": 100407.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.57, "peak": 13.79, "min": 13.19}, "VDD_GPU": {"avg": 28.05, "peak": 31.91, "min": 23.25}, "VIN": {"avg": 60.93, "peak": 72.54, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 28.05, "energy_joules_est": 20.9, "sample_count": 5, "duration_seconds": 0.745}, "timestamp": "2026-01-17T17:40:33.798779"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 922.245, "latencies_ms": [922.245], "images_per_second": 1.084, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A person in winter clothing, equipped with ski poles, stands on snow-covered ground, facing a vast snowy mountain landscape under a blue sky dotted with clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25364.9, "ram_available_mb": 100407.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 26.79, "peak": 33.48, "min": 20.89}, "VIN": {"avg": 58.92, "peak": 74.17, "min": 54.41}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.79, "energy_joules_est": 24.72, "sample_count": 7, "duration_seconds": 0.923}, "timestamp": "2026-01-17T17:40:34.734276"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1266.118, "latencies_ms": [1266.118], "images_per_second": 0.79, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "sky: 4\nclouds: 3\nperson: 1\nski poles: 2\nsnow: 6\nrocks: 2\ntree: 1\nmountain: 4", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25365.2, "ram_available_mb": 100407.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.81, "peak": 32.3, "min": 19.29}, "VIN": {"avg": 59.73, "peak": 81.14, "min": 53.36}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 24.81, "energy_joules_est": 31.42, "sample_count": 9, "duration_seconds": 1.267}, "timestamp": "2026-01-17T17:40:36.007717"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1152.024, "latencies_ms": [1152.024], "images_per_second": 0.868, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The skier is positioned in the foreground, facing the snowy mountain landscape. The foreground is relatively clear of snow and features the skier's tracks. The background is filled with snow-covered mountains, creating a vast and serene setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25365.2, "ram_available_mb": 100407.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 24.87, "peak": 31.11, "min": 19.7}, "VIN": {"avg": 60.18, "peak": 87.87, "min": 45.27}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.87, "energy_joules_est": 28.66, "sample_count": 8, "duration_seconds": 1.152}, "timestamp": "2026-01-17T17:40:37.166449"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1106.994, "latencies_ms": [1106.994], "images_per_second": 0.903, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A person is cross-country skiing across a snowy mountain landscape under a bright blue sky with fluffy white clouds. The scene captures the beauty and tranquility of winter sports amidst the vast, snow-covered terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 25.02, "peak": 31.51, "min": 19.7}, "VIN": {"avg": 59.71, "peak": 80.93, "min": 50.61}, "VDD_CPU_SOC_MSS": {"avg": 13.92, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.02, "energy_joules_est": 27.71, "sample_count": 8, "duration_seconds": 1.108}, "timestamp": "2026-01-17T17:40:38.281121"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1128.708, "latencies_ms": [1128.708], "images_per_second": 0.886, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The sky is a vibrant blue with fluffy white clouds. The snow on the ground is bright white, contrasting with the darker tones of the mountains and trees. The lighting suggests a sunny day, with shadows cast by the snow-covered landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25364.7, "ram_available_mb": 100407.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.97, "peak": 31.51, "min": 19.7}, "VIN": {"avg": 57.87, "peak": 68.63, "min": 51.3}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.97, "energy_joules_est": 28.2, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T17:40:39.416369"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 538.928, "latencies_ms": [538.928], "images_per_second": 1.856, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A chocolate-glazed donut and a ripe banana are placed together on a brown surface.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25364.7, "ram_available_mb": 100407.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25364.7, "ram_available_mb": 100407.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.52, "peak": 13.79, "min": 13.19}, "VDD_GPU": {"avg": 22.46, "peak": 24.83, "min": 20.49}, "VIN": {"avg": 54.91, "peak": 58.17, "min": 51.69}, "VDD_CPU_SOC_MSS": {"avg": 13.58, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 22.46, "energy_joules_est": 12.11, "sample_count": 4, "duration_seconds": 0.539}, "timestamp": "2026-01-17T17:40:39.965339"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 654.885, "latencies_ms": [654.885], "images_per_second": 1.527, "prompt_tokens": 21, "response_tokens_est": 20, "n_tiles": 1, "output_text": "donut: 1\nbanana: 1\nplastic bag: 1\nbrown surface: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.7, "ram_available_mb": 100407.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.43, "peak": 13.9, "min": 12.78}, "VDD_GPU": {"avg": 22.54, "peak": 26.0, "min": 20.1}, "VIN": {"avg": 56.21, "peak": 57.12, "min": 55.48}, "VDD_CPU_SOC_MSS": {"avg": 13.46, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 22.54, "energy_joules_est": 14.77, "sample_count": 5, "duration_seconds": 0.655}, "timestamp": "2026-01-17T17:40:40.626899"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 704.043, "latencies_ms": [704.043], "images_per_second": 1.42, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The banana is positioned in the foreground, partially obscuring the donut. The donut is situated in the background, partially obscured by the banana.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.49, "peak": 13.9, "min": 12.88}, "VDD_GPU": {"avg": 22.37, "peak": 25.6, "min": 19.7}, "VIN": {"avg": 55.27, "peak": 58.55, "min": 50.66}, "VDD_CPU_SOC_MSS": {"avg": 13.55, "peak": 13.79, "min": 12.99}}, "power_watts_avg": 22.37, "energy_joules_est": 15.76, "sample_count": 5, "duration_seconds": 0.704}, "timestamp": "2026-01-17T17:40:41.336388"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 833.196, "latencies_ms": [833.196], "images_per_second": 1.2, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A banana and a chocolate-glazed donut are placed together on a brown surface, possibly a table or countertop. The scene suggests a casual snack or treat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.68, "peak": 14.1, "min": 12.98}, "VDD_GPU": {"avg": 21.67, "peak": 25.61, "min": 19.3}, "VIN": {"avg": 57.4, "peak": 58.7, "min": 54.57}, "VDD_CPU_SOC_MSS": {"avg": 13.58, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 21.67, "energy_joules_est": 18.07, "sample_count": 6, "duration_seconds": 0.834}, "timestamp": "2026-01-17T17:40:42.177534"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 769.154, "latencies_ms": [769.154], "images_per_second": 1.3, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The donut is brown and appears glazed. The banana is yellow and has a slight bruise. The bag appears to be made of a clear plastic material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.9, "ram_available_mb": 100407.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25364.8, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 21.99, "peak": 24.82, "min": 19.71}, "VIN": {"avg": 56.56, "peak": 59.3, "min": 52.98}, "VDD_CPU_SOC_MSS": {"avg": 13.54, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 21.99, "energy_joules_est": 16.92, "sample_count": 5, "duration_seconds": 0.77}, "timestamp": "2026-01-17T17:40:42.957414"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 587.371, "latencies_ms": [587.371], "images_per_second": 1.703, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A white mug with a skull and crossbones design sits on a glass countertop, accompanied by a serrated knife.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25364.8, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25364.7, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.39, "peak": 13.79, "min": 12.98}, "VDD_GPU": {"avg": 22.46, "peak": 24.83, "min": 20.48}, "VIN": {"avg": 54.79, "peak": 59.19, "min": 50.03}, "VDD_CPU_SOC_MSS": {"avg": 13.49, "peak": 13.79, "min": 12.99}}, "power_watts_avg": 22.46, "energy_joules_est": 13.2, "sample_count": 4, "duration_seconds": 0.588}, "timestamp": "2026-01-17T17:40:43.555551"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1160.092, "latencies_ms": [1160.092], "images_per_second": 0.862, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "mug: 1\nskull: 1\ncrossbones: 2\nknife: 1\nhandle: 1\nplate: 1\ncountertop: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25364.7, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25364.7, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.2, "min": 12.88}, "VDD_GPU": {"avg": 21.03, "peak": 25.6, "min": 18.52}, "VIN": {"avg": 56.42, "peak": 58.97, "min": 52.22}, "VDD_CPU_SOC_MSS": {"avg": 13.63, "peak": 14.17, "min": 12.99}}, "power_watts_avg": 21.03, "energy_joules_est": 24.4, "sample_count": 8, "duration_seconds": 1.16}, "timestamp": "2026-01-17T17:40:44.726379"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 588.145, "latencies_ms": [588.145], "images_per_second": 1.7, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The mug is positioned in the foreground, slightly to the right of the knife. The knife is placed in the background, closer to the center of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25364.7, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 25364.7, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.49, "peak": 13.79, "min": 13.09}, "VDD_GPU": {"avg": 21.68, "peak": 23.64, "min": 20.11}, "VIN": {"avg": 57.2, "peak": 58.81, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 13.58, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 21.68, "energy_joules_est": 12.76, "sample_count": 4, "duration_seconds": 0.589}, "timestamp": "2026-01-17T17:40:45.321009"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 857.689, "latencies_ms": [857.689], "images_per_second": 1.166, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A white mug with a skull and crossbones design sits on a light-colored surface. Next to it is a serrated knife with a black handle. The scene suggests a kitchen or dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25364.7, "ram_available_mb": 100407.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25365.7, "ram_available_mb": 100406.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 14.1, "min": 12.98}, "VDD_GPU": {"avg": 21.61, "peak": 25.23, "min": 19.31}, "VIN": {"avg": 56.56, "peak": 58.96, "min": 50.73}, "VDD_CPU_SOC_MSS": {"avg": 13.64, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 21.61, "energy_joules_est": 18.54, "sample_count": 6, "duration_seconds": 0.858}, "timestamp": "2026-01-17T17:40:46.186275"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 860.04, "latencies_ms": [860.04], "images_per_second": 1.163, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The mug is white and appears to be made of ceramic. The knife has a black handle and a silver blade. The lighting is bright, likely from overhead lighting. The materials appear to be standard kitchen utensils.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25365.7, "ram_available_mb": 100406.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25366.2, "ram_available_mb": 100406.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.65, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 21.2, "peak": 23.64, "min": 19.32}, "VIN": {"avg": 58.09, "peak": 59.74, "min": 56.19}, "VDD_CPU_SOC_MSS": {"avg": 13.7, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 21.2, "energy_joules_est": 18.24, "sample_count": 5, "duration_seconds": 0.86}, "timestamp": "2026-01-17T17:40:47.052967"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 654.795, "latencies_ms": [654.795], "images_per_second": 1.527, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A group of people are gathered around a wooden bar, examining various wine bottles and discussing the selection.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25366.2, "ram_available_mb": 100406.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25367.2, "ram_available_mb": 100405.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.62, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 29.64, "peak": 32.3, "min": 25.6}, "VIN": {"avg": 67.16, "peak": 104.08, "min": 52.67}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 29.64, "energy_joules_est": 19.42, "sample_count": 4, "duration_seconds": 0.655}, "timestamp": "2026-01-17T17:40:47.721190"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1272.127, "latencies_ms": [1272.127], "images_per_second": 0.786, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "bottle: 5\ntable: 1\nwine bottle: 5\nglasses: 2\nman: 4\nwoman: 2\nman: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.2, "ram_available_mb": 100405.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25367.7, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 26.27, "peak": 35.45, "min": 19.7}, "VIN": {"avg": 61.18, "peak": 107.93, "min": 49.04}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 26.27, "energy_joules_est": 33.43, "sample_count": 9, "duration_seconds": 1.273}, "timestamp": "2026-01-17T17:40:48.999616"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1138.716, "latencies_ms": [1138.716], "images_per_second": 0.878, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The wine bottles are placed in the foreground, while the people are gathered around them, interacting with the display. The background features additional elements, such as a wooden cabinet and a wine barrel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.7, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25367.7, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.97, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 26.0, "peak": 33.48, "min": 20.1}, "VIN": {"avg": 61.82, "peak": 112.97, "min": 43.16}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.0, "energy_joules_est": 29.61, "sample_count": 8, "duration_seconds": 1.139}, "timestamp": "2026-01-17T17:40:50.144681"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1253.055, "latencies_ms": [1253.055], "images_per_second": 0.798, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 1, "output_text": "A group of people are gathered around a wooden bar, seemingly in a wine tasting event or similar social setting. The bar features various wine bottles and glasses, indicating a focus on wine tasting.  The setting appears to be indoors, with a wooden barrel visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.7, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 25.03, "peak": 32.71, "min": 19.29}, "VIN": {"avg": 58.69, "peak": 69.24, "min": 55.22}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.03, "energy_joules_est": 31.37, "sample_count": 9, "duration_seconds": 1.253}, "timestamp": "2026-01-17T17:40:51.403878"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1092.288, "latencies_ms": [1092.288], "images_per_second": 0.916, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The room has a warm color scheme with teal walls and wooden elements. The lighting is soft and diffused, creating a comfortable atmosphere. The materials appear to be wood and glass, contributing to the overall aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25367.7, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 25.7, "peak": 32.68, "min": 20.1}, "VIN": {"avg": 61.3, "peak": 105.74, "min": 48.83}, "VDD_CPU_SOC_MSS": {"avg": 13.88, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 25.7, "energy_joules_est": 28.08, "sample_count": 8, "duration_seconds": 1.093}, "timestamp": "2026-01-17T17:40:52.502104"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 561.105, "latencies_ms": [561.105], "images_per_second": 1.782, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "Two white birds, possibly egrets, are standing in a grassy field near a harbor filled with boats.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25367.7, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25368.2, "ram_available_mb": 100404.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.54, "peak": 13.9, "min": 13.09}, "VDD_GPU": {"avg": 22.65, "peak": 25.22, "min": 20.49}, "VIN": {"avg": 57.38, "peak": 60.16, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 22.65, "energy_joules_est": 12.72, "sample_count": 4, "duration_seconds": 0.562}, "timestamp": "2026-01-17T17:40:53.072838"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 882.636, "latencies_ms": [882.636], "images_per_second": 1.133, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "boats: 3\nseagulls: 2\ngrass: 8\nsky: 1\nwater: 1\nbuildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.2, "ram_available_mb": 100404.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25368.1, "ram_available_mb": 100404.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 14.0, "min": 12.98}, "VDD_GPU": {"avg": 22.0, "peak": 26.01, "min": 19.32}, "VIN": {"avg": 56.24, "peak": 60.91, "min": 48.21}, "VDD_CPU_SOC_MSS": {"avg": 13.65, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 22.0, "energy_joules_est": 19.43, "sample_count": 6, "duration_seconds": 0.883}, "timestamp": "2026-01-17T17:40:53.961669"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1120.858, "latencies_ms": [1120.858], "images_per_second": 0.892, "prompt_tokens": 25, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the boats and seagulls in the background. The boats are situated near the water's edge, while the seagulls are spread across the foreground. The boats appear to be relatively close to the viewer, while the seagulls are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.1, "ram_available_mb": 100404.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.68, "peak": 14.1, "min": 12.98}, "VDD_GPU": {"avg": 20.64, "peak": 24.83, "min": 18.12}, "VIN": {"avg": 54.64, "peak": 59.27, "min": 48.24}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 20.64, "energy_joules_est": 23.14, "sample_count": 8, "duration_seconds": 1.121}, "timestamp": "2026-01-17T17:40:55.088359"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 995.981, "latencies_ms": [995.981], "images_per_second": 1.004, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene depicts a marshy area with a harbor filled with fishing boats and a small white building nearby. Two white birds, possibly egrets, are visible in the foreground. The sky is cloudy, suggesting a potentially overcast day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.1, "min": 13.09}, "VDD_GPU": {"avg": 20.82, "peak": 24.42, "min": 18.52}, "VIN": {"avg": 56.16, "peak": 58.56, "min": 49.78}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 20.82, "energy_joules_est": 20.75, "sample_count": 7, "duration_seconds": 0.996}, "timestamp": "2026-01-17T17:40:56.090533"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 779.962, "latencies_ms": [779.962], "images_per_second": 1.282, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The sky is cloudy and gray. The grass is tall and green. The boats are primarily white and blue. The overall scene suggests a coastal or maritime setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.53, "peak": 13.9, "min": 12.98}, "VDD_GPU": {"avg": 21.36, "peak": 24.04, "min": 19.31}, "VIN": {"avg": 56.17, "peak": 58.44, "min": 50.52}, "VDD_CPU_SOC_MSS": {"avg": 13.46, "peak": 13.78, "min": 12.99}}, "power_watts_avg": 21.36, "energy_joules_est": 16.67, "sample_count": 5, "duration_seconds": 0.78}, "timestamp": "2026-01-17T17:40:56.877348"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 691.638, "latencies_ms": [691.638], "images_per_second": 1.446, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A man in work attire kneels beside a white toilet in a bathroom, appearing to inspect or repair it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 27.34, "peak": 31.13, "min": 22.85}, "VIN": {"avg": 62.74, "peak": 78.02, "min": 57.14}, "VDD_CPU_SOC_MSS": {"avg": 13.62, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 27.34, "energy_joules_est": 18.92, "sample_count": 5, "duration_seconds": 0.692}, "timestamp": "2026-01-17T17:40:57.579184"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1399.608, "latencies_ms": [1399.608], "images_per_second": 0.714, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "toilet: 1\nsink: 1\nmirror: 1\nfaucet: 1\ntoilet paper: 1\ngloves: 2\npants: 2\nfloor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25368.9, "ram_available_mb": 100403.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 24.74, "peak": 33.88, "min": 18.91}, "VIN": {"avg": 58.79, "peak": 75.31, "min": 50.74}, "VDD_CPU_SOC_MSS": {"avg": 14.06, "peak": 14.57, "min": 13.39}}, "power_watts_avg": 24.74, "energy_joules_est": 34.63, "sample_count": 10, "duration_seconds": 1.4}, "timestamp": "2026-01-17T17:40:58.984730"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 765.783, "latencies_ms": [765.783], "images_per_second": 1.306, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The main object is a person kneeling in front of a toilet. The toilet is positioned in the background, while the person appears to be working on it or inspecting it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.9, "ram_available_mb": 100403.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25368.9, "ram_available_mb": 100403.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.42, "peak": 31.12, "min": 22.86}, "VIN": {"avg": 63.75, "peak": 89.08, "min": 55.68}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.42, "energy_joules_est": 21.01, "sample_count": 5, "duration_seconds": 0.766}, "timestamp": "2026-01-17T17:40:59.756317"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 877.91, "latencies_ms": [877.91], "images_per_second": 1.139, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A man is kneeling beside a white toilet in a bathroom, appearing to be cleaning or inspecting the toilet. The bathroom features black and white checkered tiles on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.9, "ram_available_mb": 100403.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.76, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.51, "peak": 33.09, "min": 22.06}, "VIN": {"avg": 62.11, "peak": 89.61, "min": 54.61}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 27.51, "energy_joules_est": 24.16, "sample_count": 6, "duration_seconds": 0.878}, "timestamp": "2026-01-17T17:41:00.640602"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1118.687, "latencies_ms": [1118.687], "images_per_second": 0.894, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The bathroom is black and white. The lighting appears to be artificial, likely from overhead fixtures. The walls are tiled with black and white checkered tiles. The man is wearing gloves, suggesting the bathroom may be undergoing some kind of renovation or cleaning.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25368.9, "ram_available_mb": 100403.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 25.7, "peak": 32.3, "min": 20.09}, "VIN": {"avg": 59.14, "peak": 73.99, "min": 52.4}, "VDD_CPU_SOC_MSS": {"avg": 13.99, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 25.7, "energy_joules_est": 28.76, "sample_count": 8, "duration_seconds": 1.119}, "timestamp": "2026-01-17T17:41:01.767057"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 536.431, "latencies_ms": [536.431], "images_per_second": 1.864, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A group of six people stand on a snowy mountain slope, preparing to ski down the snow-covered mountain.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25368.9, "ram_available_mb": 100403.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25369.8, "ram_available_mb": 100402.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.46, "peak": 13.79, "min": 13.09}, "VDD_GPU": {"avg": 23.37, "peak": 25.21, "min": 21.66}, "VIN": {"avg": 55.8, "peak": 56.61, "min": 54.46}, "VDD_CPU_SOC_MSS": {"avg": 13.52, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 23.37, "energy_joules_est": 12.55, "sample_count": 3, "duration_seconds": 0.537}, "timestamp": "2026-01-17T17:41:02.318947"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 967.974, "latencies_ms": [967.974], "images_per_second": 1.033, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "mountains: 5\nsnow: 6\nskiers: 4\nsnowboarders: 1\ntracks: 2\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25369.8, "ram_available_mb": 100402.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.62, "peak": 14.1, "min": 12.88}, "VDD_GPU": {"avg": 21.62, "peak": 26.01, "min": 18.91}, "VIN": {"avg": 56.6, "peak": 59.58, "min": 53.75}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 21.62, "energy_joules_est": 20.94, "sample_count": 7, "duration_seconds": 0.968}, "timestamp": "2026-01-17T17:41:03.293845"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 832.95, "latencies_ms": [832.95], "images_per_second": 1.201, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The snow-covered mountain dominates the background, creating a sense of distance and scale.  In the foreground, a group of people is gathered, possibly preparing for a skiing activity or taking a break.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.1, "min": 13.09}, "VDD_GPU": {"avg": 21.34, "peak": 24.82, "min": 18.91}, "VIN": {"avg": 56.68, "peak": 58.71, "min": 54.41}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.34, "energy_joules_est": 17.78, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:41:04.132535"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 987.87, "latencies_ms": [987.87], "images_per_second": 1.012, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A group of people are gathered on a snowy mountain, preparing for a skiing trip. They are surrounded by tracks in the snow, indicating recent activity. The scene is set against a clear blue sky, highlighting the pristine beauty of the snow-covered landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25370.6, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.2, "min": 13.09}, "VDD_GPU": {"avg": 21.05, "peak": 24.82, "min": 18.52}, "VIN": {"avg": 57.45, "peak": 59.47, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.05, "energy_joules_est": 20.8, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-17T17:41:05.126796"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 894.025, "latencies_ms": [894.025], "images_per_second": 1.119, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The snow is white and appears undisturbed, creating a striking contrast against the blue sky. The lighting is bright and clear, illuminating the scene and highlighting the textures of the snow and rock formations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.6, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 21.34, "peak": 24.43, "min": 19.3}, "VIN": {"avg": 57.17, "peak": 60.39, "min": 55.65}, "VDD_CPU_SOC_MSS": {"avg": 13.72, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.34, "energy_joules_est": 19.08, "sample_count": 6, "duration_seconds": 0.894}, "timestamp": "2026-01-17T17:41:06.028534"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 759.006, "latencies_ms": [759.006], "images_per_second": 1.318, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A white bowl contains a serving of white rice topped with a vibrant red and orange chili mixture, accompanied by steamed broccoli.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 28.43, "peak": 32.68, "min": 23.64}, "VIN": {"avg": 66.33, "peak": 110.93, "min": 50.94}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 28.43, "energy_joules_est": 21.59, "sample_count": 5, "duration_seconds": 0.759}, "timestamp": "2026-01-17T17:41:06.799474"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1306.025, "latencies_ms": [1306.025], "images_per_second": 0.766, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "chili: 2\nbroccoli: 2\nrice: 1\nbeans: 1\ncarrots: 1\nonions: 1\ntomatoes: 1\npeppers: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25370.3, "ram_available_mb": 100401.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25370.5, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.08, "peak": 14.4, "min": 13.39}, "VDD_GPU": {"avg": 25.41, "peak": 34.66, "min": 19.29}, "VIN": {"avg": 61.82, "peak": 112.83, "min": 51.47}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.41, "energy_joules_est": 33.19, "sample_count": 10, "duration_seconds": 1.306}, "timestamp": "2026-01-17T17:41:08.111386"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 809.033, "latencies_ms": [809.033], "images_per_second": 1.236, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The bowl is positioned in the foreground, with the broccoli to the left and the chili to the right. The rice is situated in the background, partially obscured by the bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.5, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25370.9, "ram_available_mb": 100401.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 27.5, "peak": 32.68, "min": 22.06}, "VIN": {"avg": 63.64, "peak": 112.2, "min": 50.16}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.5, "energy_joules_est": 22.26, "sample_count": 6, "duration_seconds": 0.81}, "timestamp": "2026-01-17T17:41:08.928580"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 671.433, "latencies_ms": [671.433], "images_per_second": 1.489, "prompt_tokens": 19, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A white bowl contains a meal of rice, beans, and broccoli. The dish is served on a wooden table.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25370.9, "ram_available_mb": 100401.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25371.0, "ram_available_mb": 100401.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 31.01, "peak": 34.26, "min": 26.38}, "VIN": {"avg": 61.85, "peak": 77.37, "min": 55.24}, "VDD_CPU_SOC_MSS": {"avg": 13.88, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 31.01, "energy_joules_est": 20.83, "sample_count": 4, "duration_seconds": 0.672}, "timestamp": "2026-01-17T17:41:09.605998"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 852.475, "latencies_ms": [852.475], "images_per_second": 1.173, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The dish features a vibrant red sauce, contrasting with the bright green broccoli and white rice. The lighting is soft and warm, creating a pleasant and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.0, "ram_available_mb": 100401.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25370.8, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 29.41, "peak": 35.45, "min": 22.84}, "VIN": {"avg": 58.64, "peak": 75.87, "min": 50.52}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 29.41, "energy_joules_est": 25.08, "sample_count": 6, "duration_seconds": 0.853}, "timestamp": "2026-01-17T17:41:10.464888"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 808.218, "latencies_ms": [808.218], "images_per_second": 1.237, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A person in black pants and black and white Vans sneakers is riding a skateboard on a small wooden ramp in a grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.8, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25371.0, "ram_available_mb": 100401.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 27.7, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 59.37, "peak": 80.39, "min": 50.1}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.7, "energy_joules_est": 22.4, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-17T17:41:11.286371"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1119.589, "latencies_ms": [1119.589], "images_per_second": 0.893, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "shoe: 2\nskateboard: 1\nboard: 1\ngrass: 2\nwood: 4\npants: 1\nground: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.0, "ram_available_mb": 100401.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25370.5, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 25.81, "peak": 33.48, "min": 20.09}, "VIN": {"avg": 60.61, "peak": 78.75, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.81, "energy_joules_est": 28.91, "sample_count": 8, "duration_seconds": 1.12}, "timestamp": "2026-01-17T17:41:12.412137"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 803.082, "latencies_ms": [803.082], "images_per_second": 1.245, "prompt_tokens": 25, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, moving towards the background. The wooden planks and grass create a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.5, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25370.8, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.2, "min": 13.49}, "VDD_GPU": {"avg": 26.79, "peak": 31.91, "min": 21.66}, "VIN": {"avg": 62.04, "peak": 85.8, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.79, "energy_joules_est": 21.52, "sample_count": 6, "duration_seconds": 0.803}, "timestamp": "2026-01-17T17:41:13.221339"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 822.863, "latencies_ms": [822.863], "images_per_second": 1.215, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A person is skateboarding on a wooden platform in a grassy area. They are wearing Vans sneakers and appear to be performing a trick or maneuver.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.8, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25370.8, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.64, "peak": 33.48, "min": 22.07}, "VIN": {"avg": 60.92, "peak": 88.5, "min": 46.04}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.64, "energy_joules_est": 22.76, "sample_count": 6, "duration_seconds": 0.823}, "timestamp": "2026-01-17T17:41:14.050395"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1127.303, "latencies_ms": [1127.303], "images_per_second": 0.887, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The skateboarder is wearing black and white sneakers. The scene is lit by natural sunlight, creating a bright and airy atmosphere. The skateboard appears to be made of wood and has a checkered pattern on its underside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.8, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25370.7, "ram_available_mb": 100401.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 25.86, "peak": 33.5, "min": 20.1}, "VIN": {"avg": 59.61, "peak": 74.73, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.86, "energy_joules_est": 29.16, "sample_count": 8, "duration_seconds": 1.128}, "timestamp": "2026-01-17T17:41:15.183889"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 673.903, "latencies_ms": [673.903], "images_per_second": 1.484, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A bunch of ripe yellow bananas is arranged on a wooden table, partially obscuring a computer keyboard in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25370.7, "ram_available_mb": 100401.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25370.5, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 13.9, "min": 13.39}, "VDD_GPU": {"avg": 28.76, "peak": 31.91, "min": 24.83}, "VIN": {"avg": 61.54, "peak": 73.12, "min": 57.65}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 28.76, "energy_joules_est": 19.39, "sample_count": 4, "duration_seconds": 0.674}, "timestamp": "2026-01-17T17:41:15.868929"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1013.327, "latencies_ms": [1013.327], "images_per_second": 0.987, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "bananas: 4\nkeyboard: 1\nlaptop: 1\ncup: 1\ntable: 1\npaper: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25370.5, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25370.5, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 27.18, "peak": 34.27, "min": 20.88}, "VIN": {"avg": 61.19, "peak": 82.61, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 13.84, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.18, "energy_joules_est": 27.55, "sample_count": 7, "duration_seconds": 1.014}, "timestamp": "2026-01-17T17:41:16.888134"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 875.272, "latencies_ms": [875.272], "images_per_second": 1.143, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with a computer keyboard and monitor visible in the background. The bananas are situated near the center, partially obscuring the keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.5, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 26.99, "peak": 31.91, "min": 22.07}, "VIN": {"avg": 62.02, "peak": 92.12, "min": 53.69}, "VDD_CPU_SOC_MSS": {"avg": 13.84, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 26.99, "energy_joules_est": 23.64, "sample_count": 6, "duration_seconds": 0.876}, "timestamp": "2026-01-17T17:41:17.769539"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 784.405, "latencies_ms": [784.405], "images_per_second": 1.275, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The scene is set on a wooden desk with a computer keyboard and a computer monitor in the background. A bunch of ripe bananas is prominently featured in the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25371.0, "ram_available_mb": 100401.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 28.28, "peak": 32.7, "min": 23.23}, "VIN": {"avg": 61.72, "peak": 82.15, "min": 52.13}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 28.28, "energy_joules_est": 22.19, "sample_count": 5, "duration_seconds": 0.785}, "timestamp": "2026-01-17T17:41:18.560093"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 966.663, "latencies_ms": [966.663], "images_per_second": 1.034, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The bananas are yellow, indicating they are ripe. The lighting appears to be natural, possibly from daylight, creating a warm ambiance. The bananas appear to be made of natural materials like fruit and possibly plant matter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.0, "ram_available_mb": 100401.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.84, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 26.78, "peak": 33.47, "min": 20.88}, "VIN": {"avg": 60.85, "peak": 78.08, "min": 57.05}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.78, "energy_joules_est": 25.9, "sample_count": 7, "duration_seconds": 0.967}, "timestamp": "2026-01-17T17:41:19.532790"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 769.683, "latencies_ms": [769.683], "images_per_second": 1.299, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A plate of stir-fried chicken, cauliflower, broccoli, carrots, and rice is presented on a wooden table, accompanied by a fork and a glass of water.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.57, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 22.45, "peak": 25.6, "min": 20.09}, "VIN": {"avg": 56.11, "peak": 58.64, "min": 53.79}, "VDD_CPU_SOC_MSS": {"avg": 13.62, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 22.45, "energy_joules_est": 17.29, "sample_count": 5, "duration_seconds": 0.77}, "timestamp": "2026-01-17T17:41:20.313971"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1365.544, "latencies_ms": [1365.544], "images_per_second": 0.732, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "chicken: 6\nrice: 1\ncarrots: 3\nbroccoli: 2\ncauliflower: 2\ncorn: 1\nsauce: 1\nfork: 1\nglass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.3, "min": 12.98}, "VDD_GPU": {"avg": 20.42, "peak": 25.22, "min": 18.13}, "VIN": {"avg": 56.98, "peak": 59.01, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 13.74, "peak": 14.17, "min": 13.39}}, "power_watts_avg": 20.42, "energy_joules_est": 27.89, "sample_count": 10, "duration_seconds": 1.366}, "timestamp": "2026-01-17T17:41:21.685307"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 907.603, "latencies_ms": [907.603], "images_per_second": 1.102, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The plate is positioned in the foreground of the image, with the main dish (rice and vegetables) occupying the central area. The table occupies the background, and the glass of water is situated near the top left corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.7, "ram_available_mb": 100401.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25371.2, "ram_available_mb": 100401.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 21.0, "peak": 24.03, "min": 18.9}, "VIN": {"avg": 56.48, "peak": 57.41, "min": 54.63}, "VDD_CPU_SOC_MSS": {"avg": 13.65, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 21.0, "energy_joules_est": 19.07, "sample_count": 6, "duration_seconds": 0.908}, "timestamp": "2026-01-17T17:41:22.598767"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 994.475, "latencies_ms": [994.475], "images_per_second": 1.006, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a plate of Asian cuisine featuring white rice, chicken, broccoli, carrots, and cauliflower, served on a colorful plate. A glass of water and a fork are also present on the wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.2, "ram_available_mb": 100401.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25371.1, "ram_available_mb": 100401.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 20.94, "peak": 24.82, "min": 18.53}, "VIN": {"avg": 56.92, "peak": 57.9, "min": 55.25}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 20.94, "energy_joules_est": 20.83, "sample_count": 7, "duration_seconds": 0.995}, "timestamp": "2026-01-17T17:41:23.599593"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 898.469, "latencies_ms": [898.469], "images_per_second": 1.113, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The plate features a vibrant mix of colors, including orange, green, and white. The lighting is soft and warm, creating a pleasant atmosphere. The food appears to be made of high-quality ingredients, enhancing its visual appeal.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.1, "ram_available_mb": 100401.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25371.6, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.2, "min": 13.09}, "VDD_GPU": {"avg": 21.41, "peak": 24.83, "min": 18.91}, "VIN": {"avg": 56.85, "peak": 58.75, "min": 54.8}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 21.41, "energy_joules_est": 19.24, "sample_count": 6, "duration_seconds": 0.899}, "timestamp": "2026-01-17T17:41:24.504713"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 526.444, "latencies_ms": [526.444], "images_per_second": 1.9, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A young girl in a white dress and colorful skirt is playing with a white toy in a living room, surrounded by people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.6, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25371.3, "ram_available_mb": 100400.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.49, "peak": 13.69, "min": 13.19}, "VDD_GPU": {"avg": 22.98, "peak": 24.42, "min": 21.66}, "VIN": {"avg": 56.31, "peak": 56.38, "min": 56.2}, "VDD_CPU_SOC_MSS": {"avg": 13.52, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 22.98, "energy_joules_est": 12.12, "sample_count": 3, "duration_seconds": 0.527}, "timestamp": "2026-01-17T17:41:25.045120"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1178.273, "latencies_ms": [1178.273], "images_per_second": 0.849, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "woman: 2\ngirl: 1\ndress: 1\nskirt: 1\ntable: 1\nchair: 1\nrug: 1\nstairs: 1\nbar: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.3, "ram_available_mb": 100400.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25371.4, "ram_available_mb": 100400.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.51, "min": 12.88}, "VDD_GPU": {"avg": 21.17, "peak": 25.99, "min": 18.51}, "VIN": {"avg": 56.99, "peak": 59.42, "min": 52.68}, "VDD_CPU_SOC_MSS": {"avg": 13.73, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 21.17, "energy_joules_est": 24.95, "sample_count": 8, "duration_seconds": 1.179}, "timestamp": "2026-01-17T17:41:26.231108"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 619.667, "latencies_ms": [619.667], "images_per_second": 1.614, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The girl is positioned in the foreground, interacting with the partygoers. The living room extends in the background, creating a sense of depth and space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.4, "ram_available_mb": 100400.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25371.1, "ram_available_mb": 100401.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 22.25, "peak": 24.42, "min": 20.47}, "VIN": {"avg": 56.69, "peak": 58.55, "min": 53.75}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 22.25, "energy_joules_est": 13.8, "sample_count": 4, "duration_seconds": 0.62}, "timestamp": "2026-01-17T17:41:26.856635"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 808.533, "latencies_ms": [808.533], "images_per_second": 1.237, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A group of people are gathered in a living room, enjoying a casual social gathering. A young girl is actively playing a game, possibly wii, while others watch and engage in conversation.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.1, "ram_available_mb": 100401.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25371.4, "ram_available_mb": 100400.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.76, "peak": 14.3, "min": 12.98}, "VDD_GPU": {"avg": 21.84, "peak": 25.58, "min": 19.28}, "VIN": {"avg": 56.25, "peak": 58.22, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.84, "energy_joules_est": 17.67, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-17T17:41:27.671351"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 997.962, "latencies_ms": [997.962], "images_per_second": 1.002, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The room is lit by natural light, creating a warm and inviting atmosphere. The colors are predominantly neutral, with pops of pink and green from the leis and decorations. The flooring appears to be polished wood, complementing the overall aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.4, "ram_available_mb": 100400.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 21.32, "peak": 25.19, "min": 18.89}, "VIN": {"avg": 55.51, "peak": 58.84, "min": 49.67}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 21.32, "energy_joules_est": 21.28, "sample_count": 7, "duration_seconds": 0.998}, "timestamp": "2026-01-17T17:41:28.675336"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 682.786, "latencies_ms": [682.786], "images_per_second": 1.465, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Two men in formal attire are shaking hands at a banquet, engaging in a business or professional interaction.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.2, "min": 13.49}, "VDD_GPU": {"avg": 27.39, "peak": 31.09, "min": 22.83}, "VIN": {"avg": 59.35, "peak": 73.76, "min": 52.22}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.39, "energy_joules_est": 18.71, "sample_count": 5, "duration_seconds": 0.683}, "timestamp": "2026-01-17T17:41:29.369698"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1175.289, "latencies_ms": [1175.289], "images_per_second": 0.851, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "man: 2\ntie: 1\nglasses: 1\nsuit: 2\ntable: 2\nplate: 2\nwater: 1\nfood: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 25.99, "peak": 34.26, "min": 20.09}, "VIN": {"avg": 58.43, "peak": 72.9, "min": 50.95}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.19, "min": 13.38}}, "power_watts_avg": 25.99, "energy_joules_est": 30.55, "sample_count": 8, "duration_seconds": 1.176}, "timestamp": "2026-01-17T17:41:30.555130"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 887.626, "latencies_ms": [887.626], "images_per_second": 1.127, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with the man on the left shaking hands with the man on the right. The background features other individuals and tables, suggesting a larger event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25370.6, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 26.79, "peak": 31.51, "min": 21.67}, "VIN": {"avg": 60.96, "peak": 80.74, "min": 53.02}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 26.79, "energy_joules_est": 23.79, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T17:41:31.449093"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1200.538, "latencies_ms": [1200.538], "images_per_second": 0.833, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a formal event or conference taking place in a large, well-lit room.  Two men in suits are shaking hands, indicating a successful interaction or agreement. The setting suggests a professional gathering, possibly related to business, politics, or diplomacy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.6, "ram_available_mb": 100401.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.4, "min": 13.39}, "VDD_GPU": {"avg": 24.72, "peak": 33.1, "min": 19.29}, "VIN": {"avg": 59.05, "peak": 78.28, "min": 54.83}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 24.72, "energy_joules_est": 29.69, "sample_count": 9, "duration_seconds": 1.201}, "timestamp": "2026-01-17T17:41:32.655597"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1153.498, "latencies_ms": [1153.498], "images_per_second": 0.867, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The room is lit with warm yellow lighting, creating a welcoming atmosphere. The walls are made of wood, and the tables are covered with white tablecloths. The men are dressed in formal attire, contributing to the overall elegance of the event.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25370.9, "ram_available_mb": 100401.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25370.7, "ram_available_mb": 100401.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 25.06, "peak": 31.5, "min": 19.69}, "VIN": {"avg": 58.82, "peak": 75.89, "min": 52.96}, "VDD_CPU_SOC_MSS": {"avg": 13.97, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 25.06, "energy_joules_est": 28.91, "sample_count": 8, "duration_seconds": 1.154}, "timestamp": "2026-01-17T17:41:33.819063"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 711.386, "latencies_ms": [711.386], "images_per_second": 1.406, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A man in a white dress shirt and striped tie stands in front of a dark background, looking off to the side with a serious expression.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25370.7, "ram_available_mb": 100401.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25370.5, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.58, "peak": 31.51, "min": 23.26}, "VIN": {"avg": 61.0, "peak": 73.56, "min": 56.72}, "VDD_CPU_SOC_MSS": {"avg": 13.7, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 27.58, "energy_joules_est": 19.63, "sample_count": 5, "duration_seconds": 0.712}, "timestamp": "2026-01-17T17:41:34.541298"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1090.246, "latencies_ms": [1090.246], "images_per_second": 0.917, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "shirt: 1\ntie: 1\npocket: 1\nhand: 1\nface: 1\nhair: 1\neyes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.5, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25370.4, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 25.97, "peak": 33.9, "min": 20.11}, "VIN": {"avg": 60.5, "peak": 84.61, "min": 53.67}, "VDD_CPU_SOC_MSS": {"avg": 13.92, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.97, "energy_joules_est": 28.32, "sample_count": 8, "duration_seconds": 1.091}, "timestamp": "2026-01-17T17:41:35.638450"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 712.086, "latencies_ms": [712.086], "images_per_second": 1.404, "prompt_tokens": 25, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The man is positioned in the foreground, with the background blurred and out of focus. The man is facing towards the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.4, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25370.4, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 28.3, "peak": 32.31, "min": 23.64}, "VIN": {"avg": 59.78, "peak": 74.4, "min": 54.0}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 28.3, "energy_joules_est": 20.16, "sample_count": 5, "duration_seconds": 0.712}, "timestamp": "2026-01-17T17:41:36.356476"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 822.339, "latencies_ms": [822.339], "images_per_second": 1.216, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A man is standing indoors, wearing a white dress shirt and a striped tie. He appears to be in a professional or business setting, possibly a meeting or office.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25370.4, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25370.4, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 28.03, "peak": 33.88, "min": 22.04}, "VIN": {"avg": 59.86, "peak": 74.11, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 28.03, "energy_joules_est": 23.06, "sample_count": 6, "duration_seconds": 0.823}, "timestamp": "2026-01-17T17:41:37.184945"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1115.187, "latencies_ms": [1115.187], "images_per_second": 0.897, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The man is wearing a light yellow dress shirt and a dark-colored tie with thin, light-colored stripes. His hair is dark and neatly styled. The lighting in the image is soft and somewhat dim, creating a subdued atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25370.4, "ram_available_mb": 100401.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 25.8, "peak": 33.47, "min": 20.1}, "VIN": {"avg": 58.27, "peak": 75.98, "min": 52.27}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.8, "energy_joules_est": 28.78, "sample_count": 8, "duration_seconds": 1.116}, "timestamp": "2026-01-17T17:41:38.306192"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 621.075, "latencies_ms": [621.075], "images_per_second": 1.61, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The living room features a plaid couch, a wooden entertainment center with a TV, and a red plaid armchair, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 22.75, "peak": 25.21, "min": 20.88}, "VIN": {"avg": 55.11, "peak": 57.49, "min": 51.97}, "VDD_CPU_SOC_MSS": {"avg": 13.58, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 22.75, "energy_joules_est": 14.14, "sample_count": 4, "duration_seconds": 0.622}, "timestamp": "2026-01-17T17:41:38.937997"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1041.234, "latencies_ms": [1041.234], "images_per_second": 0.96, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "chair: 2\ncouch: 2\ntv: 1\ncabinet: 1\nwhiteboard: 1\npicture: 1\nspeakers: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25372.2, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.2, "min": 13.09}, "VDD_GPU": {"avg": 21.56, "peak": 26.0, "min": 18.91}, "VIN": {"avg": 56.88, "peak": 58.53, "min": 55.4}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.58, "min": 13.38}}, "power_watts_avg": 21.56, "energy_joules_est": 22.45, "sample_count": 7, "duration_seconds": 1.042}, "timestamp": "2026-01-17T17:41:39.986914"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 642.875, "latencies_ms": [642.875], "images_per_second": 1.556, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The chair is positioned to the left of the couch, closer to the viewer. The couch is situated further back in the room, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.2, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25371.9, "ram_available_mb": 100400.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.59, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 22.45, "peak": 24.82, "min": 20.49}, "VIN": {"avg": 55.55, "peak": 57.09, "min": 52.77}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.79, "min": 13.39}}, "power_watts_avg": 22.45, "energy_joules_est": 14.44, "sample_count": 4, "duration_seconds": 0.643}, "timestamp": "2026-01-17T17:41:40.635892"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1158.98, "latencies_ms": [1158.98], "images_per_second": 0.863, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The scene depicts a cozy living room or common area with comfortable seating arrangements, including a plaid couch and a plaid armchair. A television set is positioned nearby, offering entertainment options. The room features a whiteboard with notes, a framed black and white photo, and a small wooden cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.9, "ram_available_mb": 100400.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25371.9, "ram_available_mb": 100400.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.4, "min": 12.98}, "VDD_GPU": {"avg": 21.12, "peak": 25.61, "min": 18.51}, "VIN": {"avg": 57.76, "peak": 60.11, "min": 56.28}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.12, "energy_joules_est": 24.49, "sample_count": 8, "duration_seconds": 1.159}, "timestamp": "2026-01-17T17:41:41.801130"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 912.426, "latencies_ms": [912.426], "images_per_second": 1.096, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The room features a plaid couch in blue and red hues. The lighting is soft and warm, creating a cozy atmosphere. The walls are painted a pale yellow, and the floor is carpeted in a neutral tone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.9, "ram_available_mb": 100400.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25372.6, "ram_available_mb": 100399.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 21.35, "peak": 24.41, "min": 19.32}, "VIN": {"avg": 56.88, "peak": 60.43, "min": 53.59}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 14.16, "min": 13.38}}, "power_watts_avg": 21.35, "energy_joules_est": 19.49, "sample_count": 6, "duration_seconds": 0.913}, "timestamp": "2026-01-17T17:41:42.723854"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 736.77, "latencies_ms": [736.77], "images_per_second": 1.357, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A surfer in a yellow shirt and maroon shorts rides a wave on a white surfboard with orange accents.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25372.6, "ram_available_mb": 100399.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25372.1, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 28.45, "peak": 32.7, "min": 23.64}, "VIN": {"avg": 67.86, "peak": 113.95, "min": 53.28}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.17, "min": 13.77}}, "power_watts_avg": 28.45, "energy_joules_est": 20.97, "sample_count": 5, "duration_seconds": 0.737}, "timestamp": "2026-01-17T17:41:43.472595"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1066.195, "latencies_ms": [1066.195], "images_per_second": 0.938, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "surfboard: 1\nwetsuit: 1\nwater: 1\nwaves: 1\nsurfer: 1\nclothing: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25372.1, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 26.92, "peak": 34.66, "min": 20.48}, "VIN": {"avg": 63.54, "peak": 110.61, "min": 53.7}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.19, "min": 13.77}}, "power_watts_avg": 26.92, "energy_joules_est": 28.71, "sample_count": 8, "duration_seconds": 1.067}, "timestamp": "2026-01-17T17:41:44.545977"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 809.139, "latencies_ms": [809.139], "images_per_second": 1.236, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The surfer is positioned in the foreground of the image, riding a wave. The wave is situated in the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25372.1, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 27.9, "peak": 33.09, "min": 22.46}, "VIN": {"avg": 61.87, "peak": 84.71, "min": 55.26}, "VDD_CPU_SOC_MSS": {"avg": 14.04, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 27.9, "energy_joules_est": 22.58, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-17T17:41:45.361243"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1029.583, "latencies_ms": [1029.583], "images_per_second": 0.971, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A surfer in a yellow wetsuit skillfully rides a wave on a white surfboard. The setting appears to be a river or ocean with chokeboxes, indicating a possible artificial wave pool.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.1, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.94, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 64.87, "peak": 118.59, "min": 50.98}, "VDD_CPU_SOC_MSS": {"avg": 13.94, "peak": 14.17, "min": 13.76}}, "power_watts_avg": 27.8, "energy_joules_est": 28.64, "sample_count": 7, "duration_seconds": 1.03}, "timestamp": "2026-01-17T17:41:46.401367"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1163.544, "latencies_ms": [1163.544], "images_per_second": 0.859, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The surfer is wearing a yellow top and dark shorts. The wave is teal and white, indicating a likely cloudy or overcast day. The surfboard appears to be made of a light-colored material, possibly fiberglass or foam.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 26.25, "peak": 33.88, "min": 20.49}, "VIN": {"avg": 63.64, "peak": 117.66, "min": 50.43}, "VDD_CPU_SOC_MSS": {"avg": 14.08, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.25, "energy_joules_est": 30.55, "sample_count": 8, "duration_seconds": 1.164}, "timestamp": "2026-01-17T17:41:47.571161"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 665.225, "latencies_ms": [665.225], "images_per_second": 1.503, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A long-haired black cat is sitting in front of a computer monitor, attentively gazing at the content displayed.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25372.4, "ram_available_mb": 100399.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25372.0, "ram_available_mb": 100400.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 27.88, "peak": 31.89, "min": 23.24}, "VIN": {"avg": 60.17, "peak": 80.8, "min": 51.01}, "VDD_CPU_SOC_MSS": {"avg": 13.94, "peak": 14.18, "min": 13.79}}, "power_watts_avg": 27.88, "energy_joules_est": 18.56, "sample_count": 5, "duration_seconds": 0.666}, "timestamp": "2026-01-17T17:41:48.249195"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1282.354, "latencies_ms": [1282.354], "images_per_second": 0.78, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "laptop: 2\nmouse: 1\nkeyboard: 1\nmousepad: 1\ncat: 1\ntelephone: 1\nmonitor: 2\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.0, "ram_available_mb": 100400.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.94, "peak": 14.4, "min": 13.19}, "VDD_GPU": {"avg": 25.01, "peak": 33.88, "min": 18.91}, "VIN": {"avg": 59.04, "peak": 72.69, "min": 54.94}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.01, "energy_joules_est": 32.09, "sample_count": 10, "duration_seconds": 1.283}, "timestamp": "2026-01-17T17:41:49.538407"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1161.992, "latencies_ms": [1161.992], "images_per_second": 0.861, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The cat is positioned in the foreground, partially obscuring the laptop screen. The laptop is situated near the cat, occupying the left side of the image. The monitor and phone are placed in the background, extending to the right and slightly behind the cat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25371.5, "ram_available_mb": 100400.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 24.96, "peak": 31.51, "min": 19.69}, "VIN": {"avg": 59.44, "peak": 84.36, "min": 51.63}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.96, "energy_joules_est": 29.01, "sample_count": 8, "duration_seconds": 1.162}, "timestamp": "2026-01-17T17:41:50.710104"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 853.681, "latencies_ms": [853.681], "images_per_second": 1.171, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A cat is sitting in front of a computer monitor, seemingly observing the screen. The computer is displaying a webpage or document, possibly related to business or technology.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25372.0, "ram_available_mb": 100400.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 26.72, "peak": 31.51, "min": 21.67}, "VIN": {"avg": 60.09, "peak": 75.38, "min": 52.89}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.72, "energy_joules_est": 22.82, "sample_count": 6, "duration_seconds": 0.854}, "timestamp": "2026-01-17T17:41:51.570303"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1133.488, "latencies_ms": [1133.488], "images_per_second": 0.882, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The cat is primarily black with gray fur. The lighting in the image appears to be soft and warm, likely from natural light coming in from the window. The desk surface appears to be made of a light-colored material, possibly wood or plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.0, "ram_available_mb": 100400.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25372.3, "ram_available_mb": 100399.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.3, "min": 13.19}, "VDD_GPU": {"avg": 25.71, "peak": 32.7, "min": 20.09}, "VIN": {"avg": 59.9, "peak": 84.47, "min": 50.37}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 25.71, "energy_joules_est": 29.15, "sample_count": 8, "duration_seconds": 1.134}, "timestamp": "2026-01-17T17:41:52.713857"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 702.359, "latencies_ms": [702.359], "images_per_second": 1.424, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A man and a child are cutting a red ribbon together in front of a building, accompanied by a crowd of onlookers.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25372.3, "ram_available_mb": 100399.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25372.3, "ram_available_mb": 100399.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.65, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 27.66, "peak": 31.13, "min": 23.25}, "VIN": {"avg": 60.91, "peak": 81.63, "min": 51.34}, "VDD_CPU_SOC_MSS": {"avg": 13.7, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 27.66, "energy_joules_est": 19.44, "sample_count": 5, "duration_seconds": 0.703}, "timestamp": "2026-01-17T17:41:53.428776"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1128.898, "latencies_ms": [1128.898], "images_per_second": 0.886, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "ribbon: 2\nscissors: 2\nballoon: 1\nhelmet: 1\nchild: 1\nman: 3\nwoman: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.3, "ram_available_mb": 100399.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25372.5, "ram_available_mb": 100399.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.77, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 26.16, "peak": 33.89, "min": 20.11}, "VIN": {"avg": 58.55, "peak": 77.7, "min": 50.65}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 26.16, "energy_joules_est": 29.54, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T17:41:54.563908"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 788.237, "latencies_ms": [788.237], "images_per_second": 1.269, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The scissors are positioned in the foreground, cutting the red ribbon. The crowd surrounds the scissors, creating a sense of proximity and anticipation for the event.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25372.5, "ram_available_mb": 100399.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25372.5, "ram_available_mb": 100399.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.76, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 27.12, "peak": 31.91, "min": 22.06}, "VIN": {"avg": 59.06, "peak": 79.9, "min": 49.12}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.77}}, "power_watts_avg": 27.12, "energy_joules_est": 21.39, "sample_count": 6, "duration_seconds": 0.789}, "timestamp": "2026-01-17T17:41:55.358959"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1339.231, "latencies_ms": [1339.231], "images_per_second": 0.747, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The scene depicts a lively outdoor event, possibly a ribbon-cutting ceremony, where a young child is cutting a red ribbon with a pair of scissors. Several adults and children surround the child, observing and participating in the event. A blue balloon adds a festive touch to the atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.5, "ram_available_mb": 100399.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25372.7, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 24.54, "peak": 33.09, "min": 18.91}, "VIN": {"avg": 59.11, "peak": 74.7, "min": 55.91}, "VDD_CPU_SOC_MSS": {"avg": 13.97, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 24.54, "energy_joules_est": 32.87, "sample_count": 10, "duration_seconds": 1.34}, "timestamp": "2026-01-17T17:41:56.704214"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1082.972, "latencies_ms": [1082.972], "images_per_second": 0.923, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The red ribbon and the blue balloon stand out against the muted colors of the surrounding environment. The lighting appears to be natural, illuminating the scene evenly. The materials used appear to be standard construction materials, suitable for outdoor events.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.7, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25372.7, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 25.16, "peak": 31.51, "min": 20.1}, "VIN": {"avg": 60.95, "peak": 82.58, "min": 51.1}, "VDD_CPU_SOC_MSS": {"avg": 13.88, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.16, "energy_joules_est": 27.26, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T17:41:57.793234"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 837.241, "latencies_ms": [837.241], "images_per_second": 1.194, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A First Group bus numbered 65745 is parked on the side of the road, displaying its free Wi-Fi and board signage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.7, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25373.5, "ram_available_mb": 100398.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 26.46, "peak": 31.51, "min": 21.67}, "VIN": {"avg": 60.12, "peak": 73.38, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.46, "energy_joules_est": 22.16, "sample_count": 6, "duration_seconds": 0.838}, "timestamp": "2026-01-17T17:41:58.641909"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1412.752, "latencies_ms": [1412.752], "images_per_second": 0.708, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "bus: 1\nwindshield wipers: 2\nheadlights: 4\nbumper: 1\nlicense plate: 1\nside mirror: 1\nwindshield: 2\ndoor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.5, "ram_available_mb": 100398.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25374.2, "ram_available_mb": 100398.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 24.38, "peak": 33.09, "min": 18.9}, "VIN": {"avg": 60.49, "peak": 92.54, "min": 51.71}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.38, "energy_joules_est": 34.46, "sample_count": 10, "duration_seconds": 1.413}, "timestamp": "2026-01-17T17:42:00.061354"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 847.882, "latencies_ms": [847.882], "images_per_second": 1.179, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The bus is positioned in the foreground, moving towards the left side of the image. The background features buildings and a street with pedestrians, creating a sense of a typical urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.2, "ram_available_mb": 100398.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25373.9, "ram_available_mb": 100398.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 26.39, "peak": 31.5, "min": 21.28}, "VIN": {"avg": 63.67, "peak": 90.0, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.77}}, "power_watts_avg": 26.39, "energy_joules_est": 22.39, "sample_count": 6, "duration_seconds": 0.848}, "timestamp": "2026-01-17T17:42:00.916259"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1255.105, "latencies_ms": [1255.105], "images_per_second": 0.797, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The scene depicts a city street with a First Group bus parked at the curb. The bus is labeled \"First Group\" and has a \"Free Wi-Fi on Board\" sign displayed. The street is lined with buildings, and pedestrians can be seen walking nearby.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25373.9, "ram_available_mb": 100398.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 24.72, "peak": 33.1, "min": 19.29}, "VIN": {"avg": 58.86, "peak": 77.02, "min": 50.6}, "VDD_CPU_SOC_MSS": {"avg": 14.05, "peak": 14.19, "min": 13.38}}, "power_watts_avg": 24.72, "energy_joules_est": 31.04, "sample_count": 9, "duration_seconds": 1.256}, "timestamp": "2026-01-17T17:42:02.177730"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 987.956, "latencies_ms": [987.956], "images_per_second": 1.012, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The bus is primarily white with pink and blue accents. The bus has multiple headlights and fog lights. The bus is parked on a street with buildings in the background. The sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 25.71, "peak": 31.89, "min": 20.48}, "VIN": {"avg": 60.53, "peak": 82.39, "min": 52.17}, "VDD_CPU_SOC_MSS": {"avg": 14.06, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 25.71, "energy_joules_est": 25.41, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-17T17:42:03.172020"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 672.447, "latencies_ms": [672.447], "images_per_second": 1.487, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A man is sitting cross-legged on a wooden floor in front of a large mirror, capturing his reflection while holding a phone.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 28.27, "peak": 31.91, "min": 23.62}, "VIN": {"avg": 60.9, "peak": 73.49, "min": 56.15}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 28.27, "energy_joules_est": 19.02, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T17:42:03.856197"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1605.992, "latencies_ms": [1605.992], "images_per_second": 0.623, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 1, "output_text": "mirror: 1\nwooden frame: 1\nman's reflection: 1\ngreen shirt: 1\nslippers: 1\nwindow: 1\nbed: 1\ntoys: 1\ncabinet: 1\nrug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.98, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 23.7, "peak": 34.26, "min": 18.13}, "VIN": {"avg": 59.07, "peak": 83.73, "min": 51.1}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 23.7, "energy_joules_est": 38.07, "sample_count": 12, "duration_seconds": 1.606}, "timestamp": "2026-01-17T17:42:05.468458"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1059.094, "latencies_ms": [1059.094], "images_per_second": 0.944, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The man is sitting in front of the mirror, which reflects his image. The mirror is positioned between the man and the background, creating a sense of depth. The foreground consists of wooden flooring, while the background features the interior of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 24.91, "peak": 31.12, "min": 19.69}, "VIN": {"avg": 61.51, "peak": 92.27, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 14.03, "peak": 14.19, "min": 13.78}}, "power_watts_avg": 24.91, "energy_joules_est": 26.39, "sample_count": 8, "duration_seconds": 1.06}, "timestamp": "2026-01-17T17:42:06.534417"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 942.552, "latencies_ms": [942.552], "images_per_second": 1.061, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "A man is sitting cross-legged in front of a large mirror, taking a selfie. He appears to be in a living room with hardwood floors, a green couch, and a wooden dresser visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 25.88, "peak": 31.89, "min": 20.47}, "VIN": {"avg": 58.91, "peak": 82.33, "min": 49.5}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 25.88, "energy_joules_est": 24.41, "sample_count": 7, "duration_seconds": 0.943}, "timestamp": "2026-01-17T17:42:07.483297"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1169.866, "latencies_ms": [1169.866], "images_per_second": 0.855, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The room has light-colored wood flooring and walls painted in a muted green hue. The lighting appears to be soft and diffused, creating a calm atmosphere. A wooden ornate mirror is prominently featured, reflecting the room's interior.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 24.9, "peak": 32.27, "min": 19.29}, "VIN": {"avg": 58.79, "peak": 76.53, "min": 53.8}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 24.9, "energy_joules_est": 29.14, "sample_count": 9, "duration_seconds": 1.17}, "timestamp": "2026-01-17T17:42:08.659775"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 693.824, "latencies_ms": [693.824], "images_per_second": 1.441, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A group of young men are standing next to surfboards, posing for a photo while a man takes their picture.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.65, "peak": 31.5, "min": 23.24}, "VIN": {"avg": 62.01, "peak": 89.27, "min": 48.6}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.77}}, "power_watts_avg": 27.65, "energy_joules_est": 19.2, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:42:09.364014"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1079.191, "latencies_ms": [1079.191], "images_per_second": 0.927, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "surfboard: 2\nman: 3\nwoman: 1\nchild: 2\nman: 1\nman: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 26.3, "peak": 33.88, "min": 20.1}, "VIN": {"avg": 59.96, "peak": 78.56, "min": 52.54}, "VDD_CPU_SOC_MSS": {"avg": 13.88, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 26.3, "energy_joules_est": 28.4, "sample_count": 8, "duration_seconds": 1.08}, "timestamp": "2026-01-17T17:42:10.449623"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1214.605, "latencies_ms": [1214.605], "images_per_second": 0.823, "prompt_tokens": 25, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The man is on the left side of the image, taking a picture of the group of young boys. The boys are positioned in the foreground, partially obscured by the man's arm and camera. The surfboards are placed in the background, extending from the foreground to the far right of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 24.74, "peak": 31.92, "min": 19.32}, "VIN": {"avg": 60.1, "peak": 79.65, "min": 52.12}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 24.74, "energy_joules_est": 30.06, "sample_count": 9, "duration_seconds": 1.215}, "timestamp": "2026-01-17T17:42:11.670020"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 810.321, "latencies_ms": [810.321], "images_per_second": 1.234, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A group of young men are gathered in a room, holding surfboards and posing for a photo. A person is taking a picture of them with a digital camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 26.6, "peak": 31.53, "min": 21.66}, "VIN": {"avg": 59.04, "peak": 76.44, "min": 49.77}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.6, "energy_joules_est": 21.56, "sample_count": 6, "duration_seconds": 0.811}, "timestamp": "2026-01-17T17:42:12.486502"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1113.704, "latencies_ms": [1113.704], "images_per_second": 0.898, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The surfboards are brightly colored, featuring yellow, red, and blue. The lighting in the room is bright, likely from overhead fluorescent fixtures, creating a lively atmosphere. The surfboards appear to be made of durable materials like fiberglass or plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25371.9, "ram_available_mb": 100400.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 25.81, "peak": 33.48, "min": 20.1}, "VIN": {"avg": 60.31, "peak": 80.82, "min": 56.22}, "VDD_CPU_SOC_MSS": {"avg": 14.07, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 25.81, "energy_joules_est": 28.76, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T17:42:13.606424"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 759.548, "latencies_ms": [759.548], "images_per_second": 1.317, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A gold-colored Polish Airlines Boeing 737-800, marked with the LOT logo and Polish text, is taxiing on the tarmac.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.9, "ram_available_mb": 100400.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25372.9, "ram_available_mb": 100399.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 14.0, "min": 13.19}, "VDD_GPU": {"avg": 22.45, "peak": 25.61, "min": 20.09}, "VIN": {"avg": 56.71, "peak": 60.02, "min": 54.13}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 22.45, "energy_joules_est": 17.07, "sample_count": 5, "duration_seconds": 0.76}, "timestamp": "2026-01-17T17:42:14.376430"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1186.49, "latencies_ms": [1186.49], "images_per_second": 0.843, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "airplane: 1\ntwin-engine: 2\nwings: 2\ntail: 1\nwindow: 8\ndoor: 2\ncockpit: 1\nengine: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25373.4, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.3, "min": 13.09}, "VDD_GPU": {"avg": 20.75, "peak": 25.21, "min": 18.52}, "VIN": {"avg": 57.57, "peak": 59.69, "min": 54.21}, "VDD_CPU_SOC_MSS": {"avg": 14.01, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 20.75, "energy_joules_est": 24.63, "sample_count": 9, "duration_seconds": 1.187}, "timestamp": "2026-01-17T17:42:15.571032"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 875.233, "latencies_ms": [875.233], "images_per_second": 1.143, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main object is a large passenger airplane positioned in the foreground of the image. The airplane is situated on a runway, with other aircraft visible in the background. The plane appears to be parked or taxiing on the tarmac.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.4, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 21.34, "peak": 24.43, "min": 19.3}, "VIN": {"avg": 57.21, "peak": 58.82, "min": 51.92}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.34, "energy_joules_est": 18.68, "sample_count": 6, "duration_seconds": 0.876}, "timestamp": "2026-01-17T17:42:16.452289"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 925.156, "latencies_ms": [925.156], "images_per_second": 1.081, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "An LOT Polish Airlines Boeing 737-800 aircraft is parked on the tarmac at an airport, facing right. The sky is partly cloudy, and there are other aircraft visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25373.4, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.2, "min": 13.09}, "VDD_GPU": {"avg": 21.16, "peak": 24.82, "min": 18.91}, "VIN": {"avg": 55.17, "peak": 60.28, "min": 50.96}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 21.16, "energy_joules_est": 19.58, "sample_count": 7, "duration_seconds": 0.925}, "timestamp": "2026-01-17T17:42:17.384311"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 853.189, "latencies_ms": [853.189], "images_per_second": 1.172, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The plane is primarily gold in color. The lighting appears to be natural daylight, illuminating the scene. The plane appears to be made of metal and has a polished finish. The sky is partly cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.4, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 21.47, "peak": 24.83, "min": 19.32}, "VIN": {"avg": 56.04, "peak": 59.05, "min": 49.21}, "VDD_CPU_SOC_MSS": {"avg": 13.84, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 21.47, "energy_joules_est": 18.32, "sample_count": 6, "duration_seconds": 0.854}, "timestamp": "2026-01-17T17:42:18.243543"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 487.5, "latencies_ms": [487.5], "images_per_second": 2.051, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A person is standing in front of a white toilet with a raised lid, releasing water into the bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.56, "peak": 13.9, "min": 13.19}, "VDD_GPU": {"avg": 23.12, "peak": 24.83, "min": 21.67}, "VIN": {"avg": 56.84, "peak": 57.45, "min": 55.94}, "VDD_CPU_SOC_MSS": {"avg": 13.51, "peak": 13.77, "min": 13.38}}, "power_watts_avg": 23.12, "energy_joules_est": 11.29, "sample_count": 3, "duration_seconds": 0.488}, "timestamp": "2026-01-17T17:42:18.740535"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1775.747, "latencies_ms": [1775.747], "images_per_second": 0.563, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 1, "output_text": "toilet: 1\ntoilet paper: 1\ntoilet seat: 1\ntoilet seat cover: 1\ntoilet handle: 1\ntoilet seat cover: 1\ntoilet bowl: 1\ntoilet seat: 1\ntoilet paper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.51, "min": 12.98}, "VDD_GPU": {"avg": 20.03, "peak": 26.0, "min": 17.73}, "VIN": {"avg": 56.31, "peak": 58.36, "min": 51.93}, "VDD_CPU_SOC_MSS": {"avg": 13.96, "peak": 14.58, "min": 12.99}}, "power_watts_avg": 20.03, "energy_joules_est": 35.58, "sample_count": 13, "duration_seconds": 1.776}, "timestamp": "2026-01-17T17:42:20.522475"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 692.964, "latencies_ms": [692.964], "images_per_second": 1.443, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The toilet is positioned in the foreground, close to the person's feet. The toilet is situated next to a wall and partially obscured by a towel hanging nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 21.67, "peak": 24.42, "min": 19.7}, "VIN": {"avg": 57.17, "peak": 58.11, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 21.67, "energy_joules_est": 15.03, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:42:21.221992"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 533.969, "latencies_ms": [533.969], "images_per_second": 1.873, "prompt_tokens": 19, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A person is urinating into a toilet in a bathroom. The bathroom appears simple and uncluttered.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.46, "peak": 13.79, "min": 13.09}, "VDD_GPU": {"avg": 23.38, "peak": 25.21, "min": 21.67}, "VIN": {"avg": 56.19, "peak": 57.47, "min": 54.54}, "VDD_CPU_SOC_MSS": {"avg": 13.52, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 23.38, "energy_joules_est": 12.5, "sample_count": 3, "duration_seconds": 0.535}, "timestamp": "2026-01-17T17:42:21.763526"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 928.456, "latencies_ms": [928.456], "images_per_second": 1.077, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The toilet is white and appears to be made of porcelain or ceramic. The lighting in the bathroom is dim, creating a somewhat shadowy atmosphere. The materials visible include the toilet itself, the floor, and the towel hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.66, "peak": 14.2, "min": 12.78}, "VDD_GPU": {"avg": 22.0, "peak": 26.01, "min": 19.3}, "VIN": {"avg": 57.03, "peak": 58.69, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 13.51, "peak": 13.77, "min": 12.99}}, "power_watts_avg": 22.0, "energy_joules_est": 20.43, "sample_count": 6, "duration_seconds": 0.929}, "timestamp": "2026-01-17T17:42:22.698417"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 988.331, "latencies_ms": [988.331], "images_per_second": 1.012, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A skier dressed in blue and black, wearing a white helmet and goggles, skillfully navigates a snowy mountain slope, leaning into a turn with ski poles in hand.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 25.79, "peak": 31.12, "min": 20.9}, "VIN": {"avg": 59.05, "peak": 76.2, "min": 51.9}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.16, "min": 13.38}}, "power_watts_avg": 25.79, "energy_joules_est": 25.5, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T17:42:23.698676"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1274.835, "latencies_ms": [1274.835], "images_per_second": 0.784, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "person: 1\nhelmet: 1\ngloves: 1\nski poles: 2\nskis: 2\nsnow: 6\ntrees: 4\nsnow: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 24.79, "peak": 32.33, "min": 19.3}, "VIN": {"avg": 58.9, "peak": 78.85, "min": 51.39}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 24.79, "energy_joules_est": 31.62, "sample_count": 9, "duration_seconds": 1.275}, "timestamp": "2026-01-17T17:42:24.979834"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 850.127, "latencies_ms": [850.127], "images_per_second": 1.176, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The skier is positioned in the foreground of the image, moving towards the left side of the frame. The snowy landscape and trees in the background create a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 26.79, "peak": 31.51, "min": 21.67}, "VIN": {"avg": 58.8, "peak": 72.04, "min": 53.99}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.79, "energy_joules_est": 22.78, "sample_count": 6, "duration_seconds": 0.85}, "timestamp": "2026-01-17T17:42:25.836278"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 988.404, "latencies_ms": [988.404], "images_per_second": 1.012, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A skier is captured mid-turn on a snowy slope, surrounded by snow-covered trees. The skier is wearing a blue jacket and helmet, holding ski poles and navigating the terrain.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.45, "peak": 32.68, "min": 20.89}, "VIN": {"avg": 62.52, "peak": 88.27, "min": 55.91}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.45, "energy_joules_est": 26.16, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T17:42:26.832202"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 987.228, "latencies_ms": [987.228], "images_per_second": 1.013, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The skier is wearing a blue jacket and white helmet. The lighting is bright and appears to be natural daylight. The snow appears to be fresh and undisturbed. The overall scene conveys a serene winter atmosphere.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 26.34, "peak": 32.31, "min": 20.88}, "VIN": {"avg": 61.68, "peak": 84.4, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.34, "energy_joules_est": 26.01, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-17T17:42:27.825649"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 574.329, "latencies_ms": [574.329], "images_per_second": 1.741, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A female tennis player in a yellow shirt and black shorts is poised to hit the ball during a professional match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 23.14, "peak": 26.0, "min": 20.88}, "VIN": {"avg": 57.23, "peak": 58.02, "min": 56.67}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.79, "min": 13.38}}, "power_watts_avg": 23.14, "energy_joules_est": 13.31, "sample_count": 4, "duration_seconds": 0.575}, "timestamp": "2026-01-17T17:42:28.408033"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1387.875, "latencies_ms": [1387.875], "images_per_second": 0.721, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "Tennis player: 1\nTennis racket: 1\nTennis ball: 1\nTennis court: 1\nSpectators: 1\nReferee: 1\nScoreboard: 1\nAdvertising boards: 4", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.4, "min": 13.09}, "VDD_GPU": {"avg": 20.76, "peak": 26.0, "min": 18.13}, "VIN": {"avg": 56.7, "peak": 59.55, "min": 53.34}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 20.76, "energy_joules_est": 28.82, "sample_count": 10, "duration_seconds": 1.388}, "timestamp": "2026-01-17T17:42:29.806125"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 778.889, "latencies_ms": [778.889], "images_per_second": 1.284, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The tennis player is positioned near the center of the court, facing the audience. The ball court is situated in the background, extending beyond the immediate foreground of the player and spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.5, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 21.74, "peak": 24.43, "min": 19.7}, "VIN": {"avg": 55.7, "peak": 56.98, "min": 54.31}, "VDD_CPU_SOC_MSS": {"avg": 13.94, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 21.74, "energy_joules_est": 16.94, "sample_count": 5, "duration_seconds": 0.779}, "timestamp": "2026-01-17T17:42:30.595074"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 667.253, "latencies_ms": [667.253], "images_per_second": 1.499, "prompt_tokens": 19, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A tennis match is taking place on a blue court, with players in action and spectators in the stands. The atmosphere is lively and energetic.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.69, "peak": 14.1, "min": 13.09}, "VDD_GPU": {"avg": 22.06, "peak": 25.21, "min": 19.69}, "VIN": {"avg": 57.01, "peak": 60.14, "min": 52.27}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 22.06, "energy_joules_est": 14.73, "sample_count": 5, "duration_seconds": 0.668}, "timestamp": "2026-01-17T17:42:31.268579"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 782.757, "latencies_ms": [782.757], "images_per_second": 1.278, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The tennis court is predominantly blue. The lighting appears to be artificial, likely from spotlights. The court surface appears to be made of a synthetic material like rubber or synthetic turf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.68, "peak": 14.2, "min": 13.09}, "VDD_GPU": {"avg": 21.93, "peak": 25.6, "min": 19.3}, "VIN": {"avg": 56.82, "peak": 60.04, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.93, "energy_joules_est": 17.18, "sample_count": 6, "duration_seconds": 0.783}, "timestamp": "2026-01-17T17:42:32.057622"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 578.732, "latencies_ms": [578.732], "images_per_second": 1.728, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A metal bowl on a plate contains an orange-colored dish with what appears to be meat and vegetables, accompanied by a spoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 25376.8, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.57, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 22.65, "peak": 25.21, "min": 20.48}, "VIN": {"avg": 57.28, "peak": 59.73, "min": 55.27}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 22.65, "energy_joules_est": 13.12, "sample_count": 4, "duration_seconds": 0.579}, "timestamp": "2026-01-17T17:42:32.646755"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1060.29, "latencies_ms": [1060.29], "images_per_second": 0.943, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "bowl: 2\nspoon: 1\nplate: 1\nnapkin: 1\nfood: 6\nonions: 6\nred onion: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25376.8, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.2, "min": 12.98}, "VDD_GPU": {"avg": 21.18, "peak": 26.0, "min": 18.51}, "VIN": {"avg": 57.86, "peak": 59.41, "min": 55.06}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 21.18, "energy_joules_est": 22.46, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T17:42:33.713023"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 878.138, "latencies_ms": [878.138], "images_per_second": 1.139, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The bowl containing meat and vegetables is positioned in the foreground, while the bowl of pickled onions is situated in the background. The meat and vegetables are placed closer to the viewer, while the pickled onions are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 21.46, "peak": 24.82, "min": 19.3}, "VIN": {"avg": 56.63, "peak": 58.51, "min": 51.65}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 21.46, "energy_joules_est": 18.85, "sample_count": 6, "duration_seconds": 0.878}, "timestamp": "2026-01-17T17:42:34.597314"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1192.848, "latencies_ms": [1192.848], "images_per_second": 0.838, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The scene depicts a meal served on a dark table with a white plate and a metal bowl containing two distinct dishes: one with reddish-brown meat and vegetables, and the other with sliced orange vegetables, possibly pickled beets. A spoon rests on the plate, ready for use.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.4, "min": 13.19}, "VDD_GPU": {"avg": 20.57, "peak": 24.82, "min": 18.11}, "VIN": {"avg": 56.63, "peak": 60.56, "min": 50.94}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 20.57, "energy_joules_est": 24.55, "sample_count": 9, "duration_seconds": 1.193}, "timestamp": "2026-01-17T17:42:35.796154"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1144.652, "latencies_ms": [1144.652], "images_per_second": 0.874, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The food in the bowls is predominantly orange and pinkish-orange. The lighting is soft and warm, enhancing the colors of the food. The bowls appear to be made of metal and have a metallic sheen. The table setting suggests a formal or elegant dining environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 20.63, "peak": 24.41, "min": 18.51}, "VIN": {"avg": 55.91, "peak": 58.31, "min": 52.93}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 20.63, "energy_joules_est": 23.62, "sample_count": 8, "duration_seconds": 1.145}, "timestamp": "2026-01-17T17:42:36.951800"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 698.878, "latencies_ms": [698.878], "images_per_second": 1.431, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A group of sheep, including one with a distinctive black face, are standing together near a brick building in a grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 27.18, "peak": 30.73, "min": 22.85}, "VIN": {"avg": 60.99, "peak": 84.32, "min": 52.28}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 27.18, "energy_joules_est": 19.02, "sample_count": 5, "duration_seconds": 0.7}, "timestamp": "2026-01-17T17:42:37.663118"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1228.101, "latencies_ms": [1228.101], "images_per_second": 0.814, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "sheep: 5\nbrick wall: 1\ngrass: 2\nslide: 1\nwooden structure: 1\nfence: 1\nwooden beams: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.2, "ram_available_mb": 100397.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25375.2, "ram_available_mb": 100397.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 25.38, "peak": 33.48, "min": 19.7}, "VIN": {"avg": 59.51, "peak": 73.82, "min": 54.73}, "VDD_CPU_SOC_MSS": {"avg": 14.04, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 25.38, "energy_joules_est": 31.18, "sample_count": 9, "duration_seconds": 1.229}, "timestamp": "2026-01-17T17:42:38.897847"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 803.797, "latencies_ms": [803.797], "images_per_second": 1.244, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The main sheep is positioned in the foreground, slightly to the left of the image. The brick wall and wooden structure in the background are further back, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.2, "ram_available_mb": 100397.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 26.93, "peak": 31.51, "min": 22.08}, "VIN": {"avg": 61.94, "peak": 79.58, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.17, "min": 13.76}}, "power_watts_avg": 26.93, "energy_joules_est": 21.66, "sample_count": 6, "duration_seconds": 0.804}, "timestamp": "2026-01-17T17:42:39.708009"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 824.714, "latencies_ms": [824.714], "images_per_second": 1.213, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A group of sheep, including brown and white ones, are gathered in a grassy area near a brick building.  Some sheep are standing, while others appear to be interacting with each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 27.65, "peak": 33.1, "min": 22.07}, "VIN": {"avg": 62.23, "peak": 82.77, "min": 57.11}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.65, "energy_joules_est": 22.82, "sample_count": 6, "duration_seconds": 0.825}, "timestamp": "2026-01-17T17:42:40.539406"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 903.289, "latencies_ms": [903.289], "images_per_second": 1.107, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The sheep have thick, brown wool that appears slightly dirty. The lighting suggests a sunny day, and the brick wall in the background adds a rustic touch to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 27.45, "peak": 32.7, "min": 22.08}, "VIN": {"avg": 60.58, "peak": 79.55, "min": 53.52}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 27.45, "energy_joules_est": 24.81, "sample_count": 6, "duration_seconds": 0.904}, "timestamp": "2026-01-17T17:42:41.449823"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 693.392, "latencies_ms": [693.392], "images_per_second": 1.442, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A bunch of ripe bananas is arranged in a circular pattern, partially overlapping a vibrant red and yellow fruit in the center.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 29.48, "peak": 33.89, "min": 24.43}, "VIN": {"avg": 67.04, "peak": 117.56, "min": 49.11}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 29.48, "energy_joules_est": 20.45, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:42:42.155316"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1260.98, "latencies_ms": [1260.98], "images_per_second": 0.793, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "bananas: 5\npomegranate: 1\nbanana: 5\nbanana: 5\nbanana: 5\nbanana: 5\nbanana: 5\nbanana: 5", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25374.9, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.4, "min": 13.39}, "VDD_GPU": {"avg": 26.66, "peak": 35.85, "min": 20.09}, "VIN": {"avg": 61.86, "peak": 102.91, "min": 51.75}, "VDD_CPU_SOC_MSS": {"avg": 13.99, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 26.66, "energy_joules_est": 33.63, "sample_count": 9, "duration_seconds": 1.261}, "timestamp": "2026-01-17T17:42:43.426453"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 989.709, "latencies_ms": [989.709], "images_per_second": 1.01, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The bananas are positioned in the foreground, partially overlapping the pomegranate. The pomegranate is situated near the center of the image, slightly behind and to the right of the bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 26.85, "peak": 33.47, "min": 21.28}, "VIN": {"avg": 64.48, "peak": 116.52, "min": 46.13}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.17, "min": 13.77}}, "power_watts_avg": 26.85, "energy_joules_est": 26.59, "sample_count": 7, "duration_seconds": 0.99}, "timestamp": "2026-01-17T17:42:44.422579"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1032.16, "latencies_ms": [1032.16], "images_per_second": 0.969, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene is set against a textured, teal background with leaf-like patterns. A bunch of green bananas is arranged around a partially peeled, red and yellow apple, creating a visually striking and artistic composition.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 26.49, "peak": 33.88, "min": 20.48}, "VIN": {"avg": 63.72, "peak": 116.88, "min": 47.1}, "VDD_CPU_SOC_MSS": {"avg": 14.12, "peak": 14.56, "min": 13.78}}, "power_watts_avg": 26.49, "energy_joules_est": 27.36, "sample_count": 8, "duration_seconds": 1.033}, "timestamp": "2026-01-17T17:42:45.461419"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 993.237, "latencies_ms": [993.237], "images_per_second": 1.007, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The fruits are predominantly green and have a vibrant red-yellow color. The lighting creates a warm, inviting atmosphere, and the image appears to be digitally manipulated, giving it a surreal and artistic quality.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25375.0, "ram_available_mb": 100397.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 27.12, "peak": 33.48, "min": 21.27}, "VIN": {"avg": 64.15, "peak": 115.47, "min": 50.49}, "VDD_CPU_SOC_MSS": {"avg": 14.06, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.12, "energy_joules_est": 26.95, "sample_count": 7, "duration_seconds": 0.994}, "timestamp": "2026-01-17T17:42:46.461717"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 512.067, "latencies_ms": [512.067], "images_per_second": 1.953, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A blue and white tram numbered 2 is traveling down a street lined with trees and buildings.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25375.0, "ram_available_mb": 100397.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25375.0, "ram_available_mb": 100397.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.49, "peak": 13.79, "min": 13.19}, "VDD_GPU": {"avg": 23.89, "peak": 26.0, "min": 22.05}, "VIN": {"avg": 56.25, "peak": 58.32, "min": 53.97}, "VDD_CPU_SOC_MSS": {"avg": 13.65, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 23.89, "energy_joules_est": 12.25, "sample_count": 3, "duration_seconds": 0.513}, "timestamp": "2026-01-17T17:42:46.984596"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1098.822, "latencies_ms": [1098.822], "images_per_second": 0.91, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Train: 8\nTrees: 2\nSky: 1\nWires: 6\nBuildings: 1\nCars: 1\nTrain tracks: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.0, "ram_available_mb": 100397.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.78, "peak": 14.3, "min": 12.98}, "VDD_GPU": {"avg": 21.62, "peak": 26.39, "min": 18.91}, "VIN": {"avg": 56.27, "peak": 58.7, "min": 50.49}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 12.99}}, "power_watts_avg": 21.62, "energy_joules_est": 23.76, "sample_count": 8, "duration_seconds": 1.099}, "timestamp": "2026-01-17T17:42:48.089201"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 762.054, "latencies_ms": [762.054], "images_per_second": 1.312, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The blue and white tram is positioned in the foreground, moving towards the left side of the image. The street and surrounding environment extend into the background, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.63, "peak": 13.9, "min": 13.29}, "VDD_GPU": {"avg": 21.9, "peak": 24.82, "min": 19.7}, "VIN": {"avg": 56.18, "peak": 59.02, "min": 52.64}, "VDD_CPU_SOC_MSS": {"avg": 13.87, "peak": 14.19, "min": 13.38}}, "power_watts_avg": 21.9, "energy_joules_est": 16.69, "sample_count": 5, "duration_seconds": 0.762}, "timestamp": "2026-01-17T17:42:48.857004"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 739.637, "latencies_ms": [739.637], "images_per_second": 1.352, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A blue and white tram is traveling down a street, carrying passengers and passing by trees and a building. The scene suggests a modern, urban transit system.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.65, "peak": 14.1, "min": 12.98}, "VDD_GPU": {"avg": 22.13, "peak": 25.21, "min": 20.09}, "VIN": {"avg": 56.86, "peak": 59.13, "min": 54.9}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 22.13, "energy_joules_est": 16.39, "sample_count": 5, "duration_seconds": 0.74}, "timestamp": "2026-01-17T17:42:49.603463"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 791.911, "latencies_ms": [791.911], "images_per_second": 1.263, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The tram is primarily blue and white. The lighting is bright, illuminating the scene. The tram appears to be made of metal and plastic. The weather is sunny and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.63, "peak": 14.0, "min": 12.98}, "VDD_GPU": {"avg": 21.8, "peak": 25.21, "min": 19.31}, "VIN": {"avg": 55.15, "peak": 60.87, "min": 48.36}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.8, "energy_joules_est": 17.27, "sample_count": 6, "duration_seconds": 0.792}, "timestamp": "2026-01-17T17:42:50.401473"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 855.543, "latencies_ms": [855.543], "images_per_second": 1.169, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The bathroom features a white bathtub, a glass shower enclosure, a red shower curtain, and two sinks with dark wood cabinets and marble countertops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.68, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 26.78, "peak": 31.91, "min": 21.66}, "VIN": {"avg": 62.24, "peak": 83.43, "min": 56.09}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.78, "energy_joules_est": 22.92, "sample_count": 6, "duration_seconds": 0.856}, "timestamp": "2026-01-17T17:42:51.267379"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1665.808, "latencies_ms": [1665.808], "images_per_second": 0.6, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 1, "output_text": "shower curtain: 1\nbathtub: 1\nshower head: 1\ntoilet paper holder: 1\nmirrors: 2\nvanity: 2\nsink: 2\ncountertop: 2\nfloor: 1\nrug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.11, "peak": 14.51, "min": 13.19}, "VDD_GPU": {"avg": 23.03, "peak": 32.7, "min": 18.11}, "VIN": {"avg": 58.78, "peak": 77.56, "min": 54.34}, "VDD_CPU_SOC_MSS": {"avg": 14.15, "peak": 14.57, "min": 13.39}}, "power_watts_avg": 23.03, "energy_joules_est": 38.37, "sample_count": 13, "duration_seconds": 1.666}, "timestamp": "2026-01-17T17:42:52.939130"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 938.679, "latencies_ms": [938.679], "images_per_second": 1.065, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The bathtub is positioned to the left of the shower and partially behind the shower curtain. The sink is located in the foreground, closer to the viewer, while the mirrors are situated in the background, slightly further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.94, "peak": 14.2, "min": 13.49}, "VDD_GPU": {"avg": 25.66, "peak": 30.71, "min": 20.48}, "VIN": {"avg": 61.85, "peak": 75.78, "min": 57.39}, "VDD_CPU_SOC_MSS": {"avg": 14.12, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 25.66, "energy_joules_est": 24.1, "sample_count": 7, "duration_seconds": 0.939}, "timestamp": "2026-01-17T17:42:53.884331"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1403.794, "latencies_ms": [1403.794], "images_per_second": 0.712, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The bathroom features a double vanity with dark wood cabinets, a white countertop, and marble-like backsplash. A bathtub with a shower curtain is situated next to the vanity, complemented by a red rug on the floor. The bathroom is well-lit with wall sconces and mirrors.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25380.7, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 24.14, "peak": 32.3, "min": 18.9}, "VIN": {"avg": 57.9, "peak": 72.14, "min": 52.57}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.14, "energy_joules_est": 33.9, "sample_count": 10, "duration_seconds": 1.404}, "timestamp": "2026-01-17T17:42:55.295036"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1304.869, "latencies_ms": [1304.869], "images_per_second": 0.766, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The bathroom features a beige color scheme, accented by red shower curtains and dark wood cabinets. The lighting is warm and ambient, contributing to the overall ambiance. The bathroom includes a glass shower enclosure, a bathtub, and a double vanity with dark wood cabinets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.7, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.4, "min": 13.39}, "VDD_GPU": {"avg": 23.88, "peak": 31.12, "min": 18.91}, "VIN": {"avg": 57.74, "peak": 75.62, "min": 47.97}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 23.88, "energy_joules_est": 31.17, "sample_count": 10, "duration_seconds": 1.305}, "timestamp": "2026-01-17T17:42:56.606072"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 757.87, "latencies_ms": [757.87], "images_per_second": 1.319, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A surfer in a wetsuit rides a wave on a surfboard, skillfully carving through the water as it crashes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25380.7, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.58, "peak": 31.51, "min": 23.26}, "VIN": {"avg": 61.12, "peak": 71.99, "min": 57.04}, "VDD_CPU_SOC_MSS": {"avg": 13.85, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 27.58, "energy_joules_est": 20.92, "sample_count": 5, "duration_seconds": 0.759}, "timestamp": "2026-01-17T17:42:57.375122"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1150.126, "latencies_ms": [1150.126], "images_per_second": 0.869, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "surfboard: 1\nwetsuit: 1\nwater: 1\nwave: 1\ncable: 1\nperson: 1\nocean: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25380.7, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25381.1, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 25.39, "peak": 33.5, "min": 19.7}, "VIN": {"avg": 60.01, "peak": 83.23, "min": 53.15}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 25.39, "energy_joules_est": 29.22, "sample_count": 9, "duration_seconds": 1.151}, "timestamp": "2026-01-17T17:42:58.532216"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1178.542, "latencies_ms": [1178.542], "images_per_second": 0.849, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The surfer is positioned near the center of the image, riding a wave. The wave is breaking towards the left side of the frame, creating a dynamic contrast with the surfer's position. The ocean extends into the background, providing a sense of scale and vastness.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25381.1, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25380.9, "ram_available_mb": 100391.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 24.6, "peak": 31.91, "min": 19.31}, "VIN": {"avg": 59.97, "peak": 94.07, "min": 48.82}, "VDD_CPU_SOC_MSS": {"avg": 14.04, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 24.6, "energy_joules_est": 29.01, "sample_count": 9, "duration_seconds": 1.179}, "timestamp": "2026-01-17T17:42:59.719373"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1091.919, "latencies_ms": [1091.919], "images_per_second": 0.916, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A surfer in a wetsuit is skillfully riding a wave in the ocean. The wave is breaking, creating a dynamic scene with splashing water. The image is black and white, enhancing the dramatic effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.9, "ram_available_mb": 100391.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25381.1, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 24.97, "peak": 31.91, "min": 19.7}, "VIN": {"avg": 58.19, "peak": 70.67, "min": 51.24}, "VDD_CPU_SOC_MSS": {"avg": 13.87, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 24.97, "energy_joules_est": 27.27, "sample_count": 8, "duration_seconds": 1.092}, "timestamp": "2026-01-17T17:43:00.818150"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1517.384, "latencies_ms": [1517.384], "images_per_second": 0.659, "prompt_tokens": 18, "response_tokens_est": 72, "n_tiles": 1, "output_text": "The black and white image captures a dynamic moment of action in the ocean. The lighting is dramatic, with strong contrasts between light and shadow, enhancing the visual impact of the surfer and the wave. The wave appears to be breaking, creating a powerful visual effect. The surfer is wearing a wetsuit, which suggests the presence of cold water conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.1, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25381.1, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 23.35, "peak": 31.91, "min": 18.51}, "VIN": {"avg": 58.85, "peak": 74.68, "min": 55.92}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 23.35, "energy_joules_est": 35.44, "sample_count": 11, "duration_seconds": 1.518}, "timestamp": "2026-01-17T17:43:02.342017"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 902.754, "latencies_ms": [902.754], "images_per_second": 1.108, "prompt_tokens": 8, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A baseball player in a white uniform with orange and black accents is poised at home plate, ready to swing the bat, while a catcher in black and red gear crouches behind him.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25381.1, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 25385.9, "ram_available_mb": 100386.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.2, "min": 13.19}, "VDD_GPU": {"avg": 21.15, "peak": 24.43, "min": 18.91}, "VIN": {"avg": 58.04, "peak": 59.46, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.15, "energy_joules_est": 19.1, "sample_count": 6, "duration_seconds": 0.903}, "timestamp": "2026-01-17T17:43:03.257256"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1331.872, "latencies_ms": [1331.872], "images_per_second": 0.751, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "baseball bat: 1\nbaseball glove: 1\nbaseball mitt: 1\nbaseball: 1\ncatcher: 2\numpire: 1\nbaseball field: 1\nspectators: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25386.2, "ram_available_mb": 100386.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25387.5, "ram_available_mb": 100384.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 20.25, "peak": 24.82, "min": 18.12}, "VIN": {"avg": 56.08, "peak": 58.44, "min": 49.73}, "VDD_CPU_SOC_MSS": {"avg": 14.17, "peak": 14.57, "min": 13.38}}, "power_watts_avg": 20.25, "energy_joules_est": 26.98, "sample_count": 10, "duration_seconds": 1.332}, "timestamp": "2026-01-17T17:43:04.595255"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1230.158, "latencies_ms": [1230.158], "images_per_second": 0.813, "prompt_tokens": 25, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The main objects are positioned in a spatial arrangement that suggests a game in progress. The batter is in the foreground, preparing to swing, while the catcher and umpire are in the background, observing the play. The setting appears to be a baseball field, with players and spectators visible in the surrounding area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25387.5, "ram_available_mb": 100384.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25387.5, "ram_available_mb": 100384.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.3, "min": 13.19}, "VDD_GPU": {"avg": 20.26, "peak": 24.42, "min": 18.11}, "VIN": {"avg": 57.08, "peak": 59.98, "min": 52.32}, "VDD_CPU_SOC_MSS": {"avg": 14.01, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 20.26, "energy_joules_est": 24.93, "sample_count": 9, "duration_seconds": 1.231}, "timestamp": "2026-01-17T17:43:05.832103"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 813.57, "latencies_ms": [813.57], "images_per_second": 1.229, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A baseball game is in progress on a field with spectators in the stands. A batter, catcher, and umpire are actively engaged in the game, preparing for the pitch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25387.5, "ram_available_mb": 100384.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25387.7, "ram_available_mb": 100384.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.1, "min": 13.19}, "VDD_GPU": {"avg": 21.2, "peak": 24.42, "min": 18.91}, "VIN": {"avg": 55.93, "peak": 58.06, "min": 50.33}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.2, "energy_joules_est": 17.25, "sample_count": 6, "duration_seconds": 0.814}, "timestamp": "2026-01-17T17:43:06.653753"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 789.991, "latencies_ms": [789.991], "images_per_second": 1.266, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The batter is wearing white pants and an orange jersey. The catcher is wearing a black jersey and protective gear. The field is green, and the lighting appears to be natural daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25387.7, "ram_available_mb": 100384.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25387.7, "ram_available_mb": 100384.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.1, "min": 13.09}, "VDD_GPU": {"avg": 21.47, "peak": 24.82, "min": 19.3}, "VIN": {"avg": 57.29, "peak": 58.99, "min": 54.59}, "VDD_CPU_SOC_MSS": {"avg": 13.71, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.47, "energy_joules_est": 16.97, "sample_count": 6, "duration_seconds": 0.79}, "timestamp": "2026-01-17T17:43:07.449659"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 731.184, "latencies_ms": [731.184], "images_per_second": 1.368, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The black and white image showcases a collection of fruits, including apples, oranges, grapes, and peanuts, arranged in a visually appealing manner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25387.7, "ram_available_mb": 100384.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25388.2, "ram_available_mb": 100384.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.73, "peak": 31.12, "min": 23.24}, "VIN": {"avg": 63.59, "peak": 93.78, "min": 54.77}, "VDD_CPU_SOC_MSS": {"avg": 13.94, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.73, "energy_joules_est": 20.29, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T17:43:08.191335"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 915.49, "latencies_ms": [915.49], "images_per_second": 1.092, "prompt_tokens": 21, "response_tokens_est": 23, "n_tiles": 1, "output_text": "grapes: 5\norange: 1\napple: 2\npeanuts: 10\nother fruit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25388.2, "ram_available_mb": 100384.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25388.5, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 27.01, "peak": 33.86, "min": 21.27}, "VIN": {"avg": 60.17, "peak": 79.26, "min": 52.91}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 27.01, "energy_joules_est": 24.74, "sample_count": 7, "duration_seconds": 0.916}, "timestamp": "2026-01-17T17:43:09.113198"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 881.754, "latencies_ms": [881.754], "images_per_second": 1.134, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The left side of the image features the fruits and nuts, while the right side shows the apple. The fruits and nuts are positioned in the foreground, while the apple is situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25388.5, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25388.4, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 27.32, "peak": 33.09, "min": 21.67}, "VIN": {"avg": 59.17, "peak": 73.12, "min": 52.57}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.32, "energy_joules_est": 24.1, "sample_count": 6, "duration_seconds": 0.882}, "timestamp": "2026-01-17T17:43:10.001232"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1129.479, "latencies_ms": [1129.479], "images_per_second": 0.885, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a still life arrangement featuring various fruits, including apples, grapes, and a round fruit resembling an orange or mandarin. The fruits are placed on a bed of peanuts, creating a visually appealing and textured composition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25388.4, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25388.4, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 25.76, "peak": 33.09, "min": 20.08}, "VIN": {"avg": 60.79, "peak": 88.67, "min": 50.97}, "VDD_CPU_SOC_MSS": {"avg": 13.82, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 25.76, "energy_joules_est": 29.11, "sample_count": 8, "duration_seconds": 1.13}, "timestamp": "2026-01-17T17:43:11.136826"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1318.137, "latencies_ms": [1318.137], "images_per_second": 0.759, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The fruits are depicted in shades of white and gray, creating a monochromatic effect. The lighting in the image is soft and diffused, enhancing the textures and details of the fruits and nuts. The fruits appear to be whole and whole, showcasing their natural shapes and colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25388.4, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25388.4, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.11, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 24.02, "peak": 32.29, "min": 18.91}, "VIN": {"avg": 60.1, "peak": 76.05, "min": 57.6}, "VDD_CPU_SOC_MSS": {"avg": 14.06, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 24.02, "energy_joules_est": 31.67, "sample_count": 10, "duration_seconds": 1.319}, "timestamp": "2026-01-17T17:43:12.460954"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 571.123, "latencies_ms": [571.123], "images_per_second": 1.751, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A city street scene features modern buildings, a bike path, parked cars, and a bus traveling down the road.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25388.4, "ram_available_mb": 100383.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.77, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 22.44, "peak": 24.81, "min": 20.48}, "VIN": {"avg": 56.65, "peak": 57.8, "min": 54.46}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 22.44, "energy_joules_est": 12.83, "sample_count": 4, "duration_seconds": 0.572}, "timestamp": "2026-01-17T17:43:13.043337"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1123.709, "latencies_ms": [1123.709], "images_per_second": 0.89, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "building: 5\ncar: 2\nbus: 1\nstreet: 4\nsidewalk: 1\nbike path: 1\ntrees: 2\npower lines: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25386.0, "ram_available_mb": 100386.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.3, "min": 13.09}, "VDD_GPU": {"avg": 21.37, "peak": 26.0, "min": 18.52}, "VIN": {"avg": 56.37, "peak": 58.17, "min": 50.16}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.37, "energy_joules_est": 24.02, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-17T17:43:14.177681"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1076.832, "latencies_ms": [1076.832], "images_per_second": 0.929, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The foreground features a paved road with a bike path running alongside it.  The background includes buildings, parked cars, and a bus, indicating a residential or urban setting. The buildings are situated on both sides of the road, contributing to the overall urban landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25386.0, "ram_available_mb": 100386.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25385.9, "ram_available_mb": 100386.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.3, "min": 13.19}, "VDD_GPU": {"avg": 20.78, "peak": 24.82, "min": 18.51}, "VIN": {"avg": 56.36, "peak": 59.27, "min": 53.75}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 20.78, "energy_joules_est": 22.39, "sample_count": 8, "duration_seconds": 1.077}, "timestamp": "2026-01-17T17:43:15.260630"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1009.917, "latencies_ms": [1009.917], "images_per_second": 0.99, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The scene depicts a modern urban street with modern buildings lining both sides. A bike path runs alongside the road, providing a green space for cyclists. The street is relatively quiet, with only a few cars and a bus visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25386.7, "ram_available_mb": 100385.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.82, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 21.11, "peak": 24.82, "min": 18.91}, "VIN": {"avg": 57.01, "peak": 58.3, "min": 52.01}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.11, "energy_joules_est": 21.33, "sample_count": 7, "duration_seconds": 1.01}, "timestamp": "2026-01-17T17:43:16.276721"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 939.203, "latencies_ms": [939.203], "images_per_second": 1.065, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The buildings exhibit a mix of colors, including white, red, and brown. The lighting is bright, likely from streetlights and building windows. The materials appear to be primarily concrete and metal. The weather appears to be partly cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25386.9, "ram_available_mb": 100385.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25386.7, "ram_available_mb": 100385.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 21.11, "peak": 24.82, "min": 18.91}, "VIN": {"avg": 56.49, "peak": 60.23, "min": 52.03}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 21.11, "energy_joules_est": 19.83, "sample_count": 7, "duration_seconds": 0.939}, "timestamp": "2026-01-17T17:43:17.221725"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 532.51, "latencies_ms": [532.51], "images_per_second": 1.878, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A man and a woman are seated at a wooden table in a cozy pub, smiling and embracing each other.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25386.7, "ram_available_mb": 100385.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25386.0, "ram_available_mb": 100386.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.64, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 22.46, "peak": 24.82, "min": 20.5}, "VIN": {"avg": 56.86, "peak": 58.36, "min": 54.94}, "VDD_CPU_SOC_MSS": {"avg": 13.68, "peak": 13.78, "min": 13.38}}, "power_watts_avg": 22.46, "energy_joules_est": 11.97, "sample_count": 4, "duration_seconds": 0.533}, "timestamp": "2026-01-17T17:43:17.764560"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1052.639, "latencies_ms": [1052.639], "images_per_second": 0.95, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "man: 2\nwoman: 2\ntie: 1\ntable: 1\nphone: 1\nscreen: 1\nchair: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25386.0, "ram_available_mb": 100386.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.4, "min": 12.98}, "VDD_GPU": {"avg": 21.33, "peak": 26.02, "min": 18.52}, "VIN": {"avg": 57.15, "peak": 60.46, "min": 54.55}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 21.33, "energy_joules_est": 22.46, "sample_count": 8, "duration_seconds": 1.053}, "timestamp": "2026-01-17T17:43:18.824072"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 758.856, "latencies_ms": [758.856], "images_per_second": 1.318, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with a man and woman seated at a table in the background. The man is on the left side of the image, and the woman is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.71, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 22.14, "peak": 24.82, "min": 20.1}, "VIN": {"avg": 57.26, "peak": 58.17, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 13.78, "peak": 14.17, "min": 13.39}}, "power_watts_avg": 22.14, "energy_joules_est": 16.81, "sample_count": 5, "duration_seconds": 0.759}, "timestamp": "2026-01-17T17:43:19.588637"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1033.36, "latencies_ms": [1033.36], "images_per_second": 0.968, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "A man and a woman are seated at a table in a dimly lit bar or pub. The man is wearing a blue striped shirt and glasses, while the woman is wearing a white tank top. They are both smiling and appear to be enjoying each other's company.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.4, "min": 13.09}, "VDD_GPU": {"avg": 21.03, "peak": 25.21, "min": 18.51}, "VIN": {"avg": 55.85, "peak": 57.95, "min": 49.35}, "VDD_CPU_SOC_MSS": {"avg": 13.92, "peak": 14.16, "min": 13.39}}, "power_watts_avg": 21.03, "energy_joules_est": 21.75, "sample_count": 8, "duration_seconds": 1.034}, "timestamp": "2026-01-17T17:43:20.628670"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 987.21, "latencies_ms": [987.21], "images_per_second": 1.013, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The man is wearing a purple striped shirt and a dark tie. The lighting in the pub is warm and dim, creating a cozy atmosphere. The walls are painted a muted green, and there are framed pictures and signs visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 21.12, "peak": 24.82, "min": 18.92}, "VIN": {"avg": 57.0, "peak": 59.05, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 13.83, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 21.12, "energy_joules_est": 20.86, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-17T17:43:21.621806"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 666.864, "latencies_ms": [666.864], "images_per_second": 1.5, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A woman dressed in a costume and wearing a golden helmet is talking on her cell phone amidst a crowd of people.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.51, "peak": 31.13, "min": 23.26}, "VIN": {"avg": 64.02, "peak": 94.28, "min": 53.54}, "VDD_CPU_SOC_MSS": {"avg": 13.77, "peak": 14.17, "min": 13.38}}, "power_watts_avg": 27.51, "energy_joules_est": 18.37, "sample_count": 5, "duration_seconds": 0.668}, "timestamp": "2026-01-17T17:43:22.300312"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1164.077, "latencies_ms": [1164.077], "images_per_second": 0.859, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "woman: 1\nhelmet: 1\nman: 1\nman: 1\nman: 1\nman: 1\nman: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.3, "ram_available_mb": 100386.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25385.8, "ram_available_mb": 100386.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 25.61, "peak": 33.89, "min": 19.7}, "VIN": {"avg": 62.01, "peak": 94.19, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.61, "energy_joules_est": 29.82, "sample_count": 9, "duration_seconds": 1.165}, "timestamp": "2026-01-17T17:43:23.471486"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1131.114, "latencies_ms": [1131.114], "images_per_second": 0.884, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The woman is positioned in the foreground, slightly to the right of the man. She is talking on a cell phone while wearing a costume. The man is to the left of the woman, partially out of focus. The background includes other individuals, suggesting a public setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25385.8, "ram_available_mb": 100386.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25382.5, "ram_available_mb": 100389.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.2, "min": 13.49}, "VDD_GPU": {"avg": 25.21, "peak": 31.51, "min": 19.7}, "VIN": {"avg": 58.87, "peak": 76.94, "min": 50.68}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 25.21, "energy_joules_est": 28.52, "sample_count": 8, "duration_seconds": 1.131}, "timestamp": "2026-01-17T17:43:24.608769"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 813.135, "latencies_ms": [813.135], "images_per_second": 1.23, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The scene takes place outdoors, possibly during a convention or festival. A woman dressed in costume is talking on her cell phone amidst a crowd of people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25382.5, "ram_available_mb": 100389.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.5, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 26.59, "peak": 31.91, "min": 21.67}, "VIN": {"avg": 63.89, "peak": 89.25, "min": 56.88}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.59, "energy_joules_est": 21.63, "sample_count": 6, "duration_seconds": 0.814}, "timestamp": "2026-01-17T17:43:25.428701"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1015.845, "latencies_ms": [1015.845], "images_per_second": 0.984, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The woman is wearing a gold and black costume with a helmet-like headpiece. Her hair is dark and appears to be styled in loose waves. The lighting suggests an outdoor setting, possibly in bright sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.5, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.81, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.73, "peak": 33.1, "min": 20.88}, "VIN": {"avg": 60.1, "peak": 76.5, "min": 56.67}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 26.73, "energy_joules_est": 27.17, "sample_count": 7, "duration_seconds": 1.016}, "timestamp": "2026-01-17T17:43:26.450682"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 601.02, "latencies_ms": [601.02], "images_per_second": 1.664, "prompt_tokens": 8, "response_tokens_est": 17, "n_tiles": 1, "output_text": "The bathroom features a white toilet situated beneath a window, with two buckets placed nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.6, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.62, "peak": 13.79, "min": 13.29}, "VDD_GPU": {"avg": 29.25, "peak": 32.3, "min": 25.22}, "VIN": {"avg": 63.27, "peak": 80.59, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 13.88, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 29.25, "energy_joules_est": 17.6, "sample_count": 4, "duration_seconds": 0.602}, "timestamp": "2026-01-17T17:43:27.061936"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1507.84, "latencies_ms": [1507.84], "images_per_second": 0.663, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "Shower head: 1\nPipe: 2\nToilet: 1\nToilet paper holder: 1\nBucket: 2\nFloor tiles: 8\nShelf: 1\nCleaning supplies: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.3, "ram_available_mb": 100391.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25381.0, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.4, "min": 13.19}, "VDD_GPU": {"avg": 24.61, "peak": 35.06, "min": 18.91}, "VIN": {"avg": 59.55, "peak": 81.14, "min": 52.77}, "VDD_CPU_SOC_MSS": {"avg": 13.99, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 24.61, "energy_joules_est": 37.12, "sample_count": 11, "duration_seconds": 1.509}, "timestamp": "2026-01-17T17:43:28.577216"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 733.636, "latencies_ms": [733.636], "images_per_second": 1.363, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The toilet is located in the foreground, close to the shower and bucket. The shower and bucket are situated in the background, further away from the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.0, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25381.0, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 27.48, "peak": 31.12, "min": 22.84}, "VIN": {"avg": 62.71, "peak": 86.44, "min": 53.12}, "VDD_CPU_SOC_MSS": {"avg": 13.94, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.48, "energy_joules_est": 20.17, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T17:43:29.318914"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1037.197, "latencies_ms": [1037.197], "images_per_second": 0.964, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The bathroom features a white tiled floor, a toilet, a shower area with a handheld shower head, and a green bucket. The scene suggests a functional and possibly temporary space for personal hygiene and cleaning.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.0, "ram_available_mb": 100391.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.84, "peak": 14.3, "min": 13.29}, "VDD_GPU": {"avg": 26.61, "peak": 33.47, "min": 20.87}, "VIN": {"avg": 57.7, "peak": 67.2, "min": 49.49}, "VDD_CPU_SOC_MSS": {"avg": 13.89, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.61, "energy_joules_est": 27.62, "sample_count": 7, "duration_seconds": 1.038}, "timestamp": "2026-01-17T17:43:30.362330"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 972.846, "latencies_ms": [972.846], "images_per_second": 1.028, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The bathroom is predominantly white with gray tiled flooring. The lighting appears to be artificial, creating a somewhat dim atmosphere. Various materials like plastic pipes, fixtures, and cleaning supplies are present.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 26.11, "peak": 32.68, "min": 20.48}, "VIN": {"avg": 60.18, "peak": 84.84, "min": 47.77}, "VDD_CPU_SOC_MSS": {"avg": 14.06, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.11, "energy_joules_est": 25.42, "sample_count": 7, "duration_seconds": 0.974}, "timestamp": "2026-01-17T17:43:31.341577"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 627.07, "latencies_ms": [627.07], "images_per_second": 1.595, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A man wearing glasses and a green shirt stands next to an elephant with its trunk touching his face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 13.9, "min": 13.39}, "VDD_GPU": {"avg": 29.25, "peak": 32.3, "min": 25.22}, "VIN": {"avg": 63.54, "peak": 79.86, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 13.87, "peak": 14.17, "min": 13.77}}, "power_watts_avg": 29.25, "energy_joules_est": 18.35, "sample_count": 4, "duration_seconds": 0.627}, "timestamp": "2026-01-17T17:43:31.979437"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1174.405, "latencies_ms": [1174.405], "images_per_second": 0.851, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "elephant: 2\nman: 1\nglasses: 1\nt-shirt: 1\ntrunk: 1\nhills: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.6, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.89, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 25.73, "peak": 34.66, "min": 19.69}, "VIN": {"avg": 60.03, "peak": 77.99, "min": 50.57}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 25.73, "energy_joules_est": 30.23, "sample_count": 9, "duration_seconds": 1.175}, "timestamp": "2026-01-17T17:43:33.160560"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 997.273, "latencies_ms": [997.273], "images_per_second": 1.003, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The elephant is positioned close to the man, creating a close and intimate interaction. The elephant's trunk is partially visible, extending towards the man's face, further emphasizing the proximity and interaction between the two.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25380.6, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25380.6, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.84, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 26.06, "peak": 31.89, "min": 20.89}, "VIN": {"avg": 59.45, "peak": 81.51, "min": 51.97}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.06, "energy_joules_est": 26.0, "sample_count": 7, "duration_seconds": 0.998}, "timestamp": "2026-01-17T17:43:34.163958"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 839.645, "latencies_ms": [839.645], "images_per_second": 1.191, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A man is standing next to an elephant, appearing to be interacting or playing with it. The setting suggests a natural environment, possibly a forest or jungle, with a backdrop of trees and hills.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.6, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25380.9, "ram_available_mb": 100391.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 27.57, "peak": 33.08, "min": 22.45}, "VIN": {"avg": 67.52, "peak": 95.64, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 14.37, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 27.57, "energy_joules_est": 23.17, "sample_count": 6, "duration_seconds": 0.84}, "timestamp": "2026-01-17T17:43:35.010105"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1095.657, "latencies_ms": [1095.657], "images_per_second": 0.913, "prompt_tokens": 18, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The elephant's skin is a dark brown color. The lighting appears to be bright and sunny, creating a clear contrast between the elephant and the man's shirt. The materials appear to be natural and sturdy, consistent with the elephant's skin and overall structure. The weather appears to be sunny and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.9, "ram_available_mb": 100391.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25380.6, "ram_available_mb": 100391.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 26.54, "peak": 34.65, "min": 20.88}, "VIN": {"avg": 63.22, "peak": 88.33, "min": 53.85}, "VDD_CPU_SOC_MSS": {"avg": 14.71, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 26.54, "energy_joules_est": 29.09, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T17:43:36.111661"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 621.212, "latencies_ms": [621.212], "images_per_second": 1.61, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Five young children sit on the grass, each holding a white frisbee with a black design.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25380.6, "ram_available_mb": 100391.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 29.54, "peak": 32.7, "min": 25.59}, "VIN": {"avg": 60.94, "peak": 74.04, "min": 52.69}, "VDD_CPU_SOC_MSS": {"avg": 14.37, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.54, "energy_joules_est": 18.36, "sample_count": 4, "duration_seconds": 0.622}, "timestamp": "2026-01-17T17:43:36.745166"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1348.84, "latencies_ms": [1348.84], "images_per_second": 0.741, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Frisbee: 2\nUltimate: 1\nBoy: 3\nBoy: 2\nBoy: 1\nBoy: 1\nBoy: 1\nBoy: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.5, "ram_available_mb": 100391.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 25.37, "peak": 35.06, "min": 19.31}, "VIN": {"avg": 60.44, "peak": 88.33, "min": 49.9}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.37, "energy_joules_est": 34.23, "sample_count": 10, "duration_seconds": 1.349}, "timestamp": "2026-01-17T17:43:38.100333"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 848.341, "latencies_ms": [848.341], "images_per_second": 1.179, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The children's feet are positioned in the foreground, close to the frisbees. The frisbees are situated near the children, creating a sense of proximity and playfulness.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.5, "ram_available_mb": 100391.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 27.0, "peak": 31.51, "min": 22.07}, "VIN": {"avg": 61.0, "peak": 84.67, "min": 51.41}, "VDD_CPU_SOC_MSS": {"avg": 13.97, "peak": 14.17, "min": 13.77}}, "power_watts_avg": 27.0, "energy_joules_est": 22.91, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T17:43:38.955507"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 870.114, "latencies_ms": [870.114], "images_per_second": 1.149, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A group of young children is sitting on grass in a park-like setting, enjoying outdoor activities. They appear to be playing with frisbees and possibly other toys.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 27.77, "peak": 33.09, "min": 22.44}, "VIN": {"avg": 63.63, "peak": 89.55, "min": 56.53}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.77}}, "power_watts_avg": 27.77, "energy_joules_est": 24.18, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T17:43:39.832094"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1072.911, "latencies_ms": [1072.911], "images_per_second": 0.932, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The children are wearing light brown and tan clothing. The lighting appears to be natural, possibly overcast. The grass they are sitting on is green and appears to be well-maintained. The overall atmosphere suggests a casual outdoor setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 26.05, "peak": 33.09, "min": 20.1}, "VIN": {"avg": 60.18, "peak": 77.9, "min": 54.93}, "VDD_CPU_SOC_MSS": {"avg": 14.03, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.05, "energy_joules_est": 27.96, "sample_count": 8, "duration_seconds": 1.073}, "timestamp": "2026-01-17T17:43:40.911176"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 689.773, "latencies_ms": [689.773], "images_per_second": 1.45, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A young girl in a red coat and blue jeans holds a blue and pink umbrella while standing on a wet sidewalk.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25380.8, "ram_available_mb": 100391.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.0, "min": 13.39}, "VDD_GPU": {"avg": 27.97, "peak": 31.91, "min": 23.24}, "VIN": {"avg": 63.94, "peak": 86.23, "min": 57.62}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 27.97, "energy_joules_est": 19.31, "sample_count": 5, "duration_seconds": 0.69}, "timestamp": "2026-01-17T17:43:41.615550"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1318.843, "latencies_ms": [1318.843], "images_per_second": 0.758, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "Umbrella: 1\nGirl: 1\nHedge: 1\nTruck: 1\nHouse: 2\nTrees: 3\nSky: 1\nPavement: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.4, "min": 13.29}, "VDD_GPU": {"avg": 24.98, "peak": 34.27, "min": 18.91}, "VIN": {"avg": 57.65, "peak": 79.14, "min": 49.38}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 24.98, "energy_joules_est": 32.96, "sample_count": 10, "duration_seconds": 1.319}, "timestamp": "2026-01-17T17:43:42.941066"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 896.727, "latencies_ms": [896.727], "images_per_second": 1.115, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The girl is positioned in the foreground of the image, standing on a wet sidewalk near a bush and a parked truck. The background includes houses and trees, indicating a residential setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25381.3, "ram_available_mb": 100390.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.2, "min": 13.49}, "VDD_GPU": {"avg": 26.72, "peak": 31.51, "min": 21.66}, "VIN": {"avg": 60.64, "peak": 81.06, "min": 48.58}, "VDD_CPU_SOC_MSS": {"avg": 14.04, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.72, "energy_joules_est": 23.97, "sample_count": 6, "duration_seconds": 0.897}, "timestamp": "2026-01-17T17:43:43.844382"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 896.467, "latencies_ms": [896.467], "images_per_second": 1.115, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A young girl in a red coat is standing on a wet sidewalk, holding a blue and purple umbrella. The scene takes place on a rainy day, with houses and trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.3, "ram_available_mb": 100390.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 27.24, "peak": 33.08, "min": 21.66}, "VIN": {"avg": 62.25, "peak": 92.2, "min": 48.95}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.24, "energy_joules_est": 24.43, "sample_count": 6, "duration_seconds": 0.897}, "timestamp": "2026-01-17T17:43:44.747223"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 918.716, "latencies_ms": [918.716], "images_per_second": 1.088, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The girl is wearing a bright red coat. The umbrella is black with white and pink designs. The scene is wet, suggesting rain. The overall atmosphere is calm and peaceful.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25381.3, "ram_available_mb": 100390.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.85, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 27.17, "peak": 33.09, "min": 21.66}, "VIN": {"avg": 61.17, "peak": 81.23, "min": 52.87}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 27.17, "energy_joules_est": 24.97, "sample_count": 6, "duration_seconds": 0.919}, "timestamp": "2026-01-17T17:43:45.672887"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 763.314, "latencies_ms": [763.314], "images_per_second": 1.31, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A group of elephants, including two young ones, stand together near a body of water, their dusty bodies and expressive faces creating a captivating scene.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25381.3, "ram_available_mb": 100390.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.1, "min": 13.39}, "VDD_GPU": {"avg": 28.44, "peak": 32.7, "min": 23.62}, "VIN": {"avg": 63.65, "peak": 89.36, "min": 54.55}, "VDD_CPU_SOC_MSS": {"avg": 13.86, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 28.44, "energy_joules_est": 21.72, "sample_count": 5, "duration_seconds": 0.764}, "timestamp": "2026-01-17T17:43:46.447554"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 850.353, "latencies_ms": [850.353], "images_per_second": 1.176, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "elephant: 3\nwater: 1\ndirt: 2\ntrees: 1\nsky: 1\nground: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 28.42, "peak": 35.04, "min": 22.85}, "VIN": {"avg": 63.49, "peak": 82.08, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 14.63, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.42, "energy_joules_est": 24.18, "sample_count": 6, "duration_seconds": 0.851}, "timestamp": "2026-01-17T17:43:47.303805"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 792.441, "latencies_ms": [792.441], "images_per_second": 1.262, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the elephant closest to the camera appearing larger and closer to the viewer. The background includes other elephants and a body of water, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25381.5, "ram_available_mb": 100390.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 28.43, "peak": 35.06, "min": 22.85}, "VIN": {"avg": 64.45, "peak": 83.1, "min": 53.16}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.97, "min": 14.57}}, "power_watts_avg": 28.43, "energy_joules_est": 22.54, "sample_count": 6, "duration_seconds": 0.793}, "timestamp": "2026-01-17T17:43:48.102730"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1049.663, "latencies_ms": [1049.663], "images_per_second": 0.953, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A group of elephants, including two young ones, is seen near a body of water. The elephants are dusty, indicating recent activity or movement. The setting appears to be a natural environment, possibly a savanna or dry grassland.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.5, "ram_available_mb": 100390.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25381.5, "ram_available_mb": 100390.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 26.4, "peak": 34.26, "min": 20.49}, "VIN": {"avg": 59.67, "peak": 84.84, "min": 52.37}, "VDD_CPU_SOC_MSS": {"avg": 14.27, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 26.4, "energy_joules_est": 27.72, "sample_count": 8, "duration_seconds": 1.05}, "timestamp": "2026-01-17T17:43:49.158394"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 910.192, "latencies_ms": [910.192], "images_per_second": 1.099, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The elephants are primarily brown in color. The lighting appears to be natural, possibly sunlight, giving a warm tone to the scene. The elephants are standing on a reddish-brown dirt surface, which contrasts with their skin color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25381.5, "ram_available_mb": 100390.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 27.7, "peak": 33.47, "min": 22.45}, "VIN": {"avg": 62.47, "peak": 76.92, "min": 55.9}, "VDD_CPU_SOC_MSS": {"avg": 14.37, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 27.7, "energy_joules_est": 25.22, "sample_count": 6, "duration_seconds": 0.911}, "timestamp": "2026-01-17T17:43:50.076770"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 799.919, "latencies_ms": [799.919], "images_per_second": 1.25, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A surfer in a red and green wetsuit is skillfully riding a wave on a white surfboard, performing an aerial maneuver.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25381.0, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.2, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 28.51, "peak": 33.88, "min": 23.22}, "VIN": {"avg": 62.6, "peak": 79.09, "min": 54.3}, "VDD_CPU_SOC_MSS": {"avg": 14.26, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 28.51, "energy_joules_est": 22.82, "sample_count": 5, "duration_seconds": 0.8}, "timestamp": "2026-01-17T17:43:50.890913"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1036.174, "latencies_ms": [1036.174], "images_per_second": 0.965, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "surfboard: 1\nsurfer: 1\nwaves: 1\nwater: 1\nsky: 1\nwetsuit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.2, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25376.2, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.84, "peak": 14.2, "min": 13.29}, "VDD_GPU": {"avg": 26.73, "peak": 33.08, "min": 20.89}, "VIN": {"avg": 57.5, "peak": 66.63, "min": 51.06}, "VDD_CPU_SOC_MSS": {"avg": 13.95, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.73, "energy_joules_est": 27.7, "sample_count": 7, "duration_seconds": 1.036}, "timestamp": "2026-01-17T17:43:51.933183"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 830.405, "latencies_ms": [830.405], "images_per_second": 1.204, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The surfer is positioned in the foreground of the image, riding a wave. The wave extends towards the right side of the image, creating a dynamic and energetic scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.2, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.9, "ram_available_mb": 100396.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 27.13, "peak": 32.7, "min": 22.09}, "VIN": {"avg": 60.26, "peak": 79.77, "min": 53.79}, "VDD_CPU_SOC_MSS": {"avg": 13.9, "peak": 14.16, "min": 13.76}}, "power_watts_avg": 27.13, "energy_joules_est": 22.54, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T17:43:52.769884"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1089.417, "latencies_ms": [1089.417], "images_per_second": 0.918, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A surfer in a red and green wetsuit is performing an aerial maneuver on a large green wave, skillfully riding the crest with spray around them. The setting appears to be a coastal area with ocean waves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.9, "ram_available_mb": 100396.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25375.9, "ram_available_mb": 100396.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.2, "min": 13.39}, "VDD_GPU": {"avg": 25.86, "peak": 32.71, "min": 20.1}, "VIN": {"avg": 58.78, "peak": 74.78, "min": 52.46}, "VDD_CPU_SOC_MSS": {"avg": 13.92, "peak": 14.18, "min": 13.38}}, "power_watts_avg": 25.86, "energy_joules_est": 28.18, "sample_count": 8, "duration_seconds": 1.09}, "timestamp": "2026-01-17T17:43:53.865886"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 785.547, "latencies_ms": [785.547], "images_per_second": 1.273, "prompt_tokens": 18, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The surfer is wearing a red and green wetsuit. The wave is green and white, and the lighting suggests an overcast sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.9, "ram_available_mb": 100396.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25376.1, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 26.91, "peak": 31.91, "min": 21.66}, "VIN": {"avg": 62.39, "peak": 85.58, "min": 54.47}, "VDD_CPU_SOC_MSS": {"avg": 13.98, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 26.91, "energy_joules_est": 21.15, "sample_count": 6, "duration_seconds": 0.786}, "timestamp": "2026-01-17T17:43:54.659814"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 631.731, "latencies_ms": [631.731], "images_per_second": 1.583, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Two men in traditional white clothing are riding horses along a sandy beach, waving to onlookers as they gallop.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25376.1, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25376.1, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.52, "peak": 13.9, "min": 13.09}, "VDD_GPU": {"avg": 23.63, "peak": 26.38, "min": 21.26}, "VIN": {"avg": 57.48, "peak": 61.01, "min": 52.5}, "VDD_CPU_SOC_MSS": {"avg": 13.59, "peak": 13.78, "min": 13.39}}, "power_watts_avg": 23.63, "energy_joules_est": 14.94, "sample_count": 4, "duration_seconds": 0.632}, "timestamp": "2026-01-17T17:43:55.302944"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1017.883, "latencies_ms": [1017.883], "images_per_second": 0.982, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Horse: 2\nPerson: 2\nSword: 1\nHat: 2\nSand: 1\nOcean: 1\nSky: 1\nClouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.1, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25376.1, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.4, "min": 13.09}, "VDD_GPU": {"avg": 22.34, "peak": 26.38, "min": 19.7}, "VIN": {"avg": 58.94, "peak": 62.12, "min": 52.0}, "VDD_CPU_SOC_MSS": {"avg": 14.51, "peak": 14.96, "min": 13.79}}, "power_watts_avg": 22.34, "energy_joules_est": 22.75, "sample_count": 7, "duration_seconds": 1.018}, "timestamp": "2026-01-17T17:43:56.330711"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 607.706, "latencies_ms": [607.706], "images_per_second": 1.646, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the beach and ocean in the background. The horses are moving towards the ocean, suggesting they are moving towards a body of water.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25376.1, "ram_available_mb": 100396.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25375.9, "ram_available_mb": 100396.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.67, "peak": 14.0, "min": 13.29}, "VDD_GPU": {"avg": 23.15, "peak": 25.21, "min": 21.28}, "VIN": {"avg": 59.36, "peak": 61.15, "min": 54.28}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.15, "energy_joules_est": 14.07, "sample_count": 4, "duration_seconds": 0.608}, "timestamp": "2026-01-17T17:43:56.944228"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 829.439, "latencies_ms": [829.439], "images_per_second": 1.206, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Two men are riding horses on a sandy beach near the ocean, appearing to be engaged in a game or activity. The beach is populated with spectators, and the ocean stretches out in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.9, "ram_available_mb": 100396.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25377.6, "ram_available_mb": 100394.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.2, "min": 13.09}, "VDD_GPU": {"avg": 22.46, "peak": 26.4, "min": 19.7}, "VIN": {"avg": 56.41, "peak": 59.97, "min": 53.04}, "VDD_CPU_SOC_MSS": {"avg": 14.11, "peak": 14.18, "min": 13.78}}, "power_watts_avg": 22.46, "energy_joules_est": 18.63, "sample_count": 6, "duration_seconds": 0.83}, "timestamp": "2026-01-17T17:43:57.780430"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 796.232, "latencies_ms": [796.232], "images_per_second": 1.256, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The sky is bright blue with scattered white clouds. The beach is sandy and appears relatively empty except for a few people. The horses are light-colored and appear to be running on the sand.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25377.6, "ram_available_mb": 100394.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25377.0, "ram_available_mb": 100395.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.3, "min": 13.09}, "VDD_GPU": {"avg": 21.99, "peak": 25.21, "min": 20.09}, "VIN": {"avg": 58.62, "peak": 61.94, "min": 51.13}, "VDD_CPU_SOC_MSS": {"avg": 14.11, "peak": 14.57, "min": 13.38}}, "power_watts_avg": 21.99, "energy_joules_est": 17.52, "sample_count": 6, "duration_seconds": 0.797}, "timestamp": "2026-01-17T17:43:58.582681"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 690.842, "latencies_ms": [690.842], "images_per_second": 1.448, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A custom-built trike with a silver engine and distinctive front suspension is parked in a driveway, accompanied by a small dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.0, "ram_available_mb": 100395.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25377.0, "ram_available_mb": 100395.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 28.75, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 65.08, "peak": 82.41, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 28.75, "energy_joules_est": 19.88, "sample_count": 5, "duration_seconds": 0.691}, "timestamp": "2026-01-17T17:43:59.285008"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1158.159, "latencies_ms": [1158.159], "images_per_second": 0.863, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "tire: 2\nchopper: 3\nbike: 1\nbicycle: 2\ndog: 1\ngarage: 1\ntruck: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25377.0, "ram_available_mb": 100395.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 26.44, "peak": 35.45, "min": 20.48}, "VIN": {"avg": 63.74, "peak": 94.08, "min": 56.62}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 26.44, "energy_joules_est": 30.64, "sample_count": 9, "duration_seconds": 1.159}, "timestamp": "2026-01-17T17:44:00.449845"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 688.247, "latencies_ms": [688.247], "images_per_second": 1.453, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The trike is positioned in the foreground, slightly to the right of the garage. The garage is situated in the background, partially obscured by trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 28.83, "peak": 33.86, "min": 24.03}, "VIN": {"avg": 63.46, "peak": 87.49, "min": 52.77}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.83, "energy_joules_est": 19.85, "sample_count": 5, "duration_seconds": 0.689}, "timestamp": "2026-01-17T17:44:01.144128"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1238.006, "latencies_ms": [1238.006], "images_per_second": 0.808, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The scene depicts a custom trike with a silver engine and exposed frame, parked in a driveway in front of a garage. A small dog is sitting near the trike's frame.  In the background, a pickup truck with a blue tarp is parked, and several bicycles are visible inside the garage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 26.38, "peak": 35.44, "min": 20.48}, "VIN": {"avg": 64.25, "peak": 94.03, "min": 55.37}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.38, "energy_joules_est": 32.67, "sample_count": 9, "duration_seconds": 1.238}, "timestamp": "2026-01-17T17:44:02.389330"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 810.567, "latencies_ms": [810.567], "images_per_second": 1.234, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The motorcycle is primarily silver and chrome, with red accents on the frame and wheels. The lighting suggests it's daytime, and the overall setting appears to be a garage or workshop area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 27.9, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 64.78, "peak": 94.77, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.9, "energy_joules_est": 22.63, "sample_count": 6, "duration_seconds": 0.811}, "timestamp": "2026-01-17T17:44:03.205948"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 693.141, "latencies_ms": [693.141], "images_per_second": 1.443, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man is flying a blue and white kite high above a sandy beach, surrounded by other beachgoers and enjoying the sunny day.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 29.45, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 65.49, "peak": 81.38, "min": 59.39}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.45, "energy_joules_est": 20.44, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:44:03.910336"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1191.576, "latencies_ms": [1191.576], "images_per_second": 0.839, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "kite: 1\nperson: 1\nsand: 8\nwater: 1\ntrees: 4\nbuildings: 2\nsky: 1\nclouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 26.17, "peak": 36.24, "min": 19.69}, "VIN": {"avg": 60.88, "peak": 88.7, "min": 55.51}, "VDD_CPU_SOC_MSS": {"avg": 14.26, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 26.17, "energy_joules_est": 31.2, "sample_count": 9, "duration_seconds": 1.192}, "timestamp": "2026-01-17T17:44:05.108579"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1094.242, "latencies_ms": [1094.242], "images_per_second": 0.914, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The man is standing on the sandy foreground, facing the water and kite. The kite is flying high in the sky, positioned above the water. The beach extends into the background, separating the man from the water.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.97, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 25.26, "peak": 31.91, "min": 20.09}, "VIN": {"avg": 59.57, "peak": 76.94, "min": 50.02}, "VDD_CPU_SOC_MSS": {"avg": 14.13, "peak": 14.19, "min": 13.78}}, "power_watts_avg": 25.26, "energy_joules_est": 27.65, "sample_count": 8, "duration_seconds": 1.095}, "timestamp": "2026-01-17T17:44:06.208525"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 892.175, "latencies_ms": [892.175], "images_per_second": 1.121, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene takes place on a sandy beach near a body of water, where people are enjoying various activities. A man is flying a kite high in the sky, while others are walking along the shoreline or playing in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 27.57, "peak": 33.08, "min": 22.45}, "VIN": {"avg": 65.43, "peak": 94.62, "min": 55.56}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 27.57, "energy_joules_est": 24.61, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T17:44:07.111730"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 712.382, "latencies_ms": [712.382], "images_per_second": 1.404, "prompt_tokens": 18, "response_tokens_est": 23, "n_tiles": 1, "output_text": "The kite is light blue and white. The beach is sandy and appears sunny. The sky is partly cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.96, "peak": 14.2, "min": 13.59}, "VDD_GPU": {"avg": 28.67, "peak": 33.48, "min": 23.64}, "VIN": {"avg": 62.02, "peak": 82.06, "min": 51.93}, "VDD_CPU_SOC_MSS": {"avg": 14.25, "peak": 14.56, "min": 14.16}}, "power_watts_avg": 28.67, "energy_joules_est": 20.44, "sample_count": 5, "duration_seconds": 0.713}, "timestamp": "2026-01-17T17:44:07.830467"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 877.607, "latencies_ms": [877.607], "images_per_second": 1.139, "prompt_tokens": 8, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The kitchen counter is cluttered with various items, including cleaning supplies, a green bottle, a red bow, a notebook, a wine glass, a bowl, a microwave, a sink, and a refrigerator.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 28.62, "peak": 34.27, "min": 23.23}, "VIN": {"avg": 63.33, "peak": 79.99, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 14.3, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 28.62, "energy_joules_est": 25.14, "sample_count": 6, "duration_seconds": 0.878}, "timestamp": "2026-01-17T17:44:08.719793"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1709.055, "latencies_ms": [1709.055], "images_per_second": 0.585, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 1, "output_text": "kitchen sink: 2\ncountertop: 2\noven: 1\nmicrowave: 1\ncabinets: 6\nrefrigerator: 1\ntoaster oven: 1\ngreen bottle: 1\nblue bottle: 1\nred bow: 1\nplates: 2\nglasses: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.71, "min": 13.29}, "VDD_GPU": {"avg": 23.85, "peak": 34.27, "min": 19.3}, "VIN": {"avg": 60.25, "peak": 74.84, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 23.85, "energy_joules_est": 40.78, "sample_count": 13, "duration_seconds": 1.71}, "timestamp": "2026-01-17T17:44:10.436324"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 805.436, "latencies_ms": [805.436], "images_per_second": 1.242, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The stainless steel refrigerator is positioned to the left of the kitchen counter. The sink is located in the foreground, closer to the viewer. The kitchen counter extends into the background, occupying a significant portion of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 27.32, "peak": 32.7, "min": 22.45}, "VIN": {"avg": 65.2, "peak": 92.16, "min": 55.47}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.32, "energy_joules_est": 22.02, "sample_count": 6, "duration_seconds": 0.806}, "timestamp": "2026-01-17T17:44:11.247713"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 770.25, "latencies_ms": [770.25], "images_per_second": 1.298, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The kitchen is clean and organized, with light wood cabinets, a stainless steel refrigerator, and a black countertop. A sink, stove, and various cleaning supplies are present.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 29.86, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 64.02, "peak": 80.36, "min": 56.67}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.86, "energy_joules_est": 23.02, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T17:44:12.024746"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 948.957, "latencies_ms": [948.957], "images_per_second": 1.054, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The kitchen features light brown wooden cabinets and a black countertop. The lighting is bright, likely from overhead fixtures, creating a well-lit space. The materials include wood, stainless steel, and black countertop elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 27.91, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 63.76, "peak": 97.52, "min": 54.07}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.91, "energy_joules_est": 26.5, "sample_count": 7, "duration_seconds": 0.949}, "timestamp": "2026-01-17T17:44:12.979670"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 492.566, "latencies_ms": [492.566], "images_per_second": 2.03, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A large orange and white kite soars high in the clear blue sky, tethered by multiple lines.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25378.1, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.1, "min": 13.59}, "VDD_GPU": {"avg": 24.95, "peak": 26.79, "min": 23.24}, "VIN": {"avg": 60.53, "peak": 61.24, "min": 59.91}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 24.95, "energy_joules_est": 12.31, "sample_count": 3, "duration_seconds": 0.493}, "timestamp": "2026-01-17T17:44:13.481528"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1062.804, "latencies_ms": [1062.804], "images_per_second": 0.941, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "kite: 1\nkite lines: 2\nkite fabric: 1\nkite shape: 1\nkite colors: 2\nkite design: 1\nkite control: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.1, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25378.5, "ram_available_mb": 100393.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.61, "min": 13.29}, "VDD_GPU": {"avg": 22.55, "peak": 27.56, "min": 19.7}, "VIN": {"avg": 58.8, "peak": 61.71, "min": 55.21}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.55, "energy_joules_est": 23.98, "sample_count": 8, "duration_seconds": 1.063}, "timestamp": "2026-01-17T17:44:14.550129"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 906.534, "latencies_ms": [906.534], "images_per_second": 1.103, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The main object, the kite, is positioned in the foreground, slightly to the right of the viewer. The kite is angled upwards and appears to be flying high in the sky. The background is a clear, unobstructed expanse of blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.5, "ram_available_mb": 100393.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25378.7, "ram_available_mb": 100393.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 22.05, "peak": 25.6, "min": 19.69}, "VIN": {"avg": 59.85, "peak": 65.48, "min": 56.31}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.05, "energy_joules_est": 20.0, "sample_count": 7, "duration_seconds": 0.907}, "timestamp": "2026-01-17T17:44:15.462844"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 790.447, "latencies_ms": [790.447], "images_per_second": 1.265, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A large kite soars high in the clear blue sky, controlled by a person holding the lines. The kite is primarily white with red and black accents, contrasting against the vibrant blue backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.7, "ram_available_mb": 100393.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25378.7, "ram_available_mb": 100393.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 22.45, "peak": 25.59, "min": 20.09}, "VIN": {"avg": 59.46, "peak": 61.15, "min": 57.11}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 22.45, "energy_joules_est": 17.76, "sample_count": 6, "duration_seconds": 0.791}, "timestamp": "2026-01-17T17:44:16.259541"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 514.856, "latencies_ms": [514.856], "images_per_second": 1.942, "prompt_tokens": 18, "response_tokens_est": 20, "n_tiles": 1, "output_text": "The kite is predominantly white with red accents. The lighting suggests a sunny day with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.7, "ram_available_mb": 100393.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 25378.5, "ram_available_mb": 100393.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.59}, "VDD_GPU": {"avg": 24.28, "peak": 25.99, "min": 22.84}, "VIN": {"avg": 58.98, "peak": 60.12, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.57, "min": 14.17}}, "power_watts_avg": 24.28, "energy_joules_est": 12.51, "sample_count": 3, "duration_seconds": 0.515}, "timestamp": "2026-01-17T17:44:16.782116"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 691.668, "latencies_ms": [691.668], "images_per_second": 1.446, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The room features two twin beds with white linens and gray throw blankets, accented with black and gray throw pillows.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25378.5, "ram_available_mb": 100393.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25378.5, "ram_available_mb": 100393.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.98, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 29.06, "peak": 34.24, "min": 24.03}, "VIN": {"avg": 63.81, "peak": 80.27, "min": 57.05}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.06, "energy_joules_est": 20.11, "sample_count": 5, "duration_seconds": 0.692}, "timestamp": "2026-01-17T17:44:17.484557"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1119.759, "latencies_ms": [1119.759], "images_per_second": 0.893, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "bed: 2\npillows: 4\ntowels: 2\nnightstands: 1\nlamps: 2\nartwork: 1\nwindow: 2\ndoor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.5, "ram_available_mb": 100393.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 27.33, "peak": 36.23, "min": 21.27}, "VIN": {"avg": 64.43, "peak": 96.74, "min": 54.47}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.33, "energy_joules_est": 30.61, "sample_count": 8, "duration_seconds": 1.12}, "timestamp": "2026-01-17T17:44:18.610276"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 887.909, "latencies_ms": [887.909], "images_per_second": 1.126, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The beds are in the foreground, while the artwork and window are in the background. The door is situated near the window, offering a view outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25378.9, "ram_available_mb": 100393.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.09, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 62.84, "peak": 83.87, "min": 53.26}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.09, "energy_joules_est": 24.95, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T17:44:19.504083"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1141.367, "latencies_ms": [1141.367], "images_per_second": 0.876, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The scene depicts a hotel room with two twin beds, each adorned with gray and brown bedding and towels. The room features modern decor, including artwork on the walls and warm lighting from lamps. A window offers a view of the outside, and a door is visible on the right side of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.9, "ram_available_mb": 100393.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25378.9, "ram_available_mb": 100393.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 26.22, "peak": 34.66, "min": 20.48}, "VIN": {"avg": 63.14, "peak": 88.86, "min": 54.18}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.22, "energy_joules_est": 29.94, "sample_count": 9, "duration_seconds": 1.142}, "timestamp": "2026-01-17T17:44:20.651932"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 807.132, "latencies_ms": [807.132], "images_per_second": 1.239, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The room features warm lighting from lamps and a window with natural light filtering through. The walls are painted in a light green hue, and the flooring is dark wood.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25378.9, "ram_available_mb": 100393.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25379.1, "ram_available_mb": 100393.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 27.83, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 62.5, "peak": 78.13, "min": 51.79}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.83, "energy_joules_est": 22.47, "sample_count": 6, "duration_seconds": 0.807}, "timestamp": "2026-01-17T17:44:21.465770"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 785.008, "latencies_ms": [785.008], "images_per_second": 1.274, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A motorcyclist in a white helmet and suit is skillfully maneuvering a white motorcycle with green stripes down a curving asphalt road, surrounded by spectators and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25378.9, "ram_available_mb": 100393.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.49, "peak": 35.06, "min": 23.23}, "VIN": {"avg": 62.05, "peak": 81.39, "min": 53.47}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 22.38, "sample_count": 6, "duration_seconds": 0.786}, "timestamp": "2026-01-17T17:44:22.261865"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1217.353, "latencies_ms": [1217.353], "images_per_second": 0.821, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Motorcycle: 1\nHelmet: 1\nRider: 1\nFence: 2\nGrass: 2\nRoad: 1\nSpectators: 2\nBright sunlight: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.9, "ram_available_mb": 100393.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 26.35, "peak": 35.45, "min": 20.48}, "VIN": {"avg": 64.15, "peak": 93.41, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.35, "energy_joules_est": 32.08, "sample_count": 9, "duration_seconds": 1.218}, "timestamp": "2026-01-17T17:44:23.485116"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 672.775, "latencies_ms": [672.775], "images_per_second": 1.486, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The motorcycle is positioned in the foreground, moving towards the left side of the image. The spectators are in the background, observing the motorcycle's performance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 28.68, "peak": 33.09, "min": 24.03}, "VIN": {"avg": 64.0, "peak": 79.0, "min": 57.66}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.68, "energy_joules_est": 19.31, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T17:44:24.164137"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 769.442, "latencies_ms": [769.442], "images_per_second": 1.3, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A man is riding a white motorcycle on a paved road, leaning into a turn. Spectators are gathered behind a fence, watching the motorcycle performance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 30.33, "peak": 36.24, "min": 24.82}, "VIN": {"avg": 65.83, "peak": 96.67, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.33, "energy_joules_est": 23.36, "sample_count": 5, "duration_seconds": 0.77}, "timestamp": "2026-01-17T17:44:24.940120"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 828.222, "latencies_ms": [828.222], "images_per_second": 1.207, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The motorcycle is predominantly white with green accents. The lighting suggests it might be an overcast day. The motorcycle appears to be made of metal and has a sleek, aerodynamic design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 28.82, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 63.74, "peak": 80.21, "min": 59.55}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.82, "energy_joules_est": 23.88, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T17:44:25.774948"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 689.781, "latencies_ms": [689.781], "images_per_second": 1.45, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The table is set for a romantic dinner, featuring white plates, silverware, wine glasses, and a centerpiece of white flowers in a vase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 29.62, "peak": 35.04, "min": 24.42}, "VIN": {"avg": 62.19, "peak": 79.55, "min": 54.0}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 20.45, "sample_count": 5, "duration_seconds": 0.69}, "timestamp": "2026-01-17T17:44:26.474730"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1081.161, "latencies_ms": [1081.161], "images_per_second": 0.925, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Glass: 4\nVase: 1\nFlower: 2\nTablecloth: 2\nNapkins: 2\nCutlery: 2\nChairs: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 27.23, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 62.43, "peak": 79.28, "min": 53.04}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 27.23, "energy_joules_est": 29.46, "sample_count": 8, "duration_seconds": 1.082}, "timestamp": "2026-01-17T17:44:27.562651"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1102.203, "latencies_ms": [1102.203], "images_per_second": 0.907, "prompt_tokens": 25, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the table and chairs arranged in the background. The table is situated near the center, slightly to the right of the center, while the chairs are placed along the left side. The table and chairs are separated by a small gap, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25378.6, "ram_available_mb": 100393.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 26.39, "peak": 34.66, "min": 20.88}, "VIN": {"avg": 64.1, "peak": 89.8, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.39, "energy_joules_est": 29.1, "sample_count": 8, "duration_seconds": 1.103}, "timestamp": "2026-01-17T17:44:28.670326"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1379.459, "latencies_ms": [1379.459], "images_per_second": 0.725, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The scene depicts a sophisticated dining setting with a table set for a meal, featuring white tablecloths, neatly folded napkins, wine glasses, and silverware. A centerpiece of white flowers in a glass vase adds a touch of elegance and freshness to the table. The background suggests a restaurant or upscale dining establishment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.6, "ram_available_mb": 100393.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 25.25, "peak": 34.26, "min": 20.09}, "VIN": {"avg": 61.57, "peak": 88.29, "min": 54.96}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.25, "energy_joules_est": 34.85, "sample_count": 10, "duration_seconds": 1.38}, "timestamp": "2026-01-17T17:44:30.056251"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 908.595, "latencies_ms": [908.595], "images_per_second": 1.101, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The table is covered with a pristine white tablecloth. The centerpiece is a vase filled with delicate white orchids, illuminated by a soft, warm light source. The setting appears elegant and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 26.61, "peak": 33.08, "min": 21.66}, "VIN": {"avg": 62.69, "peak": 75.23, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 26.61, "energy_joules_est": 24.19, "sample_count": 7, "duration_seconds": 0.909}, "timestamp": "2026-01-17T17:44:30.971306"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 583.909, "latencies_ms": [583.909], "images_per_second": 1.713, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A tall, ornate clock tower with two clock faces stands prominently against a hazy sky.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 31.51, "peak": 34.66, "min": 27.18}, "VIN": {"avg": 61.55, "peak": 70.63, "min": 56.17}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 31.51, "energy_joules_est": 18.41, "sample_count": 4, "duration_seconds": 0.584}, "timestamp": "2026-01-17T17:44:31.564784"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1292.965, "latencies_ms": [1292.965], "images_per_second": 0.773, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "clock: 2\npole: 1\nclock face: 2\nclock hands: 2\nclock numbers: 10, 11, 12, 13, 14, 15, 16, 17\nfield: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.4, "ram_available_mb": 100393.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 26.98, "peak": 38.21, "min": 20.48}, "VIN": {"avg": 60.98, "peak": 78.98, "min": 53.74}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 26.98, "energy_joules_est": 34.89, "sample_count": 10, "duration_seconds": 1.293}, "timestamp": "2026-01-17T17:44:32.863928"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 785.3, "latencies_ms": [785.3], "images_per_second": 1.273, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The clock is positioned in the foreground, slightly to the right of the viewer. The field in the background is further away and appears smaller, suggesting the viewer is at a distance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.63, "peak": 33.88, "min": 23.24}, "VIN": {"avg": 61.98, "peak": 75.59, "min": 56.33}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.63, "energy_joules_est": 22.49, "sample_count": 6, "duration_seconds": 0.786}, "timestamp": "2026-01-17T17:44:33.659444"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 982.788, "latencies_ms": [982.788], "images_per_second": 1.018, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The scene features a square clock mounted on a pole, displaying the time. The background is blurred, suggesting an outdoor setting, possibly a park or a field. The image is in black and white, giving it a timeless and classic feel.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 28.36, "peak": 35.85, "min": 22.07}, "VIN": {"avg": 60.78, "peak": 72.08, "min": 54.45}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 27.88, "sample_count": 7, "duration_seconds": 0.983}, "timestamp": "2026-01-17T17:44:34.648531"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1315.199, "latencies_ms": [1315.199], "images_per_second": 0.76, "prompt_tokens": 18, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The clock is primarily dark, possibly black or dark gray. The lighting conditions appear overcast, creating a muted and somewhat hazy atmosphere. The materials appear to be metal, possibly cast iron or wrought iron, giving the clock a sturdy and classic appearance. The weather conditions suggest a cloudy or possibly rainy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 25.8, "peak": 35.06, "min": 20.08}, "VIN": {"avg": 61.52, "peak": 80.93, "min": 55.71}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 25.8, "energy_joules_est": 33.94, "sample_count": 10, "duration_seconds": 1.316}, "timestamp": "2026-01-17T17:44:35.970308"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 812.866, "latencies_ms": [812.866], "images_per_second": 1.23, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A young man in a black t-shirt and baseball cap skillfully maneuvers his skateboard on a concrete surface, performing a trick that showcases his balance and control.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25378.6, "ram_available_mb": 100393.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 27.3, "peak": 33.08, "min": 22.44}, "VIN": {"avg": 64.0, "peak": 90.92, "min": 53.52}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.3, "energy_joules_est": 22.2, "sample_count": 6, "duration_seconds": 0.813}, "timestamp": "2026-01-17T17:44:36.795262"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1152.948, "latencies_ms": [1152.948], "images_per_second": 0.867, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "skateboard: 1\nhat: 1\nt-shirt: 1\npants: 1\nskateboard: 1\nground: 1\ntrees: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25378.6, "ram_available_mb": 100393.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 26.94, "peak": 35.44, "min": 21.28}, "VIN": {"avg": 59.72, "peak": 79.14, "min": 54.28}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 26.94, "energy_joules_est": 31.07, "sample_count": 8, "duration_seconds": 1.153}, "timestamp": "2026-01-17T17:44:37.958421"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 775.931, "latencies_ms": [775.931], "images_per_second": 1.289, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, moving towards the left side of the image. The skate park is situated in the background, extending beyond the immediate foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.08, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 28.74, "peak": 33.86, "min": 24.02}, "VIN": {"avg": 67.89, "peak": 93.98, "min": 61.12}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.74, "energy_joules_est": 22.31, "sample_count": 5, "duration_seconds": 0.776}, "timestamp": "2026-01-17T17:44:38.744261"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1050.339, "latencies_ms": [1050.339], "images_per_second": 0.952, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "A young man is skateboarding in a skate park, performing a trick on a paved surface. He wears a black t-shirt and baseball cap. The park is surrounded by trees, and there are other skateboarders visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.4, "ram_available_mb": 100392.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 26.58, "peak": 35.04, "min": 20.88}, "VIN": {"avg": 61.76, "peak": 79.62, "min": 54.73}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 26.58, "energy_joules_est": 27.93, "sample_count": 8, "duration_seconds": 1.051}, "timestamp": "2026-01-17T17:44:39.800604"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 926.564, "latencies_ms": [926.564], "images_per_second": 1.079, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The skateboarder is wearing a black t-shirt and gray pants. The skateboard is black and appears to be made of wood. The lighting suggests an overcast day, and the setting appears to be a skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 27.17, "peak": 33.86, "min": 21.66}, "VIN": {"avg": 62.73, "peak": 83.43, "min": 57.33}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.17, "energy_joules_est": 25.18, "sample_count": 7, "duration_seconds": 0.927}, "timestamp": "2026-01-17T17:44:40.732771"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 594.488, "latencies_ms": [594.488], "images_per_second": 1.682, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A white plate is filled with freshly peeled and cut carrots, accompanied by a blue vegetable peeler and a small pile of beets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 24.23, "peak": 26.8, "min": 22.06}, "VIN": {"avg": 59.66, "peak": 61.29, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.23, "energy_joules_est": 14.42, "sample_count": 4, "duration_seconds": 0.595}, "timestamp": "2026-01-17T17:44:41.339614"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1006.396, "latencies_ms": [1006.396], "images_per_second": 0.994, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "carrots: 20\nbeets: 3\ngreen beans: 5\ncarrot peeler: 1\nsink: 1\ncutting board: 1\nkitchen sink: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.61, "min": 13.39}, "VDD_GPU": {"avg": 22.74, "peak": 27.19, "min": 20.09}, "VIN": {"avg": 59.33, "peak": 62.51, "min": 52.14}, "VDD_CPU_SOC_MSS": {"avg": 14.74, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.74, "energy_joules_est": 22.9, "sample_count": 7, "duration_seconds": 1.007}, "timestamp": "2026-01-17T17:44:42.356266"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 673.962, "latencies_ms": [673.962], "images_per_second": 1.484, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the carrots and beets placed near the background. The carrots are situated closer to the viewer, while the beets are located further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.1, "ram_available_mb": 100393.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25379.6, "ram_available_mb": 100392.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.49}, "VDD_GPU": {"avg": 23.01, "peak": 25.6, "min": 20.89}, "VIN": {"avg": 58.92, "peak": 61.3, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.01, "energy_joules_est": 15.52, "sample_count": 5, "duration_seconds": 0.674}, "timestamp": "2026-01-17T17:44:43.036344"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 891.384, "latencies_ms": [891.384], "images_per_second": 1.122, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a kitchen counter with a plate of freshly peeled and cut carrots, accompanied by a blue vegetable peeler and a small pile of beets. The counter also features a dish rack, a knife, and a cup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.6, "ram_available_mb": 100392.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25379.8, "ram_available_mb": 100392.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.49}, "VDD_GPU": {"avg": 22.85, "peak": 26.4, "min": 20.48}, "VIN": {"avg": 60.31, "peak": 65.57, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 22.85, "energy_joules_est": 20.38, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T17:44:43.938445"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 754.034, "latencies_ms": [754.034], "images_per_second": 1.326, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The carrots are bright orange and appear to be freshly peeled. The plate is white and round. The vegetables are placed on a countertop. The lighting is soft and diffused.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.8, "ram_available_mb": 100392.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25379.6, "ram_available_mb": 100392.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.4, "min": 13.39}, "VDD_GPU": {"avg": 23.01, "peak": 26.01, "min": 20.88}, "VIN": {"avg": 59.2, "peak": 62.19, "min": 55.26}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.01, "energy_joules_est": 17.36, "sample_count": 5, "duration_seconds": 0.754}, "timestamp": "2026-01-17T17:44:44.698998"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 696.683, "latencies_ms": [696.683], "images_per_second": 1.435, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A man is giving a presentation or lecture on a large screen, gesturing with his hands as he speaks to attentive audience members.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25379.6, "ram_available_mb": 100392.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25379.6, "ram_available_mb": 100392.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.98, "peak": 14.3, "min": 13.49}, "VDD_GPU": {"avg": 28.67, "peak": 33.06, "min": 24.03}, "VIN": {"avg": 63.08, "peak": 79.33, "min": 57.48}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 28.67, "energy_joules_est": 19.99, "sample_count": 5, "duration_seconds": 0.697}, "timestamp": "2026-01-17T17:44:45.408180"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 947.497, "latencies_ms": [947.497], "images_per_second": 1.055, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "screen: 1\nman: 1\ntie: 1\ntable: 1\nwater bottles: 2\nlights: 2\npeople: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.6, "ram_available_mb": 100392.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25379.8, "ram_available_mb": 100392.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.61, "min": 13.49}, "VDD_GPU": {"avg": 27.97, "peak": 35.85, "min": 22.07}, "VIN": {"avg": 66.02, "peak": 96.7, "min": 60.09}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 27.97, "energy_joules_est": 26.51, "sample_count": 7, "duration_seconds": 0.948}, "timestamp": "2026-01-17T17:44:46.361954"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 951.688, "latencies_ms": [951.688], "images_per_second": 1.051, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The main object, a man in a suit, is positioned slightly to the right of the screen, appearing to be speaking or presenting. The audience members are positioned in the foreground, facing the screen, suggesting they are attending a presentation or lecture.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.8, "ram_available_mb": 100392.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25379.8, "ram_available_mb": 100392.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 26.95, "peak": 34.26, "min": 21.66}, "VIN": {"avg": 63.99, "peak": 84.14, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.95, "energy_joules_est": 25.66, "sample_count": 7, "duration_seconds": 0.952}, "timestamp": "2026-01-17T17:44:47.319893"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 908.477, "latencies_ms": [908.477], "images_per_second": 1.101, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a presentation taking place on a large screen, possibly in a conference room or auditorium. Two people are visible, seemingly attending the presentation, with one person gesturing while the other is seated.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.8, "ram_available_mb": 100392.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25380.3, "ram_available_mb": 100391.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.43, "peak": 34.66, "min": 22.85}, "VIN": {"avg": 64.2, "peak": 78.18, "min": 59.8}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.43, "energy_joules_est": 25.84, "sample_count": 6, "duration_seconds": 0.909}, "timestamp": "2026-01-17T17:44:48.234799"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1050.216, "latencies_ms": [1050.216], "images_per_second": 0.952, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The screen displays a vibrant teal background with geometric shapes in orange and purple. The lighting is bright, illuminating the screen and highlighting the speaker. The screen appears to be made of a flexible material, allowing for easy projection and viewing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.3, "ram_available_mb": 100391.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 26.44, "peak": 34.26, "min": 20.88}, "VIN": {"avg": 62.96, "peak": 89.81, "min": 55.88}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 26.44, "energy_joules_est": 27.77, "sample_count": 8, "duration_seconds": 1.05}, "timestamp": "2026-01-17T17:44:49.291185"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 652.847, "latencies_ms": [652.847], "images_per_second": 1.532, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "Two workers in blue uniforms are sitting on a chair outside a building, conversing while one holds a broom.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25380.5, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 30.13, "peak": 33.88, "min": 26.0}, "VIN": {"avg": 67.7, "peak": 93.25, "min": 57.36}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.13, "energy_joules_est": 19.68, "sample_count": 4, "duration_seconds": 0.653}, "timestamp": "2026-01-17T17:44:49.960030"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1073.105, "latencies_ms": [1073.105], "images_per_second": 0.932, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "scooter: 2\nchair: 1\nman: 2\nwoman: 1\nbroom: 1\nsign: 2\nstreet: 1\nbuilding: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.5, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.5, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 26.78, "peak": 35.44, "min": 20.88}, "VIN": {"avg": 63.83, "peak": 94.9, "min": 53.46}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 26.78, "energy_joules_est": 28.76, "sample_count": 8, "duration_seconds": 1.074}, "timestamp": "2026-01-17T17:44:51.040287"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 652.229, "latencies_ms": [652.229], "images_per_second": 1.533, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The mop is positioned in the foreground, close to the two workers. The mop is situated between the workers and the building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.5, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25380.9, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 30.43, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 65.41, "peak": 84.71, "min": 56.47}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.43, "energy_joules_est": 19.86, "sample_count": 4, "duration_seconds": 0.653}, "timestamp": "2026-01-17T17:44:51.698238"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 808.665, "latencies_ms": [808.665], "images_per_second": 1.237, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "Two men in blue uniforms are sitting and standing on a city street near a building with signs and advertisements. Parked motorbikes are visible nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.9, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25380.9, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.16, "peak": 36.62, "min": 24.42}, "VIN": {"avg": 63.19, "peak": 77.89, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.16, "energy_joules_est": 24.4, "sample_count": 5, "duration_seconds": 0.809}, "timestamp": "2026-01-17T17:44:52.513181"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1128.778, "latencies_ms": [1128.778], "images_per_second": 0.886, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The street scene is dominated by blue and white colors, creating a vibrant and somewhat industrial atmosphere. The lighting appears to be natural daylight, suggesting an overcast day or possibly late afternoon. The street surface is paved with asphalt, contributing to the overall urban feel of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.9, "ram_available_mb": 100391.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25380.4, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 26.64, "peak": 35.04, "min": 20.88}, "VIN": {"avg": 63.29, "peak": 82.71, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.64, "energy_joules_est": 30.08, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T17:44:53.647898"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 657.384, "latencies_ms": [657.384], "images_per_second": 1.521, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A white plate holds a serving of shredded chicken and a serving of broccoli in a creamy sauce, accompanied by a fork.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.82, "peak": 33.88, "min": 26.79}, "VIN": {"avg": 65.38, "peak": 86.4, "min": 56.46}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.82, "energy_joules_est": 20.28, "sample_count": 4, "duration_seconds": 0.658}, "timestamp": "2026-01-17T17:44:54.317052"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 795.826, "latencies_ms": [795.826], "images_per_second": 1.257, "prompt_tokens": 21, "response_tokens_est": 21, "n_tiles": 1, "output_text": "chicken: 2\nbroccoli: 2\nplate: 1\nfork: 1\nfood: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25380.4, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 31.35, "peak": 37.03, "min": 25.21}, "VIN": {"avg": 62.89, "peak": 76.59, "min": 54.43}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.35, "energy_joules_est": 24.97, "sample_count": 5, "duration_seconds": 0.796}, "timestamp": "2026-01-17T17:44:55.121343"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 735.628, "latencies_ms": [735.628], "images_per_second": 1.359, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned close together, with the chicken and broccoli placed in the foreground. The plate is situated in the background, and the fork is placed near the chicken.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25379.9, "ram_available_mb": 100392.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 31.12, "peak": 36.24, "min": 25.21}, "VIN": {"avg": 63.39, "peak": 91.14, "min": 53.13}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 31.12, "energy_joules_est": 22.91, "sample_count": 5, "duration_seconds": 0.736}, "timestamp": "2026-01-17T17:44:55.863169"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 795.868, "latencies_ms": [795.868], "images_per_second": 1.256, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A white plate holds a serving of shredded chicken and broccoli. A fork rests on the plate. The setting appears to be a casual dining environment, possibly at home or a restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.9, "ram_available_mb": 100392.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 31.35, "peak": 36.63, "min": 25.6}, "VIN": {"avg": 63.57, "peak": 72.05, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.35, "energy_joules_est": 24.96, "sample_count": 5, "duration_seconds": 0.796}, "timestamp": "2026-01-17T17:44:56.665629"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 994.149, "latencies_ms": [994.149], "images_per_second": 1.006, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The food is primarily yellow and green in color. The lighting appears to be soft and diffused, suggesting an indoor setting. The food appears to be made of plastic and possibly microwavable. The weather appears to be cloudy or overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25380.7, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.87, "peak": 36.24, "min": 22.46}, "VIN": {"avg": 61.31, "peak": 68.86, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.87, "energy_joules_est": 28.71, "sample_count": 7, "duration_seconds": 0.994}, "timestamp": "2026-01-17T17:44:57.665853"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 632.46, "latencies_ms": [632.46], "images_per_second": 1.581, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A man wearing a blue shirt, red tie, plaid cap, and round sunglasses stands in front of a building, smiling warmly.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25380.7, "ram_available_mb": 100391.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 30.43, "peak": 34.27, "min": 26.01}, "VIN": {"avg": 65.56, "peak": 82.4, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.43, "energy_joules_est": 19.26, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T17:44:58.308610"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 951.597, "latencies_ms": [951.597], "images_per_second": 1.051, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "hat: 1\nglasses: 2\nshirt: 1\ntie: 1\npool: 1\nwall: 1\nbuilding: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 28.88, "peak": 36.23, "min": 22.85}, "VIN": {"avg": 65.06, "peak": 95.88, "min": 52.79}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.88, "energy_joules_est": 27.5, "sample_count": 6, "duration_seconds": 0.952}, "timestamp": "2026-01-17T17:44:59.267083"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 832.783, "latencies_ms": [832.783], "images_per_second": 1.201, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The man is positioned in the foreground, with the background elements blurred, suggesting they are further away or out of focus. The man is wearing sunglasses, which further emphasizes the foreground aspect of his image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25380.3, "ram_available_mb": 100391.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.16, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 63.38, "peak": 82.48, "min": 54.2}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.16, "energy_joules_est": 23.47, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:45:00.106808"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 827.82, "latencies_ms": [827.82], "images_per_second": 1.208, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A man wearing a blue shirt, red tie, and plaid cap is smiling at the camera. He appears to be outdoors, possibly near a building, with a blurred background suggesting a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.3, "ram_available_mb": 100391.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.43, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 64.93, "peak": 93.46, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.43, "energy_joules_est": 23.55, "sample_count": 6, "duration_seconds": 0.828}, "timestamp": "2026-01-17T17:45:00.940839"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 785.486, "latencies_ms": [785.486], "images_per_second": 1.273, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The man is wearing a blue shirt and a red tie. His hat is black and plaid. The lighting appears to be natural, possibly sunlight, and the background suggests an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.88, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 62.99, "peak": 79.97, "min": 55.37}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.88, "energy_joules_est": 22.71, "sample_count": 6, "duration_seconds": 0.786}, "timestamp": "2026-01-17T17:45:01.733267"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 617.489, "latencies_ms": [617.489], "images_per_second": 1.619, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A collage of six close-up images showcases various types of pizza slices, highlighting their toppings and textures.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 32.2, "peak": 35.85, "min": 27.57}, "VIN": {"avg": 63.62, "peak": 75.65, "min": 59.34}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 32.2, "energy_joules_est": 19.9, "sample_count": 4, "duration_seconds": 0.618}, "timestamp": "2026-01-17T17:45:02.363518"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1894.77, "latencies_ms": [1894.77], "images_per_second": 0.528, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 1, "output_text": "slice of pizza: 2\nslice of pizza with cheese and sauce: 1\nslice of pizza with cheese and sauce: 1\nslice of pizza with cheese and sauce: 1\nslice of pizza with cheese and sauce: 1\nslice of pizza with cheese and sauce: 1\nslice of pizza with cheese and sauce: 1\nslice of pizza with cheese and sauce: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 24.66, "peak": 37.82, "min": 18.91}, "VIN": {"avg": 61.24, "peak": 76.51, "min": 55.49}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.66, "energy_joules_est": 46.73, "sample_count": 15, "duration_seconds": 1.895}, "timestamp": "2026-01-17T17:45:04.264776"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 976.004, "latencies_ms": [976.004], "images_per_second": 1.025, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The main objects are positioned in a diagonal arrangement, with the largest slice of pizza in the top left and the smallest slice in the bottom right. The foreground pizza slice is closer to the viewer, while the background pizza slices are further away.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25380.2, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.12, "peak": 33.09, "min": 21.66}, "VIN": {"avg": 63.89, "peak": 79.13, "min": 57.39}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.12, "energy_joules_est": 26.48, "sample_count": 7, "duration_seconds": 0.976}, "timestamp": "2026-01-17T17:45:05.246929"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1113.56, "latencies_ms": [1113.56], "images_per_second": 0.898, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a casual dining setting, likely in a restaurant or pizzeria, where various slices of pizza are presented on white plates. The pizza slices vary in toppings and sauces, showcasing a diverse range of flavors and styles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.57, "peak": 34.65, "min": 21.66}, "VIN": {"avg": 60.85, "peak": 79.6, "min": 55.01}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 27.57, "energy_joules_est": 30.72, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T17:45:06.367312"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1101.432, "latencies_ms": [1101.432], "images_per_second": 0.908, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The pizza slices exhibit a range of colors, including yellows, greens, and browns. The lighting in the image appears to be soft and warm, creating a pleasant ambiance. The pizza appears to be made with traditional ingredients, suggesting it might be a classic or homemade style.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25380.1, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 26.83, "peak": 34.27, "min": 20.88}, "VIN": {"avg": 60.1, "peak": 65.22, "min": 54.39}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.83, "energy_joules_est": 29.56, "sample_count": 8, "duration_seconds": 1.102}, "timestamp": "2026-01-17T17:45:07.474978"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 610.628, "latencies_ms": [610.628], "images_per_second": 1.638, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "Two young girls in colorful dresses are petting a black goat while standing behind a metal fence.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25380.1, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25380.1, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 30.53, "peak": 34.27, "min": 26.4}, "VIN": {"avg": 70.21, "peak": 95.0, "min": 61.09}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.53, "energy_joules_est": 18.65, "sample_count": 4, "duration_seconds": 0.611}, "timestamp": "2026-01-17T17:45:08.097317"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 884.553, "latencies_ms": [884.553], "images_per_second": 1.131, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "goat: 2\ngirl: 2\nfence: 2\ngrass: 2\nman: 1\ngoat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.1, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25380.1, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.75, "peak": 36.63, "min": 23.64}, "VIN": {"avg": 65.58, "peak": 97.75, "min": 56.09}, "VDD_CPU_SOC_MSS": {"avg": 14.63, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.75, "energy_joules_est": 26.33, "sample_count": 6, "duration_seconds": 0.885}, "timestamp": "2026-01-17T17:45:08.992585"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 734.11, "latencies_ms": [734.11], "images_per_second": 1.362, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the goat being the most prominent object. The children's interaction with the goat suggests a close relationship and interaction with the animal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.1, "ram_available_mb": 100392.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 29.87, "peak": 35.07, "min": 24.83}, "VIN": {"avg": 64.72, "peak": 83.9, "min": 54.21}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.87, "energy_joules_est": 21.94, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T17:45:09.733181"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 850.02, "latencies_ms": [850.02], "images_per_second": 1.176, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Two young girls are petting a black goat in a fenced-in area, possibly at a petting zoo or farm. A man is visible in the background, observing the interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.29, "peak": 36.24, "min": 23.25}, "VIN": {"avg": 64.7, "peak": 80.16, "min": 60.81}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.29, "energy_joules_est": 24.91, "sample_count": 6, "duration_seconds": 0.85}, "timestamp": "2026-01-17T17:45:10.589332"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 613.032, "latencies_ms": [613.032], "images_per_second": 1.631, "prompt_tokens": 18, "response_tokens_est": 24, "n_tiles": 1, "output_text": "The girls are wearing colorful dresses. The goat is black and white. The scene is brightly lit, suggesting sunny weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 31.32, "peak": 35.86, "min": 26.79}, "VIN": {"avg": 63.9, "peak": 82.35, "min": 53.02}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.32, "energy_joules_est": 19.21, "sample_count": 4, "duration_seconds": 0.613}, "timestamp": "2026-01-17T17:45:11.210537"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 708.491, "latencies_ms": [708.491], "images_per_second": 1.411, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The scene captures a busy intersection at dusk, with multiple traffic lights glowing green, cars waiting at the lights, and street signs visible.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25379.9, "ram_available_mb": 100392.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25380.1, "ram_available_mb": 100392.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.08, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 30.81, "peak": 36.65, "min": 25.21}, "VIN": {"avg": 64.33, "peak": 81.57, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.81, "energy_joules_est": 21.84, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T17:45:11.928369"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1042.322, "latencies_ms": [1042.322], "images_per_second": 0.959, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "traffic light: 2\nstreet sign: 1\npower lines: 6\nbuildings: 2\nmountains: 1\ncars: 2\nstreetlights: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.1, "ram_available_mb": 100392.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.58, "peak": 36.65, "min": 21.27}, "VIN": {"avg": 62.61, "peak": 77.61, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.58, "energy_joules_est": 28.76, "sample_count": 8, "duration_seconds": 1.043}, "timestamp": "2026-01-17T17:45:12.976904"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1023.703, "latencies_ms": [1023.703], "images_per_second": 0.977, "prompt_tokens": 25, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The traffic light is positioned on the left side of the image, angled towards the viewer. The street extends into the distance, disappearing into the darkness, creating a sense of depth and distance. The traffic light is situated in the foreground, contrasting with the darkened background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25380.4, "ram_available_mb": 100391.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 26.79, "peak": 34.66, "min": 21.28}, "VIN": {"avg": 62.27, "peak": 86.59, "min": 54.93}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.79, "energy_joules_est": 27.44, "sample_count": 8, "duration_seconds": 1.024}, "timestamp": "2026-01-17T17:45:14.007292"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1026.424, "latencies_ms": [1026.424], "images_per_second": 0.974, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scene depicts a city intersection at dusk, with a green traffic light illuminating the road. The sky is a deep blue, contrasting with the warm orange hues of the setting sun. Buildings line the road, and cars can be seen in the distance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.13, "peak": 34.27, "min": 21.66}, "VIN": {"avg": 66.13, "peak": 99.86, "min": 59.25}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.13, "energy_joules_est": 27.85, "sample_count": 7, "duration_seconds": 1.027}, "timestamp": "2026-01-17T17:45:15.039870"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 965.184, "latencies_ms": [965.184], "images_per_second": 1.036, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The sky is a deep blue, indicating it's likely dusk or early evening. The traffic lights are illuminated green, signaling that vehicles can proceed. The street is dark and appears empty, with some faint lights in the distance.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.12, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 61.08, "peak": 77.04, "min": 51.37}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.12, "energy_joules_est": 26.18, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T17:45:16.011097"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 570.673, "latencies_ms": [570.673], "images_per_second": 1.752, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A woman in a colorful dress stands behind a table displaying several bunches of ripe, yellow bananas.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25379.3, "ram_available_mb": 100392.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25372.8, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.63, "peak": 34.66, "min": 26.4}, "VIN": {"avg": 68.84, "peak": 96.49, "min": 57.74}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 17.49, "sample_count": 4, "duration_seconds": 0.571}, "timestamp": "2026-01-17T17:45:16.591593"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 731.961, "latencies_ms": [731.961], "images_per_second": 1.366, "prompt_tokens": 21, "response_tokens_est": 20, "n_tiles": 1, "output_text": "bananas: 5\nwoman: 1\ntable: 1\nbuilding: 2\npeople: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25372.8, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25372.6, "ram_available_mb": 100399.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.81, "peak": 37.83, "min": 24.83}, "VIN": {"avg": 68.45, "peak": 98.0, "min": 59.81}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.81, "energy_joules_est": 22.56, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T17:45:17.331501"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 730.47, "latencies_ms": [730.47], "images_per_second": 1.369, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The bananas are in the foreground, slightly blurred, while the woman stands in the background, slightly out of focus. The bananas are positioned closer to the viewer, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.6, "ram_available_mb": 100399.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25373.1, "ram_available_mb": 100399.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.73, "peak": 36.63, "min": 25.21}, "VIN": {"avg": 63.51, "peak": 80.44, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.73, "energy_joules_est": 22.46, "sample_count": 5, "duration_seconds": 0.731}, "timestamp": "2026-01-17T17:45:18.069127"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 752.592, "latencies_ms": [752.592], "images_per_second": 1.329, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A woman stands behind a table displaying several bunches of ripe, yellow bananas. The setting appears to be an outdoor market or marketplace, with a building visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25373.1, "ram_available_mb": 100399.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25372.9, "ram_available_mb": 100399.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 30.01, "peak": 35.85, "min": 24.82}, "VIN": {"avg": 61.36, "peak": 70.7, "min": 53.06}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.01, "energy_joules_est": 22.59, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T17:45:18.827680"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 903.046, "latencies_ms": [903.046], "images_per_second": 1.107, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The bananas are yellow and appear to be ripe. The lighting is bright, likely from natural sunlight, giving the bananas a vibrant appearance. The bananas appear to be displayed on a wooden surface, suggesting they are for sale or consumption.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.9, "ram_available_mb": 100399.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25373.1, "ram_available_mb": 100399.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.88, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 65.09, "peak": 80.5, "min": 60.49}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 26.09, "sample_count": 6, "duration_seconds": 0.904}, "timestamp": "2026-01-17T17:45:19.741401"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 849.643, "latencies_ms": [849.643], "images_per_second": 1.177, "prompt_tokens": 8, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A green door with graffiti and a fire escape is situated between two metal roll-up doors on a city sidewalk, accompanied by a red brick building, a fire hydrant, and a bicycle.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25373.1, "ram_available_mb": 100399.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25373.1, "ram_available_mb": 100399.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 29.34, "peak": 35.04, "min": 23.64}, "VIN": {"avg": 64.92, "peak": 97.19, "min": 55.85}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.34, "energy_joules_est": 24.95, "sample_count": 6, "duration_seconds": 0.85}, "timestamp": "2026-01-17T17:45:20.607008"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1171.395, "latencies_ms": [1171.395], "images_per_second": 0.854, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "building: 4\nfire escape: 1\ngarage doors: 2\nfire hydrant: 1\nbicycle: 1\ntree: 1\nsidewalk: 1\nstreet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.1, "ram_available_mb": 100399.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25372.2, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.37, "peak": 35.85, "min": 21.27}, "VIN": {"avg": 61.72, "peak": 80.2, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.37, "energy_joules_est": 32.07, "sample_count": 8, "duration_seconds": 1.172}, "timestamp": "2026-01-17T17:45:21.788311"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 755.965, "latencies_ms": [755.965], "images_per_second": 1.323, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The building is positioned in the foreground, with the fire escape and windows located on its right side. The bicycle is situated to the left of the building, near the sidewalk.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25372.2, "ram_available_mb": 100400.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25371.8, "ram_available_mb": 100400.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.78, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 64.35, "peak": 78.26, "min": 60.34}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 22.52, "sample_count": 5, "duration_seconds": 0.756}, "timestamp": "2026-01-17T17:45:22.551074"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 972.985, "latencies_ms": [972.985], "images_per_second": 1.028, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a red brick building with green metal shutters and graffiti on its facade. A green door is visible, flanked by a fire escape. A bicycle is parked on the sidewalk in front of the building.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.8, "ram_available_mb": 100400.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.98, "peak": 36.24, "min": 22.45}, "VIN": {"avg": 61.78, "peak": 79.59, "min": 55.72}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.98, "energy_joules_est": 28.21, "sample_count": 7, "duration_seconds": 0.973}, "timestamp": "2026-01-17T17:45:23.530173"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1054.28, "latencies_ms": [1054.28], "images_per_second": 0.949, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The building is primarily red brick with green metal accents, including shutters and a fire escape. The lighting appears to be natural daylight, creating a somewhat somber atmosphere. The materials appear to be standard brick and metal, typical of older urban structures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25371.7, "ram_available_mb": 100400.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.42, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 64.47, "peak": 94.73, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.42, "energy_joules_est": 28.92, "sample_count": 8, "duration_seconds": 1.055}, "timestamp": "2026-01-17T17:45:24.590609"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 715.936, "latencies_ms": [715.936], "images_per_second": 1.397, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A man with long curly hair, wearing a black shirt and a blue and white striped hat, is seen preparing to throw a yellow frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 28.59, "peak": 33.88, "min": 24.04}, "VIN": {"avg": 62.46, "peak": 72.65, "min": 54.35}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 20.48, "sample_count": 5, "duration_seconds": 0.716}, "timestamp": "2026-01-17T17:45:25.316185"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1187.815, "latencies_ms": [1187.815], "images_per_second": 0.842, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Frisbee: 1\nPerson: 1\nHeadband: 1\nT-shirt: 1\nGround: 1\nCeiling: 1\nLighting: 1\nAudience: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25371.7, "ram_available_mb": 100400.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 26.36, "peak": 35.47, "min": 20.48}, "VIN": {"avg": 63.83, "peak": 84.75, "min": 59.2}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.36, "energy_joules_est": 31.32, "sample_count": 9, "duration_seconds": 1.188}, "timestamp": "2026-01-17T17:45:26.512605"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1095.615, "latencies_ms": [1095.615], "images_per_second": 0.913, "prompt_tokens": 25, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The man is positioned in the foreground, throwing the frisbee towards the right side of the image. The frisbee is relatively close to the man's hand, suggesting an active throwing motion. The background is slightly blurred, indicating a focus on the man and the frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25375.4, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 26.34, "peak": 33.88, "min": 20.89}, "VIN": {"avg": 62.18, "peak": 86.99, "min": 52.26}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 28.87, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T17:45:27.614498"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 956.62, "latencies_ms": [956.62], "images_per_second": 1.045, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "A man is captured mid-throw of a yellow frisbee, seemingly in a dimly lit indoor space, possibly a park or event venue.  A crowd of people is visible in the background, observing the action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.4, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.4, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.07, "peak": 33.88, "min": 21.66}, "VIN": {"avg": 65.35, "peak": 88.21, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 27.07, "energy_joules_est": 25.9, "sample_count": 7, "duration_seconds": 0.957}, "timestamp": "2026-01-17T17:45:28.577044"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1023.433, "latencies_ms": [1023.433], "images_per_second": 0.977, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The man is wearing a black shirt and a blue and white patterned headband. His hair is long, dark, and appears to be curly or wavy. The background is dimly lit, suggesting an indoor setting. The man is holding a bright yellow frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.4, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.17, "peak": 34.26, "min": 21.66}, "VIN": {"avg": 61.42, "peak": 75.69, "min": 56.23}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.17, "energy_joules_est": 27.82, "sample_count": 7, "duration_seconds": 1.024}, "timestamp": "2026-01-17T17:45:29.606638"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 514.9, "latencies_ms": [514.9], "images_per_second": 1.942, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A group of people are sitting around a table, working on their laptops in a dimly lit restaurant.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.3, "min": 13.9}, "VDD_GPU": {"avg": 31.77, "peak": 34.27, "min": 28.76}, "VIN": {"avg": 67.8, "peak": 88.34, "min": 57.38}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.77, "energy_joules_est": 16.38, "sample_count": 3, "duration_seconds": 0.515}, "timestamp": "2026-01-17T17:45:30.131827"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1065.24, "latencies_ms": [1065.24], "images_per_second": 0.939, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "laptop: 4\nkeyboard: 2\nmouse: 1\nglass: 2\nwater: 1\nhat: 1\nman: 2\nwoman: 4", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25375.7, "ram_available_mb": 100396.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.2, "ram_available_mb": 100397.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 28.06, "peak": 37.41, "min": 21.66}, "VIN": {"avg": 61.61, "peak": 76.32, "min": 53.92}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.06, "energy_joules_est": 29.91, "sample_count": 8, "duration_seconds": 1.066}, "timestamp": "2026-01-17T17:45:31.203554"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1128.609, "latencies_ms": [1128.609], "images_per_second": 0.886, "prompt_tokens": 25, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The foreground features the laptops and glasses, while the background includes other individuals and tables. The laptops are placed in the foreground, closer to the viewer, while the glasses and bottles are situated further back, creating a sense of distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.2, "ram_available_mb": 100397.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 26.59, "peak": 34.26, "min": 20.88}, "VIN": {"avg": 64.69, "peak": 94.27, "min": 55.73}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.59, "energy_joules_est": 30.02, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T17:45:32.338204"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 737.151, "latencies_ms": [737.151], "images_per_second": 1.357, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A group of people are gathered around a table in a dimly lit restaurant or bar, working on laptops and engaging in conversation. The atmosphere appears casual and collaborative.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 28.75, "peak": 33.48, "min": 24.02}, "VIN": {"avg": 62.47, "peak": 82.3, "min": 53.37}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 21.21, "sample_count": 5, "duration_seconds": 0.738}, "timestamp": "2026-01-17T17:45:33.082556"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 956.534, "latencies_ms": [956.534], "images_per_second": 1.045, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The room is dimly lit with warm lighting, creating a cozy atmosphere. The walls have a checkered pattern, adding a unique touch to the space. The laptops are open and active, suggesting active use or collaborative work.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 27.91, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 62.38, "peak": 86.39, "min": 53.78}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 27.91, "energy_joules_est": 26.72, "sample_count": 7, "duration_seconds": 0.957}, "timestamp": "2026-01-17T17:45:34.045653"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 638.071, "latencies_ms": [638.071], "images_per_second": 1.567, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A young girl in a pink jacket and blue jeans stands under a blue umbrella, gazing at the camera with a serious expression.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 31.72, "peak": 34.66, "min": 27.59}, "VIN": {"avg": 66.59, "peak": 86.45, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.72, "energy_joules_est": 20.26, "sample_count": 4, "duration_seconds": 0.639}, "timestamp": "2026-01-17T17:45:34.702031"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 993.52, "latencies_ms": [993.52], "images_per_second": 1.007, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "Umbrella: 1\nGirl: 1\nJacket: 1\nScarf: 1\nJeans: 1\nGround: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 29.6, "peak": 37.42, "min": 22.85}, "VIN": {"avg": 64.06, "peak": 84.84, "min": 56.43}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.6, "energy_joules_est": 29.43, "sample_count": 7, "duration_seconds": 0.994}, "timestamp": "2026-01-17T17:45:35.702307"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1111.645, "latencies_ms": [1111.645], "images_per_second": 0.9, "prompt_tokens": 25, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The girl is positioned in the foreground, holding a blue umbrella that extends towards the right side of the image. The umbrella is close to her, implying a close proximity between her and the umbrella. The background is relatively empty and out of focus, drawing attention to the girl and the umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.52, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 63.76, "peak": 94.06, "min": 54.77}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 30.6, "sample_count": 8, "duration_seconds": 1.112}, "timestamp": "2026-01-17T17:45:36.819895"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 738.866, "latencies_ms": [738.866], "images_per_second": 1.353, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A young girl is standing under a blue umbrella on a gravel surface. She is wearing a pink jacket and jeans, and appears to be looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25375.4, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25371.0, "ram_available_mb": 100401.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.94, "peak": 34.27, "min": 24.81}, "VIN": {"avg": 63.62, "peak": 80.42, "min": 55.94}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 22.13, "sample_count": 5, "duration_seconds": 0.739}, "timestamp": "2026-01-17T17:45:37.565738"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 800.041, "latencies_ms": [800.041], "images_per_second": 1.25, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The girl is wearing a bright pink jacket and a colorful scarf. The umbrella is blue and appears to be made of sturdy material. The lighting suggests an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.0, "ram_available_mb": 100401.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25371.5, "ram_available_mb": 100400.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.13, "peak": 36.62, "min": 24.03}, "VIN": {"avg": 61.69, "peak": 72.95, "min": 54.47}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 30.13, "energy_joules_est": 24.12, "sample_count": 6, "duration_seconds": 0.801}, "timestamp": "2026-01-17T17:45:38.372395"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 715.222, "latencies_ms": [715.222], "images_per_second": 1.398, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A man in a suit stands in front of a large window in a room, working on a computer setup that includes a monitor, keyboard, mouse, and additional equipment.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 30.25, "peak": 35.85, "min": 24.83}, "VIN": {"avg": 63.95, "peak": 89.92, "min": 53.6}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.25, "energy_joules_est": 21.65, "sample_count": 5, "duration_seconds": 0.716}, "timestamp": "2026-01-17T17:45:39.098444"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1069.128, "latencies_ms": [1069.128], "images_per_second": 0.935, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "man: 1\ntable: 2\nkeyboard: 1\ncomputer: 1\nmonitor: 1\ncables: 2\nwindow: 1\nchair: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.5, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25371.4, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 27.33, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 62.01, "peak": 76.16, "min": 57.96}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.33, "energy_joules_est": 29.24, "sample_count": 8, "duration_seconds": 1.07}, "timestamp": "2026-01-17T17:45:40.174373"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 850.79, "latencies_ms": [850.79], "images_per_second": 1.175, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The man is standing to the left of the image, near the foreground. The computer setup is positioned in the foreground, near the man. The window is situated in the background, offering a view into the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.4, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25371.4, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.36, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 65.16, "peak": 96.12, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 24.15, "sample_count": 6, "duration_seconds": 0.851}, "timestamp": "2026-01-17T17:45:41.031633"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 832.372, "latencies_ms": [832.372], "images_per_second": 1.201, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A man is standing in a room with a desk, computer, and window, appearing to be in a training or educational setting. The window reflects another person, possibly a participant or instructor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25371.4, "ram_available_mb": 100400.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.42, "peak": 34.64, "min": 22.84}, "VIN": {"avg": 63.19, "peak": 83.15, "min": 52.86}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 23.68, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:45:41.871436"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 869.539, "latencies_ms": [869.539], "images_per_second": 1.15, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The room is lit by natural light coming through a window. The walls are white, and the overall atmosphere appears bright and airy. The materials used appear to be standard office furniture and equipment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25371.7, "ram_available_mb": 100400.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.88, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 63.39, "peak": 77.41, "min": 59.4}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 25.13, "sample_count": 6, "duration_seconds": 0.87}, "timestamp": "2026-01-17T17:45:42.747151"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 592.453, "latencies_ms": [592.453], "images_per_second": 1.688, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A group of five young men are gathered around a wooden table, enjoying a meal and engaging in conversation.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.72, "peak": 34.65, "min": 26.39}, "VIN": {"avg": 64.45, "peak": 78.06, "min": 56.27}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.72, "energy_joules_est": 18.22, "sample_count": 4, "duration_seconds": 0.593}, "timestamp": "2026-01-17T17:45:43.350650"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1493.067, "latencies_ms": [1493.067], "images_per_second": 0.67, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 1, "output_text": "cup: 2\nsaucer: 1\nplate: 1\nbowl: 2\nsauce bottle: 1\nglass: 1\ntable: 1\nchopsticks: 0\nclock: 1\nwindow: 1\ncurtains: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 25.89, "peak": 37.42, "min": 19.7}, "VIN": {"avg": 62.42, "peak": 86.91, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 25.89, "energy_joules_est": 38.67, "sample_count": 11, "duration_seconds": 1.494}, "timestamp": "2026-01-17T17:45:44.850188"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1126.07, "latencies_ms": [1126.07], "images_per_second": 0.888, "prompt_tokens": 25, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The foreground is dominated by the table and chairs, while the background features the window and wooden elements. The table occupies the central portion of the image, drawing the viewer's eye towards the people seated around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 26.04, "peak": 33.08, "min": 20.88}, "VIN": {"avg": 64.12, "peak": 90.18, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.04, "energy_joules_est": 29.33, "sample_count": 8, "duration_seconds": 1.127}, "timestamp": "2026-01-17T17:45:45.982261"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1220.87, "latencies_ms": [1220.87], "images_per_second": 0.819, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 1, "output_text": "A group of young men is gathered around a wooden table in a cozy room, enjoying a meal together. They are seated in a traditional Japanese setting with wooden floors, curtains, and a clock on the wall. Various dishes, cups, and cutlery are spread across the table, hinting at a shared meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 25.51, "peak": 33.47, "min": 20.48}, "VIN": {"avg": 63.66, "peak": 91.4, "min": 57.76}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.51, "energy_joules_est": 31.16, "sample_count": 9, "duration_seconds": 1.222}, "timestamp": "2026-01-17T17:45:47.209916"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 912.887, "latencies_ms": [912.887], "images_per_second": 1.095, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The room is lit by natural light coming through a large window with curtains. The wooden table and chairs add warmth to the setting. The men are gathered around a table, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.18, "peak": 33.88, "min": 21.66}, "VIN": {"avg": 61.66, "peak": 77.53, "min": 54.06}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 24.82, "sample_count": 7, "duration_seconds": 0.913}, "timestamp": "2026-01-17T17:45:48.129205"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 751.524, "latencies_ms": [751.524], "images_per_second": 1.331, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A person in winter clothing is standing next to a red pickup truck with a snowplow attached, parked on a snow-covered street in a residential neighborhood.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 29.22, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 64.25, "peak": 78.74, "min": 55.93}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.22, "energy_joules_est": 21.97, "sample_count": 5, "duration_seconds": 0.752}, "timestamp": "2026-01-17T17:45:48.894590"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1146.475, "latencies_ms": [1146.475], "images_per_second": 0.872, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Truck: 1\nPlow: 1\nPerson: 1\nSnow: 2\nTrees: 2\nHouses: 4\nStreet: 1\nSnow: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25374.1, "ram_available_mb": 100398.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.08, "peak": 35.44, "min": 21.27}, "VIN": {"avg": 62.58, "peak": 83.12, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.08, "energy_joules_est": 31.05, "sample_count": 8, "duration_seconds": 1.147}, "timestamp": "2026-01-17T17:45:50.051234"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 794.215, "latencies_ms": [794.215], "images_per_second": 1.259, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The red snowplow truck is positioned in the foreground, facing the viewer. The snowy street and houses in the background extend into the distance, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.1, "ram_available_mb": 100398.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25374.1, "ram_available_mb": 100398.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 27.7, "peak": 33.09, "min": 22.85}, "VIN": {"avg": 63.15, "peak": 84.42, "min": 54.33}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.7, "energy_joules_est": 22.01, "sample_count": 6, "duration_seconds": 0.795}, "timestamp": "2026-01-17T17:45:50.851646"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1024.329, "latencies_ms": [1024.329], "images_per_second": 0.976, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "A red pickup truck with a snowplow is parked on a snow-covered residential street. A person in winter clothing is standing near the truck, possibly preparing to clear the snow. The scene depicts a typical winter day in a residential neighborhood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.1, "ram_available_mb": 100398.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 27.69, "peak": 35.06, "min": 22.07}, "VIN": {"avg": 61.94, "peak": 79.85, "min": 51.03}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.69, "energy_joules_est": 28.37, "sample_count": 7, "duration_seconds": 1.025}, "timestamp": "2026-01-17T17:45:51.882044"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 812.043, "latencies_ms": [812.043], "images_per_second": 1.231, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The red truck is covered in snow. The scene is lit by natural light, suggesting an overcast day. The snow is undisturbed on the truck and the surrounding area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.1, "peak": 33.88, "min": 22.85}, "VIN": {"avg": 63.07, "peak": 72.94, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.1, "energy_joules_est": 22.83, "sample_count": 6, "duration_seconds": 0.812}, "timestamp": "2026-01-17T17:45:52.700934"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 616.725, "latencies_ms": [616.725], "images_per_second": 1.621, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A man is taking a photograph of himself in a luxurious bathroom with a large mirror, marble countertop, and two sinks.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 32.0, "peak": 35.06, "min": 27.57}, "VIN": {"avg": 61.66, "peak": 72.05, "min": 56.16}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.0, "energy_joules_est": 19.75, "sample_count": 4, "duration_seconds": 0.617}, "timestamp": "2026-01-17T17:45:53.330288"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1632.344, "latencies_ms": [1632.344], "images_per_second": 0.613, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 1, "output_text": "Bathroom: 4\nDouble vanity: 2\nSink: 2\nMirror: 2\nTowels: 2\nLighting fixtures: 2\nTelevision: 1\nBathtub: 1\nFloor: 2\nWalls: 2\nDoor: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 25.51, "peak": 37.41, "min": 19.3}, "VIN": {"avg": 59.66, "peak": 80.12, "min": 52.03}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.51, "energy_joules_est": 41.66, "sample_count": 13, "duration_seconds": 1.633}, "timestamp": "2026-01-17T17:45:54.970521"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 913.844, "latencies_ms": [913.844], "images_per_second": 1.094, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The large mirror spans the width of the bathroom, reflecting the sink and surrounding space. The sink is located in the foreground, while the mirror is positioned in the background. The bathroom is spacious and appears well-lit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25374.6, "ram_available_mb": 100397.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.37, "peak": 33.48, "min": 22.86}, "VIN": {"avg": 64.6, "peak": 92.54, "min": 54.63}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.37, "energy_joules_est": 25.95, "sample_count": 6, "duration_seconds": 0.915}, "timestamp": "2026-01-17T17:45:55.891043"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1051.563, "latencies_ms": [1051.563], "images_per_second": 0.951, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene depicts a luxurious bathroom with a double vanity, marble countertop, and large mirror reflecting the room's interior. The bathroom is well-lit with recessed lighting and wall sconces, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.6, "ram_available_mb": 100397.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.38, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 63.4, "peak": 94.95, "min": 54.18}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.38, "energy_joules_est": 28.8, "sample_count": 8, "duration_seconds": 1.052}, "timestamp": "2026-01-17T17:45:56.949159"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 973.084, "latencies_ms": [973.084], "images_per_second": 1.028, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The bathroom features a dark green marble countertop and beige walls with gold accents. The lighting is warm and inviting, illuminating the space. A mirror reflects the photographer, creating a sense of depth and connection to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 27.96, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 65.19, "peak": 93.5, "min": 53.03}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.96, "energy_joules_est": 27.21, "sample_count": 7, "duration_seconds": 0.973}, "timestamp": "2026-01-17T17:45:57.928791"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 552.741, "latencies_ms": [552.741], "images_per_second": 1.809, "prompt_tokens": 8, "response_tokens_est": 17, "n_tiles": 1, "output_text": "Two men are loading luggage into a white car parked in a busy airport parking lot.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.72, "peak": 34.65, "min": 26.39}, "VIN": {"avg": 66.55, "peak": 80.62, "min": 59.92}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.97, "min": 14.56}}, "power_watts_avg": 30.72, "energy_joules_est": 17.0, "sample_count": 4, "duration_seconds": 0.553}, "timestamp": "2026-01-17T17:45:58.492321"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1003.657, "latencies_ms": [1003.657], "images_per_second": 0.996, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "car: 2\nluggage: 4\nsuitcase: 2\ntrolley: 1\nperson: 2\nwindow: 1\nsign: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 28.97, "peak": 37.41, "min": 22.45}, "VIN": {"avg": 61.76, "peak": 82.94, "min": 52.88}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.97, "energy_joules_est": 29.1, "sample_count": 7, "duration_seconds": 1.004}, "timestamp": "2026-01-17T17:45:59.506685"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 756.724, "latencies_ms": [756.724], "images_per_second": 1.321, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The luggage cart is positioned to the left of the man and slightly in front of him. The car is parked in the background, further away than the luggage cart.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.38, "peak": 34.26, "min": 24.41}, "VIN": {"avg": 63.09, "peak": 79.81, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 14.97, "min": 14.57}}, "power_watts_avg": 29.38, "energy_joules_est": 22.24, "sample_count": 5, "duration_seconds": 0.757}, "timestamp": "2026-01-17T17:46:00.268930"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1219.772, "latencies_ms": [1219.772], "images_per_second": 0.82, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The scene depicts a busy airport parking lot where two men are loading luggage into a car parked nearby. The car is white and appears to be a station wagon or hatchback. The men are surrounded by various suitcases and bags, indicating they are either preparing for a trip or have just arrived.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 26.48, "peak": 36.24, "min": 20.48}, "VIN": {"avg": 64.5, "peak": 83.75, "min": 60.4}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.48, "energy_joules_est": 32.32, "sample_count": 9, "duration_seconds": 1.221}, "timestamp": "2026-01-17T17:46:01.495523"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 839.335, "latencies_ms": [839.335], "images_per_second": 1.191, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The car is white, and the lighting is bright, illuminating the scene. The luggage is various colors, including black and brown. The overall atmosphere is clean and well-lit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 27.78, "peak": 33.5, "min": 22.86}, "VIN": {"avg": 63.12, "peak": 78.18, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.78, "energy_joules_est": 23.33, "sample_count": 6, "duration_seconds": 0.84}, "timestamp": "2026-01-17T17:46:02.341796"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 905.622, "latencies_ms": [905.622], "images_per_second": 1.104, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The plate contains a grilled chicken sandwich with sesame seed buns, accompanied by golden brown french fries and two small bowls of ketchup and mayonnaise.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 27.3, "peak": 33.89, "min": 21.67}, "VIN": {"avg": 62.12, "peak": 76.96, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.3, "energy_joules_est": 24.74, "sample_count": 7, "duration_seconds": 0.906}, "timestamp": "2026-01-17T17:46:03.258995"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1096.881, "latencies_ms": [1096.881], "images_per_second": 0.912, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "bun: 2\nfries: 4\nchicken: 2\nketchup: 1\nlettuce: 1\ntomato: 1\nmayonnaise: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.0, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 26.69, "peak": 34.28, "min": 21.27}, "VIN": {"avg": 63.94, "peak": 91.36, "min": 53.27}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.69, "energy_joules_est": 29.28, "sample_count": 8, "duration_seconds": 1.097}, "timestamp": "2026-01-17T17:46:04.361902"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 939.18, "latencies_ms": [939.18], "images_per_second": 1.065, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The sandwich is in the foreground, while the fries and sauce are in the background. The plate is situated on a surface, providing a neutral backdrop for the food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.0, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 27.18, "peak": 33.86, "min": 21.66}, "VIN": {"avg": 62.26, "peak": 79.79, "min": 57.33}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.18, "energy_joules_est": 25.54, "sample_count": 7, "duration_seconds": 0.939}, "timestamp": "2026-01-17T17:46:05.307206"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1115.971, "latencies_ms": [1115.971], "images_per_second": 0.896, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a casual dining setting with a plate containing a grilled chicken sandwich, french fries, and two small bowls of condiments. The sandwich is served on a white plate, accompanied by a knife. The overall setting suggests a relaxed and informal dining environment.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 26.44, "peak": 34.26, "min": 20.89}, "VIN": {"avg": 62.83, "peak": 81.05, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 26.44, "energy_joules_est": 29.52, "sample_count": 8, "duration_seconds": 1.116}, "timestamp": "2026-01-17T17:46:06.429640"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1113.226, "latencies_ms": [1113.226], "images_per_second": 0.898, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The plate features a mix of golden-brown fries, vibrant red ketchup, and fresh lettuce and tomato slices. The lighting is warm and inviting, enhancing the colors and textures of the food. The plate appears to be made of ceramic or porcelain, suitable for outdoor dining.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.49, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 62.83, "peak": 88.49, "min": 55.19}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.49, "energy_joules_est": 29.5, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T17:46:07.549421"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 693.53, "latencies_ms": [693.53], "images_per_second": 1.442, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The room features a bed with a mosquito net draped over it, situated beneath a thatched roof and surrounded by large windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25374.7, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.15, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 64.31, "peak": 83.19, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 20.23, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:46:08.253467"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1080.447, "latencies_ms": [1080.447], "images_per_second": 0.926, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "bed: 2\ncurtains: 3\nnightstands: 2\ncandle: 1\ntable: 1\nchair: 1\nwindow: 3\nwall: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.7, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.12, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 65.02, "peak": 93.64, "min": 57.26}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.12, "energy_joules_est": 29.31, "sample_count": 8, "duration_seconds": 1.081}, "timestamp": "2026-01-17T17:46:09.342234"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 880.853, "latencies_ms": [880.853], "images_per_second": 1.135, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The bed occupies the foreground, positioned slightly to the right of the image. The window is situated in the background, extending from left to right. The table and chair are placed in the background, further away from the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.2, "ram_available_mb": 100398.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.29, "peak": 34.27, "min": 23.24}, "VIN": {"avg": 64.75, "peak": 91.87, "min": 55.47}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.29, "energy_joules_est": 24.93, "sample_count": 6, "duration_seconds": 0.881}, "timestamp": "2026-01-17T17:46:10.229133"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 965.137, "latencies_ms": [965.137], "images_per_second": 1.036, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene depicts a serene and cozy bedroom with a thatched roof, bathed in natural light. The room features a bed with a mosquito net, a small table with a lit candle, and several framed pictures on the walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.2, "ram_available_mb": 100398.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.29, "peak": 34.27, "min": 22.07}, "VIN": {"avg": 65.71, "peak": 95.51, "min": 59.15}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.29, "energy_joules_est": 26.35, "sample_count": 7, "duration_seconds": 0.966}, "timestamp": "2026-01-17T17:46:11.200385"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 997.102, "latencies_ms": [997.102], "images_per_second": 1.003, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The room features a warm, inviting ambiance with warm yellow walls and a thatched roof. The lighting is soft and warm, creating a cozy atmosphere. The bed is covered in a green mosquito net, adding a unique touch to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.46, "peak": 35.07, "min": 21.66}, "VIN": {"avg": 60.77, "peak": 78.93, "min": 49.35}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 27.39, "sample_count": 7, "duration_seconds": 0.998}, "timestamp": "2026-01-17T17:46:12.203837"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 512.465, "latencies_ms": [512.465], "images_per_second": 1.951, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A tabby cat with striking blue eyes is standing on the hood of a sleek black car in a garage.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 24.68, "peak": 26.38, "min": 23.24}, "VIN": {"avg": 60.6, "peak": 65.04, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.68, "energy_joules_est": 12.66, "sample_count": 3, "duration_seconds": 0.513}, "timestamp": "2026-01-17T17:46:12.726984"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1133.68, "latencies_ms": [1133.68], "images_per_second": 0.882, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "lamp: 2\nbox: 2\ncar: 1\ncat: 1\nglove: 1\nbicycle: 1\ntoolbox: 1\nfloor: 1\nstorage cabinet: 2", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 22.69, "peak": 27.17, "min": 20.09}, "VIN": {"avg": 59.4, "peak": 61.75, "min": 53.93}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.69, "energy_joules_est": 25.73, "sample_count": 8, "duration_seconds": 1.134}, "timestamp": "2026-01-17T17:46:13.866948"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 575.01, "latencies_ms": [575.01], "images_per_second": 1.739, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The black car is positioned in the foreground, slightly to the right of the cat. The garage is situated in the background, extending from left to right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25374.4, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 23.44, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 59.38, "peak": 60.86, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.44, "energy_joules_est": 13.49, "sample_count": 4, "duration_seconds": 0.575}, "timestamp": "2026-01-17T17:46:14.447776"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 654.851, "latencies_ms": [654.851], "images_per_second": 1.527, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A tabby cat is standing on the hood of a black car in a garage. The garage contains various items like tools, boxes, and bicycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25374.4, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 24.22, "peak": 26.79, "min": 22.05}, "VIN": {"avg": 56.52, "peak": 58.26, "min": 55.54}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.22, "energy_joules_est": 15.87, "sample_count": 4, "duration_seconds": 0.655}, "timestamp": "2026-01-17T17:46:15.112455"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 515.962, "latencies_ms": [515.962], "images_per_second": 1.938, "prompt_tokens": 18, "response_tokens_est": 24, "n_tiles": 1, "output_text": "The car is black. The lighting is dim. The materials appear to be metal and plastic. The weather is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25373.7, "ram_available_mb": 100398.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.1, "min": 13.49}, "VDD_GPU": {"avg": 24.95, "peak": 26.79, "min": 23.25}, "VIN": {"avg": 61.42, "peak": 63.17, "min": 59.82}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 24.95, "energy_joules_est": 12.89, "sample_count": 3, "duration_seconds": 0.517}, "timestamp": "2026-01-17T17:46:15.639123"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 733.561, "latencies_ms": [733.561], "images_per_second": 1.363, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The dish features a meat patty topped with gravy, onions, tomatoes, and herbs, served on a bun and accompanied by a knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.7, "ram_available_mb": 100398.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.08, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 30.81, "peak": 35.06, "min": 25.6}, "VIN": {"avg": 61.43, "peak": 82.86, "min": 54.53}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.81, "energy_joules_est": 22.62, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T17:46:16.386118"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1349.876, "latencies_ms": [1349.876], "images_per_second": 0.741, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "bun: 1\ntomato: 1\nsauce: 1\nonions: 1\ngrilled tomato: 1\nbacon: 1\nbacon: 1\nbacon: 1\nbacon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.1, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 26.55, "peak": 36.63, "min": 20.09}, "VIN": {"avg": 61.84, "peak": 79.2, "min": 54.9}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.55, "energy_joules_est": 35.85, "sample_count": 10, "duration_seconds": 1.35}, "timestamp": "2026-01-17T17:46:17.742712"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1016.904, "latencies_ms": [1016.904], "images_per_second": 0.983, "prompt_tokens": 25, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The main object, consisting of a sandwich with gravy, onions, and tomato, sits in the foreground of the image. The sandwich is positioned slightly to the right of the viewer. The background features a metal table with a partially visible plate of food and a glass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.8, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 64.9, "peak": 94.51, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.8, "energy_joules_est": 28.28, "sample_count": 7, "duration_seconds": 1.017}, "timestamp": "2026-01-17T17:46:18.765609"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1220.136, "latencies_ms": [1220.136], "images_per_second": 0.82, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The scene is set outdoors on a metal table with a woven surface. A white plate holds a generous serving of a meat patty topped with gravy, onions, and herbs, accompanied by a fork and knife. The overall atmosphere suggests a casual dining experience, possibly at a restaurant or food truck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 26.35, "peak": 34.66, "min": 20.48}, "VIN": {"avg": 60.92, "peak": 71.55, "min": 56.16}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.35, "energy_joules_est": 32.16, "sample_count": 9, "duration_seconds": 1.22}, "timestamp": "2026-01-17T17:46:19.991800"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1232.757, "latencies_ms": [1232.757], "images_per_second": 0.811, "prompt_tokens": 18, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The dish features a rich brown gravy drizzled over it, which adds a vibrant contrast to the white plate. The lighting is warm and inviting, enhancing the visual appeal of the food. The plate appears to be made of ceramic or porcelain, and the overall presentation suggests a cozy and inviting dining experience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 26.26, "peak": 33.88, "min": 20.89}, "VIN": {"avg": 61.59, "peak": 73.67, "min": 53.71}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.55}}, "power_watts_avg": 26.26, "energy_joules_est": 32.38, "sample_count": 9, "duration_seconds": 1.233}, "timestamp": "2026-01-17T17:46:21.230750"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 593.606, "latencies_ms": [593.606], "images_per_second": 1.685, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Three men are gathered in a cozy living room, playing a video game together while enjoying snacks and drinks.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 29.74, "peak": 33.09, "min": 26.0}, "VIN": {"avg": 68.76, "peak": 95.29, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.74, "energy_joules_est": 17.67, "sample_count": 4, "duration_seconds": 0.594}, "timestamp": "2026-01-17T17:46:21.836667"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1403.987, "latencies_ms": [1403.987], "images_per_second": 0.712, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 1, "output_text": "laptop: 1\ncans: 2\nbottles: 2\ncork: 1\ncan opener: 1\ntable: 1\nred couch: 1\npillow: 1\nwindow blinds: 2\nfloor lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.6, "ram_available_mb": 100398.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25373.3, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 25.75, "peak": 36.63, "min": 20.09}, "VIN": {"avg": 63.24, "peak": 94.54, "min": 53.85}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 25.75, "energy_joules_est": 36.16, "sample_count": 11, "duration_seconds": 1.404}, "timestamp": "2026-01-17T17:46:23.246917"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 975.899, "latencies_ms": [975.899], "images_per_second": 1.025, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The main objects are positioned in a relatively open space, with a couch on the left and a red couch on the right. The foreground is dominated by the red couch, while the background features a window and a lamp. The objects are placed close together, creating a sense of proximity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.3, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25373.3, "ram_available_mb": 100398.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 27.02, "peak": 33.88, "min": 21.67}, "VIN": {"avg": 64.41, "peak": 92.15, "min": 55.04}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.02, "energy_joules_est": 26.38, "sample_count": 7, "duration_seconds": 0.976}, "timestamp": "2026-01-17T17:46:24.228236"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1321.055, "latencies_ms": [1321.055], "images_per_second": 0.757, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 1, "output_text": "Three young men are gathered in a cozy living room, playing a video game on a Nintendo Wii console. They are surrounded by various items, including a laptop, a red couch, a coffee table with drinks and snacks, and a window with blinds. The scene suggests a casual, relaxed atmosphere where friends are enjoying leisure time together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.3, "ram_available_mb": 100398.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25373.3, "ram_available_mb": 100398.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 25.25, "peak": 34.27, "min": 20.1}, "VIN": {"avg": 62.84, "peak": 81.08, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.25, "energy_joules_est": 33.37, "sample_count": 10, "duration_seconds": 1.322}, "timestamp": "2026-01-17T17:46:25.555267"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1042.853, "latencies_ms": [1042.853], "images_per_second": 0.959, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The room is lit by warm yellow lighting, creating a cozy atmosphere. The walls are painted a light beige color, and the furniture includes a red couch, a beige sofa, and a red coffee table. The scene suggests a casual gathering or social event.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25373.3, "ram_available_mb": 100398.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25373.3, "ram_available_mb": 100398.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 26.84, "peak": 33.48, "min": 21.66}, "VIN": {"avg": 62.58, "peak": 82.47, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.84, "energy_joules_est": 28.01, "sample_count": 7, "duration_seconds": 1.043}, "timestamp": "2026-01-17T17:46:26.605141"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 636.57, "latencies_ms": [636.57], "images_per_second": 1.571, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A baseball catcher crouches behind home plate, wearing a white uniform, black helmet, and protective gear.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25373.3, "ram_available_mb": 100398.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25372.8, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.04, "peak": 33.89, "min": 26.0}, "VIN": {"avg": 68.99, "peak": 97.45, "min": 57.55}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.04, "energy_joules_est": 19.14, "sample_count": 4, "duration_seconds": 0.637}, "timestamp": "2026-01-17T17:46:27.252987"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1130.224, "latencies_ms": [1130.224], "images_per_second": 0.885, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "catcher: 1\nglove: 1\nhelmet: 1\npants: 1\nshin guards: 1\nbaseball glove: 1\nbaseball field: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25372.8, "ram_available_mb": 100399.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25367.6, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.48, "peak": 36.63, "min": 21.27}, "VIN": {"avg": 64.18, "peak": 89.53, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 27.48, "energy_joules_est": 31.07, "sample_count": 8, "duration_seconds": 1.13}, "timestamp": "2026-01-17T17:46:28.389435"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 869.486, "latencies_ms": [869.486], "images_per_second": 1.15, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The catcher is positioned in the foreground, crouched down on the baseball field. The pitcher is further in the background, near the outfield wall. The field extends into the distance, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.6, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25367.6, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.7, "peak": 33.46, "min": 22.45}, "VIN": {"avg": 61.62, "peak": 75.79, "min": 56.6}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.7, "energy_joules_est": 24.09, "sample_count": 6, "duration_seconds": 0.87}, "timestamp": "2026-01-17T17:46:29.264859"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 886.469, "latencies_ms": [886.469], "images_per_second": 1.128, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A baseball catcher is crouched behind home plate, preparing to catch a ball during a game. The field is well-maintained, and the overall atmosphere suggests a professional or competitive setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.6, "ram_available_mb": 100404.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25368.1, "ram_available_mb": 100404.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.76, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 64.68, "peak": 89.25, "min": 55.69}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 25.51, "sample_count": 6, "duration_seconds": 0.887}, "timestamp": "2026-01-17T17:46:30.158183"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 634.937, "latencies_ms": [634.937], "images_per_second": 1.575, "prompt_tokens": 18, "response_tokens_est": 23, "n_tiles": 1, "output_text": "The catcher is wearing a white uniform with black accents. The field is well-lit, suggesting sunny weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.1, "ram_available_mb": 100404.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.72, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 66.23, "peak": 95.18, "min": 53.66}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.72, "energy_joules_est": 19.51, "sample_count": 4, "duration_seconds": 0.635}, "timestamp": "2026-01-17T17:46:30.799579"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 637.958, "latencies_ms": [637.958], "images_per_second": 1.568, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The bathroom features a white bathtub, a toilet, a sink with a wooden cabinet, and a shower curtain, all set against pink and light blue walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.97, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 25.7, "peak": 29.14, "min": 22.84}, "VIN": {"avg": 59.07, "peak": 64.14, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.7, "energy_joules_est": 16.4, "sample_count": 4, "duration_seconds": 0.638}, "timestamp": "2026-01-17T17:46:31.445666"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1521.562, "latencies_ms": [1521.562], "images_per_second": 0.657, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 1, "output_text": "Bathtub: 1\nToilet: 1\nShower curtain: 1\nBathtub surround: 1\nWindow: 1\nVanity: 1\nSink: 1\nCabinet: 1\nFloor: 1\nWalls: 2\nDoor: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.9, "ram_available_mb": 100404.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25368.4, "ram_available_mb": 100403.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.49}, "VDD_GPU": {"avg": 21.59, "peak": 27.18, "min": 19.3}, "VIN": {"avg": 59.44, "peak": 64.39, "min": 53.83}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 21.59, "energy_joules_est": 32.86, "sample_count": 11, "duration_seconds": 1.522}, "timestamp": "2026-01-17T17:46:32.973878"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 832.229, "latencies_ms": [832.229], "images_per_second": 1.202, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The toilet is positioned near the bathtub and sink, occupying the foreground. The bathtub and sink are located in the background, separated by a small window. The toilet is situated further back in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.4, "ram_available_mb": 100403.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 22.32, "peak": 25.21, "min": 20.09}, "VIN": {"avg": 60.04, "peak": 61.46, "min": 56.87}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.32, "energy_joules_est": 18.58, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:46:33.812215"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 671.407, "latencies_ms": [671.407], "images_per_second": 1.489, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The bathroom features a pink and white color scheme, with a white bathtub, toilet, and sink. A green bathmat is placed on the floor, and a window provides natural light.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 23.44, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 59.84, "peak": 61.22, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.44, "energy_joules_est": 15.75, "sample_count": 4, "duration_seconds": 0.672}, "timestamp": "2026-01-17T17:46:34.490531"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1049.616, "latencies_ms": [1049.616], "images_per_second": 0.953, "prompt_tokens": 18, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The bathroom features a pink and white color scheme. The lighting appears to be natural or soft, contributing to a calm atmosphere. The materials include tiled walls, wood cabinets, and a white bathtub and toilet. The weather is sunny, enhancing the brightness and cleanliness of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 22.06, "peak": 26.38, "min": 19.7}, "VIN": {"avg": 59.49, "peak": 63.64, "min": 55.32}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 22.06, "energy_joules_est": 23.17, "sample_count": 8, "duration_seconds": 1.05}, "timestamp": "2026-01-17T17:46:35.547052"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 827.778, "latencies_ms": [827.778], "images_per_second": 1.208, "prompt_tokens": 8, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The room features a neatly made bed with a plaid comforter, positioned next to a window with sheer white curtains, allowing natural light to filter in and illuminate the space.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25368.4, "ram_available_mb": 100403.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 27.38, "peak": 33.09, "min": 22.46}, "VIN": {"avg": 64.84, "peak": 89.53, "min": 57.2}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.38, "energy_joules_est": 22.67, "sample_count": 6, "duration_seconds": 0.828}, "timestamp": "2026-01-17T17:46:36.384881"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1062.346, "latencies_ms": [1062.346], "images_per_second": 0.941, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "bed: 1\ncurtains: 2\nwindow: 1\nnightstand: 1\nlamp: 1\nheadboard: 1\nplaid bedspread: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.4, "ram_available_mb": 100403.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 26.74, "peak": 35.07, "min": 21.27}, "VIN": {"avg": 63.47, "peak": 82.56, "min": 56.45}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.74, "energy_joules_est": 28.42, "sample_count": 8, "duration_seconds": 1.063}, "timestamp": "2026-01-17T17:46:37.454407"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 714.257, "latencies_ms": [714.257], "images_per_second": 1.4, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The bed occupies the foreground, positioned slightly to the right of the image. The window and curtains are located in the background, extending from left to right.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.3, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 65.2, "peak": 81.64, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.3, "energy_joules_est": 20.94, "sample_count": 5, "duration_seconds": 0.715}, "timestamp": "2026-01-17T17:46:38.174898"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 869.898, "latencies_ms": [869.898], "images_per_second": 1.15, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The scene depicts a dimly lit bedroom with a plaid-covered bed, a window with sheer curtains, and a small nightstand with a lamp. The window offers a glimpse of an outdoor area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.6, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.82, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 64.99, "peak": 82.81, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 25.08, "sample_count": 6, "duration_seconds": 0.87}, "timestamp": "2026-01-17T17:46:39.051202"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 866.72, "latencies_ms": [866.72], "images_per_second": 1.154, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The bed has a plaid comforter in shades of yellow and white. The lighting in the room is dim, creating a cozy atmosphere. The curtains are dark brown, contrasting with the light-colored bedding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 61.69, "peak": 72.47, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.75, "energy_joules_est": 24.94, "sample_count": 6, "duration_seconds": 0.868}, "timestamp": "2026-01-17T17:46:39.924884"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 623.107, "latencies_ms": [623.107], "images_per_second": 1.605, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A young woman in a black dress is helping a young man put on a boutonniere on his lapel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 31.91, "peak": 35.45, "min": 27.59}, "VIN": {"avg": 64.67, "peak": 85.35, "min": 55.33}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 31.91, "energy_joules_est": 19.91, "sample_count": 4, "duration_seconds": 0.624}, "timestamp": "2026-01-17T17:46:40.558737"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 988.139, "latencies_ms": [988.139], "images_per_second": 1.012, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "man: 1\nwoman: 2\nflower: 1\ntie: 1\ndress: 1\nnecklace: 1\nbelt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.6, "peak": 37.42, "min": 22.85}, "VIN": {"avg": 60.98, "peak": 71.89, "min": 55.06}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.6, "energy_joules_est": 29.27, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T17:46:41.556882"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 715.28, "latencies_ms": [715.28], "images_per_second": 1.398, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The man is positioned to the left of the woman, who is positioned to the right. The woman is standing closer to the man than the man is standing to the woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.73, "peak": 35.06, "min": 25.6}, "VIN": {"avg": 63.72, "peak": 74.46, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.73, "energy_joules_est": 21.99, "sample_count": 5, "duration_seconds": 0.716}, "timestamp": "2026-01-17T17:46:42.278351"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1010.816, "latencies_ms": [1010.816], "images_per_second": 0.989, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "A young man and woman are dressed in formal attire, likely attending a prom or wedding. The woman is helping the man with his boutonniere. The setting appears to be indoors, possibly in a hallway or reception area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.26, "peak": 37.03, "min": 22.85}, "VIN": {"avg": 63.43, "peak": 80.6, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.26, "energy_joules_est": 29.59, "sample_count": 7, "duration_seconds": 1.011}, "timestamp": "2026-01-17T17:46:43.295529"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 951.63, "latencies_ms": [951.63], "images_per_second": 1.051, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The man is wearing a black suit with a gold tie. The woman is wearing a black dress with a sparkly or sequined bodice. The lighting is soft and warm, enhancing the colors and details of the attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25369.0, "ram_available_mb": 100403.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.37, "peak": 35.06, "min": 22.45}, "VIN": {"avg": 63.5, "peak": 80.62, "min": 57.61}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.37, "energy_joules_est": 27.01, "sample_count": 7, "duration_seconds": 0.952}, "timestamp": "2026-01-17T17:46:44.253360"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 669.582, "latencies_ms": [669.582], "images_per_second": 1.493, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A stop sign is mounted on a metal post behind a chain-link fence, with trash and debris scattered around the base.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25369.0, "ram_available_mb": 100403.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.33, "peak": 34.26, "min": 26.01}, "VIN": {"avg": 66.99, "peak": 85.77, "min": 59.54}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 20.33, "sample_count": 4, "duration_seconds": 0.67}, "timestamp": "2026-01-17T17:46:44.936495"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1048.446, "latencies_ms": [1048.446], "images_per_second": 0.954, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "stop sign: 1\nchain link fence: 1\npalm trees: 4\ngrass: 2\ntrash: 2\nbuilding: 1\nshrubs: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.8, "ram_available_mb": 100403.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25368.5, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.38, "peak": 36.24, "min": 21.28}, "VIN": {"avg": 62.56, "peak": 77.75, "min": 57.14}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.38, "energy_joules_est": 28.71, "sample_count": 8, "duration_seconds": 1.049}, "timestamp": "2026-01-17T17:46:45.991549"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1089.532, "latencies_ms": [1089.532], "images_per_second": 0.918, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The stop sign is positioned in the foreground, slightly to the right of the chain-link fence. The background features palm trees and a building, indicating a tropical or suburban setting. The stop sign is situated between the fence and the palm trees, further emphasizing the proximity of the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25368.5, "ram_available_mb": 100403.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25368.3, "ram_available_mb": 100403.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.39, "peak": 34.66, "min": 20.88}, "VIN": {"avg": 61.81, "peak": 75.79, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.39, "energy_joules_est": 28.76, "sample_count": 8, "duration_seconds": 1.09}, "timestamp": "2026-01-17T17:46:47.086830"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1001.942, "latencies_ms": [1001.942], "images_per_second": 0.998, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The scene depicts a stop sign situated behind a chain-link fence in a grassy area with palm trees in the background. The fence appears somewhat overgrown with weeds and debris, suggesting the location might be a park or a public space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25368.3, "ram_available_mb": 100403.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25368.3, "ram_available_mb": 100403.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.13, "peak": 34.27, "min": 21.67}, "VIN": {"avg": 63.65, "peak": 85.55, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.13, "energy_joules_est": 27.19, "sample_count": 7, "duration_seconds": 1.002}, "timestamp": "2026-01-17T17:46:48.094968"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 900.657, "latencies_ms": [900.657], "images_per_second": 1.11, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The stop sign is red and white. The scene is lit by sunlight, giving a bright and sunny appearance. The stop sign is positioned behind a chain-link fence with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25368.3, "ram_available_mb": 100403.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.91, "peak": 34.28, "min": 22.85}, "VIN": {"avg": 61.94, "peak": 75.98, "min": 54.48}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 25.15, "sample_count": 6, "duration_seconds": 0.901}, "timestamp": "2026-01-17T17:46:49.002057"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 513.295, "latencies_ms": [513.295], "images_per_second": 1.948, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A man is riding a motorcycle past two parked bicycles, one yellow and one black, with a person standing nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 24.95, "peak": 26.79, "min": 23.24}, "VIN": {"avg": 59.23, "peak": 61.38, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 24.95, "energy_joules_est": 12.81, "sample_count": 3, "duration_seconds": 0.514}, "timestamp": "2026-01-17T17:46:49.526061"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1163.01, "latencies_ms": [1163.01], "images_per_second": 0.86, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "bike: 2\nmotorcycle: 1\nbicycle: 2\nbag: 1\ntowel: 1\nperson: 1\nshorts: 1\nsocks: 1\nshoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25367.1, "ram_available_mb": 100405.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 22.6, "peak": 27.59, "min": 19.7}, "VIN": {"avg": 61.24, "peak": 64.53, "min": 55.91}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.6, "energy_joules_est": 26.29, "sample_count": 8, "duration_seconds": 1.163}, "timestamp": "2026-01-17T17:46:50.695515"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 687.304, "latencies_ms": [687.304], "images_per_second": 1.455, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The motorcycle is positioned in the foreground, slightly to the left of the image. The bicycles are positioned in the background, slightly to the right. The motorcycle is closer to the viewer than the bicycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25367.1, "ram_available_mb": 100405.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 22.84, "peak": 25.59, "min": 20.88}, "VIN": {"avg": 61.03, "peak": 62.75, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.84, "energy_joules_est": 15.71, "sample_count": 5, "duration_seconds": 0.688}, "timestamp": "2026-01-17T17:46:51.389638"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 803.73, "latencies_ms": [803.73], "images_per_second": 1.244, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "Two bicycles, one yellow and one black, are parked on a paved path near a fence and trees. A person on a motorcycle is passing by, and another person is standing nearby, observing the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25366.4, "ram_available_mb": 100405.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 22.91, "peak": 26.38, "min": 20.48}, "VIN": {"avg": 60.21, "peak": 62.33, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.91, "energy_joules_est": 18.43, "sample_count": 6, "duration_seconds": 0.804}, "timestamp": "2026-01-17T17:46:52.199916"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 516.991, "latencies_ms": [516.991], "images_per_second": 1.934, "prompt_tokens": 18, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The yellow bicycle is partially obscured by the motorcycle. The motorcycle has a bright headlight. The scene appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.4, "ram_available_mb": 100405.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 24.42, "peak": 26.0, "min": 22.85}, "VIN": {"avg": 61.1, "peak": 66.16, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.42, "energy_joules_est": 12.64, "sample_count": 3, "duration_seconds": 0.517}, "timestamp": "2026-01-17T17:46:52.727067"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 573.313, "latencies_ms": [573.313], "images_per_second": 1.744, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "Two pedestrians are standing on the sidewalk near a street corner, waiting to cross the road.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 30.73, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 69.78, "peak": 97.04, "min": 60.27}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 14.57, "min": 14.17}}, "power_watts_avg": 30.73, "energy_joules_est": 17.64, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T17:46:53.314288"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1255.932, "latencies_ms": [1255.932], "images_per_second": 0.796, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Sign: 2\nTraffic light: 2\nStreet light: 2\nBicycle: 1\nCar: 1\nPerson: 1\nTrash can: 1\nCrosswalk: 2\nBuilding: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25366.5, "ram_available_mb": 100405.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 26.96, "peak": 37.41, "min": 20.88}, "VIN": {"avg": 63.04, "peak": 87.54, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 26.96, "energy_joules_est": 33.87, "sample_count": 9, "duration_seconds": 1.256}, "timestamp": "2026-01-17T17:46:54.577012"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 886.835, "latencies_ms": [886.835], "images_per_second": 1.128, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The foreground features a brick sidewalk and a black trash can. The background includes traffic lights, street signs, cars, and pedestrians. The central focus is the street corner where the pedestrians and cars are present.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.5, "ram_available_mb": 100405.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.82, "peak": 33.46, "min": 22.85}, "VIN": {"avg": 62.32, "peak": 75.82, "min": 52.43}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 27.82, "energy_joules_est": 24.68, "sample_count": 6, "duration_seconds": 0.887}, "timestamp": "2026-01-17T17:46:55.470994"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1086.726, "latencies_ms": [1086.726], "images_per_second": 0.92, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The scene depicts a busy city intersection with pedestrians, vehicles, and street signs. A man is standing near a signpost, while another person walks nearby. The setting includes buildings, traffic lights, and streetlights, typical of an urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.8, "ram_available_mb": 100405.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25366.3, "ram_available_mb": 100405.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 26.69, "peak": 34.65, "min": 21.28}, "VIN": {"avg": 62.65, "peak": 77.23, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.69, "energy_joules_est": 29.01, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-17T17:46:56.564393"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1019.138, "latencies_ms": [1019.138], "images_per_second": 0.981, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene is lit by streetlights, giving it a slightly dim atmosphere. The buildings in the background are primarily brick and concrete, contributing to the urban feel. The street is paved with brick and asphalt, typical of city streets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.3, "ram_available_mb": 100405.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25366.5, "ram_available_mb": 100405.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 26.95, "peak": 33.48, "min": 21.66}, "VIN": {"avg": 62.14, "peak": 82.41, "min": 54.38}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.95, "energy_joules_est": 27.48, "sample_count": 7, "duration_seconds": 1.02}, "timestamp": "2026-01-17T17:46:57.593647"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 608.566, "latencies_ms": [608.566], "images_per_second": 1.643, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Two bronze statues depict women sitting on a bench, holding handbags and appearing to converse in a public area.", "error": null, "sys_before": {"cpu_percent": 21.4, "ram_used_mb": 25366.5, "ram_available_mb": 100405.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25366.3, "ram_available_mb": 100405.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.33, "peak": 33.88, "min": 26.39}, "VIN": {"avg": 68.76, "peak": 93.77, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 18.47, "sample_count": 4, "duration_seconds": 0.609}, "timestamp": "2026-01-17T17:46:58.215087"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1033.411, "latencies_ms": [1033.411], "images_per_second": 0.968, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "bench: 2\nwoman: 2\nwoman: 2\nbag: 2\nwoman: 2\nwoman: 2\nwoman: 2\nperson: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25366.3, "ram_available_mb": 100405.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.36, "peak": 37.03, "min": 22.06}, "VIN": {"avg": 62.35, "peak": 78.77, "min": 50.59}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.36, "energy_joules_est": 29.32, "sample_count": 7, "duration_seconds": 1.034}, "timestamp": "2026-01-17T17:46:59.259098"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 949.018, "latencies_ms": [949.018], "images_per_second": 1.054, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The statue is positioned in the foreground, slightly to the left of the viewer. The background features people standing and walking, further away from the statue. The statue is situated on a bench, placed in the foreground and slightly to the left of the viewer.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25374.3, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.18, "peak": 33.88, "min": 21.66}, "VIN": {"avg": 62.54, "peak": 83.04, "min": 54.46}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 25.81, "sample_count": 7, "duration_seconds": 0.95}, "timestamp": "2026-01-17T17:47:00.219617"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 947.846, "latencies_ms": [947.846], "images_per_second": 1.055, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a public outdoor space with a bronze statue of two women sitting on a bench, seemingly engaged in conversation. The setting appears to be a sidewalk or pedestrian area, with people casually walking by and passing by the statue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.3, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25374.3, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.12, "peak": 33.86, "min": 21.66}, "VIN": {"avg": 62.14, "peak": 81.55, "min": 56.34}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.12, "energy_joules_est": 25.72, "sample_count": 7, "duration_seconds": 0.948}, "timestamp": "2026-01-17T17:47:01.174025"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 906.488, "latencies_ms": [906.488], "images_per_second": 1.103, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The statue is bronze and appears weathered, suggesting it is made of metal. The lighting in the image creates a dramatic contrast, highlighting the statue's features and textures. The setting appears to be outdoors on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.3, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.36, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 61.92, "peak": 75.42, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 25.73, "sample_count": 6, "duration_seconds": 0.907}, "timestamp": "2026-01-17T17:47:02.087288"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 555.105, "latencies_ms": [555.105], "images_per_second": 1.801, "prompt_tokens": 8, "response_tokens_est": 16, "n_tiles": 1, "output_text": "A tall pole displays multiple directional signs in German, guiding travelers to various destinations.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.52, "peak": 34.65, "min": 26.39}, "VIN": {"avg": 63.91, "peak": 77.35, "min": 57.1}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.52, "energy_joules_est": 16.95, "sample_count": 4, "duration_seconds": 0.555}, "timestamp": "2026-01-17T17:47:02.652593"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1060.211, "latencies_ms": [1060.211], "images_per_second": 0.943, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Sign: 4\nAirplane: 1\nHighway: 1\nTruck: 1\nParking sign: 1\nNo parking sign: 1\nTrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.92, "peak": 37.41, "min": 21.66}, "VIN": {"avg": 62.9, "peak": 88.95, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 27.92, "energy_joules_est": 29.61, "sample_count": 8, "duration_seconds": 1.06}, "timestamp": "2026-01-17T17:47:03.718419"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 648.549, "latencies_ms": [648.549], "images_per_second": 1.542, "prompt_tokens": 25, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The signs are positioned in the foreground, slightly overlapping each other. The background consists of trees and a cloudy sky, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.54, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 64.03, "peak": 83.84, "min": 53.13}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 19.18, "sample_count": 5, "duration_seconds": 0.649}, "timestamp": "2026-01-17T17:47:04.373251"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1109.428, "latencies_ms": [1109.428], "images_per_second": 0.901, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scene depicts a street sign post with multiple directional signs in German, guiding travelers to various locations such as an airport, bus stop, and parking area. The signs are mounted on a brown pole amidst trees, indicating a public or semi-public space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.56, "peak": 36.23, "min": 21.66}, "VIN": {"avg": 61.95, "peak": 88.97, "min": 53.09}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.56, "energy_joules_est": 30.58, "sample_count": 8, "duration_seconds": 1.11}, "timestamp": "2026-01-17T17:47:05.488838"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 771.938, "latencies_ms": [771.938], "images_per_second": 1.295, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The signs are predominantly blue and white. The lighting appears to be overcast, creating a somewhat muted atmosphere. The signs appear to be made of metal and have a slightly weathered appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 28.9, "peak": 33.86, "min": 24.02}, "VIN": {"avg": 67.42, "peak": 95.58, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.9, "energy_joules_est": 22.32, "sample_count": 5, "duration_seconds": 0.772}, "timestamp": "2026-01-17T17:47:06.270920"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 788.802, "latencies_ms": [788.802], "images_per_second": 1.268, "prompt_tokens": 8, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Two women, one in a red shirt and the other in a blue shirt, are standing next to a large black suitcase at a train station, smiling and posing for the camera.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 29.54, "peak": 35.04, "min": 24.42}, "VIN": {"avg": 68.14, "peak": 93.4, "min": 60.05}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 23.32, "sample_count": 5, "duration_seconds": 0.79}, "timestamp": "2026-01-17T17:47:07.074204"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1003.114, "latencies_ms": [1003.114], "images_per_second": 0.997, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "suitcase: 1\nbackpack: 2\nhandbag: 2\nwatch: 1\njeans: 1\nshirt: 2\nsmile: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.97, "peak": 35.85, "min": 22.07}, "VIN": {"avg": 63.74, "peak": 92.75, "min": 53.36}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.97, "energy_joules_est": 28.07, "sample_count": 7, "duration_seconds": 1.003}, "timestamp": "2026-01-17T17:47:08.083499"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 776.49, "latencies_ms": [776.49], "images_per_second": 1.288, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The two women are standing close together in the foreground of the image, with a large piece of luggage between them. The background features the train platform and part of the train car.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25374.2, "ram_available_mb": 100397.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.99, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 62.98, "peak": 80.5, "min": 53.2}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.99, "energy_joules_est": 22.53, "sample_count": 5, "duration_seconds": 0.777}, "timestamp": "2026-01-17T17:47:08.866329"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 995.126, "latencies_ms": [995.126], "images_per_second": 1.005, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "Two women are standing at a train station, preparing to board a train. They are dressed casually and carrying luggage. The station has a modern design with red and white elements, and the platform is clean and well-maintained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 27.92, "peak": 35.85, "min": 22.07}, "VIN": {"avg": 62.63, "peak": 87.46, "min": 53.31}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.92, "energy_joules_est": 27.8, "sample_count": 7, "duration_seconds": 0.996}, "timestamp": "2026-01-17T17:47:09.868635"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 895.154, "latencies_ms": [895.154], "images_per_second": 1.117, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The women are wearing bright colors. The lighting in the image is bright, likely from overhead fluorescent lights. The suitcase they are standing next to appears to be made of durable material, likely durable for travel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25373.4, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.36, "peak": 34.68, "min": 22.85}, "VIN": {"avg": 63.55, "peak": 82.32, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.36, "energy_joules_est": 25.4, "sample_count": 6, "duration_seconds": 0.896}, "timestamp": "2026-01-17T17:47:10.770510"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 617.744, "latencies_ms": [617.744], "images_per_second": 1.619, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "Three zebras stand in a row, facing the camera, with a backdrop of purple flowers and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.4, "ram_available_mb": 100398.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 30.83, "peak": 35.06, "min": 26.4}, "VIN": {"avg": 70.16, "peak": 89.33, "min": 62.22}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.83, "energy_joules_est": 19.06, "sample_count": 4, "duration_seconds": 0.618}, "timestamp": "2026-01-17T17:47:11.401051"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 982.617, "latencies_ms": [982.617], "images_per_second": 1.018, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "zebra: 3\ntree: 2\nflowers: 2\nground: 2\ndirt: 1\nbranches: 2\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.2, "ram_available_mb": 100399.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 28.42, "peak": 37.44, "min": 22.06}, "VIN": {"avg": 64.46, "peak": 89.09, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 14.68, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.42, "energy_joules_est": 27.94, "sample_count": 7, "duration_seconds": 0.983}, "timestamp": "2026-01-17T17:47:12.389880"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 798.572, "latencies_ms": [798.572], "images_per_second": 1.252, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The zebras are positioned in the foreground, with the purple flowers in the background. The zebras are relatively close to the viewer, while the flowers are further away, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.36, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 62.66, "peak": 78.28, "min": 55.29}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 22.65, "sample_count": 6, "duration_seconds": 0.799}, "timestamp": "2026-01-17T17:47:13.194499"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 714.42, "latencies_ms": [714.42], "images_per_second": 1.4, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Three zebras stand in a grassy area with trees in the background, facing the camera. The scene appears to be in a zoo or wildlife park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 29.93, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 65.13, "peak": 81.75, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.97, "min": 14.57}}, "power_watts_avg": 29.93, "energy_joules_est": 21.39, "sample_count": 5, "duration_seconds": 0.715}, "timestamp": "2026-01-17T17:47:13.916132"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1488.687, "latencies_ms": [1488.687], "images_per_second": 0.672, "prompt_tokens": 18, "response_tokens_est": 78, "n_tiles": 1, "output_text": "The zebras exhibit a striking combination of black and white stripes, creating a mesmerizing pattern. The lighting in the image is bright and evenly distributed, highlighting the zebras' distinctive markings. The zebras are standing on a dirt path, which contrasts with their striped coats. The background features a mix of green foliage and purple flowers, adding a vibrant touch to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 25.32, "peak": 35.85, "min": 19.7}, "VIN": {"avg": 61.42, "peak": 76.62, "min": 54.21}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 25.32, "energy_joules_est": 37.7, "sample_count": 11, "duration_seconds": 1.489}, "timestamp": "2026-01-17T17:47:15.411122"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 570.3, "latencies_ms": [570.3], "images_per_second": 1.753, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A black tripod with a camera mounted on it is set up in a room with a table, chair, and vending machine in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.4, "min": 13.59}, "VDD_GPU": {"avg": 23.15, "peak": 25.22, "min": 21.28}, "VIN": {"avg": 59.9, "peak": 62.94, "min": 52.81}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.15, "energy_joules_est": 13.21, "sample_count": 4, "duration_seconds": 0.571}, "timestamp": "2026-01-17T17:47:15.990458"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1092.934, "latencies_ms": [1092.934], "images_per_second": 0.915, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "camera: 1\nlaptop: 1\ntripod: 1\nchair: 1\nvending machine: 1\ntable: 1\ncontainers: 2\nfloor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.71, "min": 13.39}, "VDD_GPU": {"avg": 22.31, "peak": 26.79, "min": 19.7}, "VIN": {"avg": 57.61, "peak": 60.9, "min": 52.6}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.31, "energy_joules_est": 24.39, "sample_count": 8, "duration_seconds": 1.093}, "timestamp": "2026-01-17T17:47:17.089925"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 578.344, "latencies_ms": [578.344], "images_per_second": 1.729, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The camera is positioned in the foreground, slightly to the right of the laptop. The vending machine is in the background, slightly to the left of the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25373.9, "ram_available_mb": 100398.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 23.34, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 60.27, "peak": 62.19, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.34, "energy_joules_est": 13.51, "sample_count": 4, "duration_seconds": 0.579}, "timestamp": "2026-01-17T17:47:17.674500"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 851.861, "latencies_ms": [851.861], "images_per_second": 1.174, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The scene depicts a workspace with a camera mounted on a tripod, positioned near a laptop and a chair. The background includes a vending machine, suggesting an indoor setting possibly in a commercial or public space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.4, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.61, "min": 13.49}, "VDD_GPU": {"avg": 23.11, "peak": 26.8, "min": 20.48}, "VIN": {"avg": 60.85, "peak": 63.2, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.11, "energy_joules_est": 19.7, "sample_count": 6, "duration_seconds": 0.852}, "timestamp": "2026-01-17T17:47:18.533697"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 976.247, "latencies_ms": [976.247], "images_per_second": 1.024, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The camera is black and has a green light. The lighting appears to be artificial, possibly from spotlights. The camera is mounted on a tripod, suggesting it is set up for photography or videography. The background is somewhat blurred, indicating a focus on the camera setup.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25374.5, "ram_available_mb": 100397.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 22.34, "peak": 25.99, "min": 20.09}, "VIN": {"avg": 59.94, "peak": 62.0, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 22.34, "energy_joules_est": 21.82, "sample_count": 7, "duration_seconds": 0.977}, "timestamp": "2026-01-17T17:47:19.516140"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 531.87, "latencies_ms": [531.87], "images_per_second": 1.88, "prompt_tokens": 8, "response_tokens_est": 17, "n_tiles": 1, "output_text": "A white sheep stands among piles of freshly sheared wool in a fenced area.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.3, "min": 13.9}, "VDD_GPU": {"avg": 31.38, "peak": 33.48, "min": 28.76}, "VIN": {"avg": 72.97, "peak": 95.54, "min": 58.87}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.38, "energy_joules_est": 16.71, "sample_count": 3, "duration_seconds": 0.533}, "timestamp": "2026-01-17T17:47:20.060487"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1218.403, "latencies_ms": [1218.403], "images_per_second": 0.821, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "Sheep: 2\nFelt: 2\nWool: 2\nFence: 1\nGround: 1\nBall: 1\nSheep's head: 1\nSheep's body: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.7, "ram_available_mb": 100397.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.32, "peak": 37.44, "min": 20.88}, "VIN": {"avg": 61.58, "peak": 74.41, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.32, "energy_joules_est": 33.29, "sample_count": 9, "duration_seconds": 1.219}, "timestamp": "2026-01-17T17:47:21.285233"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 770.339, "latencies_ms": [770.339], "images_per_second": 1.298, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The main sheep is positioned in the foreground, partially obscured by the pile of wool. The pile of wool is situated in the background, extending beyond the immediate foreground of the sheep.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.3, "ram_available_mb": 100397.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25374.1, "ram_available_mb": 100398.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 64.72, "peak": 81.11, "min": 59.75}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 22.53, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T17:47:22.066115"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 907.33, "latencies_ms": [907.33], "images_per_second": 1.102, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A white sheep is standing behind a pile of freshly sheared wool in a fenced area. The wool is piled up around its legs and body, indicating a recent shearing process.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.1, "ram_available_mb": 100398.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.97, "peak": 35.47, "min": 22.06}, "VIN": {"avg": 64.15, "peak": 85.29, "min": 57.84}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.97, "energy_joules_est": 25.4, "sample_count": 7, "duration_seconds": 0.908}, "timestamp": "2026-01-17T17:47:22.980309"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 699.097, "latencies_ms": [699.097], "images_per_second": 1.43, "prompt_tokens": 18, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The sheep is primarily white with patches of gray wool. The lighting appears to be natural, possibly outdoors, and the scene suggests a cold environment.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.29, "peak": 34.26, "min": 24.41}, "VIN": {"avg": 65.31, "peak": 87.44, "min": 54.39}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.29, "energy_joules_est": 20.49, "sample_count": 5, "duration_seconds": 0.7}, "timestamp": "2026-01-17T17:47:23.687768"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 633.573, "latencies_ms": [633.573], "images_per_second": 1.578, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two female tennis players are engaged in a match on a blue court, with one player near the net and the other positioned further back.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25374.8, "ram_available_mb": 100397.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25374.6, "ram_available_mb": 100397.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 31.71, "peak": 35.83, "min": 27.18}, "VIN": {"avg": 64.71, "peak": 80.86, "min": 59.05}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.71, "energy_joules_est": 20.1, "sample_count": 4, "duration_seconds": 0.634}, "timestamp": "2026-01-17T17:47:24.332226"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1160.426, "latencies_ms": [1160.426], "images_per_second": 0.862, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "net: 1\nball: 1\nracket: 1\ncourt: 2\nplayers: 2\nreferee: 1\nspectators: 2\nadvertising boards: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.6, "ram_available_mb": 100397.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.82, "peak": 37.03, "min": 21.27}, "VIN": {"avg": 64.47, "peak": 95.91, "min": 54.75}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.82, "energy_joules_est": 32.3, "sample_count": 8, "duration_seconds": 1.161}, "timestamp": "2026-01-17T17:47:25.498906"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 906.57, "latencies_ms": [906.57], "images_per_second": 1.103, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The foreground features a tennis court with a net, players, and spectators. The background includes spectators seated in bleachers and additional seating areas. The players are positioned on the court, engaging in the match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.46, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 64.26, "peak": 97.99, "min": 53.59}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 24.9, "sample_count": 7, "duration_seconds": 0.907}, "timestamp": "2026-01-17T17:47:26.411501"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 809.587, "latencies_ms": [809.587], "images_per_second": 1.235, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A tennis match is taking place on a blue court, with players in red and pink outfitting the players. The audience is seated in bleachers surrounding the court.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 28.43, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 64.36, "peak": 94.69, "min": 53.51}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.43, "energy_joules_est": 23.04, "sample_count": 6, "duration_seconds": 0.81}, "timestamp": "2026-01-17T17:47:27.228070"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 886.315, "latencies_ms": [886.315], "images_per_second": 1.128, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The tennis court is painted a vibrant blue. The lighting appears to be bright and evenly distributed, illuminating both the players and the court surface. The materials appear to be durable and suitable for the sport's surface.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 67.41, "peak": 101.84, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 25.49, "sample_count": 6, "duration_seconds": 0.887}, "timestamp": "2026-01-17T17:47:28.121285"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 592.335, "latencies_ms": [592.335], "images_per_second": 1.688, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A man dressed in black is walking through a modern airport terminal, pulling a suitcase behind him.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25374.9, "ram_available_mb": 100397.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 31.12, "peak": 35.04, "min": 26.79}, "VIN": {"avg": 66.7, "peak": 89.64, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.12, "energy_joules_est": 18.45, "sample_count": 4, "duration_seconds": 0.593}, "timestamp": "2026-01-17T17:47:28.725073"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 948.623, "latencies_ms": [948.623], "images_per_second": 1.054, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "person: 1\nsuitcase: 1\nscreen: 1\nstairs: 2\npillars: 2\nfloor: 6\nsigns: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25374.9, "ram_available_mb": 100397.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25377.8, "ram_available_mb": 100394.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.09, "peak": 37.42, "min": 22.45}, "VIN": {"avg": 63.42, "peak": 89.48, "min": 55.08}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.09, "energy_joules_est": 27.61, "sample_count": 7, "duration_seconds": 0.949}, "timestamp": "2026-01-17T17:47:29.683914"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1027.119, "latencies_ms": [1027.119], "images_per_second": 0.974, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The man is walking through the airport terminal, moving from left to right. The foreground is dominated by the airport terminal entrance, partially obscured by the glass door. The background features escalators, further emphasizing the airport's spaciousness.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25377.8, "ram_available_mb": 100394.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25377.0, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.46, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 62.88, "peak": 81.96, "min": 53.06}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 28.23, "sample_count": 7, "duration_seconds": 1.028}, "timestamp": "2026-01-17T17:47:30.721316"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 797.894, "latencies_ms": [797.894], "images_per_second": 1.253, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A man in dark clothing is walking through a modern airport terminal, pulling a rolling suitcase behind him. Large pillars and escalators are visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.0, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.03, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 61.66, "peak": 72.21, "min": 53.0}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.03, "energy_joules_est": 22.37, "sample_count": 6, "duration_seconds": 0.798}, "timestamp": "2026-01-17T17:47:31.525265"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1043.058, "latencies_ms": [1043.058], "images_per_second": 0.959, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The airport terminal features a predominantly gray color scheme, with blue and white directional signs. The lighting is bright, illuminating the space and highlighting the architectural details. The materials appear to be modern glass and metal, contributing to the sleek and clean aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.0, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.0, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.98, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 63.76, "peak": 82.43, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.98, "energy_joules_est": 28.16, "sample_count": 8, "duration_seconds": 1.044}, "timestamp": "2026-01-17T17:47:32.575665"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 715.663, "latencies_ms": [715.663], "images_per_second": 1.397, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "An elderly woman is seated at a table with two pizzas, glasses of beverages, and utensils, enjoying a meal in a cozy setting.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25375.0, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.98, "peak": 33.86, "min": 24.42}, "VIN": {"avg": 63.67, "peak": 79.71, "min": 55.73}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.98, "energy_joules_est": 20.76, "sample_count": 5, "duration_seconds": 0.716}, "timestamp": "2026-01-17T17:47:33.304018"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1083.813, "latencies_ms": [1083.813], "images_per_second": 0.923, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "pizza: 2\nglass: 2\nchicken: 0\nfork: 1\nknife: 1\ntablecloth: 1\nchair: 1\nwoman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.23, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 62.1, "peak": 85.37, "min": 53.69}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 29.53, "sample_count": 8, "duration_seconds": 1.085}, "timestamp": "2026-01-17T17:47:34.394681"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 792.326, "latencies_ms": [792.326], "images_per_second": 1.262, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the pizza boxes and glasses placed nearby. The woman in the background is situated further back, suggesting the setting is relatively open and spacious.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.91, "peak": 33.47, "min": 24.03}, "VIN": {"avg": 64.43, "peak": 86.34, "min": 57.38}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.91, "energy_joules_est": 22.93, "sample_count": 5, "duration_seconds": 0.793}, "timestamp": "2026-01-17T17:47:35.193711"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1008.47, "latencies_ms": [1008.47], "images_per_second": 0.992, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The scene depicts a casual dining setting with two pizzas on white boxes placed on a table, accompanied by glasses, cutlery, and a vase of flowers. An elderly woman is seated in the background, seemingly enjoying the meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.85, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 66.07, "peak": 96.07, "min": 58.96}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 28.09, "sample_count": 7, "duration_seconds": 1.009}, "timestamp": "2026-01-17T17:47:36.210299"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1056.064, "latencies_ms": [1056.064], "images_per_second": 0.947, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The pizza boxes are white with red lettering. The table is covered with a multicolored plaid tablecloth. The pizza appears to have a golden-brown crust and toppings. The lighting in the image is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 26.38, "peak": 33.86, "min": 20.88}, "VIN": {"avg": 61.71, "peak": 80.59, "min": 54.56}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 26.38, "energy_joules_est": 27.87, "sample_count": 8, "duration_seconds": 1.057}, "timestamp": "2026-01-17T17:47:37.273597"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 833.02, "latencies_ms": [833.02], "images_per_second": 1.2, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A youth baseball game is in progress, with a batter swinging a bat at a pitched ball, while a catcher crouches behind him, and spectators watch from the sidelines.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 27.77, "peak": 33.47, "min": 22.84}, "VIN": {"avg": 64.79, "peak": 78.56, "min": 60.24}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.77, "energy_joules_est": 23.15, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:47:38.119231"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1530.99, "latencies_ms": [1530.99], "images_per_second": 0.653, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 1, "output_text": "baseball bat: 1\nbaseball glove: 1\nbaseball helmet: 1\nbaseball: 1\nbaseball field: 1\nbaseball diamond: 1\nbaseball umpire: 1\nspectators: 2\ngrass: 2\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 25.06, "peak": 35.04, "min": 19.7}, "VIN": {"avg": 62.49, "peak": 77.77, "min": 54.58}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 25.06, "energy_joules_est": 38.38, "sample_count": 11, "duration_seconds": 1.532}, "timestamp": "2026-01-17T17:47:39.657030"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 690.359, "latencies_ms": [690.359], "images_per_second": 1.449, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The batter is positioned in the foreground, while the catcher is further back, behind the batter. The spectators are situated in the background, observing the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 28.67, "peak": 33.08, "min": 24.03}, "VIN": {"avg": 66.31, "peak": 87.79, "min": 59.67}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.67, "energy_joules_est": 19.81, "sample_count": 5, "duration_seconds": 0.691}, "timestamp": "2026-01-17T17:47:40.353543"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 904.808, "latencies_ms": [904.808], "images_per_second": 1.105, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A youth baseball game is taking place in a park. A young boy is swinging a bat, while a catcher crouches behind him. Spectators are watching the game from the sidelines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.8, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.01, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 65.25, "peak": 91.12, "min": 56.87}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.01, "energy_joules_est": 26.26, "sample_count": 6, "duration_seconds": 0.905}, "timestamp": "2026-01-17T17:47:41.264455"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 889.257, "latencies_ms": [889.257], "images_per_second": 1.125, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene is bathed in bright sunlight, creating a vibrant atmosphere. The green grass and trees provide a natural backdrop, while the dirt field and clear sky suggest a pleasant day for outdoor activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.8, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.29, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 60.79, "peak": 71.68, "min": 55.11}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 25.16, "sample_count": 6, "duration_seconds": 0.89}, "timestamp": "2026-01-17T17:47:42.159795"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 690.245, "latencies_ms": [690.245], "images_per_second": 1.449, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A black telephone with a banana attached to its earpiece sits on a white desk, accompanied by a stapler and a piece of paper with handwritten text.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.3, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 63.46, "peak": 85.05, "min": 56.62}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 20.24, "sample_count": 5, "duration_seconds": 0.691}, "timestamp": "2026-01-17T17:47:42.864191"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1004.13, "latencies_ms": [1004.13], "images_per_second": 0.996, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "phone: 8\nbanana: 1\nstaple: 1\nprinter: 1\nnotepad: 1\ntable: 1\ncord: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25376.3, "ram_available_mb": 100395.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.18, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 65.59, "peak": 91.57, "min": 59.29}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.18, "energy_joules_est": 28.31, "sample_count": 7, "duration_seconds": 1.005}, "timestamp": "2026-01-17T17:47:43.874537"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 634.089, "latencies_ms": [634.089], "images_per_second": 1.577, "prompt_tokens": 25, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The banana is positioned in the foreground, close to the phone. The phone is situated in the background, slightly further away than the banana.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.3, "ram_available_mb": 100395.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.42, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 66.47, "peak": 85.49, "min": 57.77}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.42, "energy_joules_est": 19.31, "sample_count": 4, "duration_seconds": 0.635}, "timestamp": "2026-01-17T17:47:44.515317"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 831.181, "latencies_ms": [831.181], "images_per_second": 1.203, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The scene depicts a desk with a black telephone with a banana attached to its handset. A stapler is visible in the background, further suggesting an office or workspace setting.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.4, "peak": 36.23, "min": 23.64}, "VIN": {"avg": 60.58, "peak": 73.16, "min": 55.37}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.4, "energy_joules_est": 24.45, "sample_count": 6, "duration_seconds": 0.832}, "timestamp": "2026-01-17T17:47:45.352707"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 911.267, "latencies_ms": [911.267], "images_per_second": 1.097, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The phone is black and has a yellow banana attached to its handset. The phone is situated on a white desk with yellow markings on the paper. The lighting in the image is bright and appears to be natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.74, "peak": 34.66, "min": 22.05}, "VIN": {"avg": 62.96, "peak": 78.29, "min": 59.04}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.74, "energy_joules_est": 25.29, "sample_count": 7, "duration_seconds": 0.912}, "timestamp": "2026-01-17T17:47:46.270300"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 516.504, "latencies_ms": [516.504], "images_per_second": 1.936, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A large crowd of people is gathered in a public area, engaged in various activities and conversations.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.3, "min": 13.9}, "VDD_GPU": {"avg": 32.15, "peak": 34.65, "min": 29.14}, "VIN": {"avg": 64.91, "peak": 76.83, "min": 56.89}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 32.15, "energy_joules_est": 16.62, "sample_count": 3, "duration_seconds": 0.517}, "timestamp": "2026-01-17T17:47:46.802790"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1357.806, "latencies_ms": [1357.806], "images_per_second": 0.736, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "Teddy bear: 1\nBag: 1\nHandbag: 1\nBackpack: 2\nScarf: 2\nShawl: 1\nGlasses: 1\nHair: 8\nClothing: 12", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 26.59, "peak": 37.41, "min": 20.49}, "VIN": {"avg": 61.3, "peak": 79.73, "min": 54.48}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 26.59, "energy_joules_est": 36.13, "sample_count": 10, "duration_seconds": 1.359}, "timestamp": "2026-01-17T17:47:48.167419"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 768.704, "latencies_ms": [768.704], "images_per_second": 1.301, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The teddy bear is positioned near the center of the image, slightly to the right of the center. The crowd surrounds the teddy bear, creating a sense of proximity and interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 28.83, "peak": 33.86, "min": 24.03}, "VIN": {"avg": 63.5, "peak": 77.94, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.83, "energy_joules_est": 22.17, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T17:47:48.942300"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1259.692, "latencies_ms": [1259.692], "images_per_second": 0.794, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The scene depicts a crowded public space, possibly a park or plaza, where a large group of people has gathered, possibly for a social event or gathering. A prominent feature is a large teddy bear, which stands out among the crowd. The atmosphere appears lively and energetic, with people engaging in various activities and conversations.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.43, "peak": 35.44, "min": 20.48}, "VIN": {"avg": 61.19, "peak": 69.39, "min": 52.77}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.43, "energy_joules_est": 33.31, "sample_count": 9, "duration_seconds": 1.26}, "timestamp": "2026-01-17T17:47:50.208609"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1102.222, "latencies_ms": [1102.222], "images_per_second": 0.907, "prompt_tokens": 18, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The crowd is densely packed, showcasing a variety of colors, including red, green, and white. The lighting appears to be natural daylight, creating a bright and lively atmosphere. The materials include clothing, bags, and personal items like teddy bears. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 26.49, "peak": 33.88, "min": 21.27}, "VIN": {"avg": 62.39, "peak": 86.85, "min": 52.74}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.49, "energy_joules_est": 29.21, "sample_count": 8, "duration_seconds": 1.103}, "timestamp": "2026-01-17T17:47:51.316999"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 534.754, "latencies_ms": [534.754], "images_per_second": 1.87, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A man tenderly pets a brown horse's nose while holding a toddler on his hip.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 31.65, "peak": 33.89, "min": 28.76}, "VIN": {"avg": 65.15, "peak": 78.87, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 31.65, "energy_joules_est": 16.95, "sample_count": 3, "duration_seconds": 0.535}, "timestamp": "2026-01-17T17:47:51.864467"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1202.124, "latencies_ms": [1202.124], "images_per_second": 0.832, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "horse: 1\nman: 2\nchild: 1\nred shirt: 1\njeans: 1\nstone building: 1\nporch: 1\nroof: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 27.35, "peak": 37.41, "min": 20.88}, "VIN": {"avg": 62.51, "peak": 79.99, "min": 54.89}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 27.35, "energy_joules_est": 32.89, "sample_count": 9, "duration_seconds": 1.203}, "timestamp": "2026-01-17T17:47:53.072785"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 809.288, "latencies_ms": [809.288], "images_per_second": 1.236, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The horse is positioned to the left of the man and baby, creating a spatial relationship that suggests proximity. The man and baby are standing close to the horse, implying a close interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.6, "ram_available_mb": 100396.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.84, "peak": 33.47, "min": 22.86}, "VIN": {"avg": 65.49, "peak": 94.9, "min": 54.17}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.84, "energy_joules_est": 22.54, "sample_count": 6, "duration_seconds": 0.81}, "timestamp": "2026-01-17T17:47:53.887629"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 731.732, "latencies_ms": [731.732], "images_per_second": 1.367, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A man is holding a toddler while petting a brown horse. The setting appears to be a stable or farm, with a wooden porch and stone walls visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 29.93, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 67.13, "peak": 92.95, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 21.92, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T17:47:54.626387"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 587.65, "latencies_ms": [587.65], "images_per_second": 1.702, "prompt_tokens": 18, "response_tokens_est": 22, "n_tiles": 1, "output_text": "The horse is brown. The man is wearing a red shirt. The setting appears to be sunny and outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 31.32, "peak": 35.86, "min": 26.79}, "VIN": {"avg": 63.05, "peak": 69.42, "min": 57.8}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 31.32, "energy_joules_est": 18.41, "sample_count": 4, "duration_seconds": 0.588}, "timestamp": "2026-01-17T17:47:55.220108"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 519.265, "latencies_ms": [519.265], "images_per_second": 1.926, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A ripe banana with a small cup of peanut butter sits on a plate with a brown rim, placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.73, "peak": 14.1, "min": 13.29}, "VDD_GPU": {"avg": 27.05, "peak": 29.54, "min": 24.82}, "VIN": {"avg": 60.37, "peak": 61.02, "min": 59.41}, "VDD_CPU_SOC_MSS": {"avg": 14.17, "peak": 14.56, "min": 13.78}}, "power_watts_avg": 27.05, "energy_joules_est": 14.06, "sample_count": 3, "duration_seconds": 0.52}, "timestamp": "2026-01-17T17:47:55.750284"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 574.187, "latencies_ms": [574.187], "images_per_second": 1.742, "prompt_tokens": 21, "response_tokens_est": 18, "n_tiles": 1, "output_text": "banana: 1\npeanut butter: 1\nplate: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 25.11, "peak": 27.97, "min": 22.85}, "VIN": {"avg": 59.27, "peak": 60.47, "min": 58.05}, "VDD_CPU_SOC_MSS": {"avg": 14.46, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 25.11, "energy_joules_est": 14.43, "sample_count": 4, "duration_seconds": 0.575}, "timestamp": "2026-01-17T17:47:56.330966"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 555.395, "latencies_ms": [555.395], "images_per_second": 1.801, "prompt_tokens": 25, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The banana is positioned in the foreground, partially covering the peanut butter. The plate is situated in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.5, "ram_available_mb": 100396.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 24.72, "peak": 27.57, "min": 22.45}, "VIN": {"avg": 59.27, "peak": 61.76, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.72, "energy_joules_est": 13.74, "sample_count": 4, "duration_seconds": 0.556}, "timestamp": "2026-01-17T17:47:56.892578"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 618.871, "latencies_ms": [618.871], "images_per_second": 1.616, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A ripe banana rests on a small plate, accompanied by a dollop of peanut butter. The plate is placed on a wooden table, creating a simple, casual setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 24.62, "peak": 27.18, "min": 22.45}, "VIN": {"avg": 58.51, "peak": 62.79, "min": 53.86}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.62, "energy_joules_est": 15.24, "sample_count": 4, "duration_seconds": 0.619}, "timestamp": "2026-01-17T17:47:57.517555"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 836.094, "latencies_ms": [836.094], "images_per_second": 1.196, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The banana is white and appears ripe. The peanut butter is light brown and has a smooth, glossy texture. The plate is off-white with a thin orange-brown rim. The table appears to be wooden.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25375.9, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.71, "min": 13.49}, "VDD_GPU": {"avg": 23.17, "peak": 27.17, "min": 20.48}, "VIN": {"avg": 59.62, "peak": 61.71, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.17, "energy_joules_est": 19.38, "sample_count": 6, "duration_seconds": 0.837}, "timestamp": "2026-01-17T17:47:58.359861"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 475.159, "latencies_ms": [475.159], "images_per_second": 2.105, "prompt_tokens": 8, "response_tokens_est": 15, "n_tiles": 1, "output_text": "A man in green and blue is repairing a motorcycle wheel on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25375.9, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.3, "min": 13.9}, "VDD_GPU": {"avg": 31.65, "peak": 33.88, "min": 28.77}, "VIN": {"avg": 71.12, "peak": 89.02, "min": 59.95}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.65, "energy_joules_est": 15.06, "sample_count": 3, "duration_seconds": 0.476}, "timestamp": "2026-01-17T17:47:58.847210"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1265.727, "latencies_ms": [1265.727], "images_per_second": 0.79, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "motorcycle: 2\nbicycle: 1\nspokes: 6\nwheel: 4\nman: 1\nglasses: 1\nsandals: 1\nfork: 1\nspool: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 27.4, "peak": 37.8, "min": 20.88}, "VIN": {"avg": 64.68, "peak": 90.37, "min": 57.79}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.4, "energy_joules_est": 34.69, "sample_count": 9, "duration_seconds": 1.266}, "timestamp": "2026-01-17T17:48:00.123246"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 790.068, "latencies_ms": [790.068], "images_per_second": 1.266, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The motorcycle is positioned to the left of the man, who is crouched down working on the wheel. The background features other motorcycles and bicycles, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.3, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 28.75, "peak": 33.47, "min": 24.03}, "VIN": {"avg": 65.26, "peak": 83.96, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 22.73, "sample_count": 5, "duration_seconds": 0.791}, "timestamp": "2026-01-17T17:48:00.920775"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 636.376, "latencies_ms": [636.376], "images_per_second": 1.571, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man is repairing a motorcycle wheel on a city street. He appears to be working on a spare tire next to a parked motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 31.32, "peak": 35.45, "min": 26.79}, "VIN": {"avg": 65.1, "peak": 81.53, "min": 54.16}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.32, "energy_joules_est": 19.94, "sample_count": 4, "duration_seconds": 0.637}, "timestamp": "2026-01-17T17:48:01.563204"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 874.201, "latencies_ms": [874.201], "images_per_second": 1.144, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The motorcycle is primarily blue and silver. The lighting appears to be natural, possibly from street lamps or overcast conditions. The motorcycle appears to be made of metal and plastic. The weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 29.74, "peak": 37.03, "min": 23.64}, "VIN": {"avg": 63.87, "peak": 85.56, "min": 55.61}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 29.74, "energy_joules_est": 26.02, "sample_count": 6, "duration_seconds": 0.875}, "timestamp": "2026-01-17T17:48:02.444124"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 784.043, "latencies_ms": [784.043], "images_per_second": 1.275, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A young man with dreadlocks is skillfully performing a trick on his skateboard, airborne and balanced on a concrete ramp in a skate park.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25378.1, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.74, "peak": 35.04, "min": 23.23}, "VIN": {"avg": 66.79, "peak": 99.9, "min": 56.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.74, "energy_joules_est": 22.55, "sample_count": 6, "duration_seconds": 0.785}, "timestamp": "2026-01-17T17:48:03.239225"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1152.562, "latencies_ms": [1152.562], "images_per_second": 0.868, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "skateboard: 1\nman: 1\nfence: 1\ntrees: 1\ngrass: 1\nskate ramp: 1\ncamera: 1\nwatermark: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.1, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25378.1, "ram_available_mb": 100394.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.03, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 62.02, "peak": 76.47, "min": 57.33}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.03, "energy_joules_est": 31.16, "sample_count": 8, "duration_seconds": 1.153}, "timestamp": "2026-01-17T17:48:04.398875"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1146.999, "latencies_ms": [1146.999], "images_per_second": 0.872, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, performing a trick on a ramp. The skateboard is situated near the center of the image, slightly angled towards the left. The background features a grassy area and a fence, providing a contrast to the skateboarder's action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.1, "ram_available_mb": 100394.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25378.2, "ram_available_mb": 100394.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.29, "peak": 33.88, "min": 20.88}, "VIN": {"avg": 61.74, "peak": 75.63, "min": 54.83}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.29, "energy_joules_est": 30.17, "sample_count": 8, "duration_seconds": 1.148}, "timestamp": "2026-01-17T17:48:05.556672"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1007.773, "latencies_ms": [1007.773], "images_per_second": 0.992, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A young man with dreadlocks is performing a skateboard trick on a concrete ramp in a skate park. He's wearing a black t-shirt and jeans, skillfully balancing on his skateboard while executing the trick.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.2, "ram_available_mb": 100394.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25378.1, "ram_available_mb": 100394.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.0, "peak": 33.47, "min": 21.66}, "VIN": {"avg": 64.85, "peak": 99.48, "min": 56.36}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.0, "energy_joules_est": 27.22, "sample_count": 7, "duration_seconds": 1.008}, "timestamp": "2026-01-17T17:48:06.570643"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 986.359, "latencies_ms": [986.359], "images_per_second": 1.014, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The skateboarder is wearing black clothing and a brown baseball cap. The scene appears to be outdoors in natural daylight, with some shadows cast by the trees. The skateboard is light brown and appears to be made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.1, "ram_available_mb": 100394.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.29, "peak": 34.26, "min": 22.05}, "VIN": {"avg": 65.04, "peak": 86.84, "min": 56.63}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.29, "energy_joules_est": 26.93, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T17:48:07.563433"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 512.753, "latencies_ms": [512.753], "images_per_second": 1.95, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A group of four people, dressed casually, are posing for a photo on a grassy field, each holding a frisbee.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 24.69, "peak": 26.39, "min": 23.24}, "VIN": {"avg": 60.08, "peak": 62.96, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 24.69, "energy_joules_est": 12.67, "sample_count": 3, "duration_seconds": 0.513}, "timestamp": "2026-01-17T17:48:08.086279"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1222.87, "latencies_ms": [1222.87], "images_per_second": 0.818, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 1, "output_text": "Frisbee: 3\nFrisbee: 2\nFrisbee: 1\nFrisbee: 1\nFrisbee: 1\nFrisbee: 1\nFrisbee: 1\nFrisbee: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.39}, "VDD_GPU": {"avg": 22.28, "peak": 27.57, "min": 19.7}, "VIN": {"avg": 60.02, "peak": 62.32, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.28, "energy_joules_est": 27.26, "sample_count": 9, "duration_seconds": 1.224}, "timestamp": "2026-01-17T17:48:09.316697"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 654.563, "latencies_ms": [654.563], "images_per_second": 1.528, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The frisbees are positioned in the foreground, while the players are located in the background. The players are situated closer to the camera than the frisbees, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 23.44, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 58.54, "peak": 60.7, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.44, "energy_joules_est": 15.35, "sample_count": 4, "duration_seconds": 0.655}, "timestamp": "2026-01-17T17:48:09.977873"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 755.256, "latencies_ms": [755.256], "images_per_second": 1.324, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A group of four people are enjoying a game of frisbee in a grassy field at sunset. They are posing with frisbees and smiling, showcasing their fun and friendly atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.9, "ram_available_mb": 100394.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25378.0, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 23.64, "peak": 26.79, "min": 21.27}, "VIN": {"avg": 56.18, "peak": 60.53, "min": 49.31}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.97, "min": 14.18}}, "power_watts_avg": 23.64, "energy_joules_est": 17.86, "sample_count": 5, "duration_seconds": 0.756}, "timestamp": "2026-01-17T17:48:10.743712"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1026.124, "latencies_ms": [1026.124], "images_per_second": 0.975, "prompt_tokens": 18, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The people are wearing casual clothing in colors like blue, white, and red. The lighting suggests it might be either early morning or late evening. The frisbees appear to be made of plastic or similar materials. The setting appears to be a grassy field, possibly a park or recreational area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.0, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 22.16, "peak": 26.39, "min": 19.7}, "VIN": {"avg": 60.11, "peak": 65.12, "min": 53.48}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.16, "energy_joules_est": 22.75, "sample_count": 8, "duration_seconds": 1.027}, "timestamp": "2026-01-17T17:48:11.776235"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 736.579, "latencies_ms": [736.579], "images_per_second": 1.358, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A large commercial passenger airplane, painted white with a red tail and displaying the JAL logo, is parked at an airport gate with jet bridges attached.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.6, "ram_available_mb": 100394.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 28.6, "peak": 33.09, "min": 24.03}, "VIN": {"avg": 63.87, "peak": 78.46, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.6, "energy_joules_est": 21.08, "sample_count": 5, "duration_seconds": 0.737}, "timestamp": "2026-01-17T17:48:12.523741"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1150.41, "latencies_ms": [1150.41], "images_per_second": 0.869, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "airplane: 1\ntarmac: 1\njet bridge: 1\nservice vehicles: 2\nground crew: 1\nservice carts: 1\nsky: 1\nclouds: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.6, "ram_available_mb": 100394.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25378.1, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 27.08, "peak": 35.04, "min": 21.28}, "VIN": {"avg": 62.59, "peak": 75.49, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.08, "energy_joules_est": 31.17, "sample_count": 8, "duration_seconds": 1.151}, "timestamp": "2026-01-17T17:48:13.681367"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 932.701, "latencies_ms": [932.701], "images_per_second": 1.072, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main object is a large passenger airplane parked on the tarmac, positioned in the foreground. The airplane is relatively large compared to the surrounding area. The plane is situated near a building, suggesting it is at an airport or terminal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25378.1, "ram_available_mb": 100394.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.24, "peak": 33.89, "min": 22.06}, "VIN": {"avg": 63.87, "peak": 89.31, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.24, "energy_joules_est": 25.42, "sample_count": 7, "duration_seconds": 0.933}, "timestamp": "2026-01-17T17:48:14.620711"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1240.427, "latencies_ms": [1240.427], "images_per_second": 0.806, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 1, "output_text": "A large Japan Airlines Boeing 777-300ER is parked at an airport gate, ready for boarding or disembarking passengers. Ground service equipment, including baggage carts and service vehicles, is present around the aircraft. The scene is set against a bright blue sky with fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.09, "peak": 34.66, "min": 20.49}, "VIN": {"avg": 63.3, "peak": 96.31, "min": 54.58}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.09, "energy_joules_est": 32.38, "sample_count": 9, "duration_seconds": 1.241}, "timestamp": "2026-01-17T17:48:15.868108"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 693.581, "latencies_ms": [693.581], "images_per_second": 1.442, "prompt_tokens": 18, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The airplane is predominantly white with red accents on the tail and around the engines. The sky is bright blue with scattered white clouds.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 28.83, "peak": 33.08, "min": 24.42}, "VIN": {"avg": 66.93, "peak": 85.29, "min": 61.55}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.83, "energy_joules_est": 20.01, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:48:16.567903"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 769.287, "latencies_ms": [769.287], "images_per_second": 1.3, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A young man in a yellow shirt and black pants is skillfully riding a skateboard on a concrete ramp in a skate park, surrounded by graffiti and trees.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.08, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 64.12, "peak": 82.29, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 22.39, "sample_count": 6, "duration_seconds": 0.77}, "timestamp": "2026-01-17T17:48:17.348064"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1102.444, "latencies_ms": [1102.444], "images_per_second": 0.907, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "trash can: 1\nbench: 2\npark bench: 2\nskateboard: 1\nskateboard ramp: 1\ngraffiti: 2\ncar: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.23, "peak": 35.45, "min": 21.28}, "VIN": {"avg": 63.25, "peak": 87.96, "min": 54.0}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 30.03, "sample_count": 8, "duration_seconds": 1.103}, "timestamp": "2026-01-17T17:48:18.456699"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1012.174, "latencies_ms": [1012.174], "images_per_second": 0.988, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, performing a trick on a concrete ramp. The park setting with benches, trees, and a bus stop in the background creates a spatial relationship between the skateboarder and the park elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25377.0, "ram_available_mb": 100395.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.58, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 63.67, "peak": 81.53, "min": 56.28}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.58, "energy_joules_est": 27.92, "sample_count": 7, "duration_seconds": 1.012}, "timestamp": "2026-01-17T17:48:19.475114"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 867.193, "latencies_ms": [867.193], "images_per_second": 1.153, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A young man is performing a skateboard trick on a concrete ramp in a park-like setting. The park features benches, a bus stop, and trees, creating a casual and recreational atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.0, "ram_available_mb": 100395.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25376.8, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.17, "peak": 34.27, "min": 22.85}, "VIN": {"avg": 64.39, "peak": 87.68, "min": 54.35}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 24.44, "sample_count": 6, "duration_seconds": 0.868}, "timestamp": "2026-01-17T17:48:20.348599"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 987.437, "latencies_ms": [987.437], "images_per_second": 1.013, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The skateboarder is wearing a yellow shirt and brown pants. The scene is brightly lit, suggesting sunlight. The skateboard is light brown and appears to be made of wood. The ground is paved and relatively clean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.8, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.8, "ram_available_mb": 100394.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.81, "peak": 35.06, "min": 22.07}, "VIN": {"avg": 63.46, "peak": 88.66, "min": 55.57}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.81, "energy_joules_est": 27.48, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-17T17:48:21.342691"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 865.976, "latencies_ms": [865.976], "images_per_second": 1.155, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A slice of decadent chocolate cake, drizzled with chocolate sauce and adorned with white chocolate drizzle, sits on a decorative white plate with gold floral patterns.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.8, "ram_available_mb": 100394.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.44, "peak": 34.68, "min": 23.24}, "VIN": {"avg": 62.21, "peak": 73.39, "min": 55.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.44, "energy_joules_est": 24.64, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T17:48:22.218413"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1336.749, "latencies_ms": [1336.749], "images_per_second": 0.748, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 1, "output_text": "slice of chocolate cake: 1\nchocolate frosting: 1\ncaramel sauce: 1\nflour: 0\nflour: 0\nbutter: 0\nbutter: 0\ncake: 1\nplate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 25.64, "peak": 35.06, "min": 20.09}, "VIN": {"avg": 61.99, "peak": 73.92, "min": 55.77}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 25.64, "energy_joules_est": 34.29, "sample_count": 10, "duration_seconds": 1.337}, "timestamp": "2026-01-17T17:48:23.562113"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 908.69, "latencies_ms": [908.69], "images_per_second": 1.1, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The chocolate cake is positioned in the foreground of the image, resting on a white plate with a gold floral pattern. The plate is situated on a surface, possibly a table, which further emphasizes the foreground placement of the cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.0, "ram_available_mb": 100396.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.2, "ram_available_mb": 100395.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.23, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 63.41, "peak": 89.44, "min": 56.74}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.23, "energy_joules_est": 24.76, "sample_count": 7, "duration_seconds": 0.909}, "timestamp": "2026-01-17T17:48:24.478206"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 820.894, "latencies_ms": [820.894], "images_per_second": 1.218, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A slice of chocolate cake with a drizzle of white chocolate sits on a decorative plate. The plate is placed on a wooden table, enhancing the warm and inviting ambiance of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.2, "ram_available_mb": 100395.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 28.42, "peak": 34.65, "min": 23.25}, "VIN": {"avg": 63.45, "peak": 86.54, "min": 55.65}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.42, "energy_joules_est": 23.35, "sample_count": 6, "duration_seconds": 0.822}, "timestamp": "2026-01-17T17:48:25.306308"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 831.4, "latencies_ms": [831.4], "images_per_second": 1.203, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The chocolate cake is topped with white drizzle and sits on a white plate with a gold floral pattern. The cake appears moist and rich in color, reflecting the ambient lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.69, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 63.52, "peak": 82.45, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 23.87, "sample_count": 6, "duration_seconds": 0.832}, "timestamp": "2026-01-17T17:48:26.144205"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 552.409, "latencies_ms": [552.409], "images_per_second": 1.81, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A man is working on a laptop at a cluttered desk, surrounded by various electronic devices and equipment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 31.01, "peak": 34.65, "min": 26.79}, "VIN": {"avg": 66.93, "peak": 81.39, "min": 60.73}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.01, "energy_joules_est": 17.15, "sample_count": 4, "duration_seconds": 0.553}, "timestamp": "2026-01-17T17:48:26.708763"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1402.365, "latencies_ms": [1402.365], "images_per_second": 0.713, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "laptop: 2\nkeyboard: 1\nmouse: 1\nlaptop: 2\ncord: 1\nwater bottle: 1\ncardboard box: 2\nchair: 4\ntable: 2\nperson: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.5, "ram_available_mb": 100395.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25376.2, "ram_available_mb": 100395.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 26.12, "peak": 37.01, "min": 20.1}, "VIN": {"avg": 63.19, "peak": 85.11, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.12, "energy_joules_est": 36.64, "sample_count": 10, "duration_seconds": 1.403}, "timestamp": "2026-01-17T17:48:28.121690"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1174.064, "latencies_ms": [1174.064], "images_per_second": 0.852, "prompt_tokens": 25, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The foreground features the laptop and desk, while the background includes other desks, chairs, and equipment. The laptop is situated in the foreground, closer to the viewer, while the desk and chairs are further back, creating a sense of distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.2, "ram_available_mb": 100395.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 26.05, "peak": 33.48, "min": 20.88}, "VIN": {"avg": 62.83, "peak": 86.63, "min": 54.32}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.05, "energy_joules_est": 30.6, "sample_count": 8, "duration_seconds": 1.175}, "timestamp": "2026-01-17T17:48:29.302766"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 834.142, "latencies_ms": [834.142], "images_per_second": 1.199, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The scene depicts a busy workspace with multiple people working on laptops and electronic components. Various boxes, equipment, and tools are scattered throughout the room, indicating a collaborative environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.2, "ram_available_mb": 100395.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.11, "peak": 33.88, "min": 22.86}, "VIN": {"avg": 65.64, "peak": 97.82, "min": 54.36}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.11, "energy_joules_est": 23.47, "sample_count": 6, "duration_seconds": 0.835}, "timestamp": "2026-01-17T17:48:30.143347"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1045.012, "latencies_ms": [1045.012], "images_per_second": 0.957, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The room is lit by fluorescent lighting, giving it a bright and airy atmosphere. The walls are covered with various materials, including cardboard boxes, papers, and possibly some electronic components. The floor appears to be painted in a light yellow or tan color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.2, "ram_available_mb": 100395.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 26.98, "peak": 35.06, "min": 21.26}, "VIN": {"avg": 64.16, "peak": 90.61, "min": 54.21}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.98, "energy_joules_est": 28.21, "sample_count": 8, "duration_seconds": 1.045}, "timestamp": "2026-01-17T17:48:31.195152"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 598.739, "latencies_ms": [598.739], "images_per_second": 1.67, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A group of friends is playing a video game in a cozy living room, enjoying each other's company and having a good time.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.43, "peak": 34.26, "min": 26.4}, "VIN": {"avg": 66.15, "peak": 83.18, "min": 57.14}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 30.43, "energy_joules_est": 18.23, "sample_count": 4, "duration_seconds": 0.599}, "timestamp": "2026-01-17T17:48:31.804402"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1006.249, "latencies_ms": [1006.249], "images_per_second": 0.994, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "woman: 2\nman: 2\nman: 2\nman: 2\nman: 2\nman: 2\nman: 2\nman: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.69, "peak": 36.62, "min": 22.45}, "VIN": {"avg": 65.03, "peak": 90.4, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 28.69, "energy_joules_est": 28.89, "sample_count": 7, "duration_seconds": 1.007}, "timestamp": "2026-01-17T17:48:32.821567"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 871.181, "latencies_ms": [871.181], "images_per_second": 1.148, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective in the image. The foreground features the woman and the man playing the game, while the background includes other furniture and elements of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.22, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 63.33, "peak": 79.65, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.22, "energy_joules_est": 24.59, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T17:48:33.699514"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1011.791, "latencies_ms": [1011.791], "images_per_second": 0.988, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "A group of friends is playing a video game in a cozy living room. They are standing barefoot and holding Wii remotes, enjoying their time together. The room features a couch, a coffee table, and various personal items, creating a comfortable and relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.68, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 63.22, "peak": 82.89, "min": 55.11}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.68, "energy_joules_est": 28.02, "sample_count": 7, "duration_seconds": 1.012}, "timestamp": "2026-01-17T17:48:34.722356"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 732.024, "latencies_ms": [732.024], "images_per_second": 1.366, "prompt_tokens": 18, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The room is lit by natural light coming through windows, creating a warm ambiance. The walls are painted white, and the carpet is a neutral color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 29.54, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 64.66, "peak": 80.4, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 21.64, "sample_count": 5, "duration_seconds": 0.733}, "timestamp": "2026-01-17T17:48:35.464963"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 614.06, "latencies_ms": [614.06], "images_per_second": 1.629, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A person is silhouetted against the setting sun, standing on a frozen surface with footprints nearby, holding an object in their hand.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 31.12, "peak": 35.06, "min": 26.79}, "VIN": {"avg": 65.1, "peak": 81.63, "min": 57.92}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 19.13, "sample_count": 4, "duration_seconds": 0.615}, "timestamp": "2026-01-17T17:48:36.091416"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 967.177, "latencies_ms": [967.177], "images_per_second": 1.034, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "sun: 1\nperson: 1\nfrisbee: 1\nice: 2\nwater: 2\nsky: 1\nhorizon: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25376.7, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25376.6, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 28.58, "peak": 36.62, "min": 22.45}, "VIN": {"avg": 64.56, "peak": 93.28, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.58, "energy_joules_est": 27.65, "sample_count": 7, "duration_seconds": 0.968}, "timestamp": "2026-01-17T17:48:37.064980"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1204.696, "latencies_ms": [1204.696], "images_per_second": 0.83, "prompt_tokens": 25, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The person stands in the middle of the image, facing the setting sun. The sun is positioned behind them, casting a warm glow and creating a striking contrast against the cooler tones of the sky and ice. The person's position suggests they are standing relatively close to the viewer, capturing the serene and picturesque scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.6, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.22, "peak": 34.66, "min": 20.48}, "VIN": {"avg": 63.98, "peak": 93.43, "min": 56.91}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.22, "energy_joules_est": 31.61, "sample_count": 9, "duration_seconds": 1.205}, "timestamp": "2026-01-17T17:48:38.275878"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1114.148, "latencies_ms": [1114.148], "images_per_second": 0.898, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 1, "output_text": "A person is standing on a frozen surface, possibly a beach or ice floe, during sunset. The sun is setting in the background, creating a warm glow and casting long shadows. The scene is peaceful and serene, with the person's silhouette standing out against the vibrant colors of the sky.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.34, "peak": 33.88, "min": 21.27}, "VIN": {"avg": 62.17, "peak": 72.55, "min": 57.8}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.34, "energy_joules_est": 29.36, "sample_count": 8, "duration_seconds": 1.115}, "timestamp": "2026-01-17T17:48:39.396538"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 886.088, "latencies_ms": [886.088], "images_per_second": 1.129, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The sky is a vibrant mix of orange and blue, indicating a sunset. The ground is partially covered with ice and snow, reflecting the warm light of the setting sun. The scene is peaceful and serene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.9, "ram_available_mb": 100395.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.23, "peak": 34.27, "min": 22.85}, "VIN": {"avg": 64.92, "peak": 89.52, "min": 55.04}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 25.02, "sample_count": 6, "duration_seconds": 0.886}, "timestamp": "2026-01-17T17:48:40.288752"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 787.499, "latencies_ms": [787.499], "images_per_second": 1.27, "prompt_tokens": 8, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The living room features a white sofa, a coffee table, a television, a dining table with four chairs, and various decorative elements such as vases, books, and wall art.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.76, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 65.06, "peak": 94.62, "min": 54.36}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.76, "energy_joules_est": 22.67, "sample_count": 6, "duration_seconds": 0.788}, "timestamp": "2026-01-17T17:48:41.088185"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1126.097, "latencies_ms": [1126.097], "images_per_second": 0.888, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "sofa: 2\nchairs: 3\nrug: 1\ntable: 1\ntelevision: 1\nlamp: 1\nflowers: 1\nwall decor: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25376.6, "ram_available_mb": 100395.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25375.3, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.08, "peak": 35.44, "min": 21.27}, "VIN": {"avg": 65.15, "peak": 94.46, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.08, "energy_joules_est": 30.51, "sample_count": 8, "duration_seconds": 1.127}, "timestamp": "2026-01-17T17:48:42.220874"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1082.379, "latencies_ms": [1082.379], "images_per_second": 0.924, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The white sofa and wooden coffee table are positioned in the foreground, facing the window. The dining table and chairs are situated in the background, facing the window and television. The living room area is further back, separated from the dining area by a folding screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 26.58, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 62.16, "peak": 81.95, "min": 51.51}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.58, "energy_joules_est": 28.78, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T17:48:43.309705"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1003.522, "latencies_ms": [1003.522], "images_per_second": 0.996, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The living room features a modern and stylish design with natural light from a large window, complemented by colorful accents and decorative elements. The room is furnished with a white sofa, a coffee table, a television, and a dining table with chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.1, "ram_available_mb": 100397.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25375.3, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.17, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 64.4, "peak": 91.23, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.17, "energy_joules_est": 27.28, "sample_count": 7, "duration_seconds": 1.004}, "timestamp": "2026-01-17T17:48:44.321027"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1060.922, "latencies_ms": [1060.922], "images_per_second": 0.943, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The room features a warm color palette, primarily with cream, beige, and light brown tones. The lighting is soft and warm, creating a cozy atmosphere. The furniture includes white, wood, and red elements, adding contrast and visual interest to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.3, "ram_available_mb": 100396.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.63, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 62.23, "peak": 85.13, "min": 55.0}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.63, "energy_joules_est": 28.26, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T17:48:45.388763"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 611.755, "latencies_ms": [611.755], "images_per_second": 1.635, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A curious tabby cat with a blue collar sits atop a blue refrigerator in a kitchen, gazing upwards.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.32, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 64.3, "peak": 80.39, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.32, "energy_joules_est": 18.56, "sample_count": 4, "duration_seconds": 0.612}, "timestamp": "2026-01-17T17:48:46.011580"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1187.797, "latencies_ms": [1187.797], "images_per_second": 0.842, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Refrigerator: 2\nCabinet: 1\nLight fixture: 1\nCat: 1\nWall: 1\nFloor: 1\nMirror: 1\nMagnet: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.91, "peak": 37.01, "min": 20.89}, "VIN": {"avg": 65.7, "peak": 97.37, "min": 61.38}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.91, "energy_joules_est": 31.97, "sample_count": 9, "duration_seconds": 1.188}, "timestamp": "2026-01-17T17:48:47.205960"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 840.001, "latencies_ms": [840.001], "images_per_second": 1.19, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The cat is positioned near the refrigerator, seemingly looking up at it or observing something above. The refrigerator is situated in the foreground, while the cat is positioned slightly behind and above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.83, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 64.89, "peak": 92.3, "min": 53.77}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.83, "energy_joules_est": 23.39, "sample_count": 6, "duration_seconds": 0.84}, "timestamp": "2026-01-17T17:48:48.052405"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 750.972, "latencies_ms": [750.972], "images_per_second": 1.332, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A tabby cat with a blue collar is perched atop a blue refrigerator in a kitchen. The kitchen features a white cabinet and a light fixture above the refrigerator.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.14, "peak": 34.65, "min": 24.04}, "VIN": {"avg": 62.66, "peak": 73.02, "min": 54.64}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.14, "energy_joules_est": 21.9, "sample_count": 5, "duration_seconds": 0.751}, "timestamp": "2026-01-17T17:48:48.811118"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 815.707, "latencies_ms": [815.707], "images_per_second": 1.226, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The cat is brown and gray, wearing a blue collar. The kitchen has light-colored cabinets and a blue refrigerator. The lighting is bright, likely from overhead fluorescent fixtures.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 28.95, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 63.69, "peak": 78.56, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.95, "energy_joules_est": 23.63, "sample_count": 6, "duration_seconds": 0.816}, "timestamp": "2026-01-17T17:48:49.633356"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 675.819, "latencies_ms": [675.819], "images_per_second": 1.48, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The living room is decorated with colorful balloons, including a large yellow smiley face balloon, and scattered white paper confetti on the floor.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.33, "peak": 35.83, "min": 25.6}, "VIN": {"avg": 55.78, "peak": 61.2, "min": 42.35}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 20.52, "sample_count": 4, "duration_seconds": 0.677}, "timestamp": "2026-01-17T17:48:50.321125"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1346.755, "latencies_ms": [1346.755], "images_per_second": 0.743, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 1, "output_text": "smiley face balloon: 1\nrefrigerator: 2\nwooden shelving unit: 2\nbed: 1\nrug: 1\ntable: 1\nbookshelf: 2\nplant: 1\nchandelier: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25375.8, "ram_available_mb": 100396.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25377.6, "ram_available_mb": 100394.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 26.12, "peak": 37.03, "min": 20.09}, "VIN": {"avg": 62.7, "peak": 79.28, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.12, "energy_joules_est": 35.19, "sample_count": 10, "duration_seconds": 1.347}, "timestamp": "2026-01-17T17:48:51.674409"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 855.262, "latencies_ms": [855.262], "images_per_second": 1.169, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The refrigerator is positioned on the left side of the image, near the foreground. The bedroom is situated in the background, near the center of the image. The living area is further back, near the right edge of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.6, "ram_available_mb": 100394.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.5, "peak": 33.47, "min": 22.45}, "VIN": {"avg": 64.2, "peak": 86.61, "min": 55.39}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.5, "energy_joules_est": 23.53, "sample_count": 6, "duration_seconds": 0.856}, "timestamp": "2026-01-17T17:48:52.536460"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 881.895, "latencies_ms": [881.895], "images_per_second": 1.134, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The scene depicts a cozy, cluttered apartment with a small kitchen area, living room, and bedroom.  A birthday celebration is taking place, with balloons and smiley faces decorating the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.36, "peak": 36.24, "min": 22.85}, "VIN": {"avg": 65.94, "peak": 95.74, "min": 56.12}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.36, "energy_joules_est": 25.02, "sample_count": 6, "duration_seconds": 0.882}, "timestamp": "2026-01-17T17:48:53.425160"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 914.841, "latencies_ms": [914.841], "images_per_second": 1.093, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The room is decorated with yellow and blue balloons, creating a cheerful atmosphere. The wooden floor and light-colored walls contribute to the overall warm and cozy ambiance. The lighting appears to be artificial, likely from ceiling fixtures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.16, "peak": 36.23, "min": 22.85}, "VIN": {"avg": 64.38, "peak": 78.24, "min": 59.21}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.16, "energy_joules_est": 25.77, "sample_count": 6, "duration_seconds": 0.915}, "timestamp": "2026-01-17T17:48:54.346466"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 684.126, "latencies_ms": [684.126], "images_per_second": 1.462, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man with red hair, wearing a green jacket, is sitting in a train seat, focused on his laptop computer while wearing headphones.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.32, "peak": 35.04, "min": 25.99}, "VIN": {"avg": 65.81, "peak": 80.31, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.32, "energy_joules_est": 20.77, "sample_count": 4, "duration_seconds": 0.685}, "timestamp": "2026-01-17T17:48:55.041731"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 990.542, "latencies_ms": [990.542], "images_per_second": 1.01, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "laptop: 1\ntable: 1\nchair: 2\nwindow: 1\nman: 1\nheadphones: 1\njacket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 28.29, "peak": 35.83, "min": 22.44}, "VIN": {"avg": 62.49, "peak": 73.27, "min": 57.05}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.29, "energy_joules_est": 28.03, "sample_count": 7, "duration_seconds": 0.991}, "timestamp": "2026-01-17T17:48:56.038917"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 697.762, "latencies_ms": [697.762], "images_per_second": 1.433, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The main object is a laptop positioned in the foreground, close to the viewer. The background features the train window, suggesting the train is moving or traveling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.62, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 64.63, "peak": 79.01, "min": 57.71}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 20.68, "sample_count": 5, "duration_seconds": 0.698}, "timestamp": "2026-01-17T17:48:56.743129"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 698.106, "latencies_ms": [698.106], "images_per_second": 1.432, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A man is sitting in a train car, using a laptop while looking out the window. He appears to be focused on his work or browsing the internet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.09, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 63.38, "peak": 75.58, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.09, "energy_joules_est": 21.02, "sample_count": 5, "duration_seconds": 0.699}, "timestamp": "2026-01-17T17:48:57.447642"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 580.015, "latencies_ms": [580.015], "images_per_second": 1.724, "prompt_tokens": 18, "response_tokens_est": 21, "n_tiles": 1, "output_text": "The man is wearing a green jacket. The laptop is silver. The train appears to have natural lighting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25377.1, "ram_available_mb": 100395.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 32.21, "peak": 36.63, "min": 27.59}, "VIN": {"avg": 65.12, "peak": 77.88, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 32.21, "energy_joules_est": 18.7, "sample_count": 4, "duration_seconds": 0.58}, "timestamp": "2026-01-17T17:48:58.034259"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 658.308, "latencies_ms": [658.308], "images_per_second": 1.519, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A silver train is traveling on a track beneath a large white metal bridge, passing through a suburban area with buildings and trees.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25377.1, "ram_available_mb": 100395.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.9, "peak": 14.2, "min": 13.59}, "VDD_GPU": {"avg": 32.41, "peak": 37.03, "min": 27.59}, "VIN": {"avg": 63.82, "peak": 73.3, "min": 59.34}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 32.41, "energy_joules_est": 21.35, "sample_count": 4, "duration_seconds": 0.659}, "timestamp": "2026-01-17T17:48:58.705149"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 957.933, "latencies_ms": [957.933], "images_per_second": 1.044, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "bridge: 2\ntrain: 2\nroad: 1\nsky: 1\nclouds: 2\nbuildings: 2\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 28.7, "peak": 37.44, "min": 22.44}, "VIN": {"avg": 62.3, "peak": 83.13, "min": 54.58}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.7, "energy_joules_est": 27.51, "sample_count": 7, "duration_seconds": 0.958}, "timestamp": "2026-01-17T17:48:59.669304"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 852.879, "latencies_ms": [852.879], "images_per_second": 1.172, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The train is positioned in the foreground, moving away from the viewer. The road and bridge are located in the background, extending across the image. The train is situated further back, moving away from the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.35, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 65.58, "peak": 92.41, "min": 58.03}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.35, "energy_joules_est": 24.2, "sample_count": 6, "duration_seconds": 0.853}, "timestamp": "2026-01-17T17:49:00.529214"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 965.347, "latencies_ms": [965.347], "images_per_second": 1.036, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a modern, curved white bridge spanning a train yard or railway station. A silver train is traveling on the tracks beneath the bridge. The surrounding area features residential buildings and a hilly landscape in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.63, "peak": 35.45, "min": 22.07}, "VIN": {"avg": 64.09, "peak": 82.7, "min": 59.55}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 26.69, "sample_count": 7, "duration_seconds": 0.966}, "timestamp": "2026-01-17T17:49:01.501025"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1022.044, "latencies_ms": [1022.044], "images_per_second": 0.978, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The bridge is primarily white, contrasting with the blue sky and white clouds above. The lighting suggests a sunny day, with the sun casting shadows on the bridge and train tracks. The materials appear to be steel and concrete, typical of railway infrastructure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.94, "peak": 34.68, "min": 21.27}, "VIN": {"avg": 62.55, "peak": 83.35, "min": 56.46}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.94, "energy_joules_est": 27.55, "sample_count": 8, "duration_seconds": 1.023}, "timestamp": "2026-01-17T17:49:02.531032"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 668.248, "latencies_ms": [668.248], "images_per_second": 1.496, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A group of adults and children are gathered in a vibrant park, flying a large, colorful kite with orange, yellow, and blue hues.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 66.86, "peak": 90.53, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 19.56, "sample_count": 5, "duration_seconds": 0.669}, "timestamp": "2026-01-17T17:49:03.211508"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1003.741, "latencies_ms": [1003.741], "images_per_second": 0.996, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "kite: 1\nperson: 2\ngrass: 6\nfence: 1\nsoccer ball: 1\nchairs: 1\nbags: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.4, "peak": 36.23, "min": 22.44}, "VIN": {"avg": 63.14, "peak": 88.14, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.4, "energy_joules_est": 28.53, "sample_count": 7, "duration_seconds": 1.004}, "timestamp": "2026-01-17T17:49:04.222110"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 845.013, "latencies_ms": [845.013], "images_per_second": 1.183, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The kite is positioned in the foreground, close to the people. The field extends in the background, further away from the kite. The people are scattered across the field, some closer to the kite and others further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.42, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 62.29, "peak": 69.7, "min": 57.55}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 24.04, "sample_count": 6, "duration_seconds": 0.846}, "timestamp": "2026-01-17T17:49:05.073894"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1012.168, "latencies_ms": [1012.168], "images_per_second": 0.988, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The scene takes place in a park on a sunny day. A group of people, including children, gathers on a grassy field to fly a large, colorful kite. The kite has a dragon-like design and is tethered to the ground by strings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.79, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 62.39, "peak": 85.35, "min": 53.0}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.79, "energy_joules_est": 28.15, "sample_count": 7, "duration_seconds": 1.013}, "timestamp": "2026-01-17T17:49:06.097267"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 630.212, "latencies_ms": [630.212], "images_per_second": 1.587, "prompt_tokens": 18, "response_tokens_est": 23, "n_tiles": 1, "output_text": "The kite is predominantly blue with purple and yellow accents. The scene is brightly lit, suggesting sunny weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.4, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25377.3, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.63, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 62.43, "peak": 72.66, "min": 57.64}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 19.32, "sample_count": 4, "duration_seconds": 0.631}, "timestamp": "2026-01-17T17:49:06.734718"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 711.128, "latencies_ms": [711.128], "images_per_second": 1.406, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A group of miniature workers in orange uniforms are standing next to a red and black train car labeled \"Virgin\" on a set of train tracks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.3, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25377.3, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.65, "peak": 36.63, "min": 25.22}, "VIN": {"avg": 64.66, "peak": 80.41, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.65, "energy_joules_est": 21.81, "sample_count": 5, "duration_seconds": 0.712}, "timestamp": "2026-01-17T17:49:07.457325"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1296.813, "latencies_ms": [1296.813], "images_per_second": 0.771, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Train: 6\nTrain car: 2\nTrain tracks: 4\nTrain wires: 4\nTrain workers: 6\nTrain engine: 1\nTrain body: 1\nTrain wheels: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.3, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.03, "peak": 36.23, "min": 20.09}, "VIN": {"avg": 61.63, "peak": 78.4, "min": 54.42}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.03, "energy_joules_est": 33.76, "sample_count": 10, "duration_seconds": 1.297}, "timestamp": "2026-01-17T17:49:08.760255"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 770.403, "latencies_ms": [770.403], "images_per_second": 1.298, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The red and black train is positioned in the foreground, moving towards the left side of the image. The miniature model train track extends into the background, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25377.3, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 28.83, "peak": 33.47, "min": 24.02}, "VIN": {"avg": 63.54, "peak": 87.8, "min": 52.06}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.83, "energy_joules_est": 22.22, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T17:49:09.537110"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 805.901, "latencies_ms": [805.901], "images_per_second": 1.241, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A model train is traveling on tracks in a miniature model setting. Workers in orange uniforms are seen alongside the train, likely performing maintenance or preparing the tracks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.3, "ram_available_mb": 100394.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.81, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 66.6, "peak": 96.46, "min": 56.93}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.81, "energy_joules_est": 23.23, "sample_count": 6, "duration_seconds": 0.806}, "timestamp": "2026-01-17T17:49:10.349329"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1183.43, "latencies_ms": [1183.43], "images_per_second": 0.845, "prompt_tokens": 18, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The train is primarily red and black. The lighting in the image creates a dramatic effect, highlighting the colors of the train and the workers. The materials appear to be standard model train construction, with tracks, wheels, and electrical components. The weather appears to be overcast, contributing to the overall mood of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.34, "peak": 35.45, "min": 20.48}, "VIN": {"avg": 62.3, "peak": 75.68, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 31.18, "sample_count": 9, "duration_seconds": 1.184}, "timestamp": "2026-01-17T17:49:11.539847"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 611.5, "latencies_ms": [611.5], "images_per_second": 1.635, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A brown and white cat's fur is visible, appearing soft and slightly ruffled against the textured background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25377.1, "ram_available_mb": 100395.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25386.1, "ram_available_mb": 100386.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.12, "peak": 33.86, "min": 25.99}, "VIN": {"avg": 68.1, "peak": 89.35, "min": 57.3}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.12, "energy_joules_est": 18.43, "sample_count": 4, "duration_seconds": 0.612}, "timestamp": "2026-01-17T17:49:12.162795"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 828.474, "latencies_ms": [828.474], "images_per_second": 1.207, "prompt_tokens": 21, "response_tokens_est": 24, "n_tiles": 1, "output_text": "cat: 2\nblanket: 2\nfur: 2\nhair: 2\nskin: 2\nbody: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25386.1, "ram_available_mb": 100386.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.48, "peak": 36.24, "min": 23.64}, "VIN": {"avg": 64.51, "peak": 85.63, "min": 58.93}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.48, "energy_joules_est": 24.44, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T17:49:12.998184"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 870.401, "latencies_ms": [870.401], "images_per_second": 1.149, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The cat is positioned in the foreground, close to the beige textured fabric. The cat's fur is primarily light brown and white, contrasting with the background. The cat's position suggests it is close to the fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.14, "peak": 35.85, "min": 23.23}, "VIN": {"avg": 65.82, "peak": 87.51, "min": 57.65}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.14, "energy_joules_est": 25.37, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T17:49:13.874764"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 633.787, "latencies_ms": [633.787], "images_per_second": 1.578, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A cat with brown and white fur is resting on a textured, beige blanket. The blanket has a subtle pattern of triangles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.7, "peak": 34.64, "min": 26.37}, "VIN": {"avg": 66.97, "peak": 82.59, "min": 60.42}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.7, "energy_joules_est": 19.47, "sample_count": 4, "duration_seconds": 0.634}, "timestamp": "2026-01-17T17:49:14.514763"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 933.22, "latencies_ms": [933.22], "images_per_second": 1.072, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The cat's fur is predominantly light brown with white patches. The lighting appears to be soft and diffused, possibly from indoor lighting. The material appears to be a soft, textured fabric, possibly a blanket or bedspread.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25385.8, "ram_available_mb": 100386.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.18, "peak": 36.21, "min": 22.06}, "VIN": {"avg": 63.5, "peak": 82.45, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.18, "energy_joules_est": 26.31, "sample_count": 7, "duration_seconds": 0.934}, "timestamp": "2026-01-17T17:49:15.455834"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 654.199, "latencies_ms": [654.199], "images_per_second": 1.529, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A cow is standing in a milking parlor, receiving milk from a series of red and white tubes connected to a machine.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25385.8, "ram_available_mb": 100386.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.82, "peak": 35.06, "min": 26.39}, "VIN": {"avg": 67.77, "peak": 97.98, "min": 56.83}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.82, "energy_joules_est": 20.17, "sample_count": 4, "duration_seconds": 0.655}, "timestamp": "2026-01-17T17:49:16.122404"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1301.661, "latencies_ms": [1301.661], "images_per_second": 0.768, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Cow: 4\nMilking machine: 3\nMilk tub: 2\nPumps: 2\nPipes: 2\nMetal: 1\nWood: 1\nPlastic: 1\nMetal: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 26.47, "peak": 36.23, "min": 20.48}, "VIN": {"avg": 65.27, "peak": 96.61, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.47, "energy_joules_est": 34.46, "sample_count": 9, "duration_seconds": 1.302}, "timestamp": "2026-01-17T17:49:17.430947"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 840.964, "latencies_ms": [840.964], "images_per_second": 1.189, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The cow's legs are positioned in the foreground, partially obscuring the milking machine. The milking machine is situated in the background, partially obscured by the cow's legs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.82, "peak": 32.68, "min": 22.85}, "VIN": {"avg": 62.87, "peak": 76.89, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 27.82, "energy_joules_est": 23.41, "sample_count": 6, "duration_seconds": 0.841}, "timestamp": "2026-01-17T17:49:18.278343"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 716.381, "latencies_ms": [716.381], "images_per_second": 1.396, "prompt_tokens": 19, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A cow is milking a cow in a milking parlor. The parlor features milking equipment and materials scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 29.38, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 65.15, "peak": 86.18, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.38, "energy_joules_est": 21.06, "sample_count": 5, "duration_seconds": 0.717}, "timestamp": "2026-01-17T17:49:19.001060"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 923.604, "latencies_ms": [923.604], "images_per_second": 1.083, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The cow's legs are predominantly black with white patches. The lighting in the image appears to be artificial, likely from overhead fixtures. The cow's legs are made of metal and plastic, contributing to the overall industrial aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.14, "peak": 36.24, "min": 22.05}, "VIN": {"avg": 65.02, "peak": 96.31, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.14, "energy_joules_est": 26.0, "sample_count": 7, "duration_seconds": 0.924}, "timestamp": "2026-01-17T17:49:19.930696"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 613.7, "latencies_ms": [613.7], "images_per_second": 1.629, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A half-eaten sandwich with red filling and scattered seeds rests on a patterned plate, accompanied by a knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.32, "peak": 34.65, "min": 25.99}, "VIN": {"avg": 68.8, "peak": 95.0, "min": 56.17}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.32, "energy_joules_est": 18.62, "sample_count": 4, "duration_seconds": 0.614}, "timestamp": "2026-01-17T17:49:20.555628"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1019.081, "latencies_ms": [1019.081], "images_per_second": 0.981, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "sandwich: 3\nbread: 2\njelly: 1\nbutter: 1\nplate: 1\nknife: 1\ngreen napkin: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25385.6, "ram_available_mb": 100386.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.58, "peak": 37.01, "min": 22.45}, "VIN": {"avg": 63.15, "peak": 93.46, "min": 54.12}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.58, "energy_joules_est": 29.13, "sample_count": 7, "duration_seconds": 1.019}, "timestamp": "2026-01-17T17:49:21.580527"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 636.997, "latencies_ms": [636.997], "images_per_second": 1.57, "prompt_tokens": 25, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The sandwich is positioned in the foreground, partially obscuring the background. The knife is placed near the sandwich, close to the plate.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25385.9, "ram_available_mb": 100386.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25389.8, "ram_available_mb": 100382.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.73, "peak": 35.06, "min": 26.39}, "VIN": {"avg": 67.14, "peak": 84.14, "min": 59.2}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.73, "energy_joules_est": 19.58, "sample_count": 4, "duration_seconds": 0.637}, "timestamp": "2026-01-17T17:49:22.223185"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 785.479, "latencies_ms": [785.479], "images_per_second": 1.273, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A partially eaten sandwich with dark red filling sits on a patterned plate, accompanied by a knife. The setting appears to be a dimly lit dining table with a green napkin.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.8, "ram_available_mb": 100382.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25389.4, "ram_available_mb": 100382.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.4, "peak": 36.62, "min": 24.82}, "VIN": {"avg": 63.32, "peak": 75.23, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 30.4, "energy_joules_est": 23.89, "sample_count": 5, "duration_seconds": 0.786}, "timestamp": "2026-01-17T17:49:23.014780"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1019.877, "latencies_ms": [1019.877], "images_per_second": 0.981, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The sandwich is primarily light brown in color. The bread appears soft and slightly moist. The sandwich is cut in half, revealing a dark red filling. The plate has a subtle pattern and rests on a green napkin.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.4, "ram_available_mb": 100382.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25389.9, "ram_available_mb": 100382.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.13, "peak": 36.23, "min": 22.06}, "VIN": {"avg": 61.99, "peak": 73.15, "min": 57.55}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.13, "energy_joules_est": 28.7, "sample_count": 7, "duration_seconds": 1.02}, "timestamp": "2026-01-17T17:49:24.041533"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 719.359, "latencies_ms": [719.359], "images_per_second": 1.39, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The meal includes a variety of colorful and healthy food items, including pasta, vegetables, grapes, and carrots, arranged in four distinct containers.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25389.9, "ram_available_mb": 100382.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.32, "peak": 34.66, "min": 24.43}, "VIN": {"avg": 66.36, "peak": 86.46, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.32, "energy_joules_est": 21.1, "sample_count": 5, "duration_seconds": 0.72}, "timestamp": "2026-01-17T17:49:24.771851"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1194.695, "latencies_ms": [1194.695], "images_per_second": 0.837, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Pasta: 2\nGrapes: 6\nCarrots: 2\nQuinoa: 1\nTomato: 1\nBeans: 1\nMeat: 1\nCheese: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 26.35, "peak": 35.47, "min": 20.48}, "VIN": {"avg": 60.95, "peak": 79.0, "min": 55.2}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.35, "energy_joules_est": 31.5, "sample_count": 9, "duration_seconds": 1.195}, "timestamp": "2026-01-17T17:49:25.973465"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 873.532, "latencies_ms": [873.532], "images_per_second": 1.145, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main objects are arranged in a visually appealing manner, with the main dish (left) positioned in the foreground and the side dishes (right) in the background. The vegetables are placed near the right side of the main dish.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.77, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 65.27, "peak": 86.81, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.77, "energy_joules_est": 24.27, "sample_count": 6, "duration_seconds": 0.874}, "timestamp": "2026-01-17T17:49:26.853557"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 904.533, "latencies_ms": [904.533], "images_per_second": 1.106, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a colorful bento box meal on a dark surface. The meal includes a variety of food items such as pasta, vegetables, meat, and cheese, arranged in four separate containers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 27.41, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 63.74, "peak": 85.46, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.41, "energy_joules_est": 24.8, "sample_count": 7, "duration_seconds": 0.905}, "timestamp": "2026-01-17T17:49:27.764566"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 975.451, "latencies_ms": [975.451], "images_per_second": 1.025, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The meal is presented in a vibrant purple lunchbox. The food items are arranged in colorful containers, highlighting the variety of textures and colors. The lighting appears to be soft and warm, enhancing the visual appeal of the meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.64, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 66.74, "peak": 93.33, "min": 59.16}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.64, "energy_joules_est": 26.97, "sample_count": 7, "duration_seconds": 0.976}, "timestamp": "2026-01-17T17:49:28.746727"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 635.026, "latencies_ms": [635.026], "images_per_second": 1.575, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A traffic light with red lights illuminated is mounted on a pole above a tree covered in blooming cherry blossoms.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.7, "ram_available_mb": 100382.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25389.9, "ram_available_mb": 100382.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.62, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 63.04, "peak": 76.79, "min": 53.47}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 19.46, "sample_count": 4, "duration_seconds": 0.635}, "timestamp": "2026-01-17T17:49:29.400024"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 702.667, "latencies_ms": [702.667], "images_per_second": 1.423, "prompt_tokens": 21, "response_tokens_est": 21, "n_tiles": 1, "output_text": "traffic light: 3\ncherry blossoms: 10\ntrees: 10\nbuilding: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.9, "ram_available_mb": 100382.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25389.9, "ram_available_mb": 100382.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.08, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 30.72, "peak": 36.26, "min": 25.21}, "VIN": {"avg": 63.17, "peak": 80.04, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.72, "energy_joules_est": 21.6, "sample_count": 5, "duration_seconds": 0.703}, "timestamp": "2026-01-17T17:49:30.109277"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 722.607, "latencies_ms": [722.607], "images_per_second": 1.384, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The cherry blossoms are in the foreground, partially obscuring the traffic light. The traffic light is positioned slightly above and to the right of the blossoms.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25389.9, "ram_available_mb": 100382.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25389.9, "ram_available_mb": 100382.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 30.49, "peak": 36.65, "min": 24.81}, "VIN": {"avg": 63.21, "peak": 79.49, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.49, "energy_joules_est": 22.05, "sample_count": 5, "duration_seconds": 0.723}, "timestamp": "2026-01-17T17:49:30.838316"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1070.632, "latencies_ms": [1070.632], "images_per_second": 0.934, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene depicts a cherry blossom tree in full bloom, with its branches adorned with delicate pink flowers. A traffic light hangs above the tree, displaying a red light. The setting suggests a city street or roadway where these beautiful blossoms are present.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25389.9, "ram_available_mb": 100382.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25390.2, "ram_available_mb": 100382.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.28, "peak": 36.23, "min": 21.27}, "VIN": {"avg": 63.32, "peak": 93.36, "min": 53.24}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.28, "energy_joules_est": 29.21, "sample_count": 8, "duration_seconds": 1.071}, "timestamp": "2026-01-17T17:49:31.915525"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 837.014, "latencies_ms": [837.014], "images_per_second": 1.195, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The traffic light is red, indicating a stop. The cherry blossoms are pink and white, creating a beautiful contrast against the sky. The scene appears to be during springtime, with the blossoms in full bloom.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25390.2, "ram_available_mb": 100382.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25391.8, "ram_available_mb": 100380.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 28.43, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 64.73, "peak": 89.67, "min": 51.03}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.43, "energy_joules_est": 23.81, "sample_count": 6, "duration_seconds": 0.837}, "timestamp": "2026-01-17T17:49:32.758896"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 753.746, "latencies_ms": [753.746], "images_per_second": 1.327, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A white plate holds a generous serving of saut\u00e9ed broccoli, alongside a piece of grilled salmon, arranged neatly on a white tablecloth.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25391.8, "ram_available_mb": 100380.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25391.7, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.54, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 62.68, "peak": 75.8, "min": 56.5}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 22.28, "sample_count": 5, "duration_seconds": 0.754}, "timestamp": "2026-01-17T17:49:33.524071"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1247.206, "latencies_ms": [1247.206], "images_per_second": 0.802, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "broccoli: 8\nsalmon: 1\ngarlic: 1\nred pepper flakes: 1\nonions: 1\nbutter: 1\ngarlic powder: 1\nsauce: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25391.7, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25391.9, "ram_available_mb": 100380.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.74, "peak": 35.85, "min": 20.88}, "VIN": {"avg": 64.7, "peak": 98.28, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.74, "energy_joules_est": 33.36, "sample_count": 9, "duration_seconds": 1.248}, "timestamp": "2026-01-17T17:49:34.777636"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 593.016, "latencies_ms": [593.016], "images_per_second": 1.686, "prompt_tokens": 25, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The broccoli is positioned in the foreground, slightly to the left of the salmon. The salmon is placed in the background, behind the broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25391.9, "ram_available_mb": 100380.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25391.7, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.51, "min": 14.1}, "VDD_GPU": {"avg": 30.12, "peak": 33.86, "min": 25.99}, "VIN": {"avg": 69.74, "peak": 97.51, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.12, "energy_joules_est": 17.87, "sample_count": 4, "duration_seconds": 0.593}, "timestamp": "2026-01-17T17:49:35.376708"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1123.421, "latencies_ms": [1123.421], "images_per_second": 0.89, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The scene depicts a meal consisting of a piece of cooked salmon and steamed broccoli on a white plate. The broccoli appears seasoned and cooked, complementing the salmon's rich flavor. The setting suggests a home-cooked meal, possibly a family dinner or a casual gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25391.7, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25391.7, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.81, "peak": 36.6, "min": 21.66}, "VIN": {"avg": 62.87, "peak": 91.96, "min": 56.12}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.81, "energy_joules_est": 31.26, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-17T17:49:36.507125"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1005.604, "latencies_ms": [1005.604], "images_per_second": 0.994, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The broccoli appears vibrant green, contrasting with the golden-brown of the salmon. The broccoli and salmon are presented on a white plate, which highlights their colors and textures. The lighting in the image is soft and warm, creating a pleasant ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25391.7, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25391.6, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.17, "peak": 34.26, "min": 21.66}, "VIN": {"avg": 65.09, "peak": 85.44, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.17, "energy_joules_est": 27.33, "sample_count": 7, "duration_seconds": 1.006}, "timestamp": "2026-01-17T17:49:37.518983"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 550.905, "latencies_ms": [550.905], "images_per_second": 1.815, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A young boy is eating something while sitting at a table with two adults in a dimly lit restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25391.6, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25391.6, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.63, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 65.44, "peak": 83.95, "min": 56.18}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 16.89, "sample_count": 4, "duration_seconds": 0.551}, "timestamp": "2026-01-17T17:49:38.082749"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1046.921, "latencies_ms": [1046.921], "images_per_second": 0.955, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "woman: 2\nboy: 1\nman: 2\ntable: 2\nnapkins: 2\nfood: 1\nglass: 1\nchair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25391.6, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25392.1, "ram_available_mb": 100380.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.92, "peak": 37.03, "min": 21.66}, "VIN": {"avg": 62.28, "peak": 85.44, "min": 54.49}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.92, "energy_joules_est": 29.25, "sample_count": 8, "duration_seconds": 1.048}, "timestamp": "2026-01-17T17:49:39.137451"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 768.931, "latencies_ms": [768.931], "images_per_second": 1.301, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The man and woman are seated at a table in the foreground, while the boy is seated further back. The background is slightly out of focus, drawing attention to the people in the foreground.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25392.1, "ram_available_mb": 100380.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25392.1, "ram_available_mb": 100380.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.62, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 67.34, "peak": 96.64, "min": 56.7}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 22.79, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T17:49:39.916814"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 799.123, "latencies_ms": [799.123], "images_per_second": 1.251, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The scene takes place in a restaurant with people seated at tables. A young boy is eating something while two adults are nearby, possibly observing or enjoying their meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25392.1, "ram_available_mb": 100380.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25391.9, "ram_available_mb": 100380.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.88, "peak": 35.45, "min": 23.25}, "VIN": {"avg": 61.52, "peak": 69.18, "min": 56.17}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.88, "energy_joules_est": 23.1, "sample_count": 6, "duration_seconds": 0.8}, "timestamp": "2026-01-17T17:49:40.723272"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 971.566, "latencies_ms": [971.566], "images_per_second": 1.029, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The lighting in the restaurant is dim, creating a warm ambiance. The walls are painted a reddish-brown color, and the table is covered in white napkins. The scene suggests a casual dining experience in a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25391.9, "ram_available_mb": 100380.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25391.6, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.02, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 65.1, "peak": 83.05, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 28.02, "energy_joules_est": 27.24, "sample_count": 7, "duration_seconds": 0.972}, "timestamp": "2026-01-17T17:49:41.701173"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 574.582, "latencies_ms": [574.582], "images_per_second": 1.74, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A yellow bus is driving down a city street, passing a modern building with numerous windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25391.6, "ram_available_mb": 100380.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25391.4, "ram_available_mb": 100380.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.62, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 63.54, "peak": 77.29, "min": 53.65}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 17.61, "sample_count": 4, "duration_seconds": 0.575}, "timestamp": "2026-01-17T17:49:42.287641"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 964.873, "latencies_ms": [964.873], "images_per_second": 1.036, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "building: 10\nbus: 2\ncar: 1\ntrees: 4\nstreetlights: 2\nsidewalk: 2\nbush: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25391.4, "ram_available_mb": 100380.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25398.3, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.76, "peak": 36.63, "min": 22.45}, "VIN": {"avg": 62.9, "peak": 78.55, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 27.76, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T17:49:43.259599"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 884.914, "latencies_ms": [884.914], "images_per_second": 1.13, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The foreground features a yellow bus parked near a sidewalk, positioned to the right of the street. The background showcases a modern building with multiple windows, situated behind the bus and further away on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.42, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 65.02, "peak": 85.86, "min": 59.82}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 25.17, "sample_count": 6, "duration_seconds": 0.886}, "timestamp": "2026-01-17T17:49:44.150522"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 870.219, "latencies_ms": [870.219], "images_per_second": 1.149, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a city street with a yellow bus, a white bus, and a van parked near a modern building. The setting appears to be urban, with trees lining the street and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.76, "peak": 35.06, "min": 23.25}, "VIN": {"avg": 63.54, "peak": 76.04, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 25.04, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T17:49:45.027034"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 768.292, "latencies_ms": [768.292], "images_per_second": 1.302, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The building is primarily gray and light-colored. The street is paved with asphalt and features streetlights. The scene is well-lit, suggesting sunny weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.79, "peak": 35.07, "min": 24.82}, "VIN": {"avg": 65.27, "peak": 87.29, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.79, "energy_joules_est": 22.91, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T17:49:45.802250"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 571.011, "latencies_ms": [571.011], "images_per_second": 1.751, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A stop sign is mounted on a metal pole at the corner of a street, with a bus visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 31.71, "peak": 35.85, "min": 27.18}, "VIN": {"avg": 62.81, "peak": 77.45, "min": 54.44}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.71, "energy_joules_est": 18.13, "sample_count": 4, "duration_seconds": 0.572}, "timestamp": "2026-01-17T17:49:46.383914"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1098.425, "latencies_ms": [1098.425], "images_per_second": 0.91, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "stop sign: 1\npole: 1\nbus: 1\nbuildings: 2\nsign: 1\nrail: 1\ngrass: 2\nshadow: 1", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 28.01, "peak": 37.78, "min": 21.66}, "VIN": {"avg": 63.01, "peak": 79.85, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.01, "energy_joules_est": 30.78, "sample_count": 8, "duration_seconds": 1.099}, "timestamp": "2026-01-17T17:49:47.489994"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1000.621, "latencies_ms": [1000.621], "images_per_second": 0.999, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The stop sign is positioned in the foreground, slightly to the left of the image. The background features a street scene with a bus and buildings, suggesting an urban setting. The stop sign is situated near the center of the image, drawing the viewer's attention.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.34, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 62.3, "peak": 77.39, "min": 57.01}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.34, "energy_joules_est": 27.38, "sample_count": 7, "duration_seconds": 1.001}, "timestamp": "2026-01-17T17:49:48.501309"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 828.422, "latencies_ms": [828.422], "images_per_second": 1.207, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene is set in an urban area with a stop sign prominently displayed on a pole. The sun is shining brightly, casting a warm glow on the street and buildings, creating a pleasant atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25397.7, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.49, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 61.06, "peak": 70.86, "min": 57.6}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 23.61, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T17:49:49.336981"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 770.434, "latencies_ms": [770.434], "images_per_second": 1.298, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The stop sign is red and white. The lighting suggests it might be late afternoon or early evening. The stop sign appears to be made of metal and has a weathered appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.7, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25397.7, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.77, "peak": 35.04, "min": 24.42}, "VIN": {"avg": 65.24, "peak": 85.34, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.77, "energy_joules_est": 22.95, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T17:49:50.118009"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 652.371, "latencies_ms": [652.371], "images_per_second": 1.533, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A white and brown cat with green eyes lies on a black surface, gazing at the camera with its paws stretched out.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25397.7, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.18, "peak": 35.45, "min": 24.83}, "VIN": {"avg": 63.83, "peak": 76.35, "min": 57.92}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.18, "energy_joules_est": 19.71, "sample_count": 5, "duration_seconds": 0.653}, "timestamp": "2026-01-17T17:49:50.781614"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 988.18, "latencies_ms": [988.18], "images_per_second": 1.012, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "cat: 2\nmouse: 1\ncord: 1\ncouch: 1\npaws: 2\nfur: 2\neyes: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.47, "peak": 37.03, "min": 22.06}, "VIN": {"avg": 64.44, "peak": 88.7, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.47, "energy_joules_est": 28.15, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T17:49:51.776207"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 815.744, "latencies_ms": [815.744], "images_per_second": 1.226, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The cat is positioned in the foreground, slightly to the right of the mouse. The mouse is situated near the cat, closer to the viewer's perspective. The background is dark and out of focus.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.42, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 63.8, "peak": 89.22, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.42, "energy_joules_est": 23.19, "sample_count": 6, "duration_seconds": 0.816}, "timestamp": "2026-01-17T17:49:52.598536"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 908.626, "latencies_ms": [908.626], "images_per_second": 1.101, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A brown and white cat is lying on a dark surface, seemingly relaxed and possibly curious. A beige computer mouse is positioned nearby, suggesting the cat may be playing with or interacting with the device.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.0, "ram_available_mb": 100374.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25395.6, "ram_available_mb": 100376.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.43, "peak": 35.45, "min": 22.86}, "VIN": {"avg": 63.17, "peak": 76.49, "min": 56.46}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.43, "energy_joules_est": 25.84, "sample_count": 6, "duration_seconds": 0.909}, "timestamp": "2026-01-17T17:49:53.513333"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 890.57, "latencies_ms": [890.57], "images_per_second": 1.123, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The cat has striking green eyes and a coat of brown and white fur. The lighting is dim, creating a moody atmosphere. The cat is lying on a dark surface, possibly a couch or chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.6, "ram_available_mb": 100376.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25395.6, "ram_available_mb": 100376.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.49, "peak": 35.45, "min": 22.86}, "VIN": {"avg": 62.4, "peak": 77.31, "min": 52.71}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.49, "energy_joules_est": 25.39, "sample_count": 6, "duration_seconds": 0.891}, "timestamp": "2026-01-17T17:49:54.410681"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 695.633, "latencies_ms": [695.633], "images_per_second": 1.438, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A bustling city bus depot, filled with various colored buses, is situated amidst a cityscape with tall buildings and power lines overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.6, "ram_available_mb": 100376.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25395.6, "ram_available_mb": 100376.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.38, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 64.53, "peak": 79.95, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.38, "energy_joules_est": 20.45, "sample_count": 5, "duration_seconds": 0.696}, "timestamp": "2026-01-17T17:49:55.118440"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 892.34, "latencies_ms": [892.34], "images_per_second": 1.121, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "bus: 5\nbuses: 5\ntram: 1\nhighway: 1\nbuildings: 8\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.6, "ram_available_mb": 100376.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25395.6, "ram_available_mb": 100376.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.28, "peak": 36.63, "min": 23.24}, "VIN": {"avg": 63.31, "peak": 81.98, "min": 54.76}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.28, "energy_joules_est": 26.14, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T17:49:56.017259"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1016.16, "latencies_ms": [1016.16], "images_per_second": 0.984, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The main objects in the image are positioned in a spatial arrangement that suggests a perspective from above. The foreground features the buses and roadway, while the background showcases a cityscape with buildings, power lines, and possibly a tower in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.6, "ram_available_mb": 100376.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.92, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 63.55, "peak": 79.28, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 14.78, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.92, "energy_joules_est": 28.38, "sample_count": 7, "duration_seconds": 1.017}, "timestamp": "2026-01-17T17:49:57.044861"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 819.118, "latencies_ms": [819.118], "images_per_second": 1.221, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The scene depicts a busy city bus station with multiple buses parked and moving around. The setting is urban, with buildings, roads, and overhead power lines visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25395.6, "ram_available_mb": 100376.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.03, "peak": 33.88, "min": 22.86}, "VIN": {"avg": 66.99, "peak": 96.11, "min": 58.69}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.03, "energy_joules_est": 22.97, "sample_count": 6, "duration_seconds": 0.82}, "timestamp": "2026-01-17T17:49:57.870290"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1049.517, "latencies_ms": [1049.517], "images_per_second": 0.953, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The sky is partly cloudy with blue and white clouds. The lighting appears to be natural daylight, illuminating the scene. The buildings in the background are primarily white and gray, contributing to the urban setting. The overall atmosphere is bright and airy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.6, "ram_available_mb": 100376.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 26.53, "peak": 35.04, "min": 20.88}, "VIN": {"avg": 63.57, "peak": 87.71, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 14.71, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 26.53, "energy_joules_est": 27.86, "sample_count": 8, "duration_seconds": 1.05}, "timestamp": "2026-01-17T17:49:58.926558"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 666.631, "latencies_ms": [666.631], "images_per_second": 1.5, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A shirtless man wearing a straw hat is skillfully riding a skateboard up a ramp in a desert-like environment.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.06, "peak": 33.86, "min": 24.41}, "VIN": {"avg": 66.45, "peak": 88.95, "min": 59.7}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.06, "energy_joules_est": 19.39, "sample_count": 5, "duration_seconds": 0.667}, "timestamp": "2026-01-17T17:49:59.604946"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1198.516, "latencies_ms": [1198.516], "images_per_second": 0.834, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Skateboard: 1\nHat: 1\nShorts: 1\nWooden ramp: 1\nSkateboard: 1\nBicycle: 1\nTent: 2\nTable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.7, "peak": 36.23, "min": 20.89}, "VIN": {"avg": 63.92, "peak": 93.6, "min": 53.75}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.7, "energy_joules_est": 32.01, "sample_count": 9, "duration_seconds": 1.199}, "timestamp": "2026-01-17T17:50:00.809883"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1041.458, "latencies_ms": [1041.458], "images_per_second": 0.96, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, performing a trick on a ramp. The background features large tents, suggesting an outdoor event or gathering. The skateboarder's shadow is cast on the ground, indicating a relatively low sun angle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.34, "peak": 33.88, "min": 21.27}, "VIN": {"avg": 63.41, "peak": 81.85, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 27.44, "sample_count": 8, "duration_seconds": 1.042}, "timestamp": "2026-01-17T17:50:01.858440"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 818.629, "latencies_ms": [818.629], "images_per_second": 1.222, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The scene depicts a skateboarder performing a trick on a ramp in a desert-like environment. The backdrop features large tents, suggesting an outdoor event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25395.4, "ram_available_mb": 100376.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25395.6, "ram_available_mb": 100376.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.1, "peak": 34.68, "min": 22.85}, "VIN": {"avg": 62.56, "peak": 74.44, "min": 57.36}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.1, "energy_joules_est": 23.03, "sample_count": 6, "duration_seconds": 0.819}, "timestamp": "2026-01-17T17:50:02.687858"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1048.299, "latencies_ms": [1048.299], "images_per_second": 0.954, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The skateboarder is wearing light-colored shorts and a straw hat. The scene appears to be outdoors in bright sunlight. The skateboard is red and white. The background includes large green fabric structures, suggesting an outdoor event or festival.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25395.6, "ram_available_mb": 100376.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.52, "peak": 35.45, "min": 22.07}, "VIN": {"avg": 64.05, "peak": 89.11, "min": 51.91}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 28.86, "sample_count": 7, "duration_seconds": 1.049}, "timestamp": "2026-01-17T17:50:03.746735"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 485.24, "latencies_ms": [485.24], "images_per_second": 2.061, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A man is windsurfing in the ocean, surrounded by several colorful kites flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.2, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.2, "min": 13.59}, "VDD_GPU": {"avg": 24.55, "peak": 26.39, "min": 22.85}, "VIN": {"avg": 57.23, "peak": 60.61, "min": 53.75}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 24.55, "energy_joules_est": 11.92, "sample_count": 3, "duration_seconds": 0.486}, "timestamp": "2026-01-17T17:50:04.246924"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1108.646, "latencies_ms": [1108.646], "images_per_second": 0.902, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "kite: 5\nwindsurfboard: 1\nkite: 2\nkite: 1\nkite: 1\nkite: 1\nkite: 1\nkite: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25397.2, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.91, "min": 13.29}, "VDD_GPU": {"avg": 22.69, "peak": 27.17, "min": 20.09}, "VIN": {"avg": 59.78, "peak": 61.75, "min": 54.61}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 15.36, "min": 13.78}}, "power_watts_avg": 22.69, "energy_joules_est": 25.16, "sample_count": 8, "duration_seconds": 1.109}, "timestamp": "2026-01-17T17:50:05.362333"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 620.715, "latencies_ms": [620.715], "images_per_second": 1.611, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The windsurfer is positioned in the foreground, close to the water's edge. The kites are located in the background, further out from the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.2, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25397.2, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 23.44, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 58.91, "peak": 60.95, "min": 53.7}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.44, "energy_joules_est": 14.56, "sample_count": 4, "duration_seconds": 0.621}, "timestamp": "2026-01-17T17:50:05.989382"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 731.962, "latencies_ms": [731.962], "images_per_second": 1.366, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The scene depicts a windsurfer preparing to enter the ocean, alongside several other kite surfers. The setting is a sunny beach with waves crashing against the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.2, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25397.2, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 23.64, "peak": 26.8, "min": 21.27}, "VIN": {"avg": 60.32, "peak": 61.82, "min": 57.66}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.64, "energy_joules_est": 17.31, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T17:50:06.727442"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 973.494, "latencies_ms": [973.494], "images_per_second": 1.027, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The windsurfing equipment is primarily white and blue. The sky is bright blue with a few scattered clouds. The ocean appears dark blue with whitecaps. The scene suggests a sunny day with suitable wind conditions for windsurfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.2, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 22.51, "peak": 26.38, "min": 20.09}, "VIN": {"avg": 60.7, "peak": 63.97, "min": 57.6}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 22.51, "energy_joules_est": 21.92, "sample_count": 7, "duration_seconds": 0.974}, "timestamp": "2026-01-17T17:50:07.707238"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 573.988, "latencies_ms": [573.988], "images_per_second": 1.742, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "An old, rusted fire hydrant stands in a lush green lawn, contrasting with the vibrant surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.04, "peak": 33.89, "min": 26.0}, "VIN": {"avg": 64.7, "peak": 82.32, "min": 57.64}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.04, "energy_joules_est": 17.25, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T17:50:08.293069"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 893.297, "latencies_ms": [893.297], "images_per_second": 1.119, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "fire hydrant: 1\nhouse: 1\ntrees: 1\nflowers: 2\ngrass: 4\ndandelions: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.47, "peak": 36.62, "min": 23.64}, "VIN": {"avg": 61.43, "peak": 76.71, "min": 56.08}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.47, "energy_joules_est": 26.33, "sample_count": 6, "duration_seconds": 0.894}, "timestamp": "2026-01-17T17:50:09.192807"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 792.641, "latencies_ms": [792.641], "images_per_second": 1.262, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The red fire hydrant is positioned in the foreground, slightly to the right of the viewer. The house and trees in the background are situated further back, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.01, "peak": 35.85, "min": 24.81}, "VIN": {"avg": 63.0, "peak": 77.51, "min": 55.81}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.01, "energy_joules_est": 23.8, "sample_count": 5, "duration_seconds": 0.793}, "timestamp": "2026-01-17T17:50:09.991906"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 889.386, "latencies_ms": [889.386], "images_per_second": 1.124, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene is set in a residential area with a vibrant red fire hydrant in the foreground, situated on a grassy lawn near a house with purple flowers. The fire hydrant appears weathered and aged.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.81, "peak": 35.42, "min": 23.24}, "VIN": {"avg": 63.44, "peak": 82.37, "min": 52.57}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.81, "energy_joules_est": 25.63, "sample_count": 6, "duration_seconds": 0.89}, "timestamp": "2026-01-17T17:50:10.887584"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 798.575, "latencies_ms": [798.575], "images_per_second": 1.252, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The fire hydrant is predominantly red with black accents. The lighting suggests a sunny day, as evidenced by the bright colors of the hydrant and the surrounding grass.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.75, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 62.93, "peak": 81.75, "min": 54.33}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 22.98, "sample_count": 6, "duration_seconds": 0.799}, "timestamp": "2026-01-17T17:50:11.693354"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 725.012, "latencies_ms": [725.012], "images_per_second": 1.379, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A small bird with black and brown feathers is captured in mid-flight, approaching a wooden structure where several other birds are perched.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25397.1, "ram_available_mb": 100375.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.69, "peak": 35.06, "min": 24.81}, "VIN": {"avg": 67.74, "peak": 94.58, "min": 60.04}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.69, "energy_joules_est": 21.53, "sample_count": 5, "duration_seconds": 0.725}, "timestamp": "2026-01-17T17:50:12.428830"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 983.629, "latencies_ms": [983.629], "images_per_second": 1.017, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "bird: 3\nroof: 2\nwood: 2\nroof tiles: 2\ntwigs: 1\nleaves: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.07, "peak": 35.82, "min": 22.07}, "VIN": {"avg": 62.06, "peak": 75.31, "min": 57.4}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.07, "energy_joules_est": 27.62, "sample_count": 7, "duration_seconds": 0.984}, "timestamp": "2026-01-17T17:50:13.418883"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 887.351, "latencies_ms": [887.351], "images_per_second": 1.127, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The main object, a bird, is positioned in the foreground, close to the viewer. The background features the blue wooden surface, which extends into the distance. The bird is relatively close to the viewer, suggesting it is close to the viewer as well.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.29, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 62.54, "peak": 77.16, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 25.12, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T17:50:14.316761"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1031.996, "latencies_ms": [1031.996], "images_per_second": 0.969, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A bird is captured in mid-flight, seemingly preparing to land on a weathered wooden surface. Several other birds are perched nearby, observing the scene. The setting appears to be a rustic, outdoor environment with a weathered wooden wall or structure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.47, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 62.52, "peak": 78.85, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.47, "energy_joules_est": 28.37, "sample_count": 7, "duration_seconds": 1.033}, "timestamp": "2026-01-17T17:50:15.356722"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 769.659, "latencies_ms": [769.659], "images_per_second": 1.299, "prompt_tokens": 18, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The building's exterior is painted in a faded teal color. The lighting suggests an overcast day, giving the image a soft, diffused quality.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.23, "peak": 34.28, "min": 24.42}, "VIN": {"avg": 64.83, "peak": 82.35, "min": 59.64}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 22.51, "sample_count": 5, "duration_seconds": 0.77}, "timestamp": "2026-01-17T17:50:16.132825"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 738.082, "latencies_ms": [738.082], "images_per_second": 1.355, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A woman in blue jeans is leading a brown horse in a dirt-floored barn, guiding it with its reins as it trots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.85, "peak": 35.03, "min": 24.82}, "VIN": {"avg": 62.17, "peak": 77.86, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.85, "energy_joules_est": 22.05, "sample_count": 5, "duration_seconds": 0.739}, "timestamp": "2026-01-17T17:50:16.883526"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1093.659, "latencies_ms": [1093.659], "images_per_second": 0.914, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "door: 1\nladder: 1\nhorse: 1\nwoman: 1\ntable: 1\nbucket: 1\nchair: 1\nwheelbarrow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 26.98, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 62.64, "peak": 78.43, "min": 57.32}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 26.98, "energy_joules_est": 29.52, "sample_count": 8, "duration_seconds": 1.094}, "timestamp": "2026-01-17T17:50:17.983832"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 692.453, "latencies_ms": [692.453], "images_per_second": 1.444, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The horse is positioned in the foreground, moving towards the left side of the image. The woman is standing near the horse, seemingly observing or guiding it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.2, "ram_available_mb": 100374.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.31, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 65.75, "peak": 84.28, "min": 60.34}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 29.31, "energy_joules_est": 20.31, "sample_count": 5, "duration_seconds": 0.693}, "timestamp": "2026-01-17T17:50:18.682713"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1036.435, "latencies_ms": [1036.435], "images_per_second": 0.965, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "A woman is leading a brown horse in a rustic barn. The horse is trotting and wearing a saddle, indicating it is being trained or exercised. Various items, including a ladder, buckets, and a workbench, are scattered around the stable.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.6, "ram_available_mb": 100373.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.96, "peak": 35.83, "min": 22.06}, "VIN": {"avg": 63.74, "peak": 83.67, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 27.96, "energy_joules_est": 29.0, "sample_count": 7, "duration_seconds": 1.037}, "timestamp": "2026-01-17T17:50:19.726872"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 811.503, "latencies_ms": [811.503], "images_per_second": 1.232, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The horse is brown and white. The lighting is natural, likely from sunlight entering through the open door. The barn's interior is constructed of wood and has a dirt floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.96, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 63.69, "peak": 83.23, "min": 56.62}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.96, "energy_joules_est": 22.7, "sample_count": 6, "duration_seconds": 0.812}, "timestamp": "2026-01-17T17:50:20.544767"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 656.071, "latencies_ms": [656.071], "images_per_second": 1.524, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A group of zebras and rhinoceroses are grazing peacefully in a lush, grassy field enclosed by a wooden fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.91, "peak": 35.04, "min": 26.77}, "VIN": {"avg": 68.58, "peak": 96.65, "min": 53.85}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.91, "energy_joules_est": 20.29, "sample_count": 4, "duration_seconds": 0.656}, "timestamp": "2026-01-17T17:50:21.213657"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1031.199, "latencies_ms": [1031.199], "images_per_second": 0.97, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "rhino: 2\nzebra: 2\nhorse: 1\nwildebeest: 2\ntree: 1\nrocks: 2\nwater: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 28.25, "peak": 36.24, "min": 22.06}, "VIN": {"avg": 66.11, "peak": 89.04, "min": 59.87}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.25, "energy_joules_est": 29.14, "sample_count": 7, "duration_seconds": 1.032}, "timestamp": "2026-01-17T17:50:22.250889"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 868.213, "latencies_ms": [868.213], "images_per_second": 1.152, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The animals are positioned in a grassy field with a backdrop of trees, trees in the background, and trees further back. The animals are spread across the field, with some closer to the foreground and others further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.69, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 65.51, "peak": 92.86, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.69, "energy_joules_est": 24.92, "sample_count": 6, "duration_seconds": 0.869}, "timestamp": "2026-01-17T17:50:23.125319"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 873.172, "latencies_ms": [873.172], "images_per_second": 1.145, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a grassy field with various animals, including zebras, grazing and roaming. The setting appears to be a zoo or wildlife park, with a backdrop of trees and a small body of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.82, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 62.25, "peak": 78.23, "min": 56.51}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 28.82, "energy_joules_est": 25.18, "sample_count": 6, "duration_seconds": 0.874}, "timestamp": "2026-01-17T17:50:24.009341"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 829.945, "latencies_ms": [829.945], "images_per_second": 1.205, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The grass is a vibrant green, illuminated by sunlight, creating a bright and lively atmosphere. The sky is partly cloudy, casting a soft, diffused light over the scene.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.23, "peak": 34.27, "min": 22.85}, "VIN": {"avg": 63.54, "peak": 82.56, "min": 55.56}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 23.44, "sample_count": 6, "duration_seconds": 0.83}, "timestamp": "2026-01-17T17:50:24.849205"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 825.864, "latencies_ms": [825.864], "images_per_second": 1.211, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A horse-drawn trolley car, labeled \"Disneyland,\" travels down a street lined with trees and people, with yellow umbrellas providing shade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.69, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 64.64, "peak": 92.12, "min": 53.51}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 23.7, "sample_count": 6, "duration_seconds": 0.826}, "timestamp": "2026-01-17T17:50:25.685942"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1077.739, "latencies_ms": [1077.739], "images_per_second": 0.928, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "horse: 1\ncarriage: 1\nperson: 2\nbench: 1\ntree: 1\nstreet: 1\numbrella: 1\npeople: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.28, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 62.81, "peak": 80.88, "min": 56.31}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.28, "energy_joules_est": 29.41, "sample_count": 8, "duration_seconds": 1.078}, "timestamp": "2026-01-17T17:50:26.769281"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1300.244, "latencies_ms": [1300.244], "images_per_second": 0.769, "prompt_tokens": 25, "response_tokens_est": 73, "n_tiles": 1, "output_text": "The main object is a trolley car moving down the street, positioned in the foreground and slightly to the left of the horse and carriage. The trolley car is closer to the viewer, while the horse and carriage are further away in the background. The trolley car appears to be traveling on a paved road, while the horse and carriage are on a sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.8, "ram_available_mb": 100374.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25397.5, "ram_available_mb": 100374.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 25.09, "peak": 33.86, "min": 20.09}, "VIN": {"avg": 63.6, "peak": 96.0, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 25.09, "energy_joules_est": 32.63, "sample_count": 10, "duration_seconds": 1.301}, "timestamp": "2026-01-17T17:50:28.074970"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 863.536, "latencies_ms": [863.536], "images_per_second": 1.158, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The scene depicts a horse-drawn carriage traveling down a street, possibly at a theme park or fair. The carriage is decorated and carries passengers, blending traditional transportation with modern attractions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.5, "ram_available_mb": 100374.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25397.5, "ram_available_mb": 100374.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 27.89, "peak": 33.46, "min": 22.84}, "VIN": {"avg": 63.69, "peak": 83.08, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.89, "energy_joules_est": 24.1, "sample_count": 6, "duration_seconds": 0.864}, "timestamp": "2026-01-17T17:50:28.944833"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1087.247, "latencies_ms": [1087.247], "images_per_second": 0.92, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The trolley is painted in vibrant colors, including red, green, and gold. The lighting is bright, likely from sunlight, creating a pleasant atmosphere. The trolley appears to be made of sturdy materials, suitable for outdoor use. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.5, "ram_available_mb": 100374.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.83, "peak": 34.66, "min": 21.27}, "VIN": {"avg": 61.05, "peak": 77.39, "min": 54.16}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.83, "energy_joules_est": 29.19, "sample_count": 8, "duration_seconds": 1.088}, "timestamp": "2026-01-17T17:50:30.038482"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 573.42, "latencies_ms": [573.42], "images_per_second": 1.744, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A man wearing glasses is sitting on a green bench reading a newspaper, while other individuals are seated on similar benches nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25397.0, "ram_available_mb": 100375.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 25.99, "min": 21.66}, "VIN": {"avg": 60.44, "peak": 61.7, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.63, "energy_joules_est": 13.57, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T17:50:30.621345"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1299.906, "latencies_ms": [1299.906], "images_per_second": 0.769, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 1, "output_text": "Bench: 4\nTrash can: 3\nNewspaper: 1\nMan: 2\nMan in red shirt: 1\nMan in blue shirt: 1\nMan in brown shirt: 1\nBuilding: 1\nStairs: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.0, "ram_available_mb": 100375.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 22.02, "peak": 26.79, "min": 19.7}, "VIN": {"avg": 60.41, "peak": 63.11, "min": 54.12}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.02, "energy_joules_est": 28.64, "sample_count": 9, "duration_seconds": 1.301}, "timestamp": "2026-01-17T17:50:31.931691"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1007.003, "latencies_ms": [1007.003], "images_per_second": 0.993, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The main objects are positioned in a line or row, creating a sense of proximity and order. The foreground features the man reading the newspaper, while the background includes other individuals and additional benches. The scene appears to be situated in a public space, possibly a sidewalk or plaza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.0, "peak": 25.21, "min": 20.08}, "VIN": {"avg": 60.3, "peak": 62.28, "min": 59.23}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.0, "energy_joules_est": 22.16, "sample_count": 7, "duration_seconds": 1.007}, "timestamp": "2026-01-17T17:50:32.947448"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 750.849, "latencies_ms": [750.849], "images_per_second": 1.332, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene takes place on a sidewalk in front of a building with several people sitting on benches, reading newspapers. The setting appears to be an urban environment, possibly a public space or sidewalk area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.0, "peak": 25.59, "min": 20.87}, "VIN": {"avg": 60.77, "peak": 62.62, "min": 56.42}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.0, "energy_joules_est": 17.28, "sample_count": 5, "duration_seconds": 0.751}, "timestamp": "2026-01-17T17:50:33.704102"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 770.865, "latencies_ms": [770.865], "images_per_second": 1.297, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The chairs are dark green metal with ornate detailing. The lighting is bright, likely from street lamps, creating a well-lit outdoor space. The chairs appear to be made of metal and have a sturdy construction.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25397.3, "ram_available_mb": 100374.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25400.2, "ram_available_mb": 100371.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.23, "peak": 26.38, "min": 20.87}, "VIN": {"avg": 59.57, "peak": 60.76, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.23, "energy_joules_est": 17.91, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T17:50:34.481731"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 715.496, "latencies_ms": [715.496], "images_per_second": 1.398, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A wooden desk with a laptop, glass of orange juice, and a lamp is situated against a white wall, accompanied by a framed picture and a telephone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.2, "ram_available_mb": 100371.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 28.76, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 67.76, "peak": 93.79, "min": 58.29}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 20.58, "sample_count": 5, "duration_seconds": 0.716}, "timestamp": "2026-01-17T17:50:35.207236"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1262.57, "latencies_ms": [1262.57], "images_per_second": 0.792, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "laptop: 1\ntelephone: 1\ndesk: 1\nlamp: 1\nglass: 1\npicture frame: 1\npapers: 1\ncup: 1\ntray: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25399.3, "ram_available_mb": 100372.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.35, "peak": 35.83, "min": 20.49}, "VIN": {"avg": 62.23, "peak": 75.81, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.35, "energy_joules_est": 33.28, "sample_count": 9, "duration_seconds": 1.263}, "timestamp": "2026-01-17T17:50:36.475549"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 940.224, "latencies_ms": [940.224], "images_per_second": 1.064, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The laptop is positioned in the foreground, slightly to the left of the image. The desk is situated in the background, extending from the left edge to the right edge of the image. The picture hangs on the wall behind the desk, providing a backdrop for the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.3, "ram_available_mb": 100372.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.12, "peak": 33.5, "min": 21.66}, "VIN": {"avg": 63.96, "peak": 84.36, "min": 59.08}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.12, "energy_joules_est": 25.51, "sample_count": 7, "duration_seconds": 0.941}, "timestamp": "2026-01-17T17:50:37.421644"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1009.404, "latencies_ms": [1009.404], "images_per_second": 0.991, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scene depicts a home office setup with a wooden desk, a laptop, a telephone, a lamp, a glass of orange juice, and a picture on the wall. The desk is illuminated by the lamp, casting a warm glow on the surrounding area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.51, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 64.22, "peak": 90.56, "min": 51.47}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.51, "energy_joules_est": 27.78, "sample_count": 7, "duration_seconds": 1.01}, "timestamp": "2026-01-17T17:50:38.437142"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1068.733, "latencies_ms": [1068.733], "images_per_second": 0.936, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The room is lit by warm yellow light from a lamp with a black shade. The desk is made of dark wood and has a laptop, papers, a glass, and a telephone on it. The walls are painted a light color, and the overall atmosphere is cozy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25398.6, "ram_available_mb": 100373.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.53, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 63.35, "peak": 83.75, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.53, "energy_joules_est": 28.36, "sample_count": 8, "duration_seconds": 1.069}, "timestamp": "2026-01-17T17:50:39.512483"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 777.983, "latencies_ms": [777.983], "images_per_second": 1.285, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Two surfboards lean against the sand near a beach umbrella, accompanied by beach chairs, towels, and a cooler, with people enjoying the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 25398.6, "ram_available_mb": 100373.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.1, "peak": 34.27, "min": 25.22}, "VIN": {"avg": 64.09, "peak": 81.99, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 30.1, "energy_joules_est": 23.43, "sample_count": 5, "duration_seconds": 0.779}, "timestamp": "2026-01-17T17:50:40.302857"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1226.205, "latencies_ms": [1226.205], "images_per_second": 0.816, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "beach towel: 2\nsurfboard: 2\numbrella: 1\nchair: 2\ncooler: 1\nbags: 2\nsandals: 1\nfrisbee: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.27, "peak": 36.24, "min": 20.88}, "VIN": {"avg": 62.59, "peak": 78.16, "min": 52.82}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.27, "energy_joules_est": 33.46, "sample_count": 9, "duration_seconds": 1.227}, "timestamp": "2026-01-17T17:50:41.535650"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1001.101, "latencies_ms": [1001.101], "images_per_second": 0.999, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The foreground features the beach setup with towels, surfboards, and a cooler, positioned near the water's edge. The background showcases the ocean with waves and a clear sky, providing a serene backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.74, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 61.86, "peak": 79.14, "min": 55.05}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.74, "energy_joules_est": 27.78, "sample_count": 7, "duration_seconds": 1.001}, "timestamp": "2026-01-17T17:50:42.542678"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 855.455, "latencies_ms": [855.455], "images_per_second": 1.169, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a beach with several surfboards, towels, and a cooler, arranged on the sand. A green umbrella provides shade, and a person is visible in the distance near the ocean.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.28, "peak": 34.66, "min": 23.64}, "VIN": {"avg": 64.98, "peak": 90.36, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.28, "energy_joules_est": 25.07, "sample_count": 6, "duration_seconds": 0.856}, "timestamp": "2026-01-17T17:50:43.404812"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1100.759, "latencies_ms": [1100.759], "images_per_second": 0.908, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The beach scene features a bright blue sky and light blue ocean waves, creating a serene atmosphere. The sand is light-colored and appears smooth. Various beach items are scattered on the sand, including two surfboards, a beach towel, and beach chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.96, "peak": 35.85, "min": 21.66}, "VIN": {"avg": 62.68, "peak": 84.73, "min": 54.44}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.96, "energy_joules_est": 30.79, "sample_count": 8, "duration_seconds": 1.101}, "timestamp": "2026-01-17T17:50:44.512272"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 577.229, "latencies_ms": [577.229], "images_per_second": 1.732, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A white sheep with black markings on its face and legs stands confidently atop a large, rugged rock formation, gazing towards the horizon.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25398.9, "ram_available_mb": 100373.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 24.13, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 59.53, "peak": 63.01, "min": 56.09}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.13, "energy_joules_est": 13.94, "sample_count": 4, "duration_seconds": 0.578}, "timestamp": "2026-01-17T17:50:45.098751"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 909.043, "latencies_ms": [909.043], "images_per_second": 1.1, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "rock: 10\nsheep: 1\nsky: 10\nclouds: 10\ngrass: 10\nhills: 1\nstaircase: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.18, "peak": 26.8, "min": 20.88}, "VIN": {"avg": 61.27, "peak": 63.5, "min": 59.48}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.18, "energy_joules_est": 21.09, "sample_count": 6, "duration_seconds": 0.91}, "timestamp": "2026-01-17T17:50:46.014166"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 661.762, "latencies_ms": [661.762], "images_per_second": 1.511, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The sheep is positioned prominently in the foreground, contrasting with the rocky terrain in the background. The sheep appears to be standing relatively close to the viewer, emphasizing its prominence in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 23.64, "peak": 26.0, "min": 21.67}, "VIN": {"avg": 60.63, "peak": 62.66, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.64, "energy_joules_est": 15.65, "sample_count": 4, "duration_seconds": 0.662}, "timestamp": "2026-01-17T17:50:46.685870"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 558.76, "latencies_ms": [558.76], "images_per_second": 1.79, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A white sheep stands on a rocky outcrop, gazing towards the horizon. The scene is set against a clear blue sky with fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.97, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 24.13, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 59.05, "peak": 61.41, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 24.13, "energy_joules_est": 13.5, "sample_count": 4, "duration_seconds": 0.56}, "timestamp": "2026-01-17T17:50:47.251332"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 719.624, "latencies_ms": [719.624], "images_per_second": 1.39, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The sky is a bright blue with scattered white clouds. The rocky terrain is dark gray and appears to be made of slate or similar rock. The lighting suggests a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.61, "min": 13.49}, "VDD_GPU": {"avg": 23.71, "peak": 27.18, "min": 21.27}, "VIN": {"avg": 60.15, "peak": 63.75, "min": 57.67}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.71, "energy_joules_est": 17.07, "sample_count": 5, "duration_seconds": 0.72}, "timestamp": "2026-01-17T17:50:47.977308"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 661.242, "latencies_ms": [661.242], "images_per_second": 1.512, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A woman with vibrant blue hair and a blue collared shirt takes a selfie while holding a colorful phone in her right hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.4, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 29.31, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 66.41, "peak": 86.65, "min": 60.45}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 29.31, "energy_joules_est": 19.4, "sample_count": 5, "duration_seconds": 0.662}, "timestamp": "2026-01-17T17:50:48.650982"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1042.412, "latencies_ms": [1042.412], "images_per_second": 0.959, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "phone: 1\ntie: 1\nshirt: 2\nhair: 1\nring: 1\nmirror: 1\nwall: 1\nhand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.3, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.66, "peak": 36.23, "min": 21.66}, "VIN": {"avg": 63.85, "peak": 77.99, "min": 59.79}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.66, "energy_joules_est": 28.85, "sample_count": 8, "duration_seconds": 1.043}, "timestamp": "2026-01-17T17:50:49.700108"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 731.27, "latencies_ms": [731.27], "images_per_second": 1.367, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The woman is taking a selfie with her phone held in her right hand. The phone is positioned in the foreground, while the woman's reflection is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.55, "peak": 34.27, "min": 24.83}, "VIN": {"avg": 64.98, "peak": 87.61, "min": 56.69}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 29.55, "energy_joules_est": 21.62, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T17:50:50.436997"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 848.06, "latencies_ms": [848.06], "images_per_second": 1.179, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A woman with vibrant blue hair is taking a selfie in a bathroom mirror, wearing a blue shirt and tie. The mirror reflects her image, showcasing her colorful hair and outfit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.29, "peak": 35.86, "min": 23.64}, "VIN": {"avg": 66.97, "peak": 96.22, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.29, "energy_joules_est": 24.86, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T17:50:51.291704"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1026.094, "latencies_ms": [1026.094], "images_per_second": 0.975, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The woman's hair is a vibrant shade of blue. The lighting appears to be natural, possibly from a window, casting a soft glow on her face and hair. The shirt appears to be made of a sturdy, textured material, possibly denim or a similar fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.98, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 61.51, "peak": 76.71, "min": 55.13}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.98, "energy_joules_est": 27.7, "sample_count": 8, "duration_seconds": 1.027}, "timestamp": "2026-01-17T17:50:52.323735"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 784.239, "latencies_ms": [784.239], "images_per_second": 1.275, "prompt_tokens": 8, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The room features a fireplace with a black mantelpiece, a wooden cabinet with two doors, a wooden table with a book, and two chairs, one of which is a rolling office chair.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25397.8, "ram_available_mb": 100374.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.38, "peak": 35.06, "min": 24.42}, "VIN": {"avg": 65.4, "peak": 79.87, "min": 60.23}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.38, "energy_joules_est": 23.06, "sample_count": 5, "duration_seconds": 0.785}, "timestamp": "2026-01-17T17:50:53.119223"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1141.197, "latencies_ms": [1141.197], "images_per_second": 0.876, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "fireplace: 2\nvases: 2\noil lamp: 1\npicture frame: 1\nwooden cabinet: 1\ndesk: 1\nchair: 2\ntable: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25397.8, "ram_available_mb": 100374.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.28, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 65.28, "peak": 95.8, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.28, "energy_joules_est": 31.15, "sample_count": 8, "duration_seconds": 1.142}, "timestamp": "2026-01-17T17:50:54.266925"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 866.001, "latencies_ms": [866.001], "images_per_second": 1.155, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The fireplace is positioned to the left of the wooden table and chairs, creating a sense of spatial proximity. The table and chairs are situated in the foreground, closer to the viewer, while the fireplace occupies the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 27.84, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 63.38, "peak": 81.4, "min": 55.81}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.84, "energy_joules_est": 24.13, "sample_count": 6, "duration_seconds": 0.867}, "timestamp": "2026-01-17T17:50:55.138965"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 888.612, "latencies_ms": [888.612], "images_per_second": 1.125, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The scene depicts a cozy, antique-looking room with a fireplace, wooden furniture, and artwork on the walls. A small table with books is situated in the center of the room, alongside two chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.41, "peak": 34.64, "min": 23.24}, "VIN": {"avg": 65.62, "peak": 90.69, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.41, "energy_joules_est": 25.26, "sample_count": 6, "duration_seconds": 0.889}, "timestamp": "2026-01-17T17:50:56.033958"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 985.854, "latencies_ms": [985.854], "images_per_second": 1.014, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The room features a warm color palette, primarily due to the fireplace and wooden furniture. The lighting is soft and diffused, creating a cozy atmosphere. The materials include dark wood, metal, and fabric, adding to the antique ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.79, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 66.44, "peak": 97.4, "min": 59.37}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.79, "energy_joules_est": 27.41, "sample_count": 7, "duration_seconds": 0.986}, "timestamp": "2026-01-17T17:50:57.025475"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 696.573, "latencies_ms": [696.573], "images_per_second": 1.436, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A brown dog jumps to catch a red frisbee in a green yard with a tree, bushes, and a parked car in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.25, "peak": 35.06, "min": 25.21}, "VIN": {"avg": 65.63, "peak": 83.36, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 30.25, "energy_joules_est": 21.1, "sample_count": 5, "duration_seconds": 0.697}, "timestamp": "2026-01-17T17:50:57.737634"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1014.68, "latencies_ms": [1014.68], "images_per_second": 0.986, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "tree: 1\nfrisbee: 1\ndog: 1\ngrass: 1\ncar: 1\nshrubs: 2\nmulch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 29.14, "peak": 37.01, "min": 22.45}, "VIN": {"avg": 62.56, "peak": 81.74, "min": 55.25}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.14, "energy_joules_est": 29.58, "sample_count": 7, "duration_seconds": 1.015}, "timestamp": "2026-01-17T17:50:58.758305"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 779.106, "latencies_ms": [779.106], "images_per_second": 1.284, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The black car is positioned in the background, slightly to the right of the dog. The dog is positioned in the foreground, mid-jump to catch the red frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 30.71, "peak": 35.04, "min": 25.59}, "VIN": {"avg": 61.6, "peak": 72.18, "min": 57.08}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.71, "energy_joules_est": 23.94, "sample_count": 5, "duration_seconds": 0.78}, "timestamp": "2026-01-17T17:50:59.543545"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 683.865, "latencies_ms": [683.865], "images_per_second": 1.462, "prompt_tokens": 19, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A brown dog is jumping to catch a red frisbee in a grassy yard. A black car is parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 31.26, "peak": 36.23, "min": 25.59}, "VIN": {"avg": 63.79, "peak": 82.71, "min": 56.29}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.26, "energy_joules_est": 21.4, "sample_count": 5, "duration_seconds": 0.685}, "timestamp": "2026-01-17T17:51:00.233858"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 953.483, "latencies_ms": [953.483], "images_per_second": 1.049, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The dog is dark brown and appears to be jumping to catch a red frisbee. The scene is brightly lit, suggesting sunny weather. The dog's fur is thick and appears to be well-groomed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25398.5, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.6, "peak": 37.41, "min": 22.86}, "VIN": {"avg": 63.93, "peak": 81.48, "min": 59.19}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.6, "energy_joules_est": 28.23, "sample_count": 7, "duration_seconds": 0.954}, "timestamp": "2026-01-17T17:51:01.193278"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 533.66, "latencies_ms": [533.66], "images_per_second": 1.874, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A giraffe with brown and white spots and large, expressive eyes gazes directly at the camera in a lush, green environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.5, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 25.34, "peak": 27.18, "min": 23.64}, "VIN": {"avg": 59.88, "peak": 61.76, "min": 57.52}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.34, "energy_joules_est": 13.54, "sample_count": 3, "duration_seconds": 0.534}, "timestamp": "2026-01-17T17:51:01.737140"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1165.916, "latencies_ms": [1165.916], "images_per_second": 0.858, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "giraffe: 1\ntree: 1\nleaves: 1\ngiraffe's head: 1\ngiraffe's neck: 1\ngiraffe's ears: 2\ngiraffe's mouth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 22.6, "peak": 27.57, "min": 19.7}, "VIN": {"avg": 60.14, "peak": 61.89, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 22.6, "energy_joules_est": 26.36, "sample_count": 8, "duration_seconds": 1.166}, "timestamp": "2026-01-17T17:51:02.913003"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 656.538, "latencies_ms": [656.538], "images_per_second": 1.523, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The giraffe is positioned in the foreground, with its head and neck dominating the left side of the image. The background consists of blurred green foliage, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.8, "ram_available_mb": 100373.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25398.5, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 23.25, "peak": 25.22, "min": 21.66}, "VIN": {"avg": 60.29, "peak": 62.4, "min": 57.18}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.25, "energy_joules_est": 15.27, "sample_count": 4, "duration_seconds": 0.657}, "timestamp": "2026-01-17T17:51:03.575478"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1027.766, "latencies_ms": [1027.766], "images_per_second": 0.973, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The scene is set in a lush, green environment, possibly a zoo or wildlife park. The focus is on a giraffe's head and neck, with its distinctive spotted pattern and large, expressive eyes. The giraffe appears to be looking directly at the camera, creating a sense of connection between the viewer and the subject.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25398.5, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 22.57, "peak": 26.39, "min": 20.09}, "VIN": {"avg": 60.79, "peak": 65.44, "min": 55.03}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.57, "energy_joules_est": 23.2, "sample_count": 7, "duration_seconds": 1.028}, "timestamp": "2026-01-17T17:51:04.609292"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 729.541, "latencies_ms": [729.541], "images_per_second": 1.371, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The giraffe's coat is predominantly brown and white. Its eyes are dark and appear alert. The background is filled with lush green foliage, suggesting an outdoor setting with natural lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25399.8, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 22.92, "peak": 25.59, "min": 20.88}, "VIN": {"avg": 60.0, "peak": 63.26, "min": 54.43}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 22.92, "energy_joules_est": 16.73, "sample_count": 5, "duration_seconds": 0.73}, "timestamp": "2026-01-17T17:51:05.344946"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 635.437, "latencies_ms": [635.437], "images_per_second": 1.574, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "Two zebras stand side by side, facing away from the camera, showcasing their distinct black and white striped patterns.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25399.8, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25400.0, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 29.93, "peak": 33.48, "min": 25.99}, "VIN": {"avg": 63.96, "peak": 79.4, "min": 54.99}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.93, "energy_joules_est": 19.03, "sample_count": 4, "duration_seconds": 0.636}, "timestamp": "2026-01-17T17:51:05.993720"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 875.289, "latencies_ms": [875.289], "images_per_second": 1.142, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "zebra: 2\nfence: 1\nground: 1\nleaves: 1\nrocks: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.0, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.67, "peak": 37.03, "min": 23.64}, "VIN": {"avg": 64.74, "peak": 83.69, "min": 59.09}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.67, "energy_joules_est": 25.98, "sample_count": 6, "duration_seconds": 0.876}, "timestamp": "2026-01-17T17:51:06.875286"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 849.883, "latencies_ms": [849.883], "images_per_second": 1.177, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with the zebra on the left appearing slightly further away. The background is relatively open and features trees and a fence, suggesting an enclosure or open area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.88, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 61.38, "peak": 75.61, "min": 57.38}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.88, "energy_joules_est": 24.56, "sample_count": 6, "duration_seconds": 0.85}, "timestamp": "2026-01-17T17:51:07.730753"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 832.273, "latencies_ms": [832.273], "images_per_second": 1.202, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Two zebras stand side by side in a fenced enclosure, facing away from the camera. The ground is covered with grass and scattered leaves, indicating a natural habitat setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25399.7, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.89, "peak": 35.45, "min": 23.25}, "VIN": {"avg": 63.7, "peak": 79.61, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 24.05, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:51:08.568678"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 689.562, "latencies_ms": [689.562], "images_per_second": 1.45, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The zebras have distinct black and white stripes. The lighting in the image suggests a sunny day, and the ground appears to be dry and grassy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.7, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.02, "peak": 35.85, "min": 24.82}, "VIN": {"avg": 64.79, "peak": 82.5, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.02, "energy_joules_est": 20.72, "sample_count": 5, "duration_seconds": 0.69}, "timestamp": "2026-01-17T17:51:09.264859"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 747.156, "latencies_ms": [747.156], "images_per_second": 1.338, "prompt_tokens": 8, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A silver car is parked on the side of the road, while a group of horses, including brown, black, and white ones, stand nearby, seemingly curious about the vehicle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25399.7, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.48, "peak": 36.23, "min": 25.21}, "VIN": {"avg": 63.01, "peak": 76.09, "min": 55.64}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.48, "energy_joules_est": 22.79, "sample_count": 5, "duration_seconds": 0.748}, "timestamp": "2026-01-17T17:51:10.022753"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 986.741, "latencies_ms": [986.741], "images_per_second": 1.013, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "horse: 3\ncar: 1\nfence: 1\ntrees: 4\nroad: 1\nhorse droppings: 1\nbuildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.7, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.19, "peak": 36.23, "min": 22.46}, "VIN": {"avg": 64.6, "peak": 94.01, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.19, "energy_joules_est": 27.84, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T17:51:11.017296"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 668.507, "latencies_ms": [668.507], "images_per_second": 1.496, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The car is positioned in the foreground, slightly to the right of the horses. The horses are situated in the background, closer to the fence and further away.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25399.5, "ram_available_mb": 100372.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.61, "peak": 34.65, "min": 24.82}, "VIN": {"avg": 64.55, "peak": 83.37, "min": 57.69}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.61, "energy_joules_est": 19.81, "sample_count": 5, "duration_seconds": 0.669}, "timestamp": "2026-01-17T17:51:11.691711"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 844.861, "latencies_ms": [844.861], "images_per_second": 1.184, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A group of horses is gathered on a paved road near a wooden fence, with some standing close together and others further away. A car is parked on the side of the road, observing the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25399.5, "ram_available_mb": 100372.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.55, "peak": 36.63, "min": 23.64}, "VIN": {"avg": 64.45, "peak": 80.24, "min": 56.72}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.55, "energy_joules_est": 24.99, "sample_count": 6, "duration_seconds": 0.846}, "timestamp": "2026-01-17T17:51:12.543125"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 850.565, "latencies_ms": [850.565], "images_per_second": 1.176, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene is bathed in bright sunlight, creating a warm and inviting atmosphere. The trees lining the road provide a natural backdrop, while the car parked nearby adds a modern element to the setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.16, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 63.97, "peak": 81.95, "min": 56.66}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.16, "energy_joules_est": 24.81, "sample_count": 6, "duration_seconds": 0.851}, "timestamp": "2026-01-17T17:51:13.399688"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 653.926, "latencies_ms": [653.926], "images_per_second": 1.529, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A wooden desk with a stack of blue books, a red apple, and a bell sits in front of a chalkboard in a classroom setting.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25399.2, "ram_available_mb": 100373.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25399.5, "ram_available_mb": 100372.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.26, "peak": 35.47, "min": 25.22}, "VIN": {"avg": 65.74, "peak": 82.38, "min": 57.7}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.26, "energy_joules_est": 19.81, "sample_count": 5, "duration_seconds": 0.655}, "timestamp": "2026-01-17T17:51:14.063544"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1179.562, "latencies_ms": [1179.562], "images_per_second": 0.848, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Chalkboard: 10\nBooks: 5\nWooden desk: 2\nWooden chair: 1\nBell: 1\nApple: 1\nWooden floor: 1\nWhite door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.5, "ram_available_mb": 100372.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.01, "peak": 36.65, "min": 20.88}, "VIN": {"avg": 63.74, "peak": 88.22, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.01, "energy_joules_est": 31.87, "sample_count": 9, "duration_seconds": 1.18}, "timestamp": "2026-01-17T17:51:15.249724"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1047.222, "latencies_ms": [1047.222], "images_per_second": 0.955, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The main objects are positioned in a spatial arrangement that suggests a classroom setting. The books are placed on the desk in the foreground, while the bell and apple are situated near the back of the desk. The chalkboard is positioned in the background, further emphasizing the classroom setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.0, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.39, "peak": 33.48, "min": 21.27}, "VIN": {"avg": 64.69, "peak": 92.87, "min": 56.22}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.39, "energy_joules_est": 27.64, "sample_count": 8, "duration_seconds": 1.048}, "timestamp": "2026-01-17T17:51:16.302936"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 933.211, "latencies_ms": [933.211], "images_per_second": 1.072, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The scene depicts a vintage classroom setting. A wooden desk with a stack of books and a bell sits in front of a chalkboard, evoking a sense of history and learning. The overall atmosphere is calm and studious.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.35, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 65.33, "peak": 97.04, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.35, "energy_joules_est": 25.53, "sample_count": 7, "duration_seconds": 0.934}, "timestamp": "2026-01-17T17:51:17.241962"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 964.576, "latencies_ms": [964.576], "images_per_second": 1.037, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The room features a dark green chalkboard, illuminated by a soft, natural light source. The wooden desk and chair are crafted from warm-toned wood, complementing the overall ambiance of the classroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.63, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 62.5, "peak": 79.07, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 26.67, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T17:51:18.213536"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 698.044, "latencies_ms": [698.044], "images_per_second": 1.433, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A yellow bus with the number 475 and text in Hindi is driving down a busy city street, passing a silver SUV.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25399.7, "ram_available_mb": 100372.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25399.9, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.31, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 62.28, "peak": 72.6, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.31, "energy_joules_est": 20.47, "sample_count": 5, "duration_seconds": 0.699}, "timestamp": "2026-01-17T17:51:18.921977"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 952.498, "latencies_ms": [952.498], "images_per_second": 1.05, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "bus: 475\nvan: 2\ncar: 1\nmotorcycle: 1\nman: 1\nroad: 2\nbuildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.9, "ram_available_mb": 100372.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25399.7, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.25, "peak": 35.45, "min": 22.45}, "VIN": {"avg": 61.37, "peak": 77.39, "min": 55.52}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.25, "energy_joules_est": 26.92, "sample_count": 7, "duration_seconds": 0.953}, "timestamp": "2026-01-17T17:51:19.881186"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 734.517, "latencies_ms": [734.517], "images_per_second": 1.361, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The bus is positioned in the foreground, moving away from the viewer. The van is parked in the background, further away. The bus is closer to the viewer than the van.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.7, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25399.8, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.7, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 65.63, "peak": 82.63, "min": 59.54}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.7, "energy_joules_est": 21.83, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T17:51:20.621833"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1129.912, "latencies_ms": [1129.912], "images_per_second": 0.885, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scene depicts a busy urban street with a yellow bus driving down the road, alongside a white van and a motorcycle. The bus displays the number 475 and text in multiple languages, indicating its origin or destination. The overall setting suggests a bustling city environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.8, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.38, "peak": 35.85, "min": 21.27}, "VIN": {"avg": 62.54, "peak": 78.0, "min": 56.98}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 27.38, "energy_joules_est": 30.95, "sample_count": 8, "duration_seconds": 1.13}, "timestamp": "2026-01-17T17:51:21.761636"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 763.958, "latencies_ms": [763.958], "images_per_second": 1.309, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The bus is predominantly yellow with white and green accents. The lighting suggests it might be daytime. The bus appears to be made of metal and has a visible rear bumper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25399.5, "ram_available_mb": 100372.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.16, "peak": 33.89, "min": 24.43}, "VIN": {"avg": 63.26, "peak": 82.59, "min": 53.26}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.16, "energy_joules_est": 22.29, "sample_count": 5, "duration_seconds": 0.764}, "timestamp": "2026-01-17T17:51:22.531482"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 568.584, "latencies_ms": [568.584], "images_per_second": 1.759, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A bathroom features a flat-screen TV mounted on the wall, displaying a sports game between two teams.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.5, "ram_available_mb": 100372.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25399.3, "ram_available_mb": 100372.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 31.31, "peak": 35.44, "min": 27.18}, "VIN": {"avg": 69.21, "peak": 96.63, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.31, "energy_joules_est": 17.82, "sample_count": 4, "duration_seconds": 0.569}, "timestamp": "2026-01-17T17:51:23.111050"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1109.027, "latencies_ms": [1109.027], "images_per_second": 0.902, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "sink: 2\ntoilet paper dispenser: 1\nhand dryer: 1\nmirror: 1\ntile: 6\ndoor: 1\nwall: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.3, "ram_available_mb": 100372.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25399.5, "ram_available_mb": 100372.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 28.41, "peak": 38.21, "min": 21.66}, "VIN": {"avg": 64.49, "peak": 95.68, "min": 56.95}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.41, "energy_joules_est": 31.53, "sample_count": 8, "duration_seconds": 1.11}, "timestamp": "2026-01-17T17:51:24.226976"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 747.164, "latencies_ms": [747.164], "images_per_second": 1.338, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The sink is located in the foreground, while the mirror reflects the sink and door in the background. The sink and mirror are positioned close together, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.5, "ram_available_mb": 100372.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25400.5, "ram_available_mb": 100371.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.46, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 65.66, "peak": 85.69, "min": 55.0}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.97, "min": 14.57}}, "power_watts_avg": 29.46, "energy_joules_est": 22.02, "sample_count": 5, "duration_seconds": 0.747}, "timestamp": "2026-01-17T17:51:24.980177"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 734.181, "latencies_ms": [734.181], "images_per_second": 1.362, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The scene depicts a public restroom with a television mounted on the wall, displaying a sports game. Two sinks are visible, one on each side of the counter.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25400.5, "ram_available_mb": 100371.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.09, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 68.13, "peak": 95.1, "min": 60.15}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.09, "energy_joules_est": 22.12, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T17:51:25.721026"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 866.199, "latencies_ms": [866.199], "images_per_second": 1.154, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The bathroom features a beige tiled wall with brown accents, creating a warm and neutral ambiance. The lighting is bright, likely from overhead fixtures, enhancing the visibility of the sink and mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.48, "peak": 36.24, "min": 23.64}, "VIN": {"avg": 66.25, "peak": 91.74, "min": 60.1}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.48, "energy_joules_est": 25.54, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T17:51:26.592935"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 670.286, "latencies_ms": [670.286], "images_per_second": 1.492, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A man sits on a park bench, gazing thoughtfully at the surrounding trees and buildings, including a tall church with a pointed steeple.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25400.3, "ram_available_mb": 100371.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.86, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 64.64, "peak": 76.93, "min": 58.98}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 20.03, "sample_count": 5, "duration_seconds": 0.671}, "timestamp": "2026-01-17T17:51:27.273702"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1000.105, "latencies_ms": [1000.105], "images_per_second": 1.0, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "Church: 1\nTower: 1\nTree: 2\nBench: 1\nPerson: 1\nShrubs: 4\nLamp: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25400.3, "ram_available_mb": 100371.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25400.6, "ram_available_mb": 100371.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.53, "peak": 36.63, "min": 22.46}, "VIN": {"avg": 65.12, "peak": 95.73, "min": 57.41}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.53, "energy_joules_est": 28.54, "sample_count": 7, "duration_seconds": 1.0}, "timestamp": "2026-01-17T17:51:28.280066"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 853.29, "latencies_ms": [853.29], "images_per_second": 1.172, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The man is positioned in the foreground, facing the church tower and trees in the background. The church tower is situated near the center of the image, while the trees are located behind and to the right of the man.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25400.3, "ram_available_mb": 100371.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.3, "peak": 33.88, "min": 23.24}, "VIN": {"avg": 66.48, "peak": 93.92, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.3, "energy_joules_est": 24.16, "sample_count": 6, "duration_seconds": 0.854}, "timestamp": "2026-01-17T17:51:29.140058"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 826.142, "latencies_ms": [826.142], "images_per_second": 1.21, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A man sits on a park bench, appearing contemplative or relaxed. In the background, a church with a tall steeple stands amidst trees and bushes, creating a serene and peaceful atmosphere.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.82, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 64.85, "peak": 85.5, "min": 54.85}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 23.83, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T17:51:29.972530"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1120.928, "latencies_ms": [1120.928], "images_per_second": 0.892, "prompt_tokens": 18, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The photograph is in black and white, showcasing a calm and peaceful atmosphere. The lighting is soft and diffused, creating a serene ambiance. Visually, the image captures the contrast between the dark clothing of the seated man and the lighter tones of the surrounding foliage and the building.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.18, "peak": 35.85, "min": 21.27}, "VIN": {"avg": 66.15, "peak": 97.57, "min": 60.34}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.18, "energy_joules_est": 30.48, "sample_count": 8, "duration_seconds": 1.121}, "timestamp": "2026-01-17T17:51:31.099224"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 640.565, "latencies_ms": [640.565], "images_per_second": 1.561, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A bustling city street scene features various vehicles, including cars, buses, and vans, navigating through the traffic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.03, "peak": 33.48, "min": 26.0}, "VIN": {"avg": 69.31, "peak": 99.1, "min": 57.54}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.03, "energy_joules_est": 19.26, "sample_count": 4, "duration_seconds": 0.641}, "timestamp": "2026-01-17T17:51:31.755622"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1026.015, "latencies_ms": [1026.015], "images_per_second": 0.975, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "bus: 2\ncar: 4\nvan: 2\ncar: 2\nminivan: 2\ncar: 2\nbuilding: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25399.8, "ram_available_mb": 100372.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.58, "peak": 36.63, "min": 22.45}, "VIN": {"avg": 62.85, "peak": 89.51, "min": 52.99}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 28.58, "energy_joules_est": 29.34, "sample_count": 7, "duration_seconds": 1.026}, "timestamp": "2026-01-17T17:51:32.791655"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1078.148, "latencies_ms": [1078.148], "images_per_second": 0.928, "prompt_tokens": 25, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The foreground features a green bus parked on the left side of the road, partially obscuring the view of the background. The background includes various parked cars and a building, which appears to be situated further back from the bus. The bus is positioned near the center-left of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25399.8, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.64, "peak": 34.66, "min": 21.27}, "VIN": {"avg": 63.18, "peak": 92.71, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.64, "energy_joules_est": 28.73, "sample_count": 8, "duration_seconds": 1.078}, "timestamp": "2026-01-17T17:51:33.877227"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1073.353, "latencies_ms": [1073.353], "images_per_second": 0.932, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The scene depicts a busy street intersection, likely in Jerusalem, with various vehicles, including cars, vans, and a bus, navigating the road. A person is seen walking on the sidewalk near the bus stop. The buildings in the background suggest a historical or religious setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25399.8, "ram_available_mb": 100372.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.54, "peak": 34.27, "min": 21.26}, "VIN": {"avg": 62.51, "peak": 82.1, "min": 55.79}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.54, "energy_joules_est": 28.5, "sample_count": 8, "duration_seconds": 1.074}, "timestamp": "2026-01-17T17:51:34.956465"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 927.579, "latencies_ms": [927.579], "images_per_second": 1.078, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene is bathed in bright sunlight, creating a vibrant atmosphere. The buildings in the background are constructed from light-colored stone, blending harmoniously with the clear sky. The overall lighting suggests a sunny day.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25400.1, "ram_available_mb": 100372.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25397.6, "ram_available_mb": 100374.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.4, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 65.48, "peak": 97.15, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.4, "energy_joules_est": 25.43, "sample_count": 7, "duration_seconds": 0.928}, "timestamp": "2026-01-17T17:51:35.889940"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 946.742, "latencies_ms": [946.742], "images_per_second": 1.056, "prompt_tokens": 8, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A wooden table holds a plate with a slice of pancake topped with banana slices, a small bowl with sugar, a plate of fruit including watermelon, pineapple, and bananas, and a cup of coffee.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25397.6, "ram_available_mb": 100374.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.4, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 63.73, "peak": 81.31, "min": 58.04}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.4, "energy_joules_est": 25.96, "sample_count": 7, "duration_seconds": 0.948}, "timestamp": "2026-01-17T17:51:36.848180"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1439.62, "latencies_ms": [1439.62], "images_per_second": 0.695, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "Omelet: 1\nBanana slices: 2\nWatermelon: 1\nFruit: 2\nFruit salad: 1\nTea: 1\nSugar: 1\nSpoon: 1\nFork: 1\nTable: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 24.92, "peak": 34.26, "min": 19.7}, "VIN": {"avg": 62.72, "peak": 86.18, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.92, "energy_joules_est": 35.88, "sample_count": 11, "duration_seconds": 1.44}, "timestamp": "2026-01-17T17:51:38.293608"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 909.895, "latencies_ms": [909.895], "images_per_second": 1.099, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The breakfast is arranged on a table, with plates and utensils placed near each other. The table occupies the foreground, while the plates and utensils are situated in the background. The arrangement suggests a casual, intimate setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 27.5, "peak": 33.86, "min": 22.45}, "VIN": {"avg": 67.66, "peak": 94.49, "min": 59.93}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.5, "energy_joules_est": 25.03, "sample_count": 6, "duration_seconds": 0.91}, "timestamp": "2026-01-17T17:51:39.210144"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 889.594, "latencies_ms": [889.594], "images_per_second": 1.124, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a breakfast setting on a tiled floor, featuring a plate with a partially eaten omelet, a fruit platter with bananas, watermelon, and pineapple, and two cups of tea.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.75, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 63.92, "peak": 82.74, "min": 58.06}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 25.6, "sample_count": 6, "duration_seconds": 0.89}, "timestamp": "2026-01-17T17:51:40.110226"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 953.836, "latencies_ms": [953.836], "images_per_second": 1.048, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The table is brown wood, contrasting with the white tiles. The food is colorful, featuring yellow bananas, sliced watermelon, and pineapple. The table is well-lit, suggesting natural light from the surrounding area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25397.6, "ram_available_mb": 100374.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.46, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 64.09, "peak": 99.52, "min": 54.84}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 26.2, "sample_count": 7, "duration_seconds": 0.954}, "timestamp": "2026-01-17T17:51:41.069919"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 826.123, "latencies_ms": [826.123], "images_per_second": 1.21, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "An elderly woman in a green and white striped shirt and pink apron is carefully placing dough into baking pans on a kitchen table, surrounded by various baked goods and kitchenware.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25397.6, "ram_available_mb": 100374.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.29, "peak": 33.86, "min": 23.24}, "VIN": {"avg": 63.89, "peak": 83.64, "min": 54.57}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 23.39, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T17:51:41.909619"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1390.686, "latencies_ms": [1390.686], "images_per_second": 0.719, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "bread: 8\ncookies: 8\nbuns: 2\npastry: 2\nglaze: 1\nbaking pan: 1\ntable: 1\nplacemats: 2\nglasses: 2\ntoy: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.4, "ram_available_mb": 100374.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 25.77, "peak": 35.85, "min": 20.1}, "VIN": {"avg": 63.05, "peak": 83.91, "min": 57.69}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.77, "energy_joules_est": 35.85, "sample_count": 10, "duration_seconds": 1.391}, "timestamp": "2026-01-17T17:51:43.310296"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1027.798, "latencies_ms": [1027.798], "images_per_second": 0.973, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The woman is positioned on the left side of the image, preparing food at a table. The table occupies the foreground, displaying various baked goods and cooking utensils. The background features a living room with a couch and a window, providing a glimpse of the surrounding space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.9, "peak": 33.48, "min": 21.66}, "VIN": {"avg": 63.32, "peak": 80.63, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.9, "energy_joules_est": 27.67, "sample_count": 7, "duration_seconds": 1.029}, "timestamp": "2026-01-17T17:51:44.344514"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 747.406, "latencies_ms": [747.406], "images_per_second": 1.338, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A woman is preparing baked goods at a wooden table in a cozy living room. She is using a metal pan to spread something, possibly sugar or flour, on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25397.1, "ram_available_mb": 100375.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25396.9, "ram_available_mb": 100375.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.15, "peak": 33.86, "min": 24.43}, "VIN": {"avg": 65.82, "peak": 96.56, "min": 51.83}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 21.8, "sample_count": 5, "duration_seconds": 0.748}, "timestamp": "2026-01-17T17:51:45.098456"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1149.163, "latencies_ms": [1149.163], "images_per_second": 0.87, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The table is covered with a patterned tablecloth in shades of red, pink, and cream. The woman is wearing glasses and a green and white striped shirt. The table is equipped with several baking trays and ingredients, including bread rolls, cookies, and muffins.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25396.9, "ram_available_mb": 100375.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.6, "peak": 35.44, "min": 20.88}, "VIN": {"avg": 62.79, "peak": 81.76, "min": 56.71}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.6, "energy_joules_est": 30.58, "sample_count": 9, "duration_seconds": 1.149}, "timestamp": "2026-01-17T17:51:46.253859"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 610.684, "latencies_ms": [610.684], "images_per_second": 1.638, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A man in a white t-shirt and shorts stands next to a traffic light displaying a red light, surrounded by lush greenery and white flowers.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25398.2, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.2}, "VDD_GPU": {"avg": 30.53, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 68.89, "peak": 91.83, "min": 60.07}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.53, "energy_joules_est": 18.65, "sample_count": 4, "duration_seconds": 0.611}, "timestamp": "2026-01-17T17:51:46.875811"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1065.508, "latencies_ms": [1065.508], "images_per_second": 0.939, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "traffic light: 3\nsign: 1\nman: 1\ntrees: 2\nflowers: 2\nground: 2\nrocks: 2\nsand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.2, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25398.3, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.68, "peak": 36.63, "min": 21.67}, "VIN": {"avg": 62.4, "peak": 80.72, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.68, "energy_joules_est": 29.51, "sample_count": 8, "duration_seconds": 1.066}, "timestamp": "2026-01-17T17:51:47.947875"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1059.684, "latencies_ms": [1059.684], "images_per_second": 0.944, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The man is standing in the foreground of the image, positioned next to the traffic light. The traffic light is situated in the background, slightly further away than the man. The setting appears to be outdoors, with vegetation and a gravel or dirt area around the man and the traffic light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.59, "peak": 34.65, "min": 21.28}, "VIN": {"avg": 64.19, "peak": 96.19, "min": 53.16}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.59, "energy_joules_est": 28.19, "sample_count": 8, "duration_seconds": 1.06}, "timestamp": "2026-01-17T17:51:49.015828"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 807.335, "latencies_ms": [807.335], "images_per_second": 1.239, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A young man stands next to a traffic light in Australia. He is wearing a white t-shirt and shorts, smiling at the camera. The traffic light is red, indicating a stop for vehicles.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 34.27, "min": 23.25}, "VIN": {"avg": 63.14, "peak": 80.61, "min": 53.63}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 22.9, "sample_count": 6, "duration_seconds": 0.808}, "timestamp": "2026-01-17T17:51:49.830185"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 865.477, "latencies_ms": [865.477], "images_per_second": 1.155, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The traffic light is red, indicating a stop. The man is wearing sandals, suggesting warm weather. The scene is outdoors with lush greenery and a gravel path. The lighting appears to be natural daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.15, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 62.62, "peak": 76.94, "min": 57.67}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 25.24, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T17:51:50.701719"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 752.56, "latencies_ms": [752.56], "images_per_second": 1.329, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A lively kite festival is taking place in a park, with numerous vibrant kites soaring in the sky, including three clownfish-shaped kites.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.62, "peak": 35.45, "min": 24.42}, "VIN": {"avg": 64.23, "peak": 81.18, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 22.31, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T17:51:51.464970"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1127.19, "latencies_ms": [1127.19], "images_per_second": 0.887, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "kite: 4\nflag: 4\nkite: 4\nkite: 4\nkite: 4\nkite: 4\nkite: 4\nkite: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.1, "ram_available_mb": 100374.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.48, "peak": 35.85, "min": 21.66}, "VIN": {"avg": 62.23, "peak": 76.86, "min": 57.85}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.48, "energy_joules_est": 30.99, "sample_count": 8, "duration_seconds": 1.128}, "timestamp": "2026-01-17T17:51:52.598680"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 781.558, "latencies_ms": [781.558], "images_per_second": 1.279, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the red and black kites extending towards the background. The kites are spread out across the grassy field, creating a dynamic and colorful display.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.56, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 64.75, "peak": 84.03, "min": 56.46}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.56, "energy_joules_est": 22.33, "sample_count": 6, "duration_seconds": 0.782}, "timestamp": "2026-01-17T17:51:53.386590"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1168.27, "latencies_ms": [1168.27], "images_per_second": 0.856, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The scene depicts a lively kite-flying event in a park. Numerous colorful kites, including those shaped like fish, fill the sky, creating a vibrant and festive atmosphere. People are gathered on the grassy field, enjoying the activity and watching the kites soar.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.6, "ram_available_mb": 100373.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.43, "peak": 35.42, "min": 20.88}, "VIN": {"avg": 63.76, "peak": 84.26, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.43, "energy_joules_est": 30.89, "sample_count": 9, "duration_seconds": 1.169}, "timestamp": "2026-01-17T17:51:54.561895"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 820.423, "latencies_ms": [820.423], "images_per_second": 1.219, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The kites are predominantly red, orange, and purple. The lighting suggests an overcast day. The kites appear to be made of a lightweight material like fabric or plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.03, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 63.65, "peak": 81.71, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.03, "energy_joules_est": 23.01, "sample_count": 6, "duration_seconds": 0.821}, "timestamp": "2026-01-17T17:51:55.388198"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 569.807, "latencies_ms": [569.807], "images_per_second": 1.755, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A man is sharing a slice of pizza with a young boy, who appears excited and eager to eat.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25398.3, "ram_available_mb": 100373.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25404.9, "ram_available_mb": 100367.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 31.22, "peak": 35.45, "min": 26.79}, "VIN": {"avg": 68.84, "peak": 92.82, "min": 59.4}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.22, "energy_joules_est": 17.8, "sample_count": 4, "duration_seconds": 0.57}, "timestamp": "2026-01-17T17:51:55.969328"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1283.045, "latencies_ms": [1283.045], "images_per_second": 0.779, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "Pizza: 2\nPizza box: 1\nPizza slice: 1\nMan: 1\nChild: 1\nChild's shoes: 1\nWater bottle: 1\nFloor: 1\nCouch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25404.9, "ram_available_mb": 100367.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25403.9, "ram_available_mb": 100368.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.04, "peak": 37.42, "min": 20.88}, "VIN": {"avg": 65.56, "peak": 97.61, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.04, "energy_joules_est": 34.7, "sample_count": 9, "duration_seconds": 1.283}, "timestamp": "2026-01-17T17:51:57.258124"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 975.612, "latencies_ms": [975.612], "images_per_second": 1.025, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The man is on the left side of the image, holding the pizza slice near the boy. The pizza slice is positioned between the man and the boy, slightly in front of them. The background features a couch and a door, suggesting the setting is indoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.9, "ram_available_mb": 100368.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25403.7, "ram_available_mb": 100368.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.41, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 64.0, "peak": 81.4, "min": 59.55}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.41, "energy_joules_est": 26.75, "sample_count": 7, "duration_seconds": 0.976}, "timestamp": "2026-01-17T17:51:58.239629"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 686.323, "latencies_ms": [686.323], "images_per_second": 1.457, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A man and a young boy are sharing a slice of pizza together. The setting appears to be a casual indoor space, possibly a living room or bedroom.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25403.7, "ram_available_mb": 100368.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25403.9, "ram_available_mb": 100368.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.54, "peak": 35.06, "min": 24.42}, "VIN": {"avg": 67.91, "peak": 96.59, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.54, "energy_joules_est": 20.29, "sample_count": 5, "duration_seconds": 0.687}, "timestamp": "2026-01-17T17:51:58.932692"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 781.041, "latencies_ms": [781.041], "images_per_second": 1.28, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The room is lit by natural light coming in through a window. The walls are painted a warm orange color. The floor appears to be tiled with light-colored tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.9, "ram_available_mb": 100368.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.88, "peak": 37.03, "min": 25.21}, "VIN": {"avg": 69.05, "peak": 91.8, "min": 61.79}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.88, "energy_joules_est": 24.13, "sample_count": 5, "duration_seconds": 0.781}, "timestamp": "2026-01-17T17:51:59.720703"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 569.653, "latencies_ms": [569.653], "images_per_second": 1.755, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A woman is sitting in a camping chair outdoors, eating a sandwich and enjoying a plate of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.51, "min": 14.1}, "VDD_GPU": {"avg": 31.31, "peak": 35.83, "min": 26.79}, "VIN": {"avg": 64.5, "peak": 81.01, "min": 56.44}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.31, "energy_joules_est": 17.84, "sample_count": 4, "duration_seconds": 0.57}, "timestamp": "2026-01-17T17:52:00.301096"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1167.803, "latencies_ms": [1167.803], "images_per_second": 0.856, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "woman: 2\nchair: 1\nplate: 1\nfood: 1\nsauce: 1\nbread: 1\nground: 1\nrocks: 1\nfire pit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.45, "peak": 37.82, "min": 20.89}, "VIN": {"avg": 63.34, "peak": 92.7, "min": 54.2}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.45, "energy_joules_est": 32.07, "sample_count": 9, "duration_seconds": 1.168}, "timestamp": "2026-01-17T17:52:01.479063"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 935.996, "latencies_ms": [935.996], "images_per_second": 1.068, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The woman is seated in a chair, eating a sandwich. The sandwich is positioned in the foreground, close to the woman. The chair is situated in the background, slightly out of focus. The scene appears to be outdoors, near a fire pit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.4, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 63.43, "peak": 85.2, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.4, "energy_joules_est": 25.65, "sample_count": 7, "duration_seconds": 0.936}, "timestamp": "2026-01-17T17:52:02.420717"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 686.329, "latencies_ms": [686.329], "images_per_second": 1.457, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A woman is sitting outdoors, eating a sandwich and chips. She is wearing a striped shirt and appears to be in a camping or outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.4, "ram_available_mb": 100368.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25403.7, "ram_available_mb": 100368.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.62, "peak": 34.65, "min": 24.82}, "VIN": {"avg": 64.53, "peak": 77.59, "min": 59.19}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 20.34, "sample_count": 5, "duration_seconds": 0.687}, "timestamp": "2026-01-17T17:52:03.112965"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 882.914, "latencies_ms": [882.914], "images_per_second": 1.133, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The woman is wearing a green and white striped shirt. The scene is dimly lit, suggesting nighttime. The food appears to be fried and served on a white plate. The ground is dark and appears to be dirt or soil.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.7, "ram_available_mb": 100368.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25403.4, "ram_available_mb": 100368.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.41, "peak": 36.62, "min": 23.64}, "VIN": {"avg": 65.08, "peak": 79.58, "min": 60.03}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.41, "energy_joules_est": 25.97, "sample_count": 6, "duration_seconds": 0.883}, "timestamp": "2026-01-17T17:52:04.001646"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 625.758, "latencies_ms": [625.758], "images_per_second": 1.598, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A large family is gathered around a dining table, sharing a meal of diverse dishes and engaging in lively conversation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25403.4, "ram_available_mb": 100368.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25403.4, "ram_available_mb": 100368.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.32, "peak": 35.85, "min": 26.79}, "VIN": {"avg": 66.71, "peak": 84.12, "min": 60.12}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.32, "energy_joules_est": 19.61, "sample_count": 4, "duration_seconds": 0.626}, "timestamp": "2026-01-17T17:52:04.639835"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1242.89, "latencies_ms": [1242.89], "images_per_second": 0.805, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "table: 8\nplates: 8\nserving dishes: 8\nglasses: 4\nwine glasses: 2\ncutlery: 2\nchairs: 2\npeople: 10\nfamily: 10", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25403.4, "ram_available_mb": 100368.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25403.4, "ram_available_mb": 100368.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.23, "peak": 37.82, "min": 20.88}, "VIN": {"avg": 62.97, "peak": 88.88, "min": 55.86}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 33.85, "sample_count": 9, "duration_seconds": 1.243}, "timestamp": "2026-01-17T17:52:05.888563"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1059.604, "latencies_ms": [1059.604], "images_per_second": 0.944, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The main objects are positioned in a manner that creates a sense of depth and perspective. The foreground includes several people seated around the table, while the background features more distant figures. The table occupies the central portion of the image, drawing the viewer's eye towards the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25403.4, "ram_available_mb": 100368.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.34, "peak": 33.48, "min": 21.27}, "VIN": {"avg": 62.96, "peak": 80.04, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 27.92, "sample_count": 8, "duration_seconds": 1.06}, "timestamp": "2026-01-17T17:52:06.954150"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 995.72, "latencies_ms": [995.72], "images_per_second": 1.004, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A large family is gathered around a dining table, enjoying a meal together. The setting appears to be a home, with various dishes, glasses, and cutlery spread across the table. The atmosphere is warm and inviting, typical of a family gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25405.8, "ram_available_mb": 100366.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.29, "peak": 35.04, "min": 21.66}, "VIN": {"avg": 63.28, "peak": 79.53, "min": 57.9}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.29, "energy_joules_est": 27.18, "sample_count": 7, "duration_seconds": 0.996}, "timestamp": "2026-01-17T17:52:07.955842"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1156.916, "latencies_ms": [1156.916], "images_per_second": 0.864, "prompt_tokens": 18, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The table is covered with a purple tablecloth. The lighting is warm and inviting, creating a cozy atmosphere. The table is adorned with plates, bowls, cups, wine glasses, and cutlery, showcasing a variety of colors and materials. The scene appears to be set during a festive gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.8, "ram_available_mb": 100366.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25405.8, "ram_available_mb": 100366.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.63, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 63.8, "peak": 85.84, "min": 53.92}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.63, "energy_joules_est": 30.82, "sample_count": 8, "duration_seconds": 1.157}, "timestamp": "2026-01-17T17:52:09.119588"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 626.184, "latencies_ms": [626.184], "images_per_second": 1.597, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A baseball player slides into home plate, attempting to reach the base safely while other players and spectators watch.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25405.8, "ram_available_mb": 100366.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.12, "peak": 33.85, "min": 25.99}, "VIN": {"avg": 65.4, "peak": 74.07, "min": 59.53}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.12, "energy_joules_est": 18.87, "sample_count": 4, "duration_seconds": 0.627}, "timestamp": "2026-01-17T17:52:09.755154"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1745.322, "latencies_ms": [1745.322], "images_per_second": 0.573, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 1, "output_text": "baseball bat: 1\nbaseball glove: 1\nbaseball: 1\nbaseball diamond: 4\nbaseball field: 4\nbaseball player: 2\nbaseball umpire: 1\nbaseball umpire: 1\nbaseball umpire: 1\nbaseball umpire: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25405.3, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 24.88, "peak": 36.63, "min": 19.3}, "VIN": {"avg": 61.4, "peak": 77.32, "min": 56.08}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.88, "energy_joules_est": 43.44, "sample_count": 13, "duration_seconds": 1.746}, "timestamp": "2026-01-17T17:52:11.507209"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 888.522, "latencies_ms": [888.522], "images_per_second": 1.125, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The batter is positioned in the foreground, sliding into the base. The catcher is standing in the background, watching the play. The field extends beyond the immediate action, appearing to be part of a larger baseball field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.3, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 27.7, "peak": 33.08, "min": 22.85}, "VIN": {"avg": 63.24, "peak": 76.25, "min": 57.36}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.7, "energy_joules_est": 24.62, "sample_count": 6, "duration_seconds": 0.889}, "timestamp": "2026-01-17T17:52:12.401606"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 946.225, "latencies_ms": [946.225], "images_per_second": 1.057, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A baseball game is in progress on a sunny day. A batter is sliding into home plate, while a catcher prepares to catch the ball. Several other players and spectators are visible in the background, watching the play unfold.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25405.2, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.51, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 64.29, "peak": 80.84, "min": 60.37}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.51, "energy_joules_est": 26.05, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T17:52:13.354288"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 630.034, "latencies_ms": [630.034], "images_per_second": 1.587, "prompt_tokens": 18, "response_tokens_est": 24, "n_tiles": 1, "output_text": "The field is predominantly green with reddish-brown dirt. The lighting suggests it might be late afternoon or early evening.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.2, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25405.2, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.32, "peak": 34.65, "min": 26.0}, "VIN": {"avg": 64.55, "peak": 82.88, "min": 54.29}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.32, "energy_joules_est": 19.12, "sample_count": 4, "duration_seconds": 0.63}, "timestamp": "2026-01-17T17:52:13.990988"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 855.483, "latencies_ms": [855.483], "images_per_second": 1.169, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A skateboarder in black attire and helmet is performing a trick on a concrete ramp, skillfully balancing on one foot while extending the other arm for balance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.2, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 30.46, "peak": 37.03, "min": 24.03}, "VIN": {"avg": 64.3, "peak": 85.34, "min": 55.71}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.46, "energy_joules_est": 26.07, "sample_count": 6, "duration_seconds": 0.856}, "timestamp": "2026-01-17T17:52:14.857803"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1231.781, "latencies_ms": [1231.781], "images_per_second": 0.812, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "helmet: 1\nknee pads: 2\nelbow pads: 1\nshin guards: 1\nskateboard: 1\nfence: 1\nground: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25405.5, "ram_available_mb": 100366.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25405.2, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.31, "peak": 36.24, "min": 20.88}, "VIN": {"avg": 61.98, "peak": 81.89, "min": 53.58}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.31, "energy_joules_est": 33.65, "sample_count": 9, "duration_seconds": 1.232}, "timestamp": "2026-01-17T17:52:16.096242"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1103.951, "latencies_ms": [1103.951], "images_per_second": 0.906, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, performing a trick on a concrete ramp. The skateboarder's shadow is cast onto the ramp's surface. The background features a grassy area and a fence, suggesting an outdoor skatepark setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25405.2, "ram_available_mb": 100366.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25409.4, "ram_available_mb": 100362.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.18, "peak": 33.88, "min": 21.28}, "VIN": {"avg": 60.67, "peak": 70.97, "min": 55.07}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 30.02, "sample_count": 8, "duration_seconds": 1.104}, "timestamp": "2026-01-17T17:52:17.206078"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 864.561, "latencies_ms": [864.561], "images_per_second": 1.157, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A skateboarder is performing a trick on a concrete ramp in a skate park. The skate park is surrounded by trees and grass, creating a natural and urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25409.4, "ram_available_mb": 100362.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25408.2, "ram_available_mb": 100363.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.95, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 63.44, "peak": 76.27, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 25.04, "sample_count": 6, "duration_seconds": 0.865}, "timestamp": "2026-01-17T17:52:18.076542"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 955.94, "latencies_ms": [955.94], "images_per_second": 1.046, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The skateboarder is wearing a black helmet, black shirt, and black knee pads. The scene appears to be outdoors on a sunny day, with natural lighting illuminating the skateboarder's shadow on the concrete ramp.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25408.2, "ram_available_mb": 100363.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25408.5, "ram_available_mb": 100363.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.82, "peak": 35.83, "min": 22.46}, "VIN": {"avg": 62.25, "peak": 78.64, "min": 53.74}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 27.56, "sample_count": 7, "duration_seconds": 0.956}, "timestamp": "2026-01-17T17:52:19.039526"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1095.051, "latencies_ms": [1095.051], "images_per_second": 0.913, "prompt_tokens": 8, "response_tokens_est": 49, "n_tiles": 1, "output_text": "A neatly arranged meal consisting of french fries, a burger, tomato, pickle, coleslaw, lemon wedges, and condiments is presented on two white plates, accompanied by a glass of soda and a salt and pepper shaker.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25408.5, "ram_available_mb": 100363.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25408.2, "ram_available_mb": 100364.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.79, "peak": 35.45, "min": 21.28}, "VIN": {"avg": 63.78, "peak": 82.17, "min": 59.86}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.79, "energy_joules_est": 29.35, "sample_count": 8, "duration_seconds": 1.095}, "timestamp": "2026-01-17T17:52:20.145777"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1373.592, "latencies_ms": [1373.592], "images_per_second": 0.728, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "fries: 10\nbun: 2\nham patty: 1\ntomato: 2\npickle: 1\ncoleslaw: 1\nmayo: 1\nlemon: 1\nsalt and pepper shakers: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25408.2, "ram_available_mb": 100364.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25408.2, "ram_available_mb": 100364.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 25.41, "peak": 34.27, "min": 20.09}, "VIN": {"avg": 62.64, "peak": 73.67, "min": 54.35}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.41, "energy_joules_est": 34.91, "sample_count": 10, "duration_seconds": 1.374}, "timestamp": "2026-01-17T17:52:21.525607"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1072.996, "latencies_ms": [1072.996], "images_per_second": 0.932, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The main objects are positioned in a balanced and visually appealing manner. The foreground features the burger, fries, and pickle, while the background includes additional plates, condiments, and a drink. The arrangement suggests a well-prepared meal, ready to be enjoyed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25408.2, "ram_available_mb": 100364.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25408.7, "ram_available_mb": 100363.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.29, "peak": 33.48, "min": 21.27}, "VIN": {"avg": 64.86, "peak": 94.6, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 26.29, "energy_joules_est": 28.22, "sample_count": 8, "duration_seconds": 1.073}, "timestamp": "2026-01-17T17:52:22.604028"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 938.039, "latencies_ms": [938.039], "images_per_second": 1.066, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a hotel room meal featuring a burger, fries, tomato salad, pickle, coleslaw, lemon wedges, and condiments on a tray. A glass of beer is also present.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25408.7, "ram_available_mb": 100363.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25408.7, "ram_available_mb": 100363.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.35, "peak": 34.28, "min": 22.06}, "VIN": {"avg": 66.59, "peak": 93.91, "min": 60.4}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.35, "min": 14.95}}, "power_watts_avg": 27.35, "energy_joules_est": 25.67, "sample_count": 7, "duration_seconds": 0.938}, "timestamp": "2026-01-17T17:52:23.548124"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1051.977, "latencies_ms": [1051.977], "images_per_second": 0.951, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The food items display vibrant colors, including golden french fries, fresh tomatoes, and creamy coleslaw. The lighting is soft and warm, creating a pleasant and inviting atmosphere. The food appears to be served on white plates, enhancing its presentation.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25408.7, "ram_available_mb": 100363.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25408.7, "ram_available_mb": 100363.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.73, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 65.43, "peak": 93.28, "min": 57.48}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.73, "energy_joules_est": 28.13, "sample_count": 8, "duration_seconds": 1.052}, "timestamp": "2026-01-17T17:52:24.606059"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 551.827, "latencies_ms": [551.827], "images_per_second": 1.812, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A red motorcycle is parked on the side of a road near a sandy beach with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25408.7, "ram_available_mb": 100363.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25409.2, "ram_available_mb": 100363.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.83, "peak": 34.66, "min": 26.79}, "VIN": {"avg": 64.0, "peak": 76.42, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.83, "energy_joules_est": 17.03, "sample_count": 4, "duration_seconds": 0.552}, "timestamp": "2026-01-17T17:52:25.168694"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 997.797, "latencies_ms": [997.797], "images_per_second": 1.002, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "motorcycle: 1\npalm trees: 3\nbeach: 2\nsand: 2\nfence: 1\nsky: 1\nroad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25409.2, "ram_available_mb": 100363.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25408.9, "ram_available_mb": 100363.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 29.09, "peak": 38.21, "min": 22.45}, "VIN": {"avg": 63.86, "peak": 82.94, "min": 58.06}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.09, "energy_joules_est": 29.04, "sample_count": 7, "duration_seconds": 0.998}, "timestamp": "2026-01-17T17:52:26.172459"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 957.218, "latencies_ms": [957.218], "images_per_second": 1.045, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The motorcycle is positioned in the foreground, facing the left side of the image. The beach and palm trees are in the background, extending from the left to the right of the motorcycle. The motorcycle is parked near the edge of the road, suggesting it is relatively close to the beach.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25409.2, "ram_available_mb": 100363.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25410.8, "ram_available_mb": 100361.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.62, "peak": 34.64, "min": 22.06}, "VIN": {"avg": 64.22, "peak": 87.9, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.62, "energy_joules_est": 26.45, "sample_count": 7, "duration_seconds": 0.958}, "timestamp": "2026-01-17T17:52:27.135723"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 762.718, "latencies_ms": [762.718], "images_per_second": 1.311, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A red Harley Davidson motorcycle is parked on a paved area near a sandy beach with palm trees in the background. The scene suggests a sunny, tropical beach setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25410.8, "ram_available_mb": 100361.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25411.1, "ram_available_mb": 100361.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.86, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 62.18, "peak": 74.56, "min": 55.92}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.86, "energy_joules_est": 22.79, "sample_count": 5, "duration_seconds": 0.763}, "timestamp": "2026-01-17T17:52:27.908420"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 724.766, "latencies_ms": [724.766], "images_per_second": 1.38, "prompt_tokens": 18, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The motorcycle is red and appears to be made of chrome or stainless steel. The lighting suggests a sunny day, and the materials suggest a beach setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25411.1, "ram_available_mb": 100361.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25410.8, "ram_available_mb": 100361.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.41, "peak": 35.83, "min": 25.21}, "VIN": {"avg": 65.38, "peak": 91.84, "min": 55.64}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.41, "energy_joules_est": 22.05, "sample_count": 5, "duration_seconds": 0.725}, "timestamp": "2026-01-17T17:52:28.639169"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 550.322, "latencies_ms": [550.322], "images_per_second": 1.817, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A man is elegantly dressed in a black suit with a white shirt and a patterned black tie.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25410.8, "ram_available_mb": 100361.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25411.3, "ram_available_mb": 100360.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 32.0, "peak": 37.03, "min": 27.18}, "VIN": {"avg": 64.26, "peak": 76.82, "min": 57.74}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 32.0, "energy_joules_est": 17.62, "sample_count": 4, "duration_seconds": 0.551}, "timestamp": "2026-01-17T17:52:29.199725"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 989.917, "latencies_ms": [989.917], "images_per_second": 1.01, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "suit: 2\ntie: 1\nshirt: 1\njacket: 1\nvest: 1\npocket: 1\nlight switch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25411.3, "ram_available_mb": 100360.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25410.7, "ram_available_mb": 100361.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 29.61, "peak": 38.6, "min": 22.85}, "VIN": {"avg": 66.01, "peak": 93.91, "min": 59.82}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 29.61, "energy_joules_est": 29.33, "sample_count": 7, "duration_seconds": 0.99}, "timestamp": "2026-01-17T17:52:30.195684"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 730.582, "latencies_ms": [730.582], "images_per_second": 1.369, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The man is positioned in the foreground, with the suit and tie slightly behind him. The background is dark and out of focus, drawing attention to the man's attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25410.7, "ram_available_mb": 100361.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25409.8, "ram_available_mb": 100362.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.93, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 65.32, "peak": 76.96, "min": 60.79}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 21.89, "sample_count": 5, "duration_seconds": 0.731}, "timestamp": "2026-01-17T17:52:30.932738"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 814.901, "latencies_ms": [814.901], "images_per_second": 1.227, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A man is dressed in a formal black suit and tie, standing in a dimly lit room, possibly a hotel or event venue. The setting suggests a professional or formal occasion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25409.8, "ram_available_mb": 100362.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25409.7, "ram_available_mb": 100362.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.47, "peak": 36.24, "min": 23.62}, "VIN": {"avg": 64.04, "peak": 78.86, "min": 57.64}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.47, "energy_joules_est": 24.02, "sample_count": 6, "duration_seconds": 0.815}, "timestamp": "2026-01-17T17:52:31.753390"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 983.119, "latencies_ms": [983.119], "images_per_second": 1.017, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The suit is primarily dark gray or black. The lighting in the image is soft and subdued, creating a contrast with the dark suit. The material appears to be smooth, possibly silk or a similar fabric. The weather appears to be cloudy or overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25409.7, "ram_available_mb": 100362.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25410.0, "ram_available_mb": 100362.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.14, "peak": 35.85, "min": 22.46}, "VIN": {"avg": 63.4, "peak": 83.91, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.14, "energy_joules_est": 27.68, "sample_count": 7, "duration_seconds": 0.984}, "timestamp": "2026-01-17T17:52:32.743087"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 607.588, "latencies_ms": [607.588], "images_per_second": 1.646, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A gray and white cat is peacefully sleeping on a pair of sneakers, curled up with its eyes closed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25410.0, "ram_available_mb": 100362.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25411.5, "ram_available_mb": 100360.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.82, "peak": 34.66, "min": 26.79}, "VIN": {"avg": 67.22, "peak": 89.69, "min": 53.94}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.82, "energy_joules_est": 18.75, "sample_count": 4, "duration_seconds": 0.608}, "timestamp": "2026-01-17T17:52:33.360880"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1094.218, "latencies_ms": [1094.218], "images_per_second": 0.914, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "shoe: 2\ncat: 1\nwall: 1\nsneaker: 1\nshoe laces: 1\nshoe sole: 1\nconverse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25411.5, "ram_available_mb": 100360.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25411.9, "ram_available_mb": 100360.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.67, "peak": 36.62, "min": 21.27}, "VIN": {"avg": 61.06, "peak": 77.7, "min": 51.04}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 27.67, "energy_joules_est": 30.3, "sample_count": 8, "duration_seconds": 1.095}, "timestamp": "2026-01-17T17:52:34.461675"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 717.843, "latencies_ms": [717.843], "images_per_second": 1.393, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The cat is positioned close to the foreground shoe, partially covering it. The shoes are placed near the cat, creating a sense of proximity between the cat and the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25411.9, "ram_available_mb": 100360.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25411.9, "ram_available_mb": 100360.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.14, "peak": 34.65, "min": 24.02}, "VIN": {"avg": 67.6, "peak": 96.53, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.14, "energy_joules_est": 20.93, "sample_count": 5, "duration_seconds": 0.718}, "timestamp": "2026-01-17T17:52:35.185393"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 698.436, "latencies_ms": [698.436], "images_per_second": 1.432, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A cat is sleeping peacefully on a pair of sneakers placed against a textured wall. The scene takes place indoors, likely on a hardwood floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25411.9, "ram_available_mb": 100360.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25411.7, "ram_available_mb": 100360.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 30.41, "peak": 35.85, "min": 25.22}, "VIN": {"avg": 63.68, "peak": 77.06, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.41, "energy_joules_est": 21.25, "sample_count": 5, "duration_seconds": 0.699}, "timestamp": "2026-01-17T17:52:35.890135"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 986.76, "latencies_ms": [986.76], "images_per_second": 1.013, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The cat is sleeping next to a pair of sneakers. The sneakers are primarily light blue and white. The lighting appears to be soft and diffused, suggesting an indoor setting. The sneakers appear to be made of canvas or a similar material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25411.7, "ram_available_mb": 100360.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25426.8, "ram_available_mb": 100345.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.42, "peak": 36.24, "min": 22.45}, "VIN": {"avg": 63.51, "peak": 77.67, "min": 59.68}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 28.06, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T17:52:36.882900"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 771.982, "latencies_ms": [771.982], "images_per_second": 1.295, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "Two workers in safety vests and hard hats are standing on top of a green Isuzu dump truck parked on the side of a city street.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25426.8, "ram_available_mb": 100345.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 29.16, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 67.55, "peak": 88.31, "min": 60.86}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.16, "energy_joules_est": 22.52, "sample_count": 5, "duration_seconds": 0.772}, "timestamp": "2026-01-17T17:52:37.670981"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1127.884, "latencies_ms": [1127.884], "images_per_second": 0.887, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Truck: 2\nMan: 2\nWoman: 1\nHat: 1\nVest: 1\nBus: 1\nBuilding: 1\nTrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 27.08, "peak": 35.44, "min": 21.27}, "VIN": {"avg": 64.18, "peak": 91.15, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.08, "energy_joules_est": 30.56, "sample_count": 8, "duration_seconds": 1.128}, "timestamp": "2026-01-17T17:52:38.805185"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 927.34, "latencies_ms": [927.34], "images_per_second": 1.078, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main object is a green Isuzu dump truck parked on the street. The truck is positioned in the foreground, with a person standing near its front. The background features buildings, trees, and a glimpse of a red bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.46, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 62.88, "peak": 84.58, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.46, "energy_joules_est": 25.47, "sample_count": 7, "duration_seconds": 0.928}, "timestamp": "2026-01-17T17:52:39.738506"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1112.727, "latencies_ms": [1112.727], "images_per_second": 0.899, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The scene depicts a green Isuzu dump truck driving down a city street. Two workers in green uniforms are visible on the truck, one sitting on the back and the other standing near the front. The truck has various signs and markings, including one in Thai script.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.63, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 64.45, "peak": 94.97, "min": 56.05}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.63, "energy_joules_est": 29.64, "sample_count": 8, "duration_seconds": 1.113}, "timestamp": "2026-01-17T17:52:40.857068"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 910.819, "latencies_ms": [910.819], "images_per_second": 1.098, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The truck is predominantly green with red and white stripes on the front. The truck has blue headlights and orange reflective stripes on the sides. The truck is parked on a city street with buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.29, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 62.22, "peak": 76.87, "min": 55.49}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.29, "energy_joules_est": 24.87, "sample_count": 7, "duration_seconds": 0.911}, "timestamp": "2026-01-17T17:52:41.774121"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 557.475, "latencies_ms": [557.475], "images_per_second": 1.794, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A large bird, possibly a crane or heron, stands prominently in the center of the image, partially submerged in a river surrounded by rocks.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25427.0, "ram_available_mb": 100345.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 25427.2, "ram_available_mb": 100345.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 24.23, "peak": 26.79, "min": 22.06}, "VIN": {"avg": 62.03, "peak": 64.51, "min": 60.31}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.23, "energy_joules_est": 13.52, "sample_count": 4, "duration_seconds": 0.558}, "timestamp": "2026-01-17T17:52:42.342463"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1057.33, "latencies_ms": [1057.33], "images_per_second": 0.946, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "River: 10\nRocks: 20\nBird: 1\nBridge: 2\nTrees: 3\nVegetation: 4\nWater: 5\nSky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25427.2, "ram_available_mb": 100345.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25427.7, "ram_available_mb": 100344.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 22.65, "peak": 27.18, "min": 20.09}, "VIN": {"avg": 60.27, "peak": 62.92, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.65, "energy_joules_est": 23.96, "sample_count": 8, "duration_seconds": 1.058}, "timestamp": "2026-01-17T17:52:43.406133"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 604.554, "latencies_ms": [604.554], "images_per_second": 1.654, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The large bird is positioned in the foreground of the image, near the riverbank. The river extends into the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25427.7, "ram_available_mb": 100344.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25427.7, "ram_available_mb": 100344.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 23.54, "peak": 25.6, "min": 21.67}, "VIN": {"avg": 59.84, "peak": 62.22, "min": 57.04}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.54, "energy_joules_est": 14.24, "sample_count": 4, "duration_seconds": 0.605}, "timestamp": "2026-01-17T17:52:44.016398"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 798.568, "latencies_ms": [798.568], "images_per_second": 1.252, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The scene depicts a riverbank with scattered rocks and patches of green vegetation. A bird, possibly a heron, is standing on the rocks near the riverbank, seemingly observing its surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25427.7, "ram_available_mb": 100344.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25428.7, "ram_available_mb": 100343.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 23.17, "peak": 26.79, "min": 20.88}, "VIN": {"avg": 61.1, "peak": 64.43, "min": 57.12}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.17, "energy_joules_est": 18.52, "sample_count": 6, "duration_seconds": 0.799}, "timestamp": "2026-01-17T17:52:44.823602"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 518.048, "latencies_ms": [518.048], "images_per_second": 1.93, "prompt_tokens": 18, "response_tokens_est": 22, "n_tiles": 1, "output_text": "The river is clear and blue. The lighting suggests a sunny day. The riverbed is rocky and uneven.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25428.7, "ram_available_mb": 100343.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25429.2, "ram_available_mb": 100343.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 24.29, "peak": 26.0, "min": 22.85}, "VIN": {"avg": 59.67, "peak": 61.33, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.29, "energy_joules_est": 12.59, "sample_count": 3, "duration_seconds": 0.519}, "timestamp": "2026-01-17T17:52:45.347658"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 709.447, "latencies_ms": [709.447], "images_per_second": 1.41, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A row of parked motor scooters in various colors lines the sidewalk in front of a red-awning bar and restaurant, with people walking by.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25429.2, "ram_available_mb": 100343.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25429.4, "ram_available_mb": 100342.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.54, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 63.88, "peak": 77.95, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 20.98, "sample_count": 5, "duration_seconds": 0.71}, "timestamp": "2026-01-17T17:52:46.070365"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 946.59, "latencies_ms": [946.59], "images_per_second": 1.056, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "building: 5\nscooters: 10\nbar: 1\nstreet: 2\nsigns: 2\npeople: 2\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25429.4, "ram_available_mb": 100342.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25441.8, "ram_available_mb": 100330.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.14, "peak": 37.03, "min": 22.06}, "VIN": {"avg": 63.15, "peak": 84.24, "min": 53.51}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.14, "energy_joules_est": 26.65, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T17:52:47.023910"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1071.878, "latencies_ms": [1071.878], "images_per_second": 0.933, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scooters are parked along the left side of the image, occupying the foreground. The background features buildings with storefronts and awnings, extending into the distance. The scooters appear relatively close together, suggesting they are parked in a designated area.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25441.8, "ram_available_mb": 100330.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25442.3, "ram_available_mb": 100329.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.98, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 64.16, "peak": 90.97, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.98, "energy_joules_est": 28.94, "sample_count": 8, "duration_seconds": 1.073}, "timestamp": "2026-01-17T17:52:48.102597"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 911.072, "latencies_ms": [911.072], "images_per_second": 1.098, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene is set on a busy street in Paris, France, with numerous motor scooters parked along the sidewalk in front of a bar and restaurant. The atmosphere is vibrant and bustling, typical of Parisian urban life.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25442.3, "ram_available_mb": 100329.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25442.3, "ram_available_mb": 100329.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 27.23, "peak": 33.85, "min": 22.06}, "VIN": {"avg": 65.83, "peak": 95.91, "min": 56.25}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 24.82, "sample_count": 7, "duration_seconds": 0.911}, "timestamp": "2026-01-17T17:52:49.019522"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1051.575, "latencies_ms": [1051.575], "images_per_second": 0.951, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene is dominated by vibrant colors, particularly red and green from the storefront awnings and parked motorcycles. The lighting appears to be natural daylight, creating a pleasant atmosphere. The motorcycles are parked in a somewhat organized manner, suggesting an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25442.3, "ram_available_mb": 100329.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25442.8, "ram_available_mb": 100329.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.68, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 64.57, "peak": 90.12, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.68, "energy_joules_est": 29.13, "sample_count": 7, "duration_seconds": 1.052}, "timestamp": "2026-01-17T17:52:50.077866"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 533.439, "latencies_ms": [533.439], "images_per_second": 1.875, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A person's hand is holding a piece of broccoli with a brown, dried insect perched on top.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25442.8, "ram_available_mb": 100329.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 31.62, "peak": 33.86, "min": 28.73}, "VIN": {"avg": 68.82, "peak": 87.33, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.62, "energy_joules_est": 16.89, "sample_count": 3, "duration_seconds": 0.534}, "timestamp": "2026-01-17T17:52:50.622492"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1147.141, "latencies_ms": [1147.141], "images_per_second": 0.872, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Broccoli: 2\nCaterpillar: 1\nBroccoli florets: 6\nHuman fingers: 2\nCountertop: 1\nWall: 1\nStove top: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.39, "peak": 37.41, "min": 20.88}, "VIN": {"avg": 64.74, "peak": 95.46, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.39, "energy_joules_est": 31.43, "sample_count": 9, "duration_seconds": 1.147}, "timestamp": "2026-01-17T17:52:51.776666"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 618.155, "latencies_ms": [618.155], "images_per_second": 1.618, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The broccoli is held in the foreground, while the person's hand is positioned in the background. The broccoli is closer to the viewer than the hand.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.42, "peak": 33.86, "min": 26.38}, "VIN": {"avg": 65.23, "peak": 82.8, "min": 55.33}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.42, "energy_joules_est": 18.82, "sample_count": 4, "duration_seconds": 0.619}, "timestamp": "2026-01-17T17:52:52.400847"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 679.864, "latencies_ms": [679.864], "images_per_second": 1.471, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A person is holding a piece of broccoli with a brown, dried insect on it. The setting appears to be a kitchen counter with a tiled backsplash.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.88, "peak": 37.01, "min": 25.21}, "VIN": {"avg": 65.97, "peak": 90.87, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.88, "energy_joules_est": 21.01, "sample_count": 5, "duration_seconds": 0.68}, "timestamp": "2026-01-17T17:52:53.091342"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 792.023, "latencies_ms": [792.023], "images_per_second": 1.263, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The broccoli is predominantly green and has a slightly browned stem. The lighting appears to be natural, possibly from sunlight, and the material appears to be fresh and likely a leafy vegetable.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25442.6, "ram_available_mb": 100329.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25443.5, "ram_available_mb": 100328.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.96, "peak": 36.63, "min": 25.21}, "VIN": {"avg": 64.58, "peak": 82.42, "min": 54.62}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.96, "energy_joules_est": 24.54, "sample_count": 5, "duration_seconds": 0.792}, "timestamp": "2026-01-17T17:52:53.889484"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 651.808, "latencies_ms": [651.808], "images_per_second": 1.534, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Two people are standing close together, one wearing a black hooded jacket and the other wearing a black jacket with a fur hood, both with their faces partially covered.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25443.5, "ram_available_mb": 100328.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25443.5, "ram_available_mb": 100328.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 31.3, "peak": 35.83, "min": 26.77}, "VIN": {"avg": 69.23, "peak": 96.31, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.3, "energy_joules_est": 20.42, "sample_count": 4, "duration_seconds": 0.652}, "timestamp": "2026-01-17T17:52:54.556401"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 928.3, "latencies_ms": [928.3], "images_per_second": 1.077, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "woman: 2\nman: 2\nhood: 2\njacket: 2\nhair: 2\nface: 2\nfood: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25443.5, "ram_available_mb": 100328.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25443.3, "ram_available_mb": 100328.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.59, "peak": 36.63, "min": 22.45}, "VIN": {"avg": 62.95, "peak": 82.87, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 26.55, "sample_count": 7, "duration_seconds": 0.929}, "timestamp": "2026-01-17T17:52:55.491422"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 928.508, "latencies_ms": [928.508], "images_per_second": 1.077, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The woman is positioned in the foreground, partially obscured by the man's hood. The man's face is partially hidden by the hood, suggesting they are standing close to each other. The background is blurred, indicating a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25443.3, "ram_available_mb": 100328.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25443.5, "ram_available_mb": 100328.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.91, "peak": 35.45, "min": 22.05}, "VIN": {"avg": 64.2, "peak": 87.31, "min": 56.36}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 25.93, "sample_count": 7, "duration_seconds": 0.929}, "timestamp": "2026-01-17T17:52:56.426390"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 830.38, "latencies_ms": [830.38], "images_per_second": 1.204, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Two people are in a dimly lit setting, possibly a cafe or restaurant. They are wearing warm clothing and appear to be enjoying themselves, with one person covering their face with their hands and the other laughing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25443.5, "ram_available_mb": 100328.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 25451.1, "ram_available_mb": 100321.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.49, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 62.56, "peak": 81.35, "min": 54.12}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 23.67, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T17:52:57.263056"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 702.713, "latencies_ms": [702.713], "images_per_second": 1.423, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The woman is wearing a black hooded coat, and the lighting creates a warm, inviting atmosphere. The image appears to be taken indoors, possibly in a dimly lit space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25451.1, "ram_available_mb": 100321.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.78, "peak": 36.63, "min": 24.43}, "VIN": {"avg": 64.43, "peak": 85.44, "min": 56.33}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 20.94, "sample_count": 5, "duration_seconds": 0.703}, "timestamp": "2026-01-17T17:52:57.971742"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 828.806, "latencies_ms": [828.806], "images_per_second": 1.207, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A male tennis player, dressed in white and black, is poised to strike a yellow tennis ball with a racket on a blue tennis court, surrounded by empty white chairs.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25450.4, "ram_available_mb": 100321.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.35, "peak": 37.03, "min": 23.24}, "VIN": {"avg": 67.58, "peak": 95.65, "min": 59.18}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.35, "energy_joules_est": 24.34, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T17:52:58.814695"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1139.371, "latencies_ms": [1139.371], "images_per_second": 0.878, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "man: 1\ntennis racket: 1\ntennis ball: 1\ntennis shoes: 2\ntennis court: 2\nchairs: 8\ngreen surface: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.4, "ram_available_mb": 100321.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.09, "peak": 35.85, "min": 21.28}, "VIN": {"avg": 62.43, "peak": 81.2, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 27.09, "energy_joules_est": 30.87, "sample_count": 8, "duration_seconds": 1.14}, "timestamp": "2026-01-17T17:52:59.959836"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 900.818, "latencies_ms": [900.818], "images_per_second": 1.11, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The man is positioned on the left side of the image, facing towards the right. The tennis ball is in the foreground, slightly in front of him. The tennis court is situated in the background, extending beyond the man's immediate view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25449.1, "ram_available_mb": 100323.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.29, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 64.92, "peak": 87.0, "min": 60.24}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.29, "energy_joules_est": 24.59, "sample_count": 7, "duration_seconds": 0.901}, "timestamp": "2026-01-17T17:53:00.866685"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 920.696, "latencies_ms": [920.696], "images_per_second": 1.086, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A man is playing tennis on a blue court, poised to hit a yellow tennis ball. He is wearing a white shirt, black shorts, and white shoes. Empty white folding chairs are visible in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25449.1, "ram_available_mb": 100323.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25450.5, "ram_available_mb": 100321.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.74, "peak": 35.04, "min": 22.05}, "VIN": {"avg": 63.38, "peak": 92.43, "min": 52.83}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.74, "energy_joules_est": 25.55, "sample_count": 7, "duration_seconds": 0.921}, "timestamp": "2026-01-17T17:53:01.793373"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 946.652, "latencies_ms": [946.652], "images_per_second": 1.056, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The tennis court is predominantly blue. The lighting appears to be bright, likely from sunlight, creating a well-lit playing surface. The materials appear to be standard tennis court surface and netting. The weather appears to be sunny and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.39, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 64.91, "peak": 96.28, "min": 55.52}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.39, "energy_joules_est": 25.94, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T17:53:02.745738"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 627.873, "latencies_ms": [627.873], "images_per_second": 1.593, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A tall, ornate glass vase with an etched design of a woman is prominently displayed on a wooden shelf, accompanied by a lit candle and string lights.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 24.22, "peak": 26.79, "min": 22.06}, "VIN": {"avg": 58.58, "peak": 60.9, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.22, "energy_joules_est": 15.23, "sample_count": 4, "duration_seconds": 0.629}, "timestamp": "2026-01-17T17:53:03.383147"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1002.545, "latencies_ms": [1002.545], "images_per_second": 0.997, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Vase: 1\nCandle: 1\nString lights: 2\nMirror: 1\nWooden frame: 1\nGlass: 1\nFlower: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25451.0, "ram_available_mb": 100321.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 22.85, "peak": 26.79, "min": 20.48}, "VIN": {"avg": 59.94, "peak": 63.26, "min": 56.36}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.85, "energy_joules_est": 22.92, "sample_count": 7, "duration_seconds": 1.003}, "timestamp": "2026-01-17T17:53:04.391516"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 555.411, "latencies_ms": [555.411], "images_per_second": 1.8, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The vase is positioned in the foreground, slightly to the right of the candle. The candle is placed further back, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25451.0, "ram_available_mb": 100321.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.83, "peak": 26.0, "min": 22.06}, "VIN": {"avg": 61.05, "peak": 62.79, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.83, "energy_joules_est": 13.25, "sample_count": 4, "duration_seconds": 0.556}, "timestamp": "2026-01-17T17:53:04.953749"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 730.7, "latencies_ms": [730.7], "images_per_second": 1.369, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The scene is set on a wooden shelf with a decorative vase containing a painted figure, illuminated by string lights. A white candle is placed next to the vase, enhancing the cozy ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 23.79, "peak": 27.18, "min": 21.26}, "VIN": {"avg": 58.46, "peak": 62.55, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.79, "energy_joules_est": 17.39, "sample_count": 5, "duration_seconds": 0.731}, "timestamp": "2026-01-17T17:53:05.690354"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 897.634, "latencies_ms": [897.634], "images_per_second": 1.114, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The vase is primarily a reddish-orange color. The lighting in the scene is warm and inviting, illuminating the vase and the surrounding area. String lights are draped around the shelf, adding a cozy ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 22.98, "peak": 26.38, "min": 20.48}, "VIN": {"avg": 60.38, "peak": 62.07, "min": 56.57}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.98, "energy_joules_est": 20.65, "sample_count": 6, "duration_seconds": 0.898}, "timestamp": "2026-01-17T17:53:06.594740"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 615.151, "latencies_ms": [615.151], "images_per_second": 1.626, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A man is standing in a room, smiling and leaning forward, while another man stands behind him, holding a camera and possibly directing the scene.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 26.0, "min": 21.66}, "VIN": {"avg": 61.34, "peak": 63.48, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 23.63, "energy_joules_est": 14.55, "sample_count": 4, "duration_seconds": 0.616}, "timestamp": "2026-01-17T17:53:07.218709"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1043.219, "latencies_ms": [1043.219], "images_per_second": 0.959, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "suitcase: 1\ncamera: 1\nlighting stand: 1\nman: 2\ncouch: 1\njacket: 1\nclothing: 2\nfloor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.68, "peak": 26.77, "min": 20.09}, "VIN": {"avg": 60.0, "peak": 63.94, "min": 57.75}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 22.68, "energy_joules_est": 23.68, "sample_count": 7, "duration_seconds": 1.044}, "timestamp": "2026-01-17T17:53:08.272662"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 520.741, "latencies_ms": [520.741], "images_per_second": 1.92, "prompt_tokens": 25, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The suitcase is positioned in the foreground, close to the camera. The man is standing in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.8, "ram_available_mb": 100321.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 24.16, "peak": 25.6, "min": 22.85}, "VIN": {"avg": 57.37, "peak": 60.27, "min": 52.24}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.16, "energy_joules_est": 12.6, "sample_count": 3, "duration_seconds": 0.521}, "timestamp": "2026-01-17T17:53:08.800464"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1095.9, "latencies_ms": [1095.9], "images_per_second": 0.912, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 1, "output_text": "A man is standing in a room, possibly a studio, preparing for a photoshoot. He is positioned near a tripod and a suitcase, suggesting he might be a professional photographer or videographer. A second person is partially visible in the background, potentially assisting with the setup or observing the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25450.2, "ram_available_mb": 100321.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.91, "min": 13.49}, "VDD_GPU": {"avg": 22.55, "peak": 27.18, "min": 19.7}, "VIN": {"avg": 59.35, "peak": 62.89, "min": 52.28}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.55, "energy_joules_est": 24.72, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T17:53:09.902919"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 947.616, "latencies_ms": [947.616], "images_per_second": 1.055, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The room has neutral lighting, primarily from overhead spotlights. The walls appear to be a light color, possibly off-white or beige. The floor is wooden and appears to be polished. The clothing worn by the individuals suggests a casual, everyday setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.2, "ram_available_mb": 100321.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.23, "peak": 25.6, "min": 20.09}, "VIN": {"avg": 59.85, "peak": 63.02, "min": 56.59}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 22.23, "energy_joules_est": 21.08, "sample_count": 7, "duration_seconds": 0.948}, "timestamp": "2026-01-17T17:53:10.856705"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 754.129, "latencies_ms": [754.129], "images_per_second": 1.326, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A woman in a wide-brimmed hat and striped tank top smokes a cigarette, smiling warmly as she looks off to the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.77, "peak": 33.47, "min": 24.81}, "VIN": {"avg": 63.35, "peak": 76.8, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.77, "energy_joules_est": 22.47, "sample_count": 5, "duration_seconds": 0.755}, "timestamp": "2026-01-17T17:53:11.624904"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1046.809, "latencies_ms": [1046.809], "images_per_second": 0.955, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "hat: 1\nwoman: 1\ncigarette: 1\ntie: 1\ntank top: 1\nearrings: 1\nbracelet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.11, "peak": 35.83, "min": 21.66}, "VIN": {"avg": 63.48, "peak": 81.14, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.11, "energy_joules_est": 29.44, "sample_count": 8, "duration_seconds": 1.047}, "timestamp": "2026-01-17T17:53:12.677913"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 743.684, "latencies_ms": [743.684], "images_per_second": 1.345, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The woman is positioned in the foreground, wearing a hat and tie. The background is blurred and out of focus, drawing attention to the woman and her actions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 30.48, "peak": 35.04, "min": 25.21}, "VIN": {"avg": 64.88, "peak": 83.58, "min": 57.21}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.48, "energy_joules_est": 22.68, "sample_count": 5, "duration_seconds": 0.744}, "timestamp": "2026-01-17T17:53:13.428636"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 854.28, "latencies_ms": [854.28], "images_per_second": 1.171, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The black and white image shows a woman wearing a wide-brimmed hat and a striped tank top. She is smiling and holding a cigarette in her mouth, appearing relaxed and enjoying herself.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 30.39, "peak": 36.62, "min": 24.03}, "VIN": {"avg": 63.33, "peak": 76.59, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.39, "energy_joules_est": 25.98, "sample_count": 6, "duration_seconds": 0.855}, "timestamp": "2026-01-17T17:53:14.289404"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1266.749, "latencies_ms": [1266.749], "images_per_second": 0.789, "prompt_tokens": 18, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The woman is wearing a wide-brimmed hat in dark colors. The lighting appears to be soft and diffused, possibly from a light source outside the frame. The hat appears to be made of a soft material like felt or wool. The woman is smiling and holding a cigarette, which suggests a relaxed and casual atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.59, "peak": 35.83, "min": 20.48}, "VIN": {"avg": 61.15, "peak": 73.86, "min": 54.04}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.59, "energy_joules_est": 33.7, "sample_count": 10, "duration_seconds": 1.267}, "timestamp": "2026-01-17T17:53:15.562352"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 564.293, "latencies_ms": [564.293], "images_per_second": 1.772, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Two zebras are grazing on green grass in a zoo enclosure, surrounded by trees and rocks.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25450.2, "ram_available_mb": 100321.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.64, "peak": 32.7, "min": 26.0}, "VIN": {"avg": 63.12, "peak": 77.71, "min": 56.26}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.64, "energy_joules_est": 16.75, "sample_count": 4, "duration_seconds": 0.565}, "timestamp": "2026-01-17T17:53:16.139109"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 999.748, "latencies_ms": [999.748], "images_per_second": 1.0, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "zebra: 2\ngrass: 2\nrocks: 2\ntree: 2\nbush: 1\npath: 1\nfence: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25450.2, "ram_available_mb": 100321.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25450.2, "ram_available_mb": 100321.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 28.93, "peak": 37.42, "min": 22.46}, "VIN": {"avg": 63.47, "peak": 87.02, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.93, "energy_joules_est": 28.94, "sample_count": 7, "duration_seconds": 1.0}, "timestamp": "2026-01-17T17:53:17.144991"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 908.222, "latencies_ms": [908.222], "images_per_second": 1.101, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The zebra on the left is positioned closer to the viewer, while the zebra on the right is further away, occupying the foreground. The zebras are situated in a grassy area, which appears relatively open and spacious.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.2, "ram_available_mb": 100321.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25450.1, "ram_available_mb": 100322.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.29, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 62.68, "peak": 74.49, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.29, "energy_joules_est": 24.79, "sample_count": 7, "duration_seconds": 0.908}, "timestamp": "2026-01-17T17:53:18.059197"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 824.756, "latencies_ms": [824.756], "images_per_second": 1.212, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "Two zebras are grazing on green grass in a zoo enclosure. The enclosure features a rocky wall, trees, and a grassy area, providing a natural habitat for the animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.1, "ram_available_mb": 100322.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.76, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 62.53, "peak": 77.78, "min": 56.29}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 28.76, "energy_joules_est": 23.74, "sample_count": 6, "duration_seconds": 0.825}, "timestamp": "2026-01-17T17:53:18.891169"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 768.706, "latencies_ms": [768.706], "images_per_second": 1.301, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The zebras are black and white. The lighting is bright, likely from sunlight filtering through the trees. The grass is green and appears to be well-maintained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.86, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 63.81, "peak": 80.63, "min": 56.26}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.86, "energy_joules_est": 22.97, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T17:53:19.666164"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 581.657, "latencies_ms": [581.657], "images_per_second": 1.719, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A rusted fire hydrant, adorned with a chain, sits on a sidewalk next to a stone wall with a painted design.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25450.5, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 25.11, "peak": 27.97, "min": 22.86}, "VIN": {"avg": 59.1, "peak": 61.55, "min": 56.43}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.11, "energy_joules_est": 14.62, "sample_count": 4, "duration_seconds": 0.582}, "timestamp": "2026-01-17T17:53:20.257848"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1231.02, "latencies_ms": [1231.02], "images_per_second": 0.812, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "fire hydrant: 1\nchain: 1\nbolts: 4\ncap: 1\nhandle: 1\nbumper: 1\nground: 1\npavement: 1\nwall: 1\nplants: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.39}, "VDD_GPU": {"avg": 22.36, "peak": 27.57, "min": 19.7}, "VIN": {"avg": 60.61, "peak": 63.12, "min": 58.99}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.36, "energy_joules_est": 27.54, "sample_count": 9, "duration_seconds": 1.231}, "timestamp": "2026-01-17T17:53:21.494987"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 701.911, "latencies_ms": [701.911], "images_per_second": 1.425, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The fire hydrant is positioned in the foreground, slightly to the right of the image. The background features a stone wall with a painted design and a small flower bed with pink flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.0, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 59.04, "peak": 62.14, "min": 55.26}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.0, "energy_joules_est": 16.15, "sample_count": 5, "duration_seconds": 0.702}, "timestamp": "2026-01-17T17:53:22.202861"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 832.512, "latencies_ms": [832.512], "images_per_second": 1.201, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a weathered fire hydrant situated on a sidewalk next to a stone wall with a painted design. Pink flowers are visible in the background, adding a touch of color to the otherwise muted tones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.2, "ram_available_mb": 100322.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25450.4, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.91, "peak": 26.38, "min": 20.48}, "VIN": {"avg": 58.87, "peak": 62.77, "min": 53.96}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.91, "energy_joules_est": 19.09, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:53:23.042843"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1006.767, "latencies_ms": [1006.767], "images_per_second": 0.993, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The fire hydrant is primarily a light brown color, appearing weathered and possibly rusty. The lighting suggests an outdoor setting, possibly in daylight. The hydrant's materials appear to be metal and possibly painted or stained, giving it a somewhat aged appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25450.4, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25450.4, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.5, "peak": 26.38, "min": 20.08}, "VIN": {"avg": 59.16, "peak": 61.23, "min": 55.85}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 22.5, "energy_joules_est": 22.66, "sample_count": 7, "duration_seconds": 1.007}, "timestamp": "2026-01-17T17:53:24.056273"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 499.239, "latencies_ms": [499.239], "images_per_second": 2.003, "prompt_tokens": 8, "response_tokens_est": 15, "n_tiles": 1, "output_text": "Two brown bears are captured running along a dirt road in a wilderness setting.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25450.4, "ram_available_mb": 100321.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25450.9, "ram_available_mb": 100321.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.4, "min": 14.0}, "VDD_GPU": {"avg": 30.72, "peak": 33.09, "min": 27.97}, "VIN": {"avg": 70.3, "peak": 92.47, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 30.72, "energy_joules_est": 15.35, "sample_count": 3, "duration_seconds": 0.5}, "timestamp": "2026-01-17T17:53:24.566361"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 967.3, "latencies_ms": [967.3], "images_per_second": 1.034, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Bear: 2\nGround: 5\nRocks: 5\nBushes: 2\nBear fur: 5\nBear head: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25450.9, "ram_available_mb": 100321.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25451.7, "ram_available_mb": 100320.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 29.32, "peak": 38.21, "min": 22.85}, "VIN": {"avg": 62.54, "peak": 72.96, "min": 57.22}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 29.32, "energy_joules_est": 28.38, "sample_count": 7, "duration_seconds": 0.968}, "timestamp": "2026-01-17T17:53:25.540508"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 890.293, "latencies_ms": [890.293], "images_per_second": 1.123, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and distance. The foreground bear is closer and larger, while the other bear is further away and smaller, emphasizing the vastness of the environment.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25451.7, "ram_available_mb": 100320.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25451.9, "ram_available_mb": 100320.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.82, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 64.08, "peak": 76.44, "min": 60.45}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.82, "energy_joules_est": 25.67, "sample_count": 6, "duration_seconds": 0.891}, "timestamp": "2026-01-17T17:53:26.436781"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 831.867, "latencies_ms": [831.867], "images_per_second": 1.202, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "Two brown bears are seen in a dry, rocky landscape. One bear is walking towards the camera, while the other is standing in the background. The setting appears to be a natural environment with sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25451.9, "ram_available_mb": 100320.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25451.9, "ram_available_mb": 100320.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.48, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 61.91, "peak": 76.6, "min": 53.76}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.48, "energy_joules_est": 23.71, "sample_count": 6, "duration_seconds": 0.832}, "timestamp": "2026-01-17T17:53:27.276786"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 796.158, "latencies_ms": [796.158], "images_per_second": 1.256, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The bears are brown and appear to be running on a dirt road. The lighting suggests a sunny day, and the material is dirt, gravel, and possibly some rocks.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25451.9, "ram_available_mb": 100320.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 29.45, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 65.44, "peak": 84.67, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.45, "energy_joules_est": 23.46, "sample_count": 5, "duration_seconds": 0.797}, "timestamp": "2026-01-17T17:53:28.079481"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 750.036, "latencies_ms": [750.036], "images_per_second": 1.333, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A young child, dressed in a white shirt and tie, crouches down next to a metal bowl filled with dirt, appearing to be playing or exploring.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 30.33, "peak": 35.85, "min": 25.21}, "VIN": {"avg": 64.05, "peak": 81.53, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.33, "energy_joules_est": 22.77, "sample_count": 5, "duration_seconds": 0.751}, "timestamp": "2026-01-17T17:53:28.839868"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1296.028, "latencies_ms": [1296.028], "images_per_second": 0.772, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Bucket: 1\nGround: 1\nBoy: 1\nTie: 1\nShirt: 1\nPants: 1\nShoes: 1\nGround: 1\nBush: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.24, "peak": 36.24, "min": 20.48}, "VIN": {"avg": 62.83, "peak": 81.05, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.24, "energy_joules_est": 34.02, "sample_count": 10, "duration_seconds": 1.296}, "timestamp": "2026-01-17T17:53:30.142196"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 787.191, "latencies_ms": [787.191], "images_per_second": 1.27, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The child is positioned near the foreground, interacting with a metal bowl on the ground. The background is primarily composed of dark foliage, creating a contrast with the child's lighter clothing and the bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.23, "peak": 34.27, "min": 23.24}, "VIN": {"avg": 63.59, "peak": 78.84, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.23, "energy_joules_est": 22.23, "sample_count": 6, "duration_seconds": 0.788}, "timestamp": "2026-01-17T17:53:30.935796"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 949.424, "latencies_ms": [949.424], "images_per_second": 1.053, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "A young child, dressed in a white shirt and tie, is crouched down near a metal bowl filled with soil, appearing to be digging or playing in the dirt. The setting is outdoors, with a dark background of foliage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25458.6, "ram_available_mb": 100313.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25458.3, "ram_available_mb": 100313.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.07, "peak": 35.44, "min": 22.45}, "VIN": {"avg": 63.61, "peak": 83.65, "min": 54.06}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.07, "energy_joules_est": 26.66, "sample_count": 7, "duration_seconds": 0.95}, "timestamp": "2026-01-17T17:53:31.892192"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 846.552, "latencies_ms": [846.552], "images_per_second": 1.181, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The child is wearing a white shirt and a colorful tie. The lighting appears to be natural, possibly outdoors in sunlight. The child is playing with a metal bowl, possibly filled with soil or dirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25458.3, "ram_available_mb": 100313.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25458.2, "ram_available_mb": 100313.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 28.42, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 61.9, "peak": 74.61, "min": 53.36}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.42, "energy_joules_est": 24.07, "sample_count": 6, "duration_seconds": 0.847}, "timestamp": "2026-01-17T17:53:32.745127"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 595.95, "latencies_ms": [595.95], "images_per_second": 1.678, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A teddy bear wearing clothes sits on a chair amidst a barren, desert-like landscape, accompanied by various items and flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25458.2, "ram_available_mb": 100313.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25458.5, "ram_available_mb": 100313.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 24.72, "peak": 27.57, "min": 22.45}, "VIN": {"avg": 59.0, "peak": 59.9, "min": 58.1}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.72, "energy_joules_est": 14.75, "sample_count": 4, "duration_seconds": 0.597}, "timestamp": "2026-01-17T17:53:33.351645"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1213.18, "latencies_ms": [1213.18], "images_per_second": 0.824, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "teddy bear: 2\nblanket: 1\npillow: 1\nhat: 1\nsitting chair: 1\nstuffed animal: 2\ntable: 2\nflowers: 1\nbeer bottles: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25458.5, "ram_available_mb": 100313.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25458.7, "ram_available_mb": 100313.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 22.1, "peak": 27.18, "min": 19.7}, "VIN": {"avg": 60.64, "peak": 63.58, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.1, "energy_joules_est": 26.82, "sample_count": 9, "duration_seconds": 1.213}, "timestamp": "2026-01-17T17:53:34.570931"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 930.762, "latencies_ms": [930.762], "images_per_second": 1.074, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The stuffed bear is positioned in the foreground, slightly to the right of the chair. The chair and teddy bear are situated in the middle ground, separated by a small table with bottles and flowers. The background is primarily a reddish-brown dirt field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25458.7, "ram_available_mb": 100313.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25458.5, "ram_available_mb": 100313.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.17, "peak": 25.6, "min": 20.09}, "VIN": {"avg": 62.5, "peak": 65.35, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.17, "energy_joules_est": 20.65, "sample_count": 7, "duration_seconds": 0.931}, "timestamp": "2026-01-17T17:53:35.507834"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1000.318, "latencies_ms": [1000.318], "images_per_second": 1.0, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene is set in a barren, desert-like environment. A teddy bear is lying on a makeshift chair amidst various items, including clothes, flowers, and bottles. The chair appears to be repurposed as a makeshift table or bed.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25458.5, "ram_available_mb": 100313.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25458.7, "ram_available_mb": 100313.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.28, "peak": 26.0, "min": 20.09}, "VIN": {"avg": 61.27, "peak": 64.94, "min": 55.94}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.28, "energy_joules_est": 22.3, "sample_count": 7, "duration_seconds": 1.001}, "timestamp": "2026-01-17T17:53:36.514660"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1016.869, "latencies_ms": [1016.869], "images_per_second": 0.983, "prompt_tokens": 18, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The scene is dominated by earthy tones of brown and reddish-brown. The lighting appears to be natural sunlight, creating a warm and inviting atmosphere. The materials include fabric, wood, and metal, contributing to the rustic aesthetic. The setting suggests a dry, arid environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25458.7, "ram_available_mb": 100313.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25459.0, "ram_available_mb": 100313.2, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.28, "peak": 26.0, "min": 20.09}, "VIN": {"avg": 60.41, "peak": 63.83, "min": 57.08}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.28, "energy_joules_est": 22.67, "sample_count": 7, "duration_seconds": 1.017}, "timestamp": "2026-01-17T17:53:37.537722"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 592.002, "latencies_ms": [592.002], "images_per_second": 1.689, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "Two green fishing boats are docked at a wooden pier, ready for their next voyage.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25459.0, "ram_available_mb": 100313.2, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25458.4, "ram_available_mb": 100313.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 31.01, "peak": 33.86, "min": 27.17}, "VIN": {"avg": 67.78, "peak": 94.21, "min": 52.94}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.01, "energy_joules_est": 18.38, "sample_count": 4, "duration_seconds": 0.593}, "timestamp": "2026-01-17T17:53:38.142499"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1597.945, "latencies_ms": [1597.945], "images_per_second": 0.626, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 1, "output_text": "Boat: 3\nFishing boat: 2\nDock: 2\nPier: 1\nOars: 1\nBuoys: 2\nLife preservers: 2\nFishing gear: 2\nWater: 1\nSailboats: 1\nHills: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25458.4, "ram_available_mb": 100313.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25461.4, "ram_available_mb": 100310.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 25.99, "peak": 37.8, "min": 19.69}, "VIN": {"avg": 63.03, "peak": 97.42, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.99, "energy_joules_est": 41.55, "sample_count": 12, "duration_seconds": 1.599}, "timestamp": "2026-01-17T17:53:39.747118"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 664.634, "latencies_ms": [664.634], "images_per_second": 1.505, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the boats docked further back. The water extends into the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25461.4, "ram_available_mb": 100310.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25461.7, "ram_available_mb": 100310.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 30.63, "peak": 33.86, "min": 26.8}, "VIN": {"avg": 65.1, "peak": 75.47, "min": 59.68}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 20.38, "sample_count": 4, "duration_seconds": 0.665}, "timestamp": "2026-01-17T17:53:40.418172"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1082.89, "latencies_ms": [1082.89], "images_per_second": 0.923, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene depicts a harbor with several fishing boats docked near a pier. The boats are equipped with various safety features, including life jackets and orange buoys. The setting is surrounded by hills and mountains, creating a picturesque backdrop for the harbor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25461.7, "ram_available_mb": 100310.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25461.4, "ram_available_mb": 100310.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.41, "peak": 37.03, "min": 21.66}, "VIN": {"avg": 62.5, "peak": 68.39, "min": 59.38}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.41, "energy_joules_est": 30.78, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T17:53:41.506987"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1015.214, "latencies_ms": [1015.214], "images_per_second": 0.985, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The boats are primarily green and white. The lighting is soft and diffused, suggesting an overcast sky. The boats appear to be made of sturdy materials like metal and wood. The overall scene conveys a peaceful and tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25461.4, "ram_available_mb": 100310.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25461.4, "ram_available_mb": 100310.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.36, "peak": 35.04, "min": 22.44}, "VIN": {"avg": 62.91, "peak": 77.75, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 28.8, "sample_count": 7, "duration_seconds": 1.016}, "timestamp": "2026-01-17T17:53:42.529395"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 511.365, "latencies_ms": [511.365], "images_per_second": 1.956, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A woman with short brown hair bites into a hot dog, wearing a dark jacket and scarf, with a blurred background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25461.4, "ram_available_mb": 100310.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 7.0, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 24.94, "peak": 26.77, "min": 23.24}, "VIN": {"avg": 62.03, "peak": 64.44, "min": 60.29}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 24.94, "energy_joules_est": 12.76, "sample_count": 3, "duration_seconds": 0.512}, "timestamp": "2026-01-17T17:53:43.049778"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1015.97, "latencies_ms": [1015.97], "images_per_second": 0.984, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "hot dog: 1\nwoman: 1\nfood: 1\nnapkin: 1\nface: 1\nhair: 1\neyes: 1\ntongue: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 23.13, "peak": 27.57, "min": 20.48}, "VIN": {"avg": 59.6, "peak": 62.65, "min": 56.72}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 23.13, "energy_joules_est": 23.51, "sample_count": 7, "duration_seconds": 1.016}, "timestamp": "2026-01-17T17:53:44.071476"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 880.943, "latencies_ms": [880.943], "images_per_second": 1.135, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The woman is positioned in the foreground, eating a hot dog. The background is blurred, suggesting an outdoor setting at night. The hot dog is held in her right hand, while the woman's face and upper body are visible.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 22.51, "peak": 25.59, "min": 20.48}, "VIN": {"avg": 59.55, "peak": 64.14, "min": 55.46}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 22.51, "energy_joules_est": 19.84, "sample_count": 6, "duration_seconds": 0.881}, "timestamp": "2026-01-17T17:53:44.958454"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 738.535, "latencies_ms": [738.535], "images_per_second": 1.354, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A woman is eating a hot dog at night in a dimly lit setting, possibly outdoors or in a bar or restaurant. The background is blurred, drawing focus to the woman and her food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25461.4, "ram_available_mb": 100310.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 23.08, "peak": 26.0, "min": 20.88}, "VIN": {"avg": 60.97, "peak": 64.32, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.08, "energy_joules_est": 17.05, "sample_count": 5, "duration_seconds": 0.739}, "timestamp": "2026-01-17T17:53:45.703007"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 662.857, "latencies_ms": [662.857], "images_per_second": 1.509, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The woman is wearing a dark-colored jacket. The background is dimly lit, suggesting an outdoor setting at night. The food she's eating appears to be a hot dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25461.4, "ram_available_mb": 100310.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25461.7, "ram_available_mb": 100310.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 23.16, "peak": 25.99, "min": 20.88}, "VIN": {"avg": 59.7, "peak": 62.11, "min": 53.25}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.16, "energy_joules_est": 15.36, "sample_count": 5, "duration_seconds": 0.663}, "timestamp": "2026-01-17T17:53:46.372762"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 670.075, "latencies_ms": [670.075], "images_per_second": 1.492, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A man in a black suit and bow tie holds a martini glass while standing next to a woman in a long black dress.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25461.7, "ram_available_mb": 100310.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25461.7, "ram_available_mb": 100310.5, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 29.23, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 63.63, "peak": 75.61, "min": 55.85}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.23, "energy_joules_est": 19.6, "sample_count": 5, "duration_seconds": 0.67}, "timestamp": "2026-01-17T17:53:47.051793"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1248.51, "latencies_ms": [1248.51], "images_per_second": 0.801, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "woman: 2\nman: 1\ndress: 1\ngloves: 1\nbow tie: 1\ncocktail glass: 1\nman: 1\ndoor: 1\ncurtains: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25461.7, "ram_available_mb": 100310.5, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 26.57, "peak": 35.83, "min": 20.48}, "VIN": {"avg": 62.46, "peak": 97.03, "min": 53.37}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.57, "energy_joules_est": 33.18, "sample_count": 9, "duration_seconds": 1.249}, "timestamp": "2026-01-17T17:53:48.310487"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 853.297, "latencies_ms": [853.297], "images_per_second": 1.172, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The woman is positioned to the left of the man, who is standing further back in the image. The man is standing in the foreground, while the woman is positioned further back, creating a sense of depth in the composition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25461.2, "ram_available_mb": 100311.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25461.4, "ram_available_mb": 100310.8, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 27.37, "peak": 33.09, "min": 22.45}, "VIN": {"avg": 66.25, "peak": 93.73, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 27.37, "energy_joules_est": 23.36, "sample_count": 6, "duration_seconds": 0.854}, "timestamp": "2026-01-17T17:53:49.169202"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 939.014, "latencies_ms": [939.014], "images_per_second": 1.065, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A man and a woman are standing in a doorway, holding a martini glass. The woman is wearing a long dark dress, while the man is dressed in a tuxedo. They are likely attending a formal event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25461.4, "ram_available_mb": 100310.8, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25460.9, "ram_available_mb": 100311.3, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.51, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 63.64, "peak": 88.91, "min": 53.12}, "VDD_CPU_SOC_MSS": {"avg": 14.78, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.51, "energy_joules_est": 25.85, "sample_count": 7, "duration_seconds": 0.94}, "timestamp": "2026-01-17T17:53:50.114661"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1076.98, "latencies_ms": [1076.98], "images_per_second": 0.929, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The woman is wearing a dark green dress, and the man is wearing a dark suit with a bow tie. The lighting is soft and subdued, suggesting an indoor setting. The materials appear to be standard clothing, potentially silk or satin, contributing to the elegant appearance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25460.9, "ram_available_mb": 100311.3, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25461.1, "ram_available_mb": 100311.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.69, "peak": 34.27, "min": 21.28}, "VIN": {"avg": 61.88, "peak": 91.76, "min": 51.44}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.69, "energy_joules_est": 28.76, "sample_count": 8, "duration_seconds": 1.077}, "timestamp": "2026-01-17T17:53:51.198734"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 658.586, "latencies_ms": [658.586], "images_per_second": 1.518, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A blue wooden shelf holds various items, including several small metal cups, a silver teapot, a vase, and several small orange price tags.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25461.1, "ram_available_mb": 100311.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25460.8, "ram_available_mb": 100311.4, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 23.4, "peak": 26.39, "min": 21.28}, "VIN": {"avg": 59.81, "peak": 64.11, "min": 52.16}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.4, "energy_joules_est": 15.42, "sample_count": 5, "duration_seconds": 0.659}, "timestamp": "2026-01-17T17:53:51.867157"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1703.184, "latencies_ms": [1703.184], "images_per_second": 0.587, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 1, "output_text": "vase: 1\nteapot: 1\nsmall metal container: 1\nsmall metal cup: 2\nsmall metal teapot: 1\nbook: 1\nsmall metal container: 1\nsmall metal teapot: 1\nsmall metal cup: 2\nsmall metal teapot: 1\nsmall metal container: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25460.8, "ram_available_mb": 100311.4, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25460.2, "ram_available_mb": 100312.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 13.69}, "VDD_GPU": {"avg": 21.12, "peak": 26.79, "min": 18.91}, "VIN": {"avg": 60.0, "peak": 64.81, "min": 55.7}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 21.12, "energy_joules_est": 35.98, "sample_count": 13, "duration_seconds": 1.704}, "timestamp": "2026-01-17T17:53:53.576666"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 960.054, "latencies_ms": [960.054], "images_per_second": 1.042, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The silver teapot and cups are placed on the blue shelf in the foreground, while the silver vase and candlesticks are positioned on the wooden table in the background. The arrangement suggests a casual, informal setting, possibly a home or personal collection.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25460.2, "ram_available_mb": 100312.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25460.2, "ram_available_mb": 100312.0, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 22.06, "peak": 25.2, "min": 20.09}, "VIN": {"avg": 60.47, "peak": 64.53, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 22.06, "energy_joules_est": 21.19, "sample_count": 7, "duration_seconds": 0.96}, "timestamp": "2026-01-17T17:53:54.542587"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1139.165, "latencies_ms": [1139.165], "images_per_second": 0.878, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The scene depicts a cluttered outdoor setting, possibly a yard sale or flea market. A blue shelf holds various items, including vases, cups, and books, alongside other miscellaneous items.  The items are arranged haphazardly, suggesting a casual and possibly temporary arrangement.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25460.2, "ram_available_mb": 100312.0, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25452.0, "ram_available_mb": 100320.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 21.91, "peak": 25.61, "min": 19.7}, "VIN": {"avg": 61.78, "peak": 65.29, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 21.91, "energy_joules_est": 24.97, "sample_count": 8, "duration_seconds": 1.14}, "timestamp": "2026-01-17T17:53:55.688080"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 826.804, "latencies_ms": [826.804], "images_per_second": 1.209, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The blue shelf stands out against the surrounding furniture and objects. The lighting in the image is soft and diffused, creating a calm atmosphere. The shelf appears to be made of metal and has a slightly worn or weathered appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25452.0, "ram_available_mb": 100320.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25452.5, "ram_available_mb": 100319.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.58, "peak": 25.6, "min": 20.48}, "VIN": {"avg": 60.19, "peak": 62.81, "min": 57.83}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.58, "energy_joules_est": 18.68, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T17:53:56.520877"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 751.46, "latencies_ms": [751.46], "images_per_second": 1.331, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A white plate holds four square crackers topped with a white substance, likely butter or cream cheese, accompanied by a black computer keyboard and a computer mouse in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25452.5, "ram_available_mb": 100319.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25452.5, "ram_available_mb": 100319.7, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.84, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 66.06, "peak": 91.91, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.84, "energy_joules_est": 21.69, "sample_count": 5, "duration_seconds": 0.752}, "timestamp": "2026-01-17T17:53:57.283056"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 784.411, "latencies_ms": [784.411], "images_per_second": 1.275, "prompt_tokens": 21, "response_tokens_est": 22, "n_tiles": 1, "output_text": "keyboard: 2\nmouse: 1\nplate: 4\nbutter: 4\ncookies: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25452.5, "ram_available_mb": 100319.7, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25452.3, "ram_available_mb": 100319.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.88, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 65.43, "peak": 94.61, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.88, "energy_joules_est": 22.67, "sample_count": 6, "duration_seconds": 0.785}, "timestamp": "2026-01-17T17:53:58.073469"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 859.526, "latencies_ms": [859.526], "images_per_second": 1.163, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the plate of crackers slightly behind them. The background includes a keyboard, a mouse, and a partially visible cup, suggesting an office or workspace setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25452.3, "ram_available_mb": 100319.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.75, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 63.42, "peak": 69.74, "min": 61.09}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 24.72, "sample_count": 6, "duration_seconds": 0.86}, "timestamp": "2026-01-17T17:53:58.939586"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 842.979, "latencies_ms": [842.979], "images_per_second": 1.186, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A white plate with several square crackers topped with a white substance sits on a desk. A black computer keyboard and a computer mouse are visible in the background, along with a partially visible yellow object.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 64.9, "peak": 88.67, "min": 57.96}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 28.75, "energy_joules_est": 24.24, "sample_count": 6, "duration_seconds": 0.843}, "timestamp": "2026-01-17T17:53:59.788497"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 896.807, "latencies_ms": [896.807], "images_per_second": 1.115, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The plate is white and appears to be made of ceramic or porcelain. The butter on the crackers is a pale yellow color. The lighting in the image is soft and diffused, creating a gentle ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25457.6, "ram_available_mb": 100314.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.61, "peak": 35.03, "min": 23.23}, "VIN": {"avg": 66.82, "peak": 97.77, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.61, "energy_joules_est": 25.67, "sample_count": 6, "duration_seconds": 0.897}, "timestamp": "2026-01-17T17:54:00.691488"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 547.183, "latencies_ms": [547.183], "images_per_second": 1.828, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A man in a black suit and tie adjusts his colorful tie against a dark gray background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25457.6, "ram_available_mb": 100314.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 31.8, "peak": 35.04, "min": 27.57}, "VIN": {"avg": 68.81, "peak": 96.31, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.8, "energy_joules_est": 17.42, "sample_count": 4, "duration_seconds": 0.548}, "timestamp": "2026-01-17T17:54:01.249618"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 995.108, "latencies_ms": [995.108], "images_per_second": 1.005, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "man: 2\nglasses: 1\nsuit: 1\ntie: 1\nring: 1\nbelt: 1\nbackground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 29.93, "peak": 38.59, "min": 22.85}, "VIN": {"avg": 61.2, "peak": 74.1, "min": 55.11}, "VDD_CPU_SOC_MSS": {"avg": 14.74, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.93, "energy_joules_est": 29.8, "sample_count": 7, "duration_seconds": 0.996}, "timestamp": "2026-01-17T17:54:02.255330"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 720.589, "latencies_ms": [720.589], "images_per_second": 1.388, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The man is positioned in the foreground, with the illuminated tie extending towards his right. The tie is centrally placed in the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.56, "peak": 35.06, "min": 25.2}, "VIN": {"avg": 64.97, "peak": 86.41, "min": 56.25}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.56, "energy_joules_est": 22.04, "sample_count": 5, "duration_seconds": 0.721}, "timestamp": "2026-01-17T17:54:02.982432"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 605.55, "latencies_ms": [605.55], "images_per_second": 1.651, "prompt_tokens": 19, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A man in a suit is adjusting a glowing tie against a dark background. He appears to be in a professional or formal setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 33.18, "peak": 37.01, "min": 28.36}, "VIN": {"avg": 63.26, "peak": 77.0, "min": 53.68}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.18, "energy_joules_est": 20.1, "sample_count": 4, "duration_seconds": 0.606}, "timestamp": "2026-01-17T17:54:03.594034"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1110.764, "latencies_ms": [1110.764], "images_per_second": 0.9, "prompt_tokens": 18, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The tie features a vibrant array of colors, including red, green, and yellow, creating a visually striking effect. The lighting highlights the colors and creates a dynamic effect. The tie appears to be made of a smooth, potentially synthetic material. The man is wearing a suit, suggesting formal attire.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25457.1, "ram_available_mb": 100315.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.19, "peak": 38.19, "min": 22.06}, "VIN": {"avg": 61.93, "peak": 80.11, "min": 54.28}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.19, "energy_joules_est": 32.43, "sample_count": 8, "duration_seconds": 1.111}, "timestamp": "2026-01-17T17:54:04.710604"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 878.665, "latencies_ms": [878.665], "images_per_second": 1.138, "prompt_tokens": 8, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A woman in a brown jacket and blue jeans is walking across a street corner, carrying a black bag and passing by a white building with a curved facade and a \"TADURIA\" sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.1, "ram_available_mb": 100315.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25457.0, "ram_available_mb": 100315.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.3, "peak": 34.66, "min": 23.25}, "VIN": {"avg": 64.32, "peak": 83.98, "min": 57.79}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 28.3, "energy_joules_est": 24.88, "sample_count": 6, "duration_seconds": 0.879}, "timestamp": "2026-01-17T17:54:05.602804"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1064.512, "latencies_ms": [1064.512], "images_per_second": 0.939, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "building: 4\nstreetlight: 1\nperson: 1\ncar: 1\ntraffic light: 1\nshop: 1\nsign: 1\npeople: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.0, "ram_available_mb": 100315.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25457.0, "ram_available_mb": 100315.1, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.93, "peak": 34.66, "min": 21.27}, "VIN": {"avg": 63.04, "peak": 88.01, "min": 54.36}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.93, "energy_joules_est": 28.69, "sample_count": 8, "duration_seconds": 1.065}, "timestamp": "2026-01-17T17:54:06.673905"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 827.856, "latencies_ms": [827.856], "images_per_second": 1.208, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The woman is standing in the foreground, facing the camera. The building is positioned behind her, slightly to the right. The street and traffic lights are in the background, further away from the woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.0, "ram_available_mb": 100315.1, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.43, "peak": 33.88, "min": 23.24}, "VIN": {"avg": 62.01, "peak": 77.09, "min": 54.09}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.35, "min": 14.96}}, "power_watts_avg": 28.43, "energy_joules_est": 23.55, "sample_count": 6, "duration_seconds": 0.828}, "timestamp": "2026-01-17T17:54:07.507971"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1023.274, "latencies_ms": [1023.274], "images_per_second": 0.977, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "A woman is walking across a street corner in front of a white building with a \"TADURIA\" sign. A traffic light is visible, and several other pedestrians can be seen in the background. The scene is illuminated by streetlights, creating a calm and nighttime atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.3, "ram_available_mb": 100314.9, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25457.5, "ram_available_mb": 100314.6, "ram_percent": 20.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.03, "peak": 35.04, "min": 21.28}, "VIN": {"avg": 63.92, "peak": 81.72, "min": 58.92}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.03, "energy_joules_est": 27.67, "sample_count": 8, "duration_seconds": 1.024}, "timestamp": "2026-01-17T17:54:08.537270"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 862.413, "latencies_ms": [862.413], "images_per_second": 1.16, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The building is primarily white with light-colored trim and features warm lighting from street lamps and building lights. The sky is a deep blue, suggesting it's either dusk or dawn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25457.5, "ram_available_mb": 100314.6, "ram_percent": 20.2}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25469.5, "ram_available_mb": 100302.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.42, "peak": 34.66, "min": 23.25}, "VIN": {"avg": 65.06, "peak": 89.58, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 24.52, "sample_count": 6, "duration_seconds": 0.863}, "timestamp": "2026-01-17T17:54:09.405696"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 722.294, "latencies_ms": [722.294], "images_per_second": 1.384, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A young girl in a purple bikini skillfully surfs a wave on a blue surfboard, surrounded by other surfers in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25469.5, "ram_available_mb": 100302.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25469.8, "ram_available_mb": 100302.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.62, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 64.07, "peak": 74.45, "min": 60.09}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.62, "energy_joules_est": 21.41, "sample_count": 5, "duration_seconds": 0.723}, "timestamp": "2026-01-17T17:54:10.141555"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1048.004, "latencies_ms": [1048.004], "images_per_second": 0.954, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "surfboard: 2\nwoman: 1\nwoman: 1\nman: 1\nman: 1\nwoman: 1\nman: 1\nwoman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25469.8, "ram_available_mb": 100302.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25469.8, "ram_available_mb": 100302.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.47, "peak": 36.63, "min": 21.27}, "VIN": {"avg": 62.46, "peak": 86.22, "min": 56.07}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.47, "energy_joules_est": 28.8, "sample_count": 8, "duration_seconds": 1.048}, "timestamp": "2026-01-17T17:54:11.195648"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1117.721, "latencies_ms": [1117.721], "images_per_second": 0.895, "prompt_tokens": 25, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The main object, the young girl surfing, is positioned in the foreground of the image. The background features other surfers on surfboards, indicating a shared surfing environment. The girl is relatively close to the foreground, riding a wave, while the other surfers are further back in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25469.7, "ram_available_mb": 100302.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25470.0, "ram_available_mb": 100302.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.53, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 64.01, "peak": 82.1, "min": 57.84}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.53, "energy_joules_est": 29.67, "sample_count": 8, "duration_seconds": 1.118}, "timestamp": "2026-01-17T17:54:12.323381"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1105.581, "latencies_ms": [1105.581], "images_per_second": 0.905, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a lively beach surfing environment with several people enjoying the activity. A young girl in a bikini is skillfully riding a wave on a blue surfboard, while two other individuals are paddling and waiting for their turn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25470.0, "ram_available_mb": 100302.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25470.2, "ram_available_mb": 100301.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.49, "peak": 33.88, "min": 21.27}, "VIN": {"avg": 62.14, "peak": 70.15, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.49, "energy_joules_est": 29.3, "sample_count": 8, "duration_seconds": 1.106}, "timestamp": "2026-01-17T17:54:13.435412"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 848.781, "latencies_ms": [848.781], "images_per_second": 1.178, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The water is a greenish-blue color. The lighting appears to be natural daylight. The surfboards appear to be made of fiberglass or similar lightweight materials. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25470.2, "ram_available_mb": 100301.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25470.2, "ram_available_mb": 100301.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.1, "peak": 33.89, "min": 22.85}, "VIN": {"avg": 62.53, "peak": 83.15, "min": 51.7}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.1, "energy_joules_est": 23.86, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T17:54:14.290311"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 598.157, "latencies_ms": [598.157], "images_per_second": 1.672, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A man in a white shirt and khaki pants is feeding a gray elephant over a fence in a zoo enclosure.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25470.2, "ram_available_mb": 100301.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25470.2, "ram_available_mb": 100301.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.83, "peak": 34.66, "min": 26.8}, "VIN": {"avg": 64.67, "peak": 82.26, "min": 52.97}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.83, "energy_joules_est": 18.46, "sample_count": 4, "duration_seconds": 0.599}, "timestamp": "2026-01-17T17:54:14.900530"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 967.209, "latencies_ms": [967.209], "images_per_second": 1.034, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "elephant: 1\nman: 1\nfence: 2\ngrass: 2\ntree: 2\nsky: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25470.2, "ram_available_mb": 100301.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25470.2, "ram_available_mb": 100301.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 28.63, "peak": 36.62, "min": 22.06}, "VIN": {"avg": 66.11, "peak": 86.87, "min": 59.68}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 28.63, "energy_joules_est": 27.7, "sample_count": 7, "duration_seconds": 0.968}, "timestamp": "2026-01-17T17:54:15.873799"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 794.785, "latencies_ms": [794.785], "images_per_second": 1.258, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The elephant is positioned to the left of the man, who is positioned to the right of the elephant. The man is standing in the foreground, while the elephant is further back, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25470.2, "ram_available_mb": 100302.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25470.7, "ram_available_mb": 100301.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.42, "peak": 35.06, "min": 22.84}, "VIN": {"avg": 66.26, "peak": 97.52, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 22.6, "sample_count": 6, "duration_seconds": 0.795}, "timestamp": "2026-01-17T17:54:16.675367"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 839.716, "latencies_ms": [839.716], "images_per_second": 1.191, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "An elephant is standing in a fenced enclosure, reaching over a metal railing to interact with a man. The man is feeding the elephant, seemingly enjoying the interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25470.7, "ram_available_mb": 100301.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25470.7, "ram_available_mb": 100301.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.95, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 63.21, "peak": 89.56, "min": 54.78}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 24.32, "sample_count": 6, "duration_seconds": 0.84}, "timestamp": "2026-01-17T17:54:17.521140"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1163.603, "latencies_ms": [1163.603], "images_per_second": 0.859, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The elephant is gray and appears to have some grayish-brown patches on its skin. The lighting in the image suggests it might be daytime, with natural light illuminating the scene. The enclosure appears to be constructed of concrete and metal fencing, typical of zoo or wildlife park settings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25470.7, "ram_available_mb": 100301.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25470.7, "ram_available_mb": 100301.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.31, "peak": 35.07, "min": 20.88}, "VIN": {"avg": 61.76, "peak": 76.08, "min": 55.55}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.31, "energy_joules_est": 30.62, "sample_count": 9, "duration_seconds": 1.164}, "timestamp": "2026-01-17T17:54:18.691164"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 560.219, "latencies_ms": [560.219], "images_per_second": 1.785, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A brown dog sits on a bed amidst a pile of clothes, bags, and boxes.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25470.7, "ram_available_mb": 100301.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25476.8, "ram_available_mb": 100295.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.42, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 68.59, "peak": 90.41, "min": 60.62}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.42, "energy_joules_est": 17.07, "sample_count": 4, "duration_seconds": 0.561}, "timestamp": "2026-01-17T17:54:19.261908"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 986.416, "latencies_ms": [986.416], "images_per_second": 1.014, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "bed: 2\npillows: 2\nblankets: 2\nbags: 1\nbox: 1\nclothes: 2\nwindow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.8, "ram_available_mb": 100295.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25476.2, "ram_available_mb": 100296.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.97, "peak": 38.19, "min": 22.45}, "VIN": {"avg": 66.44, "peak": 95.87, "min": 55.81}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.97, "energy_joules_est": 28.59, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T17:54:20.255148"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 927.947, "latencies_ms": [927.947], "images_per_second": 1.078, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The dog is positioned on the left side of the image, near the foreground. The bed occupies the foreground and background, extending from the left to the right side of the image. The dog is situated on the bed amidst the clutter.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25476.2, "ram_available_mb": 100296.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25475.7, "ram_available_mb": 100296.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.51, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 62.43, "peak": 78.39, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.51, "energy_joules_est": 25.55, "sample_count": 7, "duration_seconds": 0.929}, "timestamp": "2026-01-17T17:54:21.189462"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 729.737, "latencies_ms": [729.737], "images_per_second": 1.37, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A brown dog sits on a bed surrounded by various items, including clothes, a backpack, and a box. The bed appears to be in a messy or disorganized state.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.7, "ram_available_mb": 100296.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.46, "peak": 35.04, "min": 24.41}, "VIN": {"avg": 65.39, "peak": 74.21, "min": 60.27}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 21.51, "sample_count": 5, "duration_seconds": 0.73}, "timestamp": "2026-01-17T17:54:21.925407"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 911.774, "latencies_ms": [911.774], "images_per_second": 1.097, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The dog is gray and sits on a bed covered in clothes and blankets. The lighting in the room appears to be soft and diffused, suggesting natural light coming from a window or possibly a nearby lamp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25476.2, "ram_available_mb": 100296.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.2, "peak": 35.85, "min": 22.46}, "VIN": {"avg": 64.94, "peak": 83.69, "min": 59.37}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.2, "energy_joules_est": 25.72, "sample_count": 7, "duration_seconds": 0.912}, "timestamp": "2026-01-17T17:54:22.843547"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 646.959, "latencies_ms": [646.959], "images_per_second": 1.546, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A man, dressed professionally in a blue shirt and tie, sits at a desk with a laptop, pen in hand, appearing deep in thought.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.2, "ram_available_mb": 100296.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 24.62, "peak": 27.18, "min": 22.45}, "VIN": {"avg": 57.43, "peak": 58.75, "min": 54.04}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.62, "energy_joules_est": 15.94, "sample_count": 4, "duration_seconds": 0.647}, "timestamp": "2026-01-17T17:54:23.502797"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1060.613, "latencies_ms": [1060.613], "images_per_second": 0.943, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "laptop: 1\nglasses: 1\nnotebook: 1\npen: 1\ntie: 1\nman: 1\ntable: 1\nclipboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 22.31, "peak": 26.79, "min": 19.7}, "VIN": {"avg": 60.47, "peak": 63.68, "min": 54.17}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.31, "energy_joules_est": 23.67, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T17:54:24.570480"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 724.342, "latencies_ms": [724.342], "images_per_second": 1.381, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The man is seated at a desk with a laptop and papers, indicating a workspace. The background is blurred, suggesting the setting is likely an office or professional environment.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 22.85, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 60.38, "peak": 62.87, "min": 57.77}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 22.85, "energy_joules_est": 16.56, "sample_count": 5, "duration_seconds": 0.725}, "timestamp": "2026-01-17T17:54:25.301341"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 676.865, "latencies_ms": [676.865], "images_per_second": 1.477, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A man is sitting at a desk in an office, appearing thoughtful as he looks up from his laptop. He is surrounded by office supplies, including a pen and papers, suggesting a professional environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25475.6, "ram_available_mb": 100296.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 23.4, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 61.11, "peak": 62.64, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.4, "energy_joules_est": 15.85, "sample_count": 5, "duration_seconds": 0.677}, "timestamp": "2026-01-17T17:54:25.986781"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 739.339, "latencies_ms": [739.339], "images_per_second": 1.353, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The man is wearing a light blue shirt and a blue tie. His glasses are silver and resting on his chin. The background is blurred, suggesting an office setting with natural lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.6, "ram_available_mb": 100296.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 23.39, "peak": 26.39, "min": 21.26}, "VIN": {"avg": 59.0, "peak": 63.0, "min": 54.42}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.39, "energy_joules_est": 17.3, "sample_count": 5, "duration_seconds": 0.74}, "timestamp": "2026-01-17T17:54:26.734648"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 630.616, "latencies_ms": [630.616], "images_per_second": 1.586, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A large commercial airplane with red accents is captured in mid-flight, passing a full moon in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 29.92, "peak": 33.47, "min": 25.99}, "VIN": {"avg": 63.78, "peak": 71.21, "min": 59.87}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 29.92, "energy_joules_est": 18.88, "sample_count": 4, "duration_seconds": 0.631}, "timestamp": "2026-01-17T17:54:27.376302"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 655.02, "latencies_ms": [655.02], "images_per_second": 1.527, "prompt_tokens": 21, "response_tokens_est": 17, "n_tiles": 1, "output_text": "airplane: 1\nmoon: 1\nclouds: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.9, "ram_available_mb": 100296.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25475.6, "ram_available_mb": 100296.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 31.79, "peak": 36.23, "min": 27.17}, "VIN": {"avg": 66.37, "peak": 82.05, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 31.79, "energy_joules_est": 20.83, "sample_count": 4, "duration_seconds": 0.655}, "timestamp": "2026-01-17T17:54:28.038827"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 676.586, "latencies_ms": [676.586], "images_per_second": 1.478, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The airplane is flying towards the right side of the image, while the moon is positioned in the lower left corner, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.6, "ram_available_mb": 100296.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25475.6, "ram_available_mb": 100296.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 30.88, "peak": 37.01, "min": 25.22}, "VIN": {"avg": 64.78, "peak": 81.83, "min": 58.14}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.88, "energy_joules_est": 20.9, "sample_count": 5, "duration_seconds": 0.677}, "timestamp": "2026-01-17T17:54:28.721440"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 776.318, "latencies_ms": [776.318], "images_per_second": 1.288, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A large commercial airplane is captured in mid-flight against a clear, light blue sky. A large, full moon is visible in the lower left corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25475.6, "ram_available_mb": 100296.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.96, "peak": 37.42, "min": 25.2}, "VIN": {"avg": 64.71, "peak": 84.12, "min": 56.74}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.96, "energy_joules_est": 24.04, "sample_count": 5, "duration_seconds": 0.777}, "timestamp": "2026-01-17T17:54:29.504304"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 972.96, "latencies_ms": [972.96], "images_per_second": 1.028, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The moon is a pale orange-yellow color, illuminated by the sun's light. The airplane is primarily gray with red accents and appears to be in flight. The sky is a light blue, suggesting fair weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.9, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 63.9, "peak": 73.37, "min": 60.26}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 27.9, "energy_joules_est": 27.16, "sample_count": 7, "duration_seconds": 0.973}, "timestamp": "2026-01-17T17:54:30.483534"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 883.219, "latencies_ms": [883.219], "images_per_second": 1.132, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A young man in a purple tie-dye shirt and black pants is skillfully performing a trick on his skateboard, airborne above a concrete ramp in a skate park.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.21, "peak": 35.04, "min": 23.64}, "VIN": {"avg": 62.39, "peak": 90.48, "min": 51.48}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.21, "energy_joules_est": 25.82, "sample_count": 6, "duration_seconds": 0.884}, "timestamp": "2026-01-17T17:54:31.377439"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1515.625, "latencies_ms": [1515.625], "images_per_second": 0.66, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 1, "output_text": "Skateboard: 2\nTie-dye shirt: 1\nSkateboard wheels: 4\nSkateboard deck: 1\nGround: 1\nPalm trees: 2\nBuildings: 2\nSky: 1\nClouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 25.6, "peak": 35.44, "min": 20.1}, "VIN": {"avg": 60.87, "peak": 73.9, "min": 53.47}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 25.6, "energy_joules_est": 38.81, "sample_count": 11, "duration_seconds": 1.516}, "timestamp": "2026-01-17T17:54:32.903808"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 866.12, "latencies_ms": [866.12], "images_per_second": 1.155, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, mid-air, performing a trick. The skate park is situated in the background, partially obscured by palm trees and other structures.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.62, "peak": 33.48, "min": 23.24}, "VIN": {"avg": 62.98, "peak": 79.77, "min": 57.32}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.62, "energy_joules_est": 24.8, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T17:54:33.775615"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1062.675, "latencies_ms": [1062.675], "images_per_second": 0.941, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "A young man is performing a skateboard trick in a skate park. He is mid-air, executing a trick while wearing a tie-dye shirt and jeans. The park is surrounded by palm trees and other greenery, with buildings visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 27.82, "peak": 35.45, "min": 21.66}, "VIN": {"avg": 62.42, "peak": 86.23, "min": 54.56}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.82, "energy_joules_est": 29.58, "sample_count": 8, "duration_seconds": 1.063}, "timestamp": "2026-01-17T17:54:34.844921"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1236.953, "latencies_ms": [1236.953], "images_per_second": 0.808, "prompt_tokens": 18, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The skateboarder is wearing a tie-dye shirt in shades of purple and blue. The lighting appears to be natural daylight, creating a bright and clear atmosphere. The skateboard itself appears to be made of wood and metal, typical materials for skateboarding. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.5, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25476.4, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.73, "peak": 34.66, "min": 20.87}, "VIN": {"avg": 61.47, "peak": 75.1, "min": 57.7}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.73, "energy_joules_est": 33.07, "sample_count": 9, "duration_seconds": 1.237}, "timestamp": "2026-01-17T17:54:36.088292"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 571.034, "latencies_ms": [571.034], "images_per_second": 1.751, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A curious sheep stands behind a wire fence in a lush green field, gazing directly at the camera.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 25476.4, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25476.4, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.13, "peak": 33.86, "min": 26.0}, "VIN": {"avg": 66.95, "peak": 88.8, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.13, "energy_joules_est": 17.22, "sample_count": 4, "duration_seconds": 0.571}, "timestamp": "2026-01-17T17:54:36.671910"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 863.823, "latencies_ms": [863.823], "images_per_second": 1.158, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "sheep: 2\nwire fence: 1\ngrass: 1\ntrees: 4\nbush: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.4, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25476.4, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.87, "peak": 37.03, "min": 23.64}, "VIN": {"avg": 67.57, "peak": 95.09, "min": 59.72}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.87, "energy_joules_est": 25.82, "sample_count": 6, "duration_seconds": 0.864}, "timestamp": "2026-01-17T17:54:37.542141"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 670.642, "latencies_ms": [670.642], "images_per_second": 1.491, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The main sheep is positioned in the foreground, partially obscured by the wire fence. The background features trees and a grassy area, creating a natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.4, "ram_available_mb": 100295.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25476.7, "ram_available_mb": 100295.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.34, "peak": 35.45, "min": 25.22}, "VIN": {"avg": 64.58, "peak": 82.72, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.34, "energy_joules_est": 20.36, "sample_count": 5, "duration_seconds": 0.671}, "timestamp": "2026-01-17T17:54:38.219893"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 679.213, "latencies_ms": [679.213], "images_per_second": 1.472, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A sheep is standing behind a wire fence in a grassy field with trees in the background. The sheep appears to be looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.7, "ram_available_mb": 100295.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25476.7, "ram_available_mb": 100295.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 30.72, "peak": 36.63, "min": 25.21}, "VIN": {"avg": 65.49, "peak": 79.01, "min": 58.66}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.72, "energy_joules_est": 20.88, "sample_count": 5, "duration_seconds": 0.68}, "timestamp": "2026-01-17T17:54:38.905315"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 920.055, "latencies_ms": [920.055], "images_per_second": 1.087, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The sheep's wool is a light beige color. The lighting in the image appears to be natural, possibly from the sun filtering through the trees. The scene is set in a grassy area with trees in the background, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25476.7, "ram_available_mb": 100295.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25481.2, "ram_available_mb": 100291.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.41, "peak": 36.62, "min": 22.45}, "VIN": {"avg": 64.56, "peak": 92.76, "min": 56.18}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.41, "energy_joules_est": 26.15, "sample_count": 7, "duration_seconds": 0.92}, "timestamp": "2026-01-17T17:54:39.831615"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 561.514, "latencies_ms": [561.514], "images_per_second": 1.781, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A close-up shot reveals the illuminated buttons and screen of a silver Sony Walkman phone.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25481.2, "ram_available_mb": 100291.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 31.01, "peak": 34.66, "min": 26.77}, "VIN": {"avg": 65.48, "peak": 79.09, "min": 60.08}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.01, "energy_joules_est": 17.43, "sample_count": 4, "duration_seconds": 0.562}, "timestamp": "2026-01-17T17:54:40.403941"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1204.715, "latencies_ms": [1204.715], "images_per_second": 0.83, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "button: 2\nscreen: 1\nvolume knob: 1\nplay button: 1\nwalkman button: 1\nback button: 1\nflash button: 1\nbrowser button: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 27.44, "peak": 37.8, "min": 20.88}, "VIN": {"avg": 63.43, "peak": 88.69, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.44, "energy_joules_est": 33.07, "sample_count": 9, "duration_seconds": 1.205}, "timestamp": "2026-01-17T17:54:41.614942"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 714.128, "latencies_ms": [714.128], "images_per_second": 1.4, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The left object is positioned in the foreground, while the right object is further away in the background. The device is angled towards the viewer, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.23, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 65.54, "peak": 84.52, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 20.89, "sample_count": 5, "duration_seconds": 0.715}, "timestamp": "2026-01-17T17:54:42.336160"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 926.532, "latencies_ms": [926.532], "images_per_second": 1.079, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Close-up view of a silver Sony Walkman phone, showcasing its illuminated buttons and screen. The phone rests on a dark surface, emphasizing its metallic finish and the orange glow emanating from the buttons.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.25, "peak": 35.44, "min": 22.46}, "VIN": {"avg": 62.31, "peak": 79.53, "min": 51.16}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.25, "energy_joules_est": 26.19, "sample_count": 7, "duration_seconds": 0.927}, "timestamp": "2026-01-17T17:54:43.268935"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 806.424, "latencies_ms": [806.424], "images_per_second": 1.24, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The phone is silver and appears to be made of metal. The lighting is warm and highlights the metallic texture of the device. The phone's surface appears smooth and reflective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.36, "peak": 34.65, "min": 22.85}, "VIN": {"avg": 64.65, "peak": 80.72, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.36, "energy_joules_est": 22.88, "sample_count": 6, "duration_seconds": 0.807}, "timestamp": "2026-01-17T17:54:44.082039"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 575.428, "latencies_ms": [575.428], "images_per_second": 1.738, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A woman in a sparkly black dress stands in front of a stainless steel refrigerator, holding a glass of orange juice.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 31.5, "peak": 35.45, "min": 27.17}, "VIN": {"avg": 65.16, "peak": 80.93, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.5, "energy_joules_est": 18.14, "sample_count": 4, "duration_seconds": 0.576}, "timestamp": "2026-01-17T17:54:44.667814"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1181.692, "latencies_ms": [1181.692], "images_per_second": 0.846, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "woman: 1\ndress: 1\nglasses: 1\nwine bottle: 1\nfoil can: 1\nrefrigerator: 1\ncounter: 1\ncabinets: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.48, "peak": 37.42, "min": 21.27}, "VIN": {"avg": 62.4, "peak": 81.4, "min": 57.69}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.48, "energy_joules_est": 32.49, "sample_count": 9, "duration_seconds": 1.182}, "timestamp": "2026-01-17T17:54:45.855873"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 724.374, "latencies_ms": [724.374], "images_per_second": 1.381, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The woman is standing near the refrigerator, which occupies the foreground of the image. The kitchen counter and cabinets are positioned in the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.08, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 64.06, "peak": 82.48, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 21.07, "sample_count": 5, "duration_seconds": 0.725}, "timestamp": "2026-01-17T17:54:46.587040"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 971.185, "latencies_ms": [971.185], "images_per_second": 1.03, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "A woman in a sparkly black dress is standing in a kitchen, holding a glass of orange juice. She is smiling and appears to be enjoying herself. The kitchen has wooden cabinets, a stainless steel refrigerator, and a tiled backsplash.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.3, "peak": 36.24, "min": 22.45}, "VIN": {"avg": 63.85, "peak": 83.18, "min": 56.16}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.3, "energy_joules_est": 27.5, "sample_count": 7, "duration_seconds": 0.972}, "timestamp": "2026-01-17T17:54:47.564738"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1032.745, "latencies_ms": [1032.745], "images_per_second": 0.968, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The kitchen features warm brown wooden cabinets and light-colored tile flooring. The stainless steel refrigerator stands out, adding a modern touch to the space. The woman is wearing black dress and glasses, complementing the overall color scheme.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.0, "ram_available_mb": 100291.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25481.4, "ram_available_mb": 100290.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.51, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 63.09, "peak": 79.34, "min": 57.24}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.51, "energy_joules_est": 28.42, "sample_count": 7, "duration_seconds": 1.033}, "timestamp": "2026-01-17T17:54:48.603596"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 650.371, "latencies_ms": [650.371], "images_per_second": 1.538, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A yellow school bus is reflected in the side mirror of a truck, driving down a road with cars and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.4, "ram_available_mb": 100290.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25481.4, "ram_available_mb": 100290.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.55, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 67.79, "peak": 92.98, "min": 59.52}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.55, "energy_joules_est": 19.24, "sample_count": 5, "duration_seconds": 0.651}, "timestamp": "2026-01-17T17:54:49.265629"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1000.538, "latencies_ms": [1000.538], "images_per_second": 0.999, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "school bus: 1\nroad: 2\ncar: 1\ntraffic light: 1\nbuildings: 2\nsign: 1\npole: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25481.4, "ram_available_mb": 100290.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.08, "peak": 35.85, "min": 22.06}, "VIN": {"avg": 63.27, "peak": 76.27, "min": 55.7}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.08, "energy_joules_est": 28.11, "sample_count": 7, "duration_seconds": 1.001}, "timestamp": "2026-01-17T17:54:50.272655"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 770.812, "latencies_ms": [770.812], "images_per_second": 1.297, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The school bus is positioned in the foreground, partially obscuring the background. The mirror reflects the school bus and the surrounding environment, indicating a relatively close proximity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.62, "peak": 34.26, "min": 24.82}, "VIN": {"avg": 65.72, "peak": 81.69, "min": 60.47}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.62, "energy_joules_est": 22.84, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T17:54:51.049782"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 852.518, "latencies_ms": [852.518], "images_per_second": 1.173, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A yellow school bus is reflected in the side mirror of a vehicle. The reflection shows a road with cars and traffic lights. The setting appears to be a street or parking lot near a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.02, "peak": 35.85, "min": 23.24}, "VIN": {"avg": 66.42, "peak": 98.33, "min": 56.44}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.02, "energy_joules_est": 24.76, "sample_count": 6, "duration_seconds": 0.853}, "timestamp": "2026-01-17T17:54:51.909189"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 781.373, "latencies_ms": [781.373], "images_per_second": 1.28, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The yellow school bus is reflected in the side mirror. The lighting conditions suggest an overcast sky. The bus appears to be made of metal and has a reflective surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.85, "peak": 35.06, "min": 24.81}, "VIN": {"avg": 65.55, "peak": 86.21, "min": 54.08}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.85, "energy_joules_est": 23.33, "sample_count": 5, "duration_seconds": 0.782}, "timestamp": "2026-01-17T17:54:52.700767"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 593.076, "latencies_ms": [593.076], "images_per_second": 1.686, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A gray cat is sitting on a wooden table near two potted plants, while a brown dog stands nearby, seemingly observing the cat.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 24.82, "peak": 27.57, "min": 22.45}, "VIN": {"avg": 61.29, "peak": 62.61, "min": 60.46}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.82, "energy_joules_est": 14.73, "sample_count": 4, "duration_seconds": 0.594}, "timestamp": "2026-01-17T17:54:53.303811"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 838.432, "latencies_ms": [838.432], "images_per_second": 1.193, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "cat: 1\ndog: 1\nplant: 2\npot: 2\ntable: 1\nwindow: 1\nfence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 23.51, "peak": 27.19, "min": 20.88}, "VIN": {"avg": 60.27, "peak": 62.5, "min": 56.76}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.51, "energy_joules_est": 19.72, "sample_count": 6, "duration_seconds": 0.839}, "timestamp": "2026-01-17T17:54:54.148450"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 529.692, "latencies_ms": [529.692], "images_per_second": 1.888, "prompt_tokens": 25, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The cat is positioned in the foreground, close to the potted plants. The dog is further in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 24.55, "peak": 26.39, "min": 22.85}, "VIN": {"avg": 59.21, "peak": 63.22, "min": 54.83}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 24.55, "energy_joules_est": 13.02, "sample_count": 3, "duration_seconds": 0.53}, "timestamp": "2026-01-17T17:54:54.684331"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 751.397, "latencies_ms": [751.397], "images_per_second": 1.331, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A dog and a cat are seen outdoors near a wooden table with potted plants. The dog is standing in the background, while the cat is sitting on the table, seemingly observing the dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 24.02, "peak": 27.17, "min": 21.66}, "VIN": {"avg": 59.33, "peak": 63.07, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 24.02, "energy_joules_est": 18.06, "sample_count": 5, "duration_seconds": 0.752}, "timestamp": "2026-01-17T17:54:55.442539"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 676.29, "latencies_ms": [676.29], "images_per_second": 1.479, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The cat is gray, and the dog is brown. The lighting appears to be natural daylight. The cat and dog are sitting near a wooden table and planters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 23.55, "peak": 26.38, "min": 21.27}, "VIN": {"avg": 60.23, "peak": 62.46, "min": 56.45}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.55, "energy_joules_est": 15.95, "sample_count": 5, "duration_seconds": 0.677}, "timestamp": "2026-01-17T17:54:56.125718"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 672.785, "latencies_ms": [672.785], "images_per_second": 1.486, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A female soccer player in a blue jersey is skillfully dribbling the ball while another player in a yellow jersey watches.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25491.8, "ram_available_mb": 100280.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.05, "peak": 33.47, "min": 24.41}, "VIN": {"avg": 66.31, "peak": 83.01, "min": 60.51}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.05, "energy_joules_est": 19.56, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T17:54:56.809289"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1065.615, "latencies_ms": [1065.615], "images_per_second": 0.938, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "woman: 1\nball: 1\njersey: 1\nshorts: 1\nheadband: 1\nhair: 1\nface: 1\nbackground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.32, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 63.86, "peak": 89.42, "min": 55.41}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.32, "energy_joules_est": 29.13, "sample_count": 8, "duration_seconds": 1.066}, "timestamp": "2026-01-17T17:54:57.881456"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 671.743, "latencies_ms": [671.743], "images_per_second": 1.489, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The ball is positioned in the foreground, slightly to the left of the main subject. The background is blurred, suggesting the focus is on the player in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25492.5, "ram_available_mb": 100279.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.23, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 62.62, "peak": 79.34, "min": 56.16}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 19.64, "sample_count": 5, "duration_seconds": 0.672}, "timestamp": "2026-01-17T17:54:58.559218"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 692.867, "latencies_ms": [692.867], "images_per_second": 1.443, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A female soccer player in a blue jersey is dribbling the ball on a field. Another player in a yellow jersey is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25492.5, "ram_available_mb": 100279.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.65, "peak": 36.24, "min": 25.21}, "VIN": {"avg": 66.39, "peak": 92.68, "min": 53.92}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.65, "energy_joules_est": 21.26, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T17:54:59.259440"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 750.802, "latencies_ms": [750.802], "images_per_second": 1.332, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The player is wearing a blue jersey with red and white accents. The ball is white with blue and red markings. The setting appears to be outdoors in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 31.12, "peak": 37.03, "min": 25.61}, "VIN": {"avg": 64.83, "peak": 80.38, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 23.38, "sample_count": 5, "duration_seconds": 0.751}, "timestamp": "2026-01-17T17:55:00.016447"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 564.429, "latencies_ms": [564.429], "images_per_second": 1.772, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "Two giraffes are in a grassy enclosure, one bending over a wooden fence and the other standing tall, facing away from the camera.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25492.3, "ram_available_mb": 100279.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25497.1, "ram_available_mb": 100275.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 25.41, "peak": 28.36, "min": 22.85}, "VIN": {"avg": 61.26, "peak": 63.85, "min": 57.61}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 25.41, "energy_joules_est": 14.36, "sample_count": 4, "duration_seconds": 0.565}, "timestamp": "2026-01-17T17:55:00.591596"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 799.123, "latencies_ms": [799.123], "images_per_second": 1.251, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "giraffe: 2\nfence: 1\ntrees: 6\ngrass: 6\ndirt: 1\npath: 1", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25497.4, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 23.56, "peak": 27.56, "min": 20.87}, "VIN": {"avg": 58.38, "peak": 62.03, "min": 53.49}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.56, "energy_joules_est": 18.84, "sample_count": 6, "duration_seconds": 0.8}, "timestamp": "2026-01-17T17:55:01.396553"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1065.371, "latencies_ms": [1065.371], "images_per_second": 0.939, "prompt_tokens": 25, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The giraffes are positioned near the fence and trees in the background, creating a sense of depth and perspective. The giraffe on the left is partially obscured by the fence, drawing attention to the giraffe on the right. The foreground is relatively open, providing a clear view of the giraffes and the surrounding environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.4, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.25, "peak": 26.38, "min": 19.7}, "VIN": {"avg": 59.74, "peak": 61.55, "min": 56.72}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 22.25, "energy_joules_est": 23.72, "sample_count": 8, "duration_seconds": 1.066}, "timestamp": "2026-01-17T17:55:02.468634"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1007.299, "latencies_ms": [1007.299], "images_per_second": 0.993, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "Two giraffes are in a zoo enclosure. One giraffe is bending over a wooden fence, seemingly reaching for something, while the other giraffe stands nearby, observing its surroundings. The enclosure is surrounded by lush green trees and grass, creating a natural habitat for the animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 22.23, "peak": 25.59, "min": 20.09}, "VIN": {"avg": 61.06, "peak": 64.23, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 22.23, "energy_joules_est": 22.4, "sample_count": 7, "duration_seconds": 1.008}, "timestamp": "2026-01-17T17:55:03.482767"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 701.559, "latencies_ms": [701.559], "images_per_second": 1.425, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The giraffes exhibit a mix of brown and white patterns, illuminated by natural daylight. The enclosure is constructed from light-colored wood and features natural greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.84, "peak": 25.59, "min": 20.87}, "VIN": {"avg": 59.31, "peak": 61.72, "min": 56.53}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 22.84, "energy_joules_est": 16.04, "sample_count": 5, "duration_seconds": 0.702}, "timestamp": "2026-01-17T17:55:04.191164"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 714.119, "latencies_ms": [714.119], "images_per_second": 1.4, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "Three pieces of luggage, including a suitcase, a duffel bag, and a smaller bag, are neatly arranged on the floor near a curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.93, "peak": 34.26, "min": 24.82}, "VIN": {"avg": 64.35, "peak": 78.78, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 21.39, "sample_count": 5, "duration_seconds": 0.715}, "timestamp": "2026-01-17T17:55:04.917948"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 923.461, "latencies_ms": [923.461], "images_per_second": 1.083, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "suitcase: 1\nbag: 1\ncarpet: 1\ncurtains: 1\nbooks: 2\nplastic bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25497.4, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 29.43, "peak": 37.01, "min": 22.85}, "VIN": {"avg": 66.26, "peak": 95.15, "min": 53.95}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.43, "energy_joules_est": 27.19, "sample_count": 7, "duration_seconds": 0.924}, "timestamp": "2026-01-17T17:55:05.847543"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 911.382, "latencies_ms": [911.382], "images_per_second": 1.097, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The main objects are positioned in a relatively close and central arrangement. The suitcase is positioned slightly in front and to the right of the plastic bag. The bags are placed further back, near the edge of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.4, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.92, "peak": 35.85, "min": 22.84}, "VIN": {"avg": 61.1, "peak": 72.8, "min": 53.54}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.92, "energy_joules_est": 26.37, "sample_count": 7, "duration_seconds": 0.912}, "timestamp": "2026-01-17T17:55:06.765129"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 950.948, "latencies_ms": [950.948], "images_per_second": 1.052, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a room with several pieces of luggage, including a suitcase, a tote bag, and a smaller bag, arranged on the floor near a curtain. The setting suggests preparation for a trip or travel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25497.4, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.63, "peak": 35.83, "min": 22.45}, "VIN": {"avg": 62.81, "peak": 74.68, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.63, "energy_joules_est": 27.23, "sample_count": 7, "duration_seconds": 0.951}, "timestamp": "2026-01-17T17:55:07.722284"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1125.946, "latencies_ms": [1125.946], "images_per_second": 0.888, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The suitcase appears to be gray or dark gray in color. The lighting in the image is soft and diffused, suggesting natural light coming from a window or indoor source. The suitcase appears to be made of a sturdy material, possibly fabric or hard-shell.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.4, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.47, "peak": 35.44, "min": 21.27}, "VIN": {"avg": 62.49, "peak": 70.94, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.47, "energy_joules_est": 30.94, "sample_count": 8, "duration_seconds": 1.126}, "timestamp": "2026-01-17T17:55:08.854539"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 554.255, "latencies_ms": [554.255], "images_per_second": 1.804, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Two men are riding donkeys down a rocky trail in a forest, surrounded by tall trees and rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.1, "ram_available_mb": 100275.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25497.3, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.42, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 65.81, "peak": 88.99, "min": 52.99}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.42, "energy_joules_est": 16.87, "sample_count": 4, "duration_seconds": 0.555}, "timestamp": "2026-01-17T17:55:09.421408"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1068.192, "latencies_ms": [1068.192], "images_per_second": 0.936, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "tree: 5\nrocks: 5\nhorse: 3\nman: 2\nbandana: 1\nbackpack: 1\nground: 2\npath: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25497.3, "ram_available_mb": 100274.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.22, "peak": 38.21, "min": 21.67}, "VIN": {"avg": 63.94, "peak": 80.89, "min": 60.23}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.22, "energy_joules_est": 30.16, "sample_count": 8, "duration_seconds": 1.069}, "timestamp": "2026-01-17T17:55:10.495836"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 988.097, "latencies_ms": [988.097], "images_per_second": 1.012, "prompt_tokens": 25, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The man and horse are positioned in the foreground of the image, with the man walking alongside the horse. The horses are further back in the scene, suggesting they are moving away from the viewer. The setting appears to be a wooded area, with trees and rocks visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.46, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 65.79, "peak": 101.2, "min": 52.25}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 27.14, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-17T17:55:11.490033"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 698.152, "latencies_ms": [698.152], "images_per_second": 1.432, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "Two people are riding donkeys along a rocky trail in a forest. The trail is surrounded by tall trees and rocks, creating a natural and rugged environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25498.8, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.91, "peak": 33.86, "min": 24.03}, "VIN": {"avg": 66.95, "peak": 90.71, "min": 59.74}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.91, "energy_joules_est": 20.2, "sample_count": 5, "duration_seconds": 0.699}, "timestamp": "2026-01-17T17:55:12.194543"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1354.219, "latencies_ms": [1354.219], "images_per_second": 0.738, "prompt_tokens": 18, "response_tokens_est": 75, "n_tiles": 1, "output_text": "The scene is bathed in sunlight, creating a warm and inviting atmosphere. The colors are vibrant, with the green of the trees contrasting with the blue of the man's shirt and the brown of the horses' coats. The lighting is natural, illuminating the path and the surrounding forest. The materials appear to be natural, earthy, and sturdy, supporting the rugged terrain.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25498.8, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.19, "peak": 36.63, "min": 20.48}, "VIN": {"avg": 63.0, "peak": 83.95, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.19, "energy_joules_est": 35.48, "sample_count": 10, "duration_seconds": 1.355}, "timestamp": "2026-01-17T17:55:13.556228"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 625.923, "latencies_ms": [625.923], "images_per_second": 1.598, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A man in a sweatshirt and jeans rides a horse, holding the reins and guiding it along a track.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25498.8, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.61, "min": 14.2}, "VDD_GPU": {"avg": 29.93, "peak": 33.48, "min": 25.99}, "VIN": {"avg": 64.07, "peak": 76.57, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 18.75, "sample_count": 4, "duration_seconds": 0.626}, "timestamp": "2026-01-17T17:55:14.192047"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1148.536, "latencies_ms": [1148.536], "images_per_second": 0.871, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "horse: 1\narman: 1\nsaddle: 1\njockey: 1\nriding crop: 1\nground: 1\nbuilding: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25498.8, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.0, "peak": 37.01, "min": 20.88}, "VIN": {"avg": 62.08, "peak": 76.78, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 27.0, "energy_joules_est": 31.02, "sample_count": 9, "duration_seconds": 1.149}, "timestamp": "2026-01-17T17:55:15.346968"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 799.587, "latencies_ms": [799.587], "images_per_second": 1.251, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The man is positioned near the foreground of the image, riding a horse. The background features blurred structures, possibly buildings or tents, which create a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.16, "peak": 34.27, "min": 22.85}, "VIN": {"avg": 62.17, "peak": 77.43, "min": 54.61}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.16, "energy_joules_est": 22.52, "sample_count": 6, "duration_seconds": 0.8}, "timestamp": "2026-01-17T17:55:16.152492"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 784.534, "latencies_ms": [784.534], "images_per_second": 1.275, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A man is riding a horse in a black and white image, possibly at a racetrack or similar venue. The horse and rider appear to be in motion, creating a dynamic scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25498.7, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.94, "peak": 35.45, "min": 24.83}, "VIN": {"avg": 64.91, "peak": 79.38, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 23.5, "sample_count": 5, "duration_seconds": 0.785}, "timestamp": "2026-01-17T17:55:16.943255"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 672.113, "latencies_ms": [672.113], "images_per_second": 1.488, "prompt_tokens": 18, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The horse and rider appear to be wearing dark-colored clothing. The background is blurred, suggesting motion, and appears to be outdoors in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25498.7, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.18, "peak": 35.45, "min": 24.83}, "VIN": {"avg": 64.98, "peak": 83.03, "min": 57.09}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.18, "energy_joules_est": 20.31, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T17:55:17.623175"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 587.464, "latencies_ms": [587.464], "images_per_second": 1.702, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Three ducks are swimming leisurely in a calm, reflective body of water, surrounded by lush greenery and tall grasses.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 31.72, "peak": 36.26, "min": 27.19}, "VIN": {"avg": 67.26, "peak": 91.52, "min": 55.48}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 31.72, "energy_joules_est": 18.65, "sample_count": 4, "duration_seconds": 0.588}, "timestamp": "2026-01-17T17:55:18.225254"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 872.884, "latencies_ms": [872.884], "images_per_second": 1.146, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "goose: 4\nwater: 4\nshore: 4\ngrass: 4\ntrees: 4\nbushes: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25498.5, "ram_available_mb": 100273.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25498.7, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 30.27, "peak": 37.83, "min": 24.03}, "VIN": {"avg": 66.8, "peak": 96.09, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.27, "energy_joules_est": 26.45, "sample_count": 6, "duration_seconds": 0.874}, "timestamp": "2026-01-17T17:55:19.105030"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 747.422, "latencies_ms": [747.422], "images_per_second": 1.338, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground of the image, with the water providing a background. The geese are situated near the water's edge, moving across the surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25498.7, "ram_available_mb": 100273.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25499.0, "ram_available_mb": 100273.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 29.85, "peak": 35.04, "min": 24.81}, "VIN": {"avg": 64.33, "peak": 78.7, "min": 57.15}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.85, "energy_joules_est": 22.34, "sample_count": 5, "duration_seconds": 0.748}, "timestamp": "2026-01-17T17:55:19.859381"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 569.454, "latencies_ms": [569.454], "images_per_second": 1.756, "prompt_tokens": 19, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A group of geese is swimming in a calm body of water near a grassy bank with trees and bushes in the background.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25499.0, "ram_available_mb": 100273.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25499.0, "ram_available_mb": 100273.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 31.51, "peak": 35.45, "min": 27.18}, "VIN": {"avg": 65.41, "peak": 80.05, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 31.51, "energy_joules_est": 17.96, "sample_count": 4, "duration_seconds": 0.57}, "timestamp": "2026-01-17T17:55:20.435153"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 721.802, "latencies_ms": [721.802], "images_per_second": 1.385, "prompt_tokens": 18, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The water is a calm, reflective blue-green. The lighting suggests a sunny day, with soft shadows cast by the surrounding trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.0, "ram_available_mb": 100273.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25502.4, "ram_available_mb": 100269.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 31.2, "peak": 37.03, "min": 25.6}, "VIN": {"avg": 64.64, "peak": 79.24, "min": 59.26}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 31.2, "energy_joules_est": 22.53, "sample_count": 5, "duration_seconds": 0.722}, "timestamp": "2026-01-17T17:55:21.163994"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 573.148, "latencies_ms": [573.148], "images_per_second": 1.745, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "An orange and white cat is comfortably resting on the hood of a black Mercedes-Benz car parked in a residential area.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25502.4, "ram_available_mb": 100269.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25502.1, "ram_available_mb": 100270.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 25.7, "peak": 28.76, "min": 23.24}, "VIN": {"avg": 60.51, "peak": 61.65, "min": 59.87}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.7, "energy_joules_est": 14.75, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T17:55:21.747602"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1001.937, "latencies_ms": [1001.937], "images_per_second": 0.998, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "cat: 1\ncar: 1\nmercedes: 1\ngrill: 1\nbush: 1\nfence: 1\nwindow: 2\nbuilding: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25502.1, "ram_available_mb": 100270.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25499.7, "ram_available_mb": 100272.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 23.19, "peak": 27.59, "min": 20.48}, "VIN": {"avg": 61.78, "peak": 63.81, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.19, "energy_joules_est": 23.25, "sample_count": 7, "duration_seconds": 1.003}, "timestamp": "2026-01-17T17:55:22.756250"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 612.301, "latencies_ms": [612.301], "images_per_second": 1.633, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The cat is positioned on the car's hood, near the front grill. The car is parked in front of a house, indicating a residential setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.7, "ram_available_mb": 100272.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25499.4, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.83, "peak": 26.0, "min": 22.06}, "VIN": {"avg": 60.75, "peak": 62.63, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.83, "energy_joules_est": 14.6, "sample_count": 4, "duration_seconds": 0.613}, "timestamp": "2026-01-17T17:55:23.374704"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 731.652, "latencies_ms": [731.652], "images_per_second": 1.367, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A calico cat is resting on the hood of a black Mercedes-Benz parked on a residential street. The car is parked in front of a house with a green bush nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.4, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25499.4, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.64, "peak": 26.79, "min": 21.27}, "VIN": {"avg": 59.06, "peak": 61.14, "min": 55.77}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.64, "energy_joules_est": 17.31, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T17:55:24.113262"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 672.768, "latencies_ms": [672.768], "images_per_second": 1.486, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The cat is orange and white. The car is dark-colored and appears to be parked outdoors. The lighting suggests it's daytime, and the car's material looks like polished metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.4, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25499.3, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.64, "peak": 26.8, "min": 21.28}, "VIN": {"avg": 59.96, "peak": 61.97, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.64, "energy_joules_est": 15.91, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T17:55:24.791982"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 737.048, "latencies_ms": [737.048], "images_per_second": 1.357, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A snowboarder in a brown jacket and yellow pants is captured mid-air, performing a trick against a clear blue sky and snow-covered mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.3, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25499.4, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.63, "peak": 34.66, "min": 24.83}, "VIN": {"avg": 68.33, "peak": 98.18, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.63, "energy_joules_est": 21.85, "sample_count": 5, "duration_seconds": 0.737}, "timestamp": "2026-01-17T17:55:25.538222"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1139.155, "latencies_ms": [1139.155], "images_per_second": 0.878, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "snowboard: 1\nsnow: 2\nsky: 1\nclouds: 0\nperson: 1\ngloves: 1\nhelmet: 1\npants: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.4, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25499.3, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.33, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 62.32, "peak": 79.36, "min": 56.49}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.33, "energy_joules_est": 31.15, "sample_count": 8, "duration_seconds": 1.14}, "timestamp": "2026-01-17T17:55:26.683638"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1062.683, "latencies_ms": [1062.683], "images_per_second": 0.941, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The snowboarder is positioned in the foreground, mid-air, against a clear blue sky. The snowboard is situated in the background, slightly elevated above the snow. The snowboarder is relatively close to the viewer, capturing a dynamic moment in the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25499.3, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25499.6, "ram_available_mb": 100272.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 26.24, "peak": 33.86, "min": 20.88}, "VIN": {"avg": 65.13, "peak": 97.19, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.24, "energy_joules_est": 27.89, "sample_count": 8, "duration_seconds": 1.063}, "timestamp": "2026-01-17T17:55:27.752374"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 813.487, "latencies_ms": [813.487], "images_per_second": 1.229, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A snowboarder is captured mid-air, performing a trick against a clear blue sky. The scene takes place on a snow-covered slope, emphasizing the winter sports aspect.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25499.6, "ram_available_mb": 100272.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25499.3, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.1, "peak": 34.66, "min": 22.85}, "VIN": {"avg": 64.08, "peak": 79.02, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.1, "energy_joules_est": 22.87, "sample_count": 6, "duration_seconds": 0.814}, "timestamp": "2026-01-17T17:55:28.572104"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 828.805, "latencies_ms": [828.805], "images_per_second": 1.207, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The snowboarder is wearing a brown jacket and yellow pants. The sky is clear and blue, indicating sunny weather. The snowboard is white and appears to be made of foam or plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.3, "ram_available_mb": 100272.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25499.6, "ram_available_mb": 100272.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.25}, "VIN": {"avg": 65.17, "peak": 82.92, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 23.84, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T17:55:29.407252"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 708.262, "latencies_ms": [708.262], "images_per_second": 1.412, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The bathroom features a vintage-style toilet with a wooden seat and a white tank, situated beneath a white pipe system against a white wall.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25499.6, "ram_available_mb": 100272.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25499.0, "ram_available_mb": 100273.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 30.02, "peak": 35.85, "min": 24.82}, "VIN": {"avg": 64.52, "peak": 77.8, "min": 59.92}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.02, "energy_joules_est": 21.27, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T17:55:30.127512"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1190.347, "latencies_ms": [1190.347], "images_per_second": 0.84, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Toilet: 1\nBathtub: 1\nPipes: 6\nChain: 1\nPaper: 1\nFloor: 1\nWalls: 2\nDoor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25499.0, "ram_available_mb": 100273.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25503.7, "ram_available_mb": 100268.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.7, "peak": 36.23, "min": 20.89}, "VIN": {"avg": 60.49, "peak": 82.72, "min": 54.16}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 26.7, "energy_joules_est": 31.79, "sample_count": 9, "duration_seconds": 1.191}, "timestamp": "2026-01-17T17:55:31.324358"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 675.758, "latencies_ms": [675.758], "images_per_second": 1.48, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The toilet is positioned in the foreground, slightly to the left of the bathtub. The bathtub is situated in the background, further away from the toilet.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25503.7, "ram_available_mb": 100268.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25503.7, "ram_available_mb": 100268.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.14, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.39, "peak": 81.58, "min": 60.15}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.14, "energy_joules_est": 19.7, "sample_count": 5, "duration_seconds": 0.676}, "timestamp": "2026-01-17T17:55:32.006186"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1109.711, "latencies_ms": [1109.711], "images_per_second": 0.901, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The bathroom features a vintage-style toilet with a wooden seat and a chain hanging from the tank. A white bathtub is situated next to the toilet, partially obscured by pipes. The space appears to be in a basement or attic area, with exposed pipes and aged fixtures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25503.7, "ram_available_mb": 100268.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25503.7, "ram_available_mb": 100268.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.43, "peak": 35.85, "min": 21.28}, "VIN": {"avg": 63.02, "peak": 80.06, "min": 56.63}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.43, "energy_joules_est": 30.46, "sample_count": 8, "duration_seconds": 1.11}, "timestamp": "2026-01-17T17:55:33.122689"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 939.572, "latencies_ms": [939.572], "images_per_second": 1.064, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The bathroom features a predominantly white color scheme with exposed pipes and fixtures. The lighting is dim, creating a somewhat subdued atmosphere. The materials appear to be old and possibly antique, contributing to the vintage feel of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25503.7, "ram_available_mb": 100268.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25503.9, "ram_available_mb": 100268.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.96, "peak": 33.88, "min": 21.66}, "VIN": {"avg": 65.43, "peak": 93.43, "min": 57.71}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.96, "energy_joules_est": 25.35, "sample_count": 7, "duration_seconds": 0.94}, "timestamp": "2026-01-17T17:55:34.068845"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 767.194, "latencies_ms": [767.194], "images_per_second": 1.303, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The statue depicts two children, one standing and one kneeling, holding a colorful kite that soars high above them, symbolizing freedom and joy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25503.9, "ram_available_mb": 100268.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25504.1, "ram_available_mb": 100268.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 29.15, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 62.77, "peak": 72.91, "min": 55.7}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.15, "energy_joules_est": 22.38, "sample_count": 5, "duration_seconds": 0.768}, "timestamp": "2026-01-17T17:55:34.847820"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 910.637, "latencies_ms": [910.637], "images_per_second": 1.098, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "kite: 1\nstatue: 2\nbuilding: 1\nsky: 1\nclouds: 1\nreflection: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25504.1, "ram_available_mb": 100268.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 28.82, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 62.53, "peak": 77.38, "min": 57.62}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.82, "energy_joules_est": 26.26, "sample_count": 6, "duration_seconds": 0.911}, "timestamp": "2026-01-17T17:55:35.768729"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 672.722, "latencies_ms": [672.722], "images_per_second": 1.486, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The statue is positioned in the foreground, slightly to the right of the kite. The kite is situated in the background, near the top of the building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25504.1, "ram_available_mb": 100268.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.86, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 68.33, "peak": 89.08, "min": 62.14}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 20.11, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T17:55:36.447696"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 799.063, "latencies_ms": [799.063], "images_per_second": 1.251, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "Two bronze statues of children stand on a platform in front of a modern glass and steel building. One statue is flying a colorful kite, while the other appears to be reaching upwards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.1, "ram_available_mb": 100268.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.41, "peak": 36.23, "min": 23.64}, "VIN": {"avg": 63.64, "peak": 75.27, "min": 59.5}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.41, "energy_joules_est": 23.51, "sample_count": 6, "duration_seconds": 0.8}, "timestamp": "2026-01-17T17:55:37.252864"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1055.381, "latencies_ms": [1055.381], "images_per_second": 0.948, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The kite is multicolored with shades of yellow, blue, and red. The lighting in the image appears to be soft and diffused, possibly suggesting an overcast sky. The materials appear to be metal, giving the statue a weathered and weathered look.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.03, "peak": 35.44, "min": 21.27}, "VIN": {"avg": 63.02, "peak": 87.44, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.03, "energy_joules_est": 28.54, "sample_count": 8, "duration_seconds": 1.056}, "timestamp": "2026-01-17T17:55:38.314777"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 929.92, "latencies_ms": [929.92], "images_per_second": 1.075, "prompt_tokens": 8, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A wooden table displays a vibrant array of fresh produce, including red radishes, green asparagus, purple cabbage, carrots, potatoes, cucumbers, broccoli, strawberries, and green beans.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.94, "peak": 33.86, "min": 21.66}, "VIN": {"avg": 63.04, "peak": 79.45, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.94, "energy_joules_est": 25.07, "sample_count": 7, "duration_seconds": 0.931}, "timestamp": "2026-01-17T17:55:39.256838"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1227.001, "latencies_ms": [1227.001], "images_per_second": 0.815, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "strawberries: 10\nbroccoli: 2\ncucumbers: 2\nasparagus: 1\nradishes: 8\npeas: 1\ncarrots: 4\npotatoes: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.0, "peak": 34.66, "min": 20.48}, "VIN": {"avg": 62.41, "peak": 78.18, "min": 56.77}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.0, "energy_joules_est": 31.91, "sample_count": 9, "duration_seconds": 1.227}, "timestamp": "2026-01-17T17:55:40.490105"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 688.9, "latencies_ms": [688.9], "images_per_second": 1.452, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The strawberries are positioned to the left of the vegetables, closer to the viewer. The vegetables are spread out across the foreground and background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25504.5, "ram_available_mb": 100267.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.91, "peak": 33.47, "min": 24.42}, "VIN": {"avg": 67.12, "peak": 89.96, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.91, "energy_joules_est": 19.92, "sample_count": 5, "duration_seconds": 0.689}, "timestamp": "2026-01-17T17:55:41.185039"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1216.727, "latencies_ms": [1216.727], "images_per_second": 0.822, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The scene depicts a vibrant display of fresh produce, including strawberries, broccoli, radishes, carrots, asparagus, and potatoes, arranged on a table or countertop. The produce suggests a home-grown or market setting, highlighting the abundance and variety of fresh vegetables available.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.5, "ram_available_mb": 100267.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.7, "peak": 35.83, "min": 20.89}, "VIN": {"avg": 62.41, "peak": 78.12, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.7, "energy_joules_est": 32.5, "sample_count": 9, "duration_seconds": 1.217}, "timestamp": "2026-01-17T17:55:42.408604"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 964.347, "latencies_ms": [964.347], "images_per_second": 1.037, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The produce displays vibrant colors, including reds, greens, and oranges. The lighting appears to be natural, possibly from overhead, and suggests an outdoor setting. The vegetables appear to be fresh and crisp, indicating good quality and care in preparation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.18, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 62.45, "peak": 82.98, "min": 55.24}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 26.22, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T17:55:43.379321"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 636.03, "latencies_ms": [636.03], "images_per_second": 1.572, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Three people are sitting on a bed in a dimly lit room, laughing and playing a video game using Wii remotes.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.51, "min": 14.1}, "VDD_GPU": {"avg": 30.23, "peak": 34.27, "min": 26.0}, "VIN": {"avg": 68.31, "peak": 90.39, "min": 59.06}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.23, "energy_joules_est": 19.25, "sample_count": 4, "duration_seconds": 0.637}, "timestamp": "2026-01-17T17:55:44.025969"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 999.397, "latencies_ms": [999.397], "images_per_second": 1.001, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "projector: 1\nscreen: 1\nwii controllers: 2\ncouch: 2\nman: 3\nman: 2\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.4, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25504.5, "ram_available_mb": 100267.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.52, "peak": 36.62, "min": 22.44}, "VIN": {"avg": 64.17, "peak": 94.72, "min": 52.3}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.52, "energy_joules_est": 28.51, "sample_count": 7, "duration_seconds": 1.0}, "timestamp": "2026-01-17T17:55:45.031672"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 780.867, "latencies_ms": [780.867], "images_per_second": 1.281, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with the projection screen and remote control placed further back. The couch and bed are situated in the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.5, "ram_available_mb": 100267.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.39, "peak": 35.06, "min": 24.42}, "VIN": {"avg": 64.45, "peak": 83.27, "min": 57.34}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.39, "energy_joules_est": 22.96, "sample_count": 5, "duration_seconds": 0.781}, "timestamp": "2026-01-17T17:55:45.818654"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 760.145, "latencies_ms": [760.145], "images_per_second": 1.316, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Three people are gathered in a dimly lit room, watching a video game on a projector. They are comfortably seated on a couch and bed, each holding a game controller.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.09, "peak": 36.24, "min": 24.82}, "VIN": {"avg": 67.63, "peak": 94.52, "min": 59.28}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.09, "energy_joules_est": 22.88, "sample_count": 5, "duration_seconds": 0.761}, "timestamp": "2026-01-17T17:55:46.585258"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1130.486, "latencies_ms": [1130.486], "images_per_second": 0.885, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The room is dimly lit, creating a moody atmosphere. The walls appear to be a light color, possibly white or off-white. The couch is covered in a patchwork quilt or fabric with vibrant colors. The overall ambiance suggests a casual, relaxed setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.6, "ram_available_mb": 100267.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25504.3, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.52, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 62.75, "peak": 85.83, "min": 54.62}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.52, "energy_joules_est": 31.12, "sample_count": 8, "duration_seconds": 1.131}, "timestamp": "2026-01-17T17:55:47.722014"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 602.694, "latencies_ms": [602.694], "images_per_second": 1.659, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A group of cows, including white, brown, and black ones, are resting and grazing peacefully in a lush green field.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25504.3, "ram_available_mb": 100267.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 29.93, "peak": 33.47, "min": 26.0}, "VIN": {"avg": 61.62, "peak": 72.05, "min": 52.58}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 18.06, "sample_count": 4, "duration_seconds": 0.603}, "timestamp": "2026-01-17T17:55:48.336546"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1139.368, "latencies_ms": [1139.368], "images_per_second": 0.878, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "tree: 1\ncow: 5\nsheep: 1\ngrass: 8\ncow: 5\ncow: 1\ncow: 1\ncow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.87, "peak": 37.03, "min": 21.67}, "VIN": {"avg": 64.53, "peak": 84.6, "min": 57.72}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 27.87, "energy_joules_est": 31.77, "sample_count": 8, "duration_seconds": 1.14}, "timestamp": "2026-01-17T17:55:49.482890"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 913.469, "latencies_ms": [913.469], "images_per_second": 1.095, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The foreground features a young lamb resting near the base of a tree, positioned between the foreground and the background. The background includes several cows lying down in the grassy field, extending from the foreground to the middle ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.35, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 63.77, "peak": 82.63, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.35, "energy_joules_est": 25.0, "sample_count": 7, "duration_seconds": 0.914}, "timestamp": "2026-01-17T17:55:50.402759"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 657.785, "latencies_ms": [657.785], "images_per_second": 1.52, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A group of cows, including some calves, are resting in a lush green field. A large tree trunk is visible near the animals, providing shade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.62, "peak": 34.65, "min": 24.83}, "VIN": {"avg": 63.78, "peak": 75.65, "min": 59.54}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 19.5, "sample_count": 5, "duration_seconds": 0.658}, "timestamp": "2026-01-17T17:55:51.066844"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 892.783, "latencies_ms": [892.783], "images_per_second": 1.12, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The field is predominantly green with patches of sunlight filtering through the trees, creating a bright and peaceful atmosphere. The grass appears lush and well-maintained, contributing to the overall tranquility of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25504.1, "ram_available_mb": 100268.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25506.1, "ram_available_mb": 100266.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.34, "peak": 37.03, "min": 23.24}, "VIN": {"avg": 63.87, "peak": 83.46, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.34, "energy_joules_est": 26.22, "sample_count": 6, "duration_seconds": 0.894}, "timestamp": "2026-01-17T17:55:51.966699"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 687.41, "latencies_ms": [687.41], "images_per_second": 1.455, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A large group of boys in uniform, sitting and standing together in front of a brick building, poses for a school photograph in April 1929.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25506.1, "ram_available_mb": 100266.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.25, "peak": 35.45, "min": 25.21}, "VIN": {"avg": 63.49, "peak": 77.51, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.25, "energy_joules_est": 20.8, "sample_count": 5, "duration_seconds": 0.688}, "timestamp": "2026-01-17T17:55:52.663599"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1124.489, "latencies_ms": [1124.489], "images_per_second": 0.889, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "group: 20\nboys: 20\nschool building: 10\nsitting: 10\nsocks: 10\nshoes: 10\njackets: 10\nsuits: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.28, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 64.32, "peak": 82.91, "min": 59.58}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.28, "energy_joules_est": 30.68, "sample_count": 8, "duration_seconds": 1.125}, "timestamp": "2026-01-17T17:55:53.795155"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 792.996, "latencies_ms": [792.996], "images_per_second": 1.261, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The main objects are positioned in a diagonal formation, with the boys standing and sitting in the foreground and the brick building in the background. The boys are grouped together, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 28.82, "peak": 33.86, "min": 24.02}, "VIN": {"avg": 65.47, "peak": 84.65, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 22.86, "sample_count": 5, "duration_seconds": 0.793}, "timestamp": "2026-01-17T17:55:54.595033"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 892.01, "latencies_ms": [892.01], "images_per_second": 1.121, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A large group of boys, likely from the Goodmayes Boys' School, poses for a group photograph in April 1929. The photo captures the school building in the background and the boys sitting on the ground in front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.81, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 65.66, "peak": 87.68, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.81, "energy_joules_est": 25.71, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T17:55:55.493348"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1076.782, "latencies_ms": [1076.782], "images_per_second": 0.929, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The photograph is in black and white and appears to be taken outdoors in natural light. The students are wearing formal attire, including suits and ties. The building in the background suggests a school setting. The overall composition and style of the photograph indicate it was taken during an earlier era.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.97, "peak": 35.03, "min": 21.27}, "VIN": {"avg": 66.39, "peak": 92.72, "min": 59.42}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.97, "energy_joules_est": 29.05, "sample_count": 8, "duration_seconds": 1.077}, "timestamp": "2026-01-17T17:55:56.576870"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 441.851, "latencies_ms": [441.851], "images_per_second": 2.263, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A rainbow-colored kite soars high in the sky above a lush green park.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 24.55, "peak": 26.39, "min": 22.85}, "VIN": {"avg": 61.85, "peak": 64.7, "min": 57.12}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.55, "energy_joules_est": 10.86, "sample_count": 3, "duration_seconds": 0.442}, "timestamp": "2026-01-17T17:55:57.028066"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 659.059, "latencies_ms": [659.059], "images_per_second": 1.517, "prompt_tokens": 21, "response_tokens_est": 22, "n_tiles": 1, "output_text": "kite: 1\nperson: 1\ntrees: 4\nbuildings: 3\nsky: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25505.9, "ram_available_mb": 100266.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25506.6, "ram_available_mb": 100265.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 24.72, "peak": 27.56, "min": 22.46}, "VIN": {"avg": 60.15, "peak": 62.08, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 14.46, "peak": 14.95, "min": 14.17}}, "power_watts_avg": 24.72, "energy_joules_est": 16.3, "sample_count": 4, "duration_seconds": 0.659}, "timestamp": "2026-01-17T17:55:57.697853"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 772.392, "latencies_ms": [772.392], "images_per_second": 1.295, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The kite is positioned to the right of the image, flying above a grassy field. The background features trees and residential buildings, suggesting the kite is flying in a park or open space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25506.6, "ram_available_mb": 100265.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25506.6, "ram_available_mb": 100265.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 23.63, "peak": 26.77, "min": 21.27}, "VIN": {"avg": 61.25, "peak": 67.04, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 18.26, "sample_count": 5, "duration_seconds": 0.773}, "timestamp": "2026-01-17T17:55:58.476049"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 639.534, "latencies_ms": [639.534], "images_per_second": 1.564, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The scene is set in a park-like area with trees and houses in the background. A colorful kite is flying high in the sky, caught in the wind.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25506.6, "ram_available_mb": 100265.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25506.4, "ram_available_mb": 100265.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 23.92, "peak": 26.38, "min": 22.06}, "VIN": {"avg": 59.92, "peak": 61.68, "min": 57.64}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.92, "energy_joules_est": 15.3, "sample_count": 4, "duration_seconds": 0.64}, "timestamp": "2026-01-17T17:55:59.121890"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 536.304, "latencies_ms": [536.304], "images_per_second": 1.865, "prompt_tokens": 18, "response_tokens_est": 22, "n_tiles": 1, "output_text": "The kite is brightly colored with a rainbow pattern. The lighting suggests a sunny day with potentially warm temperatures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25506.4, "ram_available_mb": 100265.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 25506.4, "ram_available_mb": 100265.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 24.94, "peak": 26.77, "min": 23.23}, "VIN": {"avg": 59.91, "peak": 60.76, "min": 59.38}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.94, "energy_joules_est": 13.38, "sample_count": 3, "duration_seconds": 0.537}, "timestamp": "2026-01-17T17:55:59.668260"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 575.328, "latencies_ms": [575.328], "images_per_second": 1.738, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A large cheese pizza with red sauce and melted cheese sits in a cardboard box on a black surface.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25506.4, "ram_available_mb": 100265.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25506.4, "ram_available_mb": 100265.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.43, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 64.16, "peak": 81.4, "min": 52.92}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.43, "energy_joules_est": 17.53, "sample_count": 4, "duration_seconds": 0.576}, "timestamp": "2026-01-17T17:56:00.258315"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1499.047, "latencies_ms": [1499.047], "images_per_second": 0.667, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 1, "output_text": "Pizza: 8\nCheese: 8\nTomato sauce: 8\nPepperoni: 8\nSauce: 8\nCheese: 8\nTomato sauce: 8\nSauce: 8\nCheese: 8\nCheese: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25506.4, "ram_available_mb": 100265.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25509.9, "ram_available_mb": 100262.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.06, "peak": 37.8, "min": 20.09}, "VIN": {"avg": 63.56, "peak": 93.01, "min": 52.94}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.06, "energy_joules_est": 39.08, "sample_count": 11, "duration_seconds": 1.5}, "timestamp": "2026-01-17T17:56:01.763931"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 642.058, "latencies_ms": [642.058], "images_per_second": 1.557, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The main object, the pizza, is positioned in the foreground of the image. The pizza box is situated in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25509.9, "ram_available_mb": 100262.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.04, "peak": 33.48, "min": 26.0}, "VIN": {"avg": 67.8, "peak": 88.08, "min": 57.61}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.04, "energy_joules_est": 19.3, "sample_count": 4, "duration_seconds": 0.643}, "timestamp": "2026-01-17T17:56:02.412742"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 790.766, "latencies_ms": [790.766], "images_per_second": 1.265, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A large cheese pizza sits in a cardboard box on a dark surface. The pizza is cut into eight slices and appears freshly baked with melted cheese and tomato sauce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.81, "peak": 36.63, "min": 23.64}, "VIN": {"avg": 67.21, "peak": 95.38, "min": 56.44}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.81, "energy_joules_est": 23.58, "sample_count": 6, "duration_seconds": 0.791}, "timestamp": "2026-01-17T17:56:03.209502"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1003.846, "latencies_ms": [1003.846], "images_per_second": 0.996, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The pizza is topped with melted white cheese and has a golden-brown crust. The cheese appears slightly browned, suggesting it has been cooked to perfection. The lighting in the image is bright, highlighting the colors and textures of the pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.24, "peak": 35.83, "min": 22.45}, "VIN": {"avg": 64.48, "peak": 93.08, "min": 55.25}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.24, "energy_joules_est": 28.36, "sample_count": 7, "duration_seconds": 1.004}, "timestamp": "2026-01-17T17:56:04.219455"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 632.72, "latencies_ms": [632.72], "images_per_second": 1.58, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Two women are sitting inside an open white refrigerator on a city sidewalk, one smoking a cigarette and the other holding a drink.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25509.8, "ram_available_mb": 100262.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.62, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 68.07, "peak": 89.02, "min": 59.72}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 19.39, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T17:56:04.869709"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1083.159, "latencies_ms": [1083.159], "images_per_second": 0.923, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "woman: 2\nrefrigerator: 1\ncup: 2\ncigarette: 1\nwindow: 1\nstreet: 2\nsidewalk: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25509.8, "ram_available_mb": 100262.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.66, "peak": 36.21, "min": 21.66}, "VIN": {"avg": 62.32, "peak": 75.99, "min": 58.93}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.66, "energy_joules_est": 29.98, "sample_count": 8, "duration_seconds": 1.084}, "timestamp": "2026-01-17T17:56:05.959817"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 958.585, "latencies_ms": [958.585], "images_per_second": 1.043, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The woman on the left is positioned in the foreground, leaning against the wall and holding a cigarette. The open refrigerator is situated in the background, partially obscured by the woman. The street and buildings are visible in the background, suggesting an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.24, "peak": 34.27, "min": 21.66}, "VIN": {"avg": 63.5, "peak": 81.62, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.24, "energy_joules_est": 26.12, "sample_count": 7, "duration_seconds": 0.959}, "timestamp": "2026-01-17T17:56:06.925372"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 754.498, "latencies_ms": [754.498], "images_per_second": 1.325, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Two women are sitting on a sidewalk outside a building, engaged in conversation. One woman is smoking a cigarette, while the other is sitting inside an open refrigerator.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.14, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 67.53, "peak": 98.28, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.14, "energy_joules_est": 21.99, "sample_count": 5, "duration_seconds": 0.755}, "timestamp": "2026-01-17T17:56:07.686273"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 867.717, "latencies_ms": [867.717], "images_per_second": 1.152, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The woman is wearing a brown jacket and dark blue jeans. The scene is lit by natural daylight, creating a warm ambiance. The presence of a beverage suggests a casual, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.08, "peak": 35.85, "min": 23.24}, "VIN": {"avg": 61.88, "peak": 71.4, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.08, "energy_joules_est": 25.25, "sample_count": 6, "duration_seconds": 0.868}, "timestamp": "2026-01-17T17:56:08.560073"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 588.796, "latencies_ms": [588.796], "images_per_second": 1.698, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A man wearing a straw hat is preparing a meal, grilling hot dogs on a tray with foil.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 31.01, "peak": 35.45, "min": 26.38}, "VIN": {"avg": 71.61, "peak": 96.14, "min": 61.93}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.01, "energy_joules_est": 18.27, "sample_count": 4, "duration_seconds": 0.589}, "timestamp": "2026-01-17T17:56:09.161601"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1125.021, "latencies_ms": [1125.021], "images_per_second": 0.889, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "hot dog: 8\nbun: 8\ntray: 1\nman: 1\nhat: 1\ngrass: 1\nchair: 1\nsweatshirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.76, "peak": 37.0, "min": 21.27}, "VIN": {"avg": 65.12, "peak": 96.47, "min": 56.05}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.76, "energy_joules_est": 31.24, "sample_count": 8, "duration_seconds": 1.125}, "timestamp": "2026-01-17T17:56:10.293001"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 748.218, "latencies_ms": [748.218], "images_per_second": 1.337, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The main object is a person holding a tray of hot dogs. The tray is placed in the foreground, slightly to the right of the person. The background consists of grass and a white fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25509.6, "ram_available_mb": 100262.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25509.8, "ram_available_mb": 100262.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.81, "min": 14.3}, "VDD_GPU": {"avg": 29.46, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 68.84, "peak": 98.14, "min": 57.02}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 22.06, "sample_count": 5, "duration_seconds": 0.749}, "timestamp": "2026-01-17T17:56:11.048413"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 847.141, "latencies_ms": [847.141], "images_per_second": 1.18, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A man is preparing hot dogs on a tray outdoors, likely at a picnic or outdoor gathering. He appears to be in a casual setting, possibly a backyard or park, with some grass visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25509.8, "ram_available_mb": 100262.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25511.3, "ram_available_mb": 100260.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.35, "peak": 36.24, "min": 23.64}, "VIN": {"avg": 67.12, "peak": 97.67, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.35, "energy_joules_est": 24.88, "sample_count": 6, "duration_seconds": 0.848}, "timestamp": "2026-01-17T17:56:11.902779"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 949.265, "latencies_ms": [949.265], "images_per_second": 1.053, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The hot dogs are red and appear to be cooked on a tray. The lighting is bright, likely from sunlight, creating a warm and inviting atmosphere. The tray is made of aluminum foil, suggesting it is designed for easy handling and easy cleanup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25511.3, "ram_available_mb": 100260.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25511.0, "ram_available_mb": 100261.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 62.08, "peak": 86.03, "min": 49.97}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 26.4, "sample_count": 7, "duration_seconds": 0.95}, "timestamp": "2026-01-17T17:56:12.858331"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 737.418, "latencies_ms": [737.418], "images_per_second": 1.356, "prompt_tokens": 8, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The living room features a desk with a laptop, a chair, a couch, a bookshelf, a radiator, and a star decoration, creating a cozy and functional space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25511.0, "ram_available_mb": 100261.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.89, "peak": 34.24, "min": 24.02}, "VIN": {"avg": 68.21, "peak": 98.84, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 21.32, "sample_count": 5, "duration_seconds": 0.738}, "timestamp": "2026-01-17T17:56:13.606859"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1130.599, "latencies_ms": [1130.599], "images_per_second": 0.884, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "desk: 1\nlaptop: 1\nchair: 1\nbookshelf: 2\nbooks: 2\nheater: 1\ncouch: 1\nbags: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.98, "peak": 35.42, "min": 21.27}, "VIN": {"avg": 63.02, "peak": 78.23, "min": 57.09}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.98, "energy_joules_est": 30.51, "sample_count": 8, "duration_seconds": 1.131}, "timestamp": "2026-01-17T17:56:14.743210"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 716.345, "latencies_ms": [716.345], "images_per_second": 1.396, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The desk and chair are positioned to the left of the couch, near the foreground. The bookshelf and heater are situated in the background, near the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.98, "peak": 33.88, "min": 24.41}, "VIN": {"avg": 63.31, "peak": 75.05, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.98, "energy_joules_est": 20.77, "sample_count": 5, "duration_seconds": 0.717}, "timestamp": "2026-01-17T17:56:15.466688"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1178.533, "latencies_ms": [1178.533], "images_per_second": 0.849, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 1, "output_text": "The room appears to be a home office or study area. A desk with a laptop is positioned on the left side, while a bookshelf filled with books occupies the center. A couch with personal items, including a backpack and pillows, is situated on the right side. The room is dimly lit, with a star decoration hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.23, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 64.1, "peak": 80.28, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 32.1, "sample_count": 8, "duration_seconds": 1.179}, "timestamp": "2026-01-17T17:56:16.651597"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 892.798, "latencies_ms": [892.798], "images_per_second": 1.12, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The room has neutral-colored walls and carpeting. The lighting appears to be soft and warm, creating a calm atmosphere. The furniture includes a desk with a laptop and a chair, and a couch with blankets and bags.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.56, "peak": 33.47, "min": 22.45}, "VIN": {"avg": 65.09, "peak": 82.57, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.56, "energy_joules_est": 24.61, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T17:56:17.550840"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 537.773, "latencies_ms": [537.773], "images_per_second": 1.86, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "Two majestic elephants stand in a grassy field, their trunks intertwined in a gentle interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.4, "min": 14.0}, "VDD_GPU": {"avg": 31.9, "peak": 34.65, "min": 28.76}, "VIN": {"avg": 69.52, "peak": 81.81, "min": 62.11}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.9, "energy_joules_est": 17.17, "sample_count": 3, "duration_seconds": 0.538}, "timestamp": "2026-01-17T17:56:18.104481"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1021.395, "latencies_ms": [1021.395], "images_per_second": 0.979, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "elephant: 2\ntrunk: 2\ntusks: 2\ngrass: 2\nleaves: 2\nbushes: 2\nhills: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.27, "peak": 37.82, "min": 21.67}, "VIN": {"avg": 63.71, "peak": 85.42, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.27, "energy_joules_est": 28.88, "sample_count": 8, "duration_seconds": 1.022}, "timestamp": "2026-01-17T17:56:19.132283"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 965.913, "latencies_ms": [965.913], "images_per_second": 1.035, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The main objects are positioned relatively close to the viewer, creating a sense of proximity and interaction. The foreground is dominated by the elephants, while the background features more vegetation and a hazy sky, suggesting the scene is situated in a relatively open area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.4, "peak": 34.65, "min": 21.66}, "VIN": {"avg": 63.9, "peak": 86.05, "min": 57.2}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.4, "energy_joules_est": 26.47, "sample_count": 7, "duration_seconds": 0.966}, "timestamp": "2026-01-17T17:56:20.103963"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 747.244, "latencies_ms": [747.244], "images_per_second": 1.338, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Two elephants are seen interacting in a grassy field with scattered trees and shrubs. The scene appears to be in a natural habitat, possibly a savanna or grassland.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.38, "peak": 34.66, "min": 24.41}, "VIN": {"avg": 63.67, "peak": 77.65, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.38, "energy_joules_est": 21.97, "sample_count": 5, "duration_seconds": 0.748}, "timestamp": "2026-01-17T17:56:20.858359"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 827.246, "latencies_ms": [827.246], "images_per_second": 1.209, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The elephants are dark brown, contrasting with the light green vegetation. The lighting is soft and diffused, suggesting an overcast sky. The scene appears natural and peaceful.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.13, "peak": 35.42, "min": 23.62}, "VIN": {"avg": 66.7, "peak": 96.84, "min": 57.49}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.13, "energy_joules_est": 24.1, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T17:56:21.692195"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 657.456, "latencies_ms": [657.456], "images_per_second": 1.521, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man wearing a pink cap and sunglasses is holding a white frisbee and a green bottle while standing in a grassy field.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25510.5, "ram_available_mb": 100261.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.92, "peak": 35.44, "min": 24.81}, "VIN": {"avg": 64.86, "peak": 84.55, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.92, "energy_joules_est": 19.68, "sample_count": 5, "duration_seconds": 0.658}, "timestamp": "2026-01-17T17:56:22.360983"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1236.318, "latencies_ms": [1236.318], "images_per_second": 0.809, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "man: 1\nfrisbee: 1\nbottle: 1\nsunglasses: 1\nbelt: 1\nshorts: 1\ngrass: 1\ntrees: 2\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.96, "peak": 36.63, "min": 20.88}, "VIN": {"avg": 64.41, "peak": 91.9, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.96, "energy_joules_est": 33.34, "sample_count": 9, "duration_seconds": 1.237}, "timestamp": "2026-01-17T17:56:23.603351"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 873.891, "latencies_ms": [873.891], "images_per_second": 1.144, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The man is positioned in the foreground, holding a frisbee and bottle, while another person can be seen in the background walking away. The frisbee and bottle are located close to the man, creating a sense of proximity.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.83, "peak": 33.86, "min": 22.84}, "VIN": {"avg": 62.68, "peak": 80.32, "min": 54.79}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.83, "energy_joules_est": 24.33, "sample_count": 6, "duration_seconds": 0.874}, "timestamp": "2026-01-17T17:56:24.483545"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 935.019, "latencies_ms": [935.019], "images_per_second": 1.069, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "A man is playing frisbee in a park. He is shirtless and wearing khaki shorts, carrying a frisbee and a green bottle. Another person is visible in the background, walking across the grassy field.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 65.82, "peak": 94.93, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 26.0, "sample_count": 7, "duration_seconds": 0.935}, "timestamp": "2026-01-17T17:56:25.426070"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1002.009, "latencies_ms": [1002.009], "images_per_second": 0.998, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The man is shirtless and wearing khaki shorts. He holds a white frisbee and a green bottle in his hands. The scene takes place on a sunny day with a clear blue sky. The grass appears to be well-maintained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.5, "peak": 35.06, "min": 22.05}, "VIN": {"avg": 63.13, "peak": 74.31, "min": 56.15}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.5, "energy_joules_est": 27.56, "sample_count": 7, "duration_seconds": 1.002}, "timestamp": "2026-01-17T17:56:26.434286"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 640.408, "latencies_ms": [640.408], "images_per_second": 1.562, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A young boy in a blue soccer jersey is cutting into a large chocolate cake designed to look like a skateboard, complete with wheels and flames.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 24.22, "peak": 26.77, "min": 22.05}, "VIN": {"avg": 60.24, "peak": 64.83, "min": 53.36}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.22, "energy_joules_est": 15.52, "sample_count": 4, "duration_seconds": 0.641}, "timestamp": "2026-01-17T17:56:27.084647"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 905.31, "latencies_ms": [905.31], "images_per_second": 1.105, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "cake: 1\nknife: 1\nplates: 2\ntoys: 2\ntable: 2\nfoil: 1\ntablecloth: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.9, "ram_available_mb": 100261.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.59}, "VDD_GPU": {"avg": 22.68, "peak": 26.77, "min": 20.09}, "VIN": {"avg": 60.07, "peak": 62.48, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.68, "energy_joules_est": 20.55, "sample_count": 7, "duration_seconds": 0.906}, "timestamp": "2026-01-17T17:56:27.996437"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 877.937, "latencies_ms": [877.937], "images_per_second": 1.139, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The main object, the boy, is positioned in the foreground of the image, cutting the cake. The cake itself is situated on the table in the background, partially obscured by the boy's actions. The table occupies the middle ground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.65, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 60.25, "peak": 62.65, "min": 57.96}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.65, "energy_joules_est": 19.9, "sample_count": 6, "duration_seconds": 0.878}, "timestamp": "2026-01-17T17:56:28.880474"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 919.546, "latencies_ms": [919.546], "images_per_second": 1.087, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A young boy in a blue soccer jersey is cutting a chocolate cake shaped like a skateboard on a table covered with a colorful tablecloth. The cake has chocolate wheels and flames on top, and small toy figures are scattered around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.34, "peak": 26.0, "min": 20.09}, "VIN": {"avg": 60.96, "peak": 63.61, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 22.34, "energy_joules_est": 20.55, "sample_count": 7, "duration_seconds": 0.92}, "timestamp": "2026-01-17T17:56:29.809297"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 836.232, "latencies_ms": [836.232], "images_per_second": 1.196, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The cake is primarily dark brown with red and yellow accents. The lighting in the image is warm and soft, creating a pleasant atmosphere. The materials appear to be chocolate and fondant, and the weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25510.7, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25510.6, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.65, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 60.06, "peak": 61.47, "min": 58.93}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.65, "energy_joules_est": 18.95, "sample_count": 6, "duration_seconds": 0.837}, "timestamp": "2026-01-17T17:56:30.651665"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 474.589, "latencies_ms": [474.589], "images_per_second": 2.107, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two zebras, one in the foreground and the other in the background, are seen eating from a trough in a zoo enclosure.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25510.6, "ram_available_mb": 100261.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25511.9, "ram_available_mb": 100260.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 24.29, "peak": 26.0, "min": 22.84}, "VIN": {"avg": 60.77, "peak": 62.54, "min": 59.76}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.29, "energy_joules_est": 11.55, "sample_count": 3, "duration_seconds": 0.475}, "timestamp": "2026-01-17T17:56:31.136586"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 944.574, "latencies_ms": [944.574], "images_per_second": 1.059, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "zebra: 2\nface: 2\neye: 2\nmouth: 1\nnose: 1\nhead: 1\nears: 1\nbody: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25511.9, "ram_available_mb": 100260.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25512.1, "ram_available_mb": 100260.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 23.13, "peak": 27.57, "min": 20.48}, "VIN": {"avg": 59.98, "peak": 64.67, "min": 52.14}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.13, "energy_joules_est": 21.85, "sample_count": 7, "duration_seconds": 0.945}, "timestamp": "2026-01-17T17:56:32.087325"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 858.187, "latencies_ms": [858.187], "images_per_second": 1.165, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The zebra's head is positioned in the foreground, partially obscuring the background. The zebra is situated close to the viewer, suggesting proximity. The zebra's head is angled towards the viewer, further emphasizing its presence.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25512.1, "ram_available_mb": 100260.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25521.9, "ram_available_mb": 100250.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.78, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 62.98, "peak": 65.38, "min": 59.64}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.78, "energy_joules_est": 19.57, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T17:56:32.952432"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 635.709, "latencies_ms": [635.709], "images_per_second": 1.573, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Two zebras are seen in a zoo enclosure, one in the foreground and the other partially visible in the background. The setting appears to be outdoors, with some greenery visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25521.9, "ram_available_mb": 100250.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25522.4, "ram_available_mb": 100249.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 26.0, "min": 21.66}, "VIN": {"avg": 60.97, "peak": 62.08, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 23.63, "energy_joules_est": 15.03, "sample_count": 4, "duration_seconds": 0.636}, "timestamp": "2026-01-17T17:56:33.595221"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 759.159, "latencies_ms": [759.159], "images_per_second": 1.317, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The zebra's coat is predominantly black and white. The lighting suggests an outdoor setting, likely in sunny weather. The zebra's coat appears to be made of a durable material, likely fur or wool.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25522.4, "ram_available_mb": 100249.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 23.63, "peak": 26.77, "min": 21.26}, "VIN": {"avg": 61.33, "peak": 64.14, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.63, "energy_joules_est": 17.95, "sample_count": 5, "duration_seconds": 0.76}, "timestamp": "2026-01-17T17:56:34.362605"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 636.921, "latencies_ms": [636.921], "images_per_second": 1.57, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two trains are stationed at the La Spezia Centrale train station, Italy, as indicated by the sign above the platform.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.03, "peak": 33.47, "min": 26.0}, "VIN": {"avg": 66.88, "peak": 81.23, "min": 61.45}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.03, "energy_joules_est": 19.14, "sample_count": 4, "duration_seconds": 0.637}, "timestamp": "2026-01-17T17:56:35.010084"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 982.323, "latencies_ms": [982.323], "images_per_second": 1.018, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "train: 2\nplatform: 2\nsign: 1\nbuildings: 2\npeople: 1\nmountains: 1\ntracks: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.82, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 63.26, "peak": 82.3, "min": 56.95}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 28.33, "sample_count": 7, "duration_seconds": 0.983}, "timestamp": "2026-01-17T17:56:35.999490"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 846.246, "latencies_ms": [846.246], "images_per_second": 1.182, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main objects are positioned in close proximity to each other, with the train station building on the left and the train on the right. The foreground is dominated by the platform and tracks, while the background features the mountains in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.42, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 65.21, "peak": 80.48, "min": 60.04}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 24.06, "sample_count": 6, "duration_seconds": 0.847}, "timestamp": "2026-01-17T17:56:36.852040"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1002.071, "latencies_ms": [1002.071], "images_per_second": 0.998, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The scene depicts a train station in La Spezia, Centrale, Italy. A train is stationed on the tracks, waiting for passengers. The station features a covered platform and multiple train tracks, creating a functional and bustling environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.85, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 63.72, "peak": 86.3, "min": 54.73}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 27.93, "sample_count": 7, "duration_seconds": 1.003}, "timestamp": "2026-01-17T17:56:37.861149"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 970.62, "latencies_ms": [970.62], "images_per_second": 1.03, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The station is illuminated by overhead lights, creating a bright and welcoming atmosphere. The platform is made of brick, contrasting with the metallic train cars and the surrounding infrastructure. The black and white photograph emphasizes the textures and contrasts within the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25522.4, "ram_available_mb": 100249.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.46, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 63.7, "peak": 76.74, "min": 58.92}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 26.66, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-17T17:56:38.838083"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 671.245, "latencies_ms": [671.245], "images_per_second": 1.49, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A woman in a black wetsuit sits on a red surfboard in the ocean, gazing out at the horizon as the sun sets.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25522.4, "ram_available_mb": 100249.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 62.61, "peak": 76.41, "min": 56.28}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 14.96, "min": 14.55}}, "power_watts_avg": 29.23, "energy_joules_est": 19.64, "sample_count": 5, "duration_seconds": 0.672}, "timestamp": "2026-01-17T17:56:39.521606"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1005.293, "latencies_ms": [1005.293], "images_per_second": 0.995, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "woman: 1\nsurfboard: 1\nwater: 1\nsky: 1\nclouds: 1\nsunset: 1\nmountains: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25522.4, "ram_available_mb": 100249.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.53, "peak": 36.23, "min": 22.45}, "VIN": {"avg": 62.31, "peak": 82.41, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.53, "energy_joules_est": 28.69, "sample_count": 7, "duration_seconds": 1.006}, "timestamp": "2026-01-17T17:56:40.537167"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1007.024, "latencies_ms": [1007.024], "images_per_second": 0.993, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The woman is positioned in the foreground of the image, facing the left side of the frame. The surfboard is situated in the foreground, partially submerged in the water. The setting sun in the background creates a dramatic contrast between the dark water and the warm hues of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25522.4, "ram_available_mb": 100249.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.69, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 62.41, "peak": 82.73, "min": 54.93}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.69, "energy_joules_est": 27.9, "sample_count": 7, "duration_seconds": 1.008}, "timestamp": "2026-01-17T17:56:41.554300"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 737.351, "latencies_ms": [737.351], "images_per_second": 1.356, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A woman is sitting on a red surfboard in the ocean at sunset. The sky is filled with clouds, and the water reflects the warm hues of the setting sun.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25522.1, "ram_available_mb": 100250.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25521.8, "ram_available_mb": 100250.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 64.97, "peak": 82.85, "min": 57.37}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.95, "min": 14.55}}, "power_watts_avg": 29.23, "energy_joules_est": 21.56, "sample_count": 5, "duration_seconds": 0.738}, "timestamp": "2026-01-17T17:56:42.298073"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1126.675, "latencies_ms": [1126.675], "images_per_second": 0.888, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The sunset paints the sky in warm hues of orange and yellow, creating a dramatic backdrop for the surfer. The water reflects the colors of the sky, enhancing the overall atmosphere. The surfer is wearing a dark wetsuit, blending with the colors of the sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25521.8, "ram_available_mb": 100250.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.12, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 64.08, "peak": 79.89, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.12, "energy_joules_est": 30.57, "sample_count": 8, "duration_seconds": 1.127}, "timestamp": "2026-01-17T17:56:43.432567"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 591.926, "latencies_ms": [591.926], "images_per_second": 1.689, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A man and a woman are enjoying a meal of sushi and pastries while seated on a train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.32, "peak": 33.86, "min": 26.39}, "VIN": {"avg": 67.63, "peak": 88.06, "min": 59.46}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.32, "energy_joules_est": 17.96, "sample_count": 4, "duration_seconds": 0.592}, "timestamp": "2026-01-17T17:56:44.036374"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1208.046, "latencies_ms": [1208.046], "images_per_second": 0.828, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "man: 2\nwoman: 2\nchopsticks: 2\nsushi: 2\nfood: 2\nplate: 1\nsauce: 1\nmenu: 1\ntrain: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 27.09, "peak": 37.01, "min": 20.88}, "VIN": {"avg": 61.71, "peak": 77.78, "min": 57.47}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.09, "energy_joules_est": 32.74, "sample_count": 9, "duration_seconds": 1.208}, "timestamp": "2026-01-17T17:56:45.250835"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 711.386, "latencies_ms": [711.386], "images_per_second": 1.406, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with the man on the left and the woman on the right. The background is slightly blurred, suggesting they are situated further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.23, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 66.64, "peak": 88.0, "min": 59.09}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 20.8, "sample_count": 5, "duration_seconds": 0.712}, "timestamp": "2026-01-17T17:56:45.968543"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 907.659, "latencies_ms": [907.659], "images_per_second": 1.102, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A man and a woman are enjoying a meal together on a train. The meal consists of sushi rolls, meat, and vegetables. They are seated at a table with chopsticks, eating with plates in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.13, "peak": 35.83, "min": 23.23}, "VIN": {"avg": 62.7, "peak": 82.59, "min": 54.18}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.13, "energy_joules_est": 26.45, "sample_count": 6, "duration_seconds": 0.908}, "timestamp": "2026-01-17T17:56:46.882632"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1126.72, "latencies_ms": [1126.72], "images_per_second": 0.888, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The train's interior is primarily light-colored, possibly off-white or beige. The lighting appears soft and diffused, creating a calm atmosphere. The materials visible include metal, plastic, and wood, typical of train interiors. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25524.3, "ram_available_mb": 100247.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25523.8, "ram_available_mb": 100248.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.83, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 63.98, "peak": 87.15, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.83, "energy_joules_est": 30.24, "sample_count": 8, "duration_seconds": 1.127}, "timestamp": "2026-01-17T17:56:48.016472"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 716.473, "latencies_ms": [716.473], "images_per_second": 1.396, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Two men in business attire, one in a white shirt and black tie and the other in a pink shirt and striped tie, walk down a city street at night.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25523.8, "ram_available_mb": 100248.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.14, "peak": 33.86, "min": 24.42}, "VIN": {"avg": 66.18, "peak": 82.63, "min": 60.61}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.14, "energy_joules_est": 20.89, "sample_count": 5, "duration_seconds": 0.717}, "timestamp": "2026-01-17T17:56:48.743761"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1048.184, "latencies_ms": [1048.184], "images_per_second": 0.954, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "building: 2\nstreet: 2\ncrosswalk: 2\nman: 2\ncar: 1\nman: 1\ntie: 2\nman: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.52, "peak": 35.85, "min": 21.66}, "VIN": {"avg": 60.17, "peak": 78.82, "min": 53.75}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 28.86, "sample_count": 8, "duration_seconds": 1.049}, "timestamp": "2026-01-17T17:56:49.798842"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 735.785, "latencies_ms": [735.785], "images_per_second": 1.359, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The two men are positioned in the foreground of the image, with the building and street behind them. The street appears relatively empty, with only a few parked cars visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.38, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 64.26, "peak": 88.75, "min": 55.85}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.38, "energy_joules_est": 21.63, "sample_count": 5, "duration_seconds": 0.736}, "timestamp": "2026-01-17T17:56:50.543261"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 950.699, "latencies_ms": [950.699], "images_per_second": 1.052, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Two men in business attire are walking down a city street at night, passing a building with signage that says \"Hierro Y Alberto.\" A car is parked on the street behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.19, "peak": 35.85, "min": 22.45}, "VIN": {"avg": 62.73, "peak": 74.15, "min": 58.29}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.19, "energy_joules_est": 26.81, "sample_count": 7, "duration_seconds": 0.951}, "timestamp": "2026-01-17T17:56:51.500497"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 829.122, "latencies_ms": [829.122], "images_per_second": 1.206, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The men are wearing dark suits and ties. The scene is lit by streetlights, creating a nighttime atmosphere. The buildings in the background are illuminated, contributing to the overall ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25524.1, "ram_available_mb": 100248.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25524.3, "ram_available_mb": 100247.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 34.65, "min": 23.23}, "VIN": {"avg": 62.86, "peak": 75.64, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 23.53, "sample_count": 6, "duration_seconds": 0.83}, "timestamp": "2026-01-17T17:56:52.336978"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 631.513, "latencies_ms": [631.513], "images_per_second": 1.583, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A man in a gray shirt and glasses is standing behind a bar, holding a wine bottle and wiping it with a cloth.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25524.3, "ram_available_mb": 100247.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25526.8, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 31.31, "peak": 35.44, "min": 26.79}, "VIN": {"avg": 69.01, "peak": 98.07, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.31, "energy_joules_est": 19.79, "sample_count": 4, "duration_seconds": 0.632}, "timestamp": "2026-01-17T17:56:52.979963"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1317.105, "latencies_ms": [1317.105], "images_per_second": 0.759, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "wine bottle: 1\nwine glass: 1\nwine bottle: 1\nwine glass: 1\ntowel: 1\npitcher: 1\nmenu: 1\ncounter: 1\nperson: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.8, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.47, "peak": 37.41, "min": 20.48}, "VIN": {"avg": 63.11, "peak": 84.36, "min": 54.43}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.47, "energy_joules_est": 34.88, "sample_count": 10, "duration_seconds": 1.318}, "timestamp": "2026-01-17T17:56:54.303529"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1184.884, "latencies_ms": [1184.884], "images_per_second": 0.844, "prompt_tokens": 25, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The main object, the man, is positioned in the foreground of the image, interacting with a wine bottle and towel. The wine bottle and towel are placed on the countertop, near the man. The background features additional wine bottles and glasses, further emphasizing the setting of a wine tasting event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 25.78, "peak": 33.47, "min": 20.48}, "VIN": {"avg": 63.56, "peak": 75.84, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.78, "energy_joules_est": 30.55, "sample_count": 9, "duration_seconds": 1.185}, "timestamp": "2026-01-17T17:56:55.494499"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 929.334, "latencies_ms": [929.334], "images_per_second": 1.076, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "A man is standing behind a bar, cleaning a wine bottle with a cloth. He appears to be in a wine tasting setting, with wine bottles and glasses visible on the bar. The atmosphere seems relaxed and casual.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.18, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 64.26, "peak": 88.92, "min": 54.68}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 25.27, "sample_count": 7, "duration_seconds": 0.93}, "timestamp": "2026-01-17T17:56:56.430334"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 947.044, "latencies_ms": [947.044], "images_per_second": 1.056, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The wine bottles are predominantly dark, indicating a likely red wine color. The lighting in the scene is warm and inviting, creating a cozy atmosphere. The wooden bar counter and shelving unit contribute to the overall rustic ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.63, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 63.14, "peak": 83.35, "min": 55.83}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 26.18, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T17:56:57.383740"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 790.09, "latencies_ms": [790.09], "images_per_second": 1.266, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A tennis player, dressed in white, is captured mid-swing with a blue and white racket, poised to strike a yellow tennis ball in the air.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.62, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 65.42, "peak": 98.51, "min": 53.11}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.62, "energy_joules_est": 22.63, "sample_count": 6, "duration_seconds": 0.791}, "timestamp": "2026-01-17T17:56:58.184647"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1217.657, "latencies_ms": [1217.657], "images_per_second": 0.821, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis court: 4\nTennis player: 1\nTennis shirt: 1\nTennis shorts: 1\nTennis wristbands: 2", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25526.7, "ram_available_mb": 100245.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25526.4, "ram_available_mb": 100245.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 25.9, "peak": 36.6, "min": 20.48}, "VIN": {"avg": 63.36, "peak": 83.95, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 25.9, "energy_joules_est": 31.55, "sample_count": 9, "duration_seconds": 1.218}, "timestamp": "2026-01-17T17:56:59.408830"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 732.158, "latencies_ms": [732.158], "images_per_second": 1.366, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The tennis player is positioned in the foreground, preparing to hit the tennis ball. The tennis court extends into the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.4, "ram_available_mb": 100245.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25526.4, "ram_available_mb": 100245.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.08, "peak": 33.48, "min": 24.43}, "VIN": {"avg": 63.55, "peak": 74.04, "min": 59.88}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 21.3, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T17:57:00.147369"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 848.411, "latencies_ms": [848.411], "images_per_second": 1.179, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A tennis player is executing a forehand shot on a grass court, showcasing their athletic skills and focus. The green grass court with white boundary lines provides a vibrant backdrop for the action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.4, "ram_available_mb": 100245.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 25526.4, "ram_available_mb": 100245.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.15, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 63.15, "peak": 77.45, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 24.74, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T17:57:01.002776"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 812.778, "latencies_ms": [812.778], "images_per_second": 1.23, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The tennis player is wearing a white shirt and shorts. The grass court is well-maintained and appears to be sunny. The player is holding a blue and white tennis racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.4, "ram_available_mb": 100245.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25526.4, "ram_available_mb": 100245.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.82, "peak": 35.44, "min": 23.25}, "VIN": {"avg": 62.59, "peak": 74.42, "min": 54.11}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 23.44, "sample_count": 6, "duration_seconds": 0.813}, "timestamp": "2026-01-17T17:57:01.821997"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 702.95, "latencies_ms": [702.95], "images_per_second": 1.423, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A white and orange cat is perched atop a wooden TV stand, attentively watching a television screen displaying a man's face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25526.4, "ram_available_mb": 100245.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25526.4, "ram_available_mb": 100245.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 30.09, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 64.8, "peak": 82.79, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.09, "energy_joules_est": 21.17, "sample_count": 5, "duration_seconds": 0.704}, "timestamp": "2026-01-17T17:57:02.536131"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1049.974, "latencies_ms": [1049.974], "images_per_second": 0.952, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "cat: 1\ntelevision: 1\ndvd player: 1\ncable box: 1\ncup: 1\ntable: 1\nshelf: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25526.4, "ram_available_mb": 100245.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.33, "peak": 35.85, "min": 21.28}, "VIN": {"avg": 64.23, "peak": 89.16, "min": 57.41}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.33, "energy_joules_est": 28.71, "sample_count": 8, "duration_seconds": 1.05}, "timestamp": "2026-01-17T17:57:03.592455"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 712.07, "latencies_ms": [712.07], "images_per_second": 1.404, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The cat is positioned in the foreground, close to the television and DVD player. The television and DVD player are situated in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.31, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.63, "peak": 89.19, "min": 53.79}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.31, "energy_joules_est": 20.88, "sample_count": 5, "duration_seconds": 0.712}, "timestamp": "2026-01-17T17:57:04.310354"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 960.546, "latencies_ms": [960.546], "images_per_second": 1.041, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A white and orange cat is perched atop a wooden TV stand, watching a television screen displaying a man's face. The TV stand is situated in a room with a yellow wall and a partially visible blue curtain.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.91, "peak": 36.63, "min": 22.06}, "VIN": {"avg": 63.41, "peak": 82.61, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 26.82, "sample_count": 7, "duration_seconds": 0.961}, "timestamp": "2026-01-17T17:57:05.277399"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 966.612, "latencies_ms": [966.612], "images_per_second": 1.035, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The cat is primarily white with orange markings. The lighting in the room appears to be soft and warm, creating a cozy atmosphere. The TV and entertainment center are made of wood and metal, contributing to the overall aesthetic of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.57, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 66.01, "peak": 95.52, "min": 57.3}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.57, "energy_joules_est": 26.66, "sample_count": 7, "duration_seconds": 0.967}, "timestamp": "2026-01-17T17:57:06.250387"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 613.789, "latencies_ms": [613.789], "images_per_second": 1.629, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A blue circular sign with a white silhouette of a person walking with a child on a bicycle is mounted on a metal pole.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.62, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 68.69, "peak": 98.43, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 18.81, "sample_count": 4, "duration_seconds": 0.614}, "timestamp": "2026-01-17T17:57:06.881369"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1052.622, "latencies_ms": [1052.622], "images_per_second": 0.95, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "sign: 2\nbicycle: 1\npedestrian: 1\ntree: 1\nstreet light: 1\nmetal post: 1\nclear sky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.77, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 63.67, "peak": 75.95, "min": 57.97}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 27.77, "energy_joules_est": 29.24, "sample_count": 8, "duration_seconds": 1.053}, "timestamp": "2026-01-17T17:57:07.940173"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 884.456, "latencies_ms": [884.456], "images_per_second": 1.131, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The sign is positioned in the foreground, partially obscured by the tree branches. The sign is situated near the tree, slightly above and to the left of the signpost. The background consists of more trees and sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.97, "peak": 34.27, "min": 22.85}, "VIN": {"avg": 61.53, "peak": 74.27, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 24.75, "sample_count": 6, "duration_seconds": 0.885}, "timestamp": "2026-01-17T17:57:08.831196"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1076.054, "latencies_ms": [1076.054], "images_per_second": 0.929, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scene is set outdoors on a sunny day, with a blue circular sign featuring a bicycle symbol and a pedestrian symbol. Below this sign is a smaller rectangular sign with Chinese characters. The signs are mounted on a metal pole and partially obscured by tree leaves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.93, "peak": 35.04, "min": 21.28}, "VIN": {"avg": 60.12, "peak": 75.83, "min": 50.97}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.93, "energy_joules_est": 28.99, "sample_count": 8, "duration_seconds": 1.076}, "timestamp": "2026-01-17T17:57:09.913732"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 868.646, "latencies_ms": [868.646], "images_per_second": 1.151, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The sign is blue and features a bicycle symbol. The lighting is bright, likely from sunlight, creating a clear visibility of the sign's details. The sign appears to be made of metal and has a weathered appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.17, "peak": 33.88, "min": 23.24}, "VIN": {"avg": 63.64, "peak": 77.61, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 24.48, "sample_count": 6, "duration_seconds": 0.869}, "timestamp": "2026-01-17T17:57:10.789184"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 682.129, "latencies_ms": [682.129], "images_per_second": 1.466, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A young girl with black hair and yellow earrings is sitting at a wooden table in a restaurant, enjoying a slice of pizza.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.7, "peak": 35.45, "min": 24.42}, "VIN": {"avg": 64.46, "peak": 86.74, "min": 52.68}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.7, "energy_joules_est": 20.27, "sample_count": 5, "duration_seconds": 0.683}, "timestamp": "2026-01-17T17:57:11.484640"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1196.21, "latencies_ms": [1196.21], "images_per_second": 0.836, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Pizza: 2\nPizza pan: 1\nGlass of water: 1\nPaper napkin: 1\nBook: 1\nGirl: 1\nChair: 4\nTable: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 26.61, "peak": 36.24, "min": 20.88}, "VIN": {"avg": 61.31, "peak": 73.96, "min": 54.38}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.61, "energy_joules_est": 31.84, "sample_count": 9, "duration_seconds": 1.197}, "timestamp": "2026-01-17T17:57:12.686942"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1072.165, "latencies_ms": [1072.165], "images_per_second": 0.933, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The girl is positioned in the foreground of the image, sitting at a table with a partially eaten pizza and a glass of water. The table is situated near a window, offering a view of the outside world. The background features additional tables and chairs, suggesting a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.6, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.34, "peak": 34.27, "min": 20.87}, "VIN": {"avg": 63.91, "peak": 78.58, "min": 59.34}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 28.26, "sample_count": 8, "duration_seconds": 1.073}, "timestamp": "2026-01-17T17:57:13.766220"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 985.36, "latencies_ms": [985.36], "images_per_second": 1.015, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A young girl with braided hair is seated at a wooden table in a restaurant, enjoying a slice of pizza and a glass of water. The restaurant has a casual atmosphere with wooden chairs and tables, and several other people can be seen in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.7, "ram_available_mb": 100241.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.45, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 63.75, "peak": 83.95, "min": 53.75}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.45, "energy_joules_est": 27.06, "sample_count": 7, "duration_seconds": 0.986}, "timestamp": "2026-01-17T17:57:14.757934"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1047.61, "latencies_ms": [1047.61], "images_per_second": 0.955, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The colors of the pizza are vibrant and varied. The lighting in the restaurant is warm and inviting, creating a cozy atmosphere. The pizza appears to be served on metal trays, suggesting it's likely a takeout option. The overall setting appears casual and comfortable.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.62, "peak": 33.86, "min": 21.26}, "VIN": {"avg": 65.33, "peak": 93.96, "min": 55.67}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.62, "energy_joules_est": 27.91, "sample_count": 8, "duration_seconds": 1.048}, "timestamp": "2026-01-17T17:57:15.812487"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 712.635, "latencies_ms": [712.635], "images_per_second": 1.403, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A table is set with plates of food, including broccoli, cauliflower, rice, meat, and bread, accompanied by drinks and utensils.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.4, "ram_available_mb": 100241.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25530.2, "ram_available_mb": 100242.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.3, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 65.81, "peak": 90.06, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 20.9, "sample_count": 5, "duration_seconds": 0.713}, "timestamp": "2026-01-17T17:57:16.536818"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1336.51, "latencies_ms": [1336.51], "images_per_second": 0.748, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "broccoli: 2\ncauliflower: 2\ncarrots: 1\ncorn: 1\nbread rolls: 4\ncasserole: 1\nwater: 1\naluminum foil: 1\nplate: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.2, "ram_available_mb": 100242.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25530.2, "ram_available_mb": 100242.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.04, "peak": 35.85, "min": 20.09}, "VIN": {"avg": 63.91, "peak": 87.05, "min": 59.27}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.04, "energy_joules_est": 34.81, "sample_count": 10, "duration_seconds": 1.337}, "timestamp": "2026-01-17T17:57:17.880043"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1047.481, "latencies_ms": [1047.481], "images_per_second": 0.955, "prompt_tokens": 25, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The broccoli, cauliflower, and bread are placed in the foreground, while the plates of food and the utensils are situated in the background. The arrangement suggests a casual, home-cooked meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25530.2, "ram_available_mb": 100242.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25529.9, "ram_available_mb": 100242.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.49, "peak": 33.88, "min": 21.26}, "VIN": {"avg": 64.17, "peak": 88.02, "min": 56.23}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.49, "energy_joules_est": 27.76, "sample_count": 8, "duration_seconds": 1.048}, "timestamp": "2026-01-17T17:57:18.935981"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1148.938, "latencies_ms": [1148.938], "images_per_second": 0.87, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The scene depicts a kitchen counter with various food items and utensils. A large bowl of broccoli and cauliflower is prominently displayed, along with several plates containing different dishes. A glass of water and a spoon are also visible on the counter. The setting appears to be a casual gathering or meal preparation area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25529.9, "ram_available_mb": 100242.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25529.5, "ram_available_mb": 100242.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.59, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 62.33, "peak": 79.9, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.59, "energy_joules_est": 30.56, "sample_count": 8, "duration_seconds": 1.149}, "timestamp": "2026-01-17T17:57:20.091127"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1226.87, "latencies_ms": [1226.87], "images_per_second": 0.815, "prompt_tokens": 18, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The food is brightly colored, with shades of orange, green, and white. The lighting is soft and diffused, creating a pleasant atmosphere. The dishes appear to be made of metal and glass, reflecting the ambient light. The setting suggests a casual, home-cooked meal, possibly in a kitchen or dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25529.5, "ram_available_mb": 100242.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25529.0, "ram_available_mb": 100243.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 25.56, "peak": 33.48, "min": 20.48}, "VIN": {"avg": 64.07, "peak": 88.64, "min": 59.43}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.56, "energy_joules_est": 31.37, "sample_count": 9, "duration_seconds": 1.227}, "timestamp": "2026-01-17T17:57:21.324568"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 752.998, "latencies_ms": [752.998], "images_per_second": 1.328, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A green and white bus is stopped at a traffic light on a busy city street, with several cars waiting behind it and a person walking on the sidewalk nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25529.0, "ram_available_mb": 100243.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25529.0, "ram_available_mb": 100243.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 67.49, "peak": 89.63, "min": 59.49}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 22.02, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T17:57:22.089478"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1163.378, "latencies_ms": [1163.378], "images_per_second": 0.86, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "street sign: 1\nbus: 1\ncar: 2\ntruck: 2\nvan: 1\ntree: 2\nbuildings: 5\ncars: 5\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25529.0, "ram_available_mb": 100243.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25529.0, "ram_available_mb": 100243.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.65, "peak": 36.23, "min": 20.88}, "VIN": {"avg": 63.48, "peak": 82.02, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.65, "energy_joules_est": 31.01, "sample_count": 9, "duration_seconds": 1.164}, "timestamp": "2026-01-17T17:57:23.259811"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1050.905, "latencies_ms": [1050.905], "images_per_second": 0.952, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The foreground features a street intersection with several vehicles, including a red car, a silver car, and a bus. The background showcases a city street lined with buildings, trees, and parked cars. The bus is positioned in the middle of the street, slightly to the right.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25529.0, "ram_available_mb": 100243.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 26.95, "peak": 34.26, "min": 21.67}, "VIN": {"avg": 62.66, "peak": 80.58, "min": 54.46}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.95, "energy_joules_est": 28.33, "sample_count": 7, "duration_seconds": 1.051}, "timestamp": "2026-01-17T17:57:24.317114"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 988.644, "latencies_ms": [988.644], "images_per_second": 1.011, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a busy city street with several vehicles, including a bus, cars, and a truck, navigating through traffic. Pedestrians are walking along the sidewalk, and buildings line the street, creating a typical urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 27.3, "peak": 34.27, "min": 22.07}, "VIN": {"avg": 64.4, "peak": 93.74, "min": 53.14}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.3, "energy_joules_est": 27.0, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T17:57:25.312533"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1002.371, "latencies_ms": [1002.371], "images_per_second": 0.998, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scene is dominated by various colors, including red, green, and white. The lighting appears to be natural daylight, creating a pleasant atmosphere. The buildings in the background are primarily brick and feature large windows. The overall setting suggests a bustling urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 63.3, "peak": 81.78, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 27.87, "sample_count": 7, "duration_seconds": 1.003}, "timestamp": "2026-01-17T17:57:26.325302"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 534.458, "latencies_ms": [534.458], "images_per_second": 1.871, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A black Toshiba laptop is open on a white table, accompanied by a smartphone, a small device, and a tripod.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 24.94, "peak": 26.79, "min": 23.23}, "VIN": {"avg": 59.64, "peak": 61.39, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.94, "energy_joules_est": 13.35, "sample_count": 3, "duration_seconds": 0.535}, "timestamp": "2026-01-17T17:57:26.869573"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 891.928, "latencies_ms": [891.928], "images_per_second": 1.121, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "laptop: 1\nphone: 1\ntripod: 1\nglobe: 1\ntable: 1\nperson: 1\nwindow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.43, "peak": 27.17, "min": 20.87}, "VIN": {"avg": 59.84, "peak": 61.79, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 23.43, "energy_joules_est": 20.91, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T17:57:27.768710"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 789.218, "latencies_ms": [789.218], "images_per_second": 1.267, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The laptop is positioned in the foreground, slightly to the right of the camera. The person's red shirt is partially visible in the background, near the laptop. The cell phone is situated further to the right, near the edge of the table.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.24, "peak": 26.0, "min": 21.27}, "VIN": {"avg": 59.97, "peak": 62.82, "min": 53.23}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.24, "energy_joules_est": 18.35, "sample_count": 5, "duration_seconds": 0.79}, "timestamp": "2026-01-17T17:57:28.563413"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 906.429, "latencies_ms": [906.429], "images_per_second": 1.103, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a workspace with a Toshiba laptop open on a table, accompanied by a smartphone, a small device, and a tripod. The laptop screen displays a Windows desktop, indicating a typical work or study environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 22.57, "peak": 26.39, "min": 20.09}, "VIN": {"avg": 60.35, "peak": 61.52, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.57, "energy_joules_est": 20.47, "sample_count": 7, "duration_seconds": 0.907}, "timestamp": "2026-01-17T17:57:29.476271"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 476.345, "latencies_ms": [476.345], "images_per_second": 2.099, "prompt_tokens": 18, "response_tokens_est": 20, "n_tiles": 1, "output_text": "The laptop is black. The table is white. The phone is black. The background is dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 24.28, "peak": 25.99, "min": 22.84}, "VIN": {"avg": 59.04, "peak": 61.87, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.28, "energy_joules_est": 11.58, "sample_count": 3, "duration_seconds": 0.477}, "timestamp": "2026-01-17T17:57:29.958994"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 710.752, "latencies_ms": [710.752], "images_per_second": 1.407, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A desk with a computer setup, keyboard, mouse, books, papers, and a water bottle is situated near a window in an office.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.85, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 65.22, "peak": 79.56, "min": 60.24}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.85, "energy_joules_est": 21.23, "sample_count": 5, "duration_seconds": 0.711}, "timestamp": "2026-01-17T17:57:30.679879"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1121.719, "latencies_ms": [1121.719], "images_per_second": 0.891, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "computer monitor: 1\nkeyboard: 1\nlaptop: 1\nmouse: 1\nbooks: 5\nwater bottle: 1\ndesk: 2\nwindow: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.57, "peak": 37.01, "min": 21.27}, "VIN": {"avg": 64.23, "peak": 86.15, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.57, "energy_joules_est": 30.94, "sample_count": 8, "duration_seconds": 1.122}, "timestamp": "2026-01-17T17:57:31.808784"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 892.269, "latencies_ms": [892.269], "images_per_second": 1.121, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main objects are positioned in a diagonal arrangement, with the computer monitor and keyboard in the foreground and the books and laptop in the background. The desk is situated near a window, which provides natural light and potentially a view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.81, "min": 14.3}, "VDD_GPU": {"avg": 28.09, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 67.07, "peak": 91.25, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.09, "energy_joules_est": 25.07, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T17:57:32.707663"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1307.24, "latencies_ms": [1307.24], "images_per_second": 0.765, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The scene depicts a well-organized workspace with a desk featuring a computer setup, including a monitor, keyboard, mouse, and laptop.  The desk is positioned near a window, offering natural light and possibly a view of an external building. Various books and office supplies are present on the desk, indicating a studious environment.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.0, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 25.48, "peak": 35.04, "min": 20.1}, "VIN": {"avg": 62.93, "peak": 80.39, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.48, "energy_joules_est": 33.32, "sample_count": 10, "duration_seconds": 1.308}, "timestamp": "2026-01-17T17:57:34.021029"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 872.324, "latencies_ms": [872.324], "images_per_second": 1.146, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The desk is light beige or tan. The lighting appears to be natural daylight coming in through a window. The desk is equipped with a computer setup, including a monitor, keyboard, mouse, and laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 27.77, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 65.19, "peak": 87.54, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.77, "energy_joules_est": 24.23, "sample_count": 6, "duration_seconds": 0.873}, "timestamp": "2026-01-17T17:57:34.899903"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 752.513, "latencies_ms": [752.513], "images_per_second": 1.329, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A skateboarder wearing a black shirt and helmet is performing an impressive trick mid-air, flipping their skateboard while airborne.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.71, "peak": 35.85, "min": 24.43}, "VIN": {"avg": 65.14, "peak": 88.05, "min": 57.02}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.71, "energy_joules_est": 22.37, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T17:57:35.663553"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1050.626, "latencies_ms": [1050.626], "images_per_second": 0.952, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "skateboard: 1\nhelmet: 1\ncamera: 1\nspectators: 6\nstage: 1\nbanner: 2\nwater bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.91, "peak": 35.85, "min": 22.06}, "VIN": {"avg": 62.3, "peak": 78.49, "min": 51.37}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 29.34, "sample_count": 7, "duration_seconds": 1.051}, "timestamp": "2026-01-17T17:57:36.724462"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1096.959, "latencies_ms": [1096.959], "images_per_second": 0.912, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, performing a trick above a crowd of spectators. The skateboarder is relatively close to the spectators, suggesting they are close by. The background is filled with spectators, further emphasizing the proximity of the action.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.39, "peak": 33.88, "min": 20.88}, "VIN": {"avg": 63.91, "peak": 93.9, "min": 53.76}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.39, "energy_joules_est": 28.96, "sample_count": 8, "duration_seconds": 1.097}, "timestamp": "2026-01-17T17:57:37.828156"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 951.714, "latencies_ms": [951.714], "images_per_second": 1.051, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A skateboarder is performing a trick mid-air, suspended above a ramp in a large indoor arena. Spectators watch intently from the sidelines, capturing the moment with their cameras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.95, "peak": 33.86, "min": 21.66}, "VIN": {"avg": 63.66, "peak": 87.67, "min": 56.8}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.95, "energy_joules_est": 25.66, "sample_count": 7, "duration_seconds": 0.952}, "timestamp": "2026-01-17T17:57:38.786112"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1123.542, "latencies_ms": [1123.542], "images_per_second": 0.89, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The skateboarder is wearing black clothing and a red and white helmet. The lighting is focused on the skateboarder, creating a dramatic effect. The skateboard appears to be made of wood and metal. The setting appears to be indoors, possibly in a large arena or stadium.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.84, "peak": 34.66, "min": 21.26}, "VIN": {"avg": 63.32, "peak": 83.14, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.84, "energy_joules_est": 30.17, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-17T17:57:39.915889"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 768.328, "latencies_ms": [768.328], "images_per_second": 1.302, "prompt_tokens": 8, "response_tokens_est": 36, "n_tiles": 1, "output_text": "An orange fire hydrant, adorned with a smiley face and a bowtie, stands on a sidewalk next to a tree, with cars parked on the street in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.99, "peak": 33.47, "min": 24.42}, "VIN": {"avg": 66.04, "peak": 83.38, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.99, "energy_joules_est": 22.29, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T17:57:40.698860"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1082.777, "latencies_ms": [1082.777], "images_per_second": 0.924, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "hydrant: 1\nstreet: 2\ncar: 2\ntree: 1\nbuildings: 2\ntrees: 2\nstreet: 2\nsidewalk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.13, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 63.7, "peak": 80.74, "min": 55.29}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.13, "energy_joules_est": 29.38, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T17:57:41.790178"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 722.568, "latencies_ms": [722.568], "images_per_second": 1.384, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The fire hydrant is positioned in the foreground, slightly to the right of the viewer. The street and sidewalk are in the background, with the cars parked further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.22, "peak": 33.86, "min": 24.41}, "VIN": {"avg": 67.56, "peak": 96.48, "min": 59.45}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.22, "energy_joules_est": 21.12, "sample_count": 5, "duration_seconds": 0.723}, "timestamp": "2026-01-17T17:57:42.519981"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 781.613, "latencies_ms": [781.613], "images_per_second": 1.279, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "An orange fire hydrant with a smiley face and bowtie is situated on a sidewalk next to a street with parked cars and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.01, "peak": 35.44, "min": 24.81}, "VIN": {"avg": 67.83, "peak": 95.41, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.01, "energy_joules_est": 23.47, "sample_count": 5, "duration_seconds": 0.782}, "timestamp": "2026-01-17T17:57:43.308054"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 681.432, "latencies_ms": [681.432], "images_per_second": 1.467, "prompt_tokens": 18, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The fire hydrant is painted bright orange and has a smiley face painted on it. The scene appears to be sunny and outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 30.16, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 63.32, "peak": 81.37, "min": 53.01}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.16, "energy_joules_est": 20.57, "sample_count": 5, "duration_seconds": 0.682}, "timestamp": "2026-01-17T17:57:43.995751"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 616.788, "latencies_ms": [616.788], "images_per_second": 1.621, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A green cart loaded with old suitcases in various sizes and colors is positioned in front of a green building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 31.4, "peak": 35.83, "min": 26.77}, "VIN": {"avg": 69.49, "peak": 99.13, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 14.36, "peak": 14.56, "min": 14.16}}, "power_watts_avg": 31.4, "energy_joules_est": 19.38, "sample_count": 4, "duration_seconds": 0.617}, "timestamp": "2026-01-17T17:57:44.624504"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 987.126, "latencies_ms": [987.126], "images_per_second": 1.013, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "suitcase: 8\ncart: 8\nbicycle: 1\nposter: 1\ndoor: 1\nfloor: 1\nwall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 28.64, "peak": 37.0, "min": 22.45}, "VIN": {"avg": 62.33, "peak": 73.66, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.64, "energy_joules_est": 28.28, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-17T17:57:45.618024"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1030.933, "latencies_ms": [1030.933], "images_per_second": 0.97, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The suitcases are stacked and piled together, occupying the foreground of the image. The green cart is positioned in the background, slightly out of focus. The suitcases are positioned in front of a green door, suggesting they are being transported or moved.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.46, "peak": 34.66, "min": 22.05}, "VIN": {"avg": 65.24, "peak": 96.47, "min": 54.65}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 28.32, "sample_count": 7, "duration_seconds": 1.031}, "timestamp": "2026-01-17T17:57:46.660595"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 978.737, "latencies_ms": [978.737], "images_per_second": 1.022, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a vintage luggage cart loaded with several suitcases and trunks, positioned in front of a green building. The luggage appears to be old and possibly antique, adding a nostalgic touch to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.0, "peak": 33.86, "min": 21.66}, "VIN": {"avg": 64.23, "peak": 88.37, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.0, "energy_joules_est": 26.43, "sample_count": 7, "duration_seconds": 0.979}, "timestamp": "2026-01-17T17:57:47.646066"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1019.806, "latencies_ms": [1019.806], "images_per_second": 0.981, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The luggage is primarily a mix of brown, blue, and green colors. The lighting appears to be natural, possibly from daylight, creating a warm and inviting atmosphere. The luggage appears to be made of sturdy materials, suggesting it is designed for durability and ease of transport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.68, "peak": 34.24, "min": 21.27}, "VIN": {"avg": 65.77, "peak": 99.33, "min": 56.6}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.68, "energy_joules_est": 27.22, "sample_count": 8, "duration_seconds": 1.02}, "timestamp": "2026-01-17T17:57:48.672548"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 603.113, "latencies_ms": [603.113], "images_per_second": 1.658, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A young girl in a pink dress is sitting on a brown couch, holding a white Wii remote and appearing focused.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.52, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 66.26, "peak": 81.91, "min": 60.42}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.52, "energy_joules_est": 18.42, "sample_count": 4, "duration_seconds": 0.604}, "timestamp": "2026-01-17T17:57:49.285507"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1019.228, "latencies_ms": [1019.228], "images_per_second": 0.981, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "girl: 1\ndress: 1\ncouch: 1\nwindow blinds: 2\ncontroller: 1\nwii: 1\ncamera: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.69, "peak": 37.01, "min": 22.45}, "VIN": {"avg": 64.15, "peak": 92.61, "min": 52.92}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.69, "energy_joules_est": 29.25, "sample_count": 7, "duration_seconds": 1.02}, "timestamp": "2026-01-17T17:57:50.311252"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 858.384, "latencies_ms": [858.384], "images_per_second": 1.165, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The girl is positioned near the foreground of the image, standing on a couch. She is holding a Wii remote in her right hand. The couch is situated in the background, extending from left to right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.49, "peak": 34.27, "min": 23.24}, "VIN": {"avg": 64.69, "peak": 95.01, "min": 52.64}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 24.46, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T17:57:51.176003"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 824.509, "latencies_ms": [824.509], "images_per_second": 1.213, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A young girl is sitting on a brown couch, holding a white Wii remote and appearing to play a video game. She is wearing a pink dress and is positioned in front of a window with closed blinds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.42, "peak": 35.04, "min": 22.85}, "VIN": {"avg": 62.84, "peak": 80.11, "min": 54.71}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 23.45, "sample_count": 6, "duration_seconds": 0.825}, "timestamp": "2026-01-17T17:57:52.007559"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1075.79, "latencies_ms": [1075.79], "images_per_second": 0.93, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The girl is wearing a pink dress with brown floral patterns. The lighting in the room is soft and diffused, suggesting natural light from a window. The couch appears to be upholstered in a neutral fabric, and the overall setting appears cozy and comfortable.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.98, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 62.38, "peak": 88.3, "min": 48.95}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.98, "energy_joules_est": 29.04, "sample_count": 8, "duration_seconds": 1.076}, "timestamp": "2026-01-17T17:57:53.092290"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 838.797, "latencies_ms": [838.797], "images_per_second": 1.192, "prompt_tokens": 8, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A sleek glass and metal desk is set up in a room with a window, holding an open laptop, a keyboard, a mouse, a jar, a pair of headphones, and a computer tower.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.17, "peak": 34.27, "min": 22.86}, "VIN": {"avg": 64.27, "peak": 82.93, "min": 56.26}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 23.64, "sample_count": 6, "duration_seconds": 0.839}, "timestamp": "2026-01-17T17:57:53.942119"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1611.444, "latencies_ms": [1611.444], "images_per_second": 0.621, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 1, "output_text": "laptop: 1\nkeyboard: 2\nmouse: 1\nheadphones: 1\nglass jar: 1\ntrash can: 1\ncomputer tower: 1\ndesk: 2\nchair: 1\nfloor: 1\nwindow: 1\nheater: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 24.62, "peak": 35.04, "min": 19.7}, "VIN": {"avg": 61.67, "peak": 80.07, "min": 57.83}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.62, "energy_joules_est": 39.69, "sample_count": 12, "duration_seconds": 1.612}, "timestamp": "2026-01-17T17:57:55.560881"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1119.192, "latencies_ms": [1119.192], "images_per_second": 0.894, "prompt_tokens": 25, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The main objects are positioned in a relatively open space, with a desk, chair, and computer setup in the foreground. The desk is situated near a window, offering natural light. The computer setup is situated further back, near the window and potentially in a corner of the room.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.1, "peak": 33.09, "min": 20.88}, "VIN": {"avg": 62.18, "peak": 78.41, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.1, "energy_joules_est": 29.22, "sample_count": 8, "duration_seconds": 1.12}, "timestamp": "2026-01-17T17:57:56.686698"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 999.606, "latencies_ms": [999.606], "images_per_second": 1.0, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene depicts a home office setup with a desk, laptop, keyboard, mouse, headphones, and a computer tower. A trash can and computer peripherals are also present. The room is illuminated by natural light from a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.01, "peak": 33.89, "min": 21.66}, "VIN": {"avg": 62.94, "peak": 77.24, "min": 55.01}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.01, "energy_joules_est": 27.01, "sample_count": 7, "duration_seconds": 1.0}, "timestamp": "2026-01-17T17:57:57.692878"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 859.652, "latencies_ms": [859.652], "images_per_second": 1.163, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The desk is primarily silver and glass. The chair is gray fabric. The lighting in the room appears to be natural light from a window. The desk and chair appear to be made of glass and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.43, "peak": 34.66, "min": 23.25}, "VIN": {"avg": 61.36, "peak": 71.38, "min": 50.35}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.43, "energy_joules_est": 24.45, "sample_count": 6, "duration_seconds": 0.86}, "timestamp": "2026-01-17T17:57:58.559467"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 745.742, "latencies_ms": [745.742], "images_per_second": 1.341, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A person is enjoying a freshly made vegetable pizza topped with pepperoni, mushrooms, green peppers, and bell peppers, using a knife and fork.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.7, "peak": 35.06, "min": 24.42}, "VIN": {"avg": 66.86, "peak": 86.36, "min": 60.02}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.97, "min": 14.18}}, "power_watts_avg": 29.7, "energy_joules_est": 22.16, "sample_count": 5, "duration_seconds": 0.746}, "timestamp": "2026-01-17T17:57:59.316577"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1241.169, "latencies_ms": [1241.169], "images_per_second": 0.806, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Pizza: 8\nPepperoni: 2\nMushrooms: 4\nGreen peppers: 4\nTomato: 1\nCheese: 1\nSausage: 1\nFork: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.1, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 26.34, "peak": 35.83, "min": 20.48}, "VIN": {"avg": 64.22, "peak": 86.49, "min": 59.41}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 32.7, "sample_count": 9, "duration_seconds": 1.242}, "timestamp": "2026-01-17T17:58:00.564439"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 862.648, "latencies_ms": [862.648], "images_per_second": 1.159, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The pizza is positioned in the foreground of the image, with the person's hand and fork visible in the background. The table and checkered tablecloth are placed in the background, further emphasizing the dining setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.1, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.76, "peak": 33.86, "min": 22.45}, "VIN": {"avg": 63.07, "peak": 71.46, "min": 60.64}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.76, "energy_joules_est": 23.96, "sample_count": 6, "duration_seconds": 0.863}, "timestamp": "2026-01-17T17:58:01.433714"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 878.257, "latencies_ms": [878.257], "images_per_second": 1.139, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "A person is enjoying a freshly made pizza topped with pepperoni, mushrooms, green peppers, and bell peppers. The pizza is served on a white plate on a red and white checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.4, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.76, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 63.42, "peak": 84.18, "min": 54.91}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 25.27, "sample_count": 6, "duration_seconds": 0.879}, "timestamp": "2026-01-17T17:58:02.323328"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1158.726, "latencies_ms": [1158.726], "images_per_second": 0.863, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The pizza is topped with vibrant red sauce, melted cheese, and several green bell peppers. The lighting is warm and inviting, enhancing the colors and textures of the toppings. The pizza appears to be made of traditional ingredients like tomato sauce, cheese, mushrooms, and bell peppers.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.4, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.12, "peak": 34.64, "min": 20.48}, "VIN": {"avg": 61.74, "peak": 80.85, "min": 53.92}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.12, "energy_joules_est": 30.28, "sample_count": 9, "duration_seconds": 1.159}, "timestamp": "2026-01-17T17:58:03.488487"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 705.399, "latencies_ms": [705.399], "images_per_second": 1.418, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A white and red Metropolitan Transit System bus is parked on the side of the road, carrying passengers and displaying its route number.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.94, "peak": 33.88, "min": 24.82}, "VIN": {"avg": 62.93, "peak": 84.21, "min": 53.07}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 21.14, "sample_count": 5, "duration_seconds": 0.706}, "timestamp": "2026-01-17T17:58:04.206331"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1066.494, "latencies_ms": [1066.494], "images_per_second": 0.938, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "bus: 5\ntree: 1\nbuilding: 2\nwindow: 4\nsign: 1\nbaby: 1\nman: 1\ncar: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.21, "peak": 36.62, "min": 21.66}, "VIN": {"avg": 61.3, "peak": 69.64, "min": 56.84}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.21, "energy_joules_est": 30.1, "sample_count": 8, "duration_seconds": 1.067}, "timestamp": "2026-01-17T17:58:05.279219"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 704.78, "latencies_ms": [704.78], "images_per_second": 1.419, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The bus is positioned in the foreground, slightly to the right of the image. The building in the background is further back, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.49, "peak": 34.66, "min": 25.21}, "VIN": {"avg": 67.03, "peak": 85.27, "min": 60.48}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.49, "energy_joules_est": 21.5, "sample_count": 5, "duration_seconds": 0.705}, "timestamp": "2026-01-17T17:58:05.992769"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 832.717, "latencies_ms": [832.717], "images_per_second": 1.201, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A white and red Metropolitan Transit System bus is parked near a tall building, with people visible inside and outside the bus. The scene appears to be in a city environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 30.46, "peak": 36.63, "min": 24.03}, "VIN": {"avg": 62.3, "peak": 81.73, "min": 51.56}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.46, "energy_joules_est": 25.38, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:58:06.833017"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 884.794, "latencies_ms": [884.794], "images_per_second": 1.13, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The bus is primarily white with red accents. The lighting appears to be natural daylight, creating a bright and pleasant atmosphere. The bus appears to be modern in design, constructed from metal and glass.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 30.0, "peak": 35.85, "min": 24.03}, "VIN": {"avg": 63.71, "peak": 83.23, "min": 55.23}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.0, "energy_joules_est": 26.55, "sample_count": 6, "duration_seconds": 0.885}, "timestamp": "2026-01-17T17:58:07.728399"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 399.88, "latencies_ms": [399.88], "images_per_second": 2.501, "prompt_tokens": 8, "response_tokens_est": 16, "n_tiles": 1, "output_text": "A baseball glove and cap rest on the ground, positioned next to each other.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.1, "min": 13.79}, "VDD_GPU": {"avg": 26.39, "peak": 27.56, "min": 25.21}, "VIN": {"avg": 57.03, "peak": 59.27, "min": 54.79}, "VDD_CPU_SOC_MSS": {"avg": 14.37, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 26.39, "energy_joules_est": 10.57, "sample_count": 2, "duration_seconds": 0.4}, "timestamp": "2026-01-17T17:58:08.141567"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 880.32, "latencies_ms": [880.32], "images_per_second": 1.136, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "baseball cap: 1\nbaseball glove: 1\nbaseball: 1\nfence post: 1\nground: 1\nsand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.39}, "VDD_GPU": {"avg": 23.96, "peak": 28.35, "min": 20.88}, "VIN": {"avg": 60.97, "peak": 64.22, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 23.96, "energy_joules_est": 21.1, "sample_count": 6, "duration_seconds": 0.881}, "timestamp": "2026-01-17T17:58:09.028499"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 680.653, "latencies_ms": [680.653], "images_per_second": 1.469, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The baseball glove and cap are positioned close to the ground, close to the right edge of the image. The glove is partially obscured by the cap, lying in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 23.32, "peak": 26.0, "min": 21.27}, "VIN": {"avg": 58.56, "peak": 61.8, "min": 56.21}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.32, "energy_joules_est": 15.88, "sample_count": 5, "duration_seconds": 0.681}, "timestamp": "2026-01-17T17:58:09.715892"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 701.45, "latencies_ms": [701.45], "images_per_second": 1.426, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A baseball glove and cap rest on the ground near a metal pole, likely at a baseball field. The scene suggests a moment of anticipation or preparation for a game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25542.8, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 23.55, "peak": 26.77, "min": 21.27}, "VIN": {"avg": 59.28, "peak": 62.42, "min": 54.33}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.55, "energy_joules_est": 16.53, "sample_count": 5, "duration_seconds": 0.702}, "timestamp": "2026-01-17T17:58:10.424843"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 641.137, "latencies_ms": [641.137], "images_per_second": 1.56, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The baseball cap is navy blue, and the glove is tan and leather-like. The lighting suggests it might be sunny, and the ground appears to be dirt.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 23.93, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 56.41, "peak": 62.51, "min": 52.27}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.93, "energy_joules_est": 15.35, "sample_count": 4, "duration_seconds": 0.641}, "timestamp": "2026-01-17T17:58:11.072767"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 679.794, "latencies_ms": [679.794], "images_per_second": 1.471, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A surfer in a red shirt skillfully rides a white and green striped surfboard on a powerful blue-green wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.07, "peak": 33.88, "min": 24.43}, "VIN": {"avg": 67.17, "peak": 92.5, "min": 55.08}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.07, "energy_joules_est": 19.77, "sample_count": 5, "duration_seconds": 0.68}, "timestamp": "2026-01-17T17:58:11.764351"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 878.485, "latencies_ms": [878.485], "images_per_second": 1.138, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "surfboard: 1\nperson: 1\nwaves: 2\nwater: 1\nsky: 1\nclouds: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.54, "peak": 36.63, "min": 23.64}, "VIN": {"avg": 63.41, "peak": 77.09, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 25.96, "sample_count": 6, "duration_seconds": 0.879}, "timestamp": "2026-01-17T17:58:12.649588"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 699.883, "latencies_ms": [699.883], "images_per_second": 1.429, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The surfer is positioned in the foreground of the image, riding a wave. The wave is further in the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.84, "peak": 35.44, "min": 24.4}, "VIN": {"avg": 63.74, "peak": 87.27, "min": 52.41}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.84, "energy_joules_est": 20.89, "sample_count": 5, "duration_seconds": 0.7}, "timestamp": "2026-01-17T17:58:13.355101"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1176.116, "latencies_ms": [1176.116], "images_per_second": 0.85, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "A surfer in a red rash guard skillfully rides a wave on a yellow and white surfboard. The ocean is a vibrant turquoise, contrasting with the white foam created by the wave. The surfer's focused posture suggests concentration and balance as they navigate the powerful force of nature.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.8, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.69, "peak": 36.23, "min": 20.88}, "VIN": {"avg": 63.46, "peak": 86.7, "min": 54.35}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.69, "energy_joules_est": 31.4, "sample_count": 9, "duration_seconds": 1.176}, "timestamp": "2026-01-17T17:58:14.539376"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 822.921, "latencies_ms": [822.921], "images_per_second": 1.215, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The water is a vibrant turquoise color, creating a striking contrast with the white and green surfboard. The lighting suggests a sunny day with bright, natural light illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.89, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 66.97, "peak": 86.67, "min": 60.3}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.89, "energy_joules_est": 22.96, "sample_count": 6, "duration_seconds": 0.823}, "timestamp": "2026-01-17T17:58:15.369250"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 446.344, "latencies_ms": [446.344], "images_per_second": 2.24, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "The bathroom features a white toilet, a granite countertop sink, and a mirror above the sink.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 25.48, "peak": 27.57, "min": 23.64}, "VIN": {"avg": 59.74, "peak": 62.13, "min": 55.79}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.57, "min": 14.17}}, "power_watts_avg": 25.48, "energy_joules_est": 11.38, "sample_count": 3, "duration_seconds": 0.447}, "timestamp": "2026-01-17T17:58:15.824588"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1208.287, "latencies_ms": [1208.287], "images_per_second": 0.828, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "toilet: 1\nsink: 1\nmirror: 1\nleaf: 1\nfaucet: 1\nsoap dispenser: 1\ntoilet brush: 1\ntoilet paper: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.39}, "VDD_GPU": {"avg": 22.59, "peak": 27.97, "min": 19.7}, "VIN": {"avg": 62.75, "peak": 64.42, "min": 59.23}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 13.78}}, "power_watts_avg": 22.59, "energy_joules_est": 27.31, "sample_count": 9, "duration_seconds": 1.209}, "timestamp": "2026-01-17T17:58:17.039922"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 632.902, "latencies_ms": [632.902], "images_per_second": 1.58, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The toilet is positioned to the left of the sink, which is situated in the background. The toilet and sink are located close together, creating a functional and compact bathroom layout.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.53, "peak": 25.59, "min": 21.66}, "VIN": {"avg": 58.99, "peak": 61.15, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.53, "energy_joules_est": 14.9, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T17:58:17.679291"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 732.249, "latencies_ms": [732.249], "images_per_second": 1.366, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The black and white image shows a bathroom setting with a toilet, sink, and tiled walls. A leaf-shaped object rests on the countertop, and a roll of toilet paper is visible on the floor.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.63, "peak": 26.79, "min": 21.27}, "VIN": {"avg": 60.41, "peak": 60.9, "min": 59.67}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 17.31, "sample_count": 5, "duration_seconds": 0.733}, "timestamp": "2026-01-17T17:58:18.417845"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 730.066, "latencies_ms": [730.066], "images_per_second": 1.37, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The bathroom features a black and white color scheme, illuminated by natural light coming in from the window. The walls are tiled with stone-like tiles, and the floor is dark and shiny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.64, "peak": 26.79, "min": 21.28}, "VIN": {"avg": 59.88, "peak": 62.42, "min": 57.32}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.64, "energy_joules_est": 17.27, "sample_count": 5, "duration_seconds": 0.731}, "timestamp": "2026-01-17T17:58:19.155771"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 750.625, "latencies_ms": [750.625], "images_per_second": 1.332, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A white clock tower with a green dome and clock face is situated atop a red-tiled roof, partially obscured by a decorative white structure.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.83, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 68.51, "peak": 96.07, "min": 60.22}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.83, "energy_joules_est": 21.65, "sample_count": 5, "duration_seconds": 0.751}, "timestamp": "2026-01-17T17:58:19.918281"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1049.657, "latencies_ms": [1049.657], "images_per_second": 0.953, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "gazebo: 2\nbell tower: 1\nclock face: 1\nroof tiles: 10\nbell: 1\nlight fixtures: 4\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.12, "peak": 35.44, "min": 21.28}, "VIN": {"avg": 63.52, "peak": 90.45, "min": 55.33}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.12, "energy_joules_est": 28.49, "sample_count": 8, "duration_seconds": 1.05}, "timestamp": "2026-01-17T17:58:20.974680"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1151.135, "latencies_ms": [1151.135], "images_per_second": 0.869, "prompt_tokens": 25, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The white clock tower is positioned in the foreground, slightly to the right of the gazebo. The gazebo is situated in the background, extending across the entire width of the image. The clock tower and gazebo are separated by a distance that suggests a clear separation between the foreground and the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.5, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.29, "peak": 33.86, "min": 20.88}, "VIN": {"avg": 61.61, "peak": 76.28, "min": 56.93}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.29, "energy_joules_est": 30.28, "sample_count": 8, "duration_seconds": 1.152}, "timestamp": "2026-01-17T17:58:22.132872"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 866.461, "latencies_ms": [866.461], "images_per_second": 1.154, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene is set in a historic area, featuring a white clock tower with a green dome, situated atop a tiled roof. The tower is surrounded by ornate architectural elements and stands against a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.5, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.5, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.02, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 63.66, "peak": 82.44, "min": 51.5}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.02, "energy_joules_est": 24.29, "sample_count": 6, "duration_seconds": 0.867}, "timestamp": "2026-01-17T17:58:23.005943"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 926.706, "latencies_ms": [926.706], "images_per_second": 1.079, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The building features a distinctive blue dome roof and terracotta tiles, creating a striking contrast against the clear, bright blue sky. The scene is illuminated by sunlight, giving the structure a warm, inviting glow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.5, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.85, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 62.43, "peak": 72.35, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 25.82, "sample_count": 7, "duration_seconds": 0.927}, "timestamp": "2026-01-17T17:58:23.939921"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 531.982, "latencies_ms": [531.982], "images_per_second": 1.88, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A group of elephants, including one in the foreground, is seen walking along a dirt path surrounded by trees and bushes.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 32.3, "peak": 35.04, "min": 29.15}, "VIN": {"avg": 69.17, "peak": 92.59, "min": 54.59}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 32.3, "energy_joules_est": 17.2, "sample_count": 3, "duration_seconds": 0.533}, "timestamp": "2026-01-17T17:58:24.486830"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 975.897, "latencies_ms": [975.897], "images_per_second": 1.025, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "elephant: 5\ntusk: 2\ntrunk: 1\nears: 4\nbody: 6\nhair: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 28.93, "peak": 37.42, "min": 22.46}, "VIN": {"avg": 64.06, "peak": 82.58, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 28.93, "energy_joules_est": 28.25, "sample_count": 7, "duration_seconds": 0.976}, "timestamp": "2026-01-17T17:58:25.469731"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 888.66, "latencies_ms": [888.66], "images_per_second": 1.125, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The elephant in the foreground is positioned to the left of the image, drawing the viewer's attention to its presence. The elephants in the background are slightly out of focus, creating a sense of depth and distance between the foreground and the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.68, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 66.01, "peak": 87.9, "min": 60.47}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.68, "energy_joules_est": 25.49, "sample_count": 6, "duration_seconds": 0.889}, "timestamp": "2026-01-17T17:58:26.364496"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 716.595, "latencies_ms": [716.595], "images_per_second": 1.395, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A group of elephants is gathered in a natural setting, possibly a savanna or forest. The elephants are walking along a dirt path, surrounded by trees and vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.2, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.86, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 65.78, "peak": 91.04, "min": 55.99}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 21.42, "sample_count": 5, "duration_seconds": 0.717}, "timestamp": "2026-01-17T17:58:27.087975"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 929.222, "latencies_ms": [929.222], "images_per_second": 1.076, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The elephants are primarily gray in color. The lighting suggests an overcast day, with diffused light filtering through the trees. The scene appears to be set in a natural environment with dirt paths and dense vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.53, "peak": 36.23, "min": 22.46}, "VIN": {"avg": 66.8, "peak": 93.35, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.53, "energy_joules_est": 26.52, "sample_count": 7, "duration_seconds": 0.93}, "timestamp": "2026-01-17T17:58:28.024340"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 615.523, "latencies_ms": [615.523], "images_per_second": 1.625, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "The open refrigerator door reveals an empty interior with four crisper drawers and a carton of eggs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 31.12, "peak": 35.06, "min": 26.79}, "VIN": {"avg": 67.73, "peak": 84.71, "min": 56.52}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 19.18, "sample_count": 4, "duration_seconds": 0.616}, "timestamp": "2026-01-17T17:58:28.652565"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1240.376, "latencies_ms": [1240.376], "images_per_second": 0.806, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "Egg carton: 2\nOrange juice bottle: 1\nIce maker: 1\nFridge door: 3\nPlastic food containers: 2\nRefrigerator drawers: 4\nFloor: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.23, "peak": 37.42, "min": 20.88}, "VIN": {"avg": 63.5, "peak": 82.3, "min": 59.45}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 33.79, "sample_count": 9, "duration_seconds": 1.241}, "timestamp": "2026-01-17T17:58:29.899219"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 926.458, "latencies_ms": [926.458], "images_per_second": 1.079, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The main object, the refrigerator, is positioned in the foreground, occupying a significant portion of the image. The eggs are located on the top shelf of the refrigerator, near the light. The floor is visible in the background, providing context to the overall space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.24, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 64.57, "peak": 91.94, "min": 53.11}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.24, "energy_joules_est": 25.25, "sample_count": 7, "duration_seconds": 0.927}, "timestamp": "2026-01-17T17:58:30.832005"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 944.527, "latencies_ms": [944.527], "images_per_second": 1.059, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts an empty refrigerator in a kitchen, illuminated by a light source inside. The refrigerator is stocked with various items, including a carton of eggs, a bottle of liquid, and a few other unidentified items.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.68, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 62.25, "peak": 78.19, "min": 58.66}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.68, "energy_joules_est": 26.16, "sample_count": 7, "duration_seconds": 0.945}, "timestamp": "2026-01-17T17:58:31.783132"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 825.777, "latencies_ms": [825.777], "images_per_second": 1.211, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The refrigerator is white and appears clean and empty. The lighting inside the refrigerator is bright, creating a well-lit interior. The refrigerator's shelves and compartments are made of clear plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.36, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 61.87, "peak": 72.49, "min": 55.17}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 23.43, "sample_count": 6, "duration_seconds": 0.826}, "timestamp": "2026-01-17T17:58:32.615798"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 542.899, "latencies_ms": [542.899], "images_per_second": 1.842, "prompt_tokens": 8, "response_tokens_est": 15, "n_tiles": 1, "output_text": "A bunch of ripe yellow bananas with small stickers rests on a blue surface.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 32.1, "peak": 35.45, "min": 27.56}, "VIN": {"avg": 68.26, "peak": 92.65, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.1, "energy_joules_est": 17.44, "sample_count": 4, "duration_seconds": 0.543}, "timestamp": "2026-01-17T17:58:33.169932"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 706.568, "latencies_ms": [706.568], "images_per_second": 1.415, "prompt_tokens": 21, "response_tokens_est": 20, "n_tiles": 1, "output_text": "bananas: 8\nstickers: 2\nmetal tray: 1\npurple background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 33.0, "peak": 38.98, "min": 26.79}, "VIN": {"avg": 66.35, "peak": 81.12, "min": 60.9}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.0, "energy_joules_est": 23.33, "sample_count": 5, "duration_seconds": 0.707}, "timestamp": "2026-01-17T17:58:33.883109"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 710.818, "latencies_ms": [710.818], "images_per_second": 1.407, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, partially overlapping. The background is blurred and out of focus, drawing attention to the bananas in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 32.05, "peak": 37.8, "min": 26.0}, "VIN": {"avg": 64.12, "peak": 77.53, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.05, "energy_joules_est": 22.8, "sample_count": 5, "duration_seconds": 0.711}, "timestamp": "2026-01-17T17:58:34.599599"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 943.096, "latencies_ms": [943.096], "images_per_second": 1.06, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a close-up view of a bunch of ripe bananas resting on a metallic surface, possibly a tray or stand. The bananas are yellow and appear fresh. The setting suggests a market or grocery store environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 29.65, "peak": 37.82, "min": 22.85}, "VIN": {"avg": 63.92, "peak": 81.94, "min": 58.03}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.65, "energy_joules_est": 27.97, "sample_count": 7, "duration_seconds": 0.943}, "timestamp": "2026-01-17T17:58:35.548892"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 974.337, "latencies_ms": [974.337], "images_per_second": 1.026, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The bananas are predominantly yellow, indicating they are ripe. The lighting appears to be soft and diffused, possibly from a diffused light source. The bananas appear to be resting on a smooth, reflective surface, possibly a tray or plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.75, "peak": 35.44, "min": 22.45}, "VIN": {"avg": 61.07, "peak": 77.0, "min": 54.91}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 28.03, "sample_count": 7, "duration_seconds": 0.975}, "timestamp": "2026-01-17T17:58:36.529986"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 780.826, "latencies_ms": [780.826], "images_per_second": 1.281, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A row of tall, shiny orange fire hydrants with gold knobs is lined up on a city sidewalk, contrasting with the surrounding urban landscape.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.57, "peak": 35.04, "min": 25.22}, "VIN": {"avg": 67.15, "peak": 94.41, "min": 59.18}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.57, "energy_joules_est": 23.88, "sample_count": 5, "duration_seconds": 0.781}, "timestamp": "2026-01-17T17:58:37.324738"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 949.293, "latencies_ms": [949.293], "images_per_second": 1.053, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "fire hydrant: 4\npipes: 4\nbuildings: 5\ntrees: 2\nsidewalk: 6\nstreet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.92, "peak": 36.24, "min": 22.45}, "VIN": {"avg": 62.71, "peak": 70.22, "min": 59.21}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.92, "energy_joules_est": 27.47, "sample_count": 7, "duration_seconds": 0.95}, "timestamp": "2026-01-17T17:58:38.280478"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 824.592, "latencies_ms": [824.592], "images_per_second": 1.213, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The fire hydrants are positioned in the foreground, slightly to the right of the center. The cityscape, including skyscrapers and buildings, stretches out in the background, further away from the hydrants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.74, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 61.97, "peak": 80.84, "min": 53.87}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.74, "energy_joules_est": 24.53, "sample_count": 6, "duration_seconds": 0.825}, "timestamp": "2026-01-17T17:58:39.111684"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1030.184, "latencies_ms": [1030.184], "images_per_second": 0.971, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene depicts a city street with tall buildings in the background and a row of copper-colored fire hydrants lined up in the foreground. The fire hydrants are positioned in a somewhat orderly manner, contrasting with the more casual urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.9, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.69, "peak": 35.83, "min": 22.44}, "VIN": {"avg": 62.12, "peak": 75.69, "min": 54.34}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 29.57, "sample_count": 7, "duration_seconds": 1.031}, "timestamp": "2026-01-17T17:58:40.148564"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 926.696, "latencies_ms": [926.696], "images_per_second": 1.079, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The fire hydrants are copper-colored and stand tall in the plaza. The lighting is soft and diffused, creating a calm atmosphere. The materials appear to be metal, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.9, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.41, "peak": 34.65, "min": 22.45}, "VIN": {"avg": 62.92, "peak": 79.16, "min": 54.31}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.41, "energy_joules_est": 26.33, "sample_count": 7, "duration_seconds": 0.927}, "timestamp": "2026-01-17T17:58:41.081415"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 699.644, "latencies_ms": [699.644], "images_per_second": 1.429, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A jockey wearing a white helmet and colorful shirt is guiding a small brown horse pulling a red cart down a dirt track.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.78, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 64.37, "peak": 89.6, "min": 54.59}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 14.97, "min": 14.57}}, "power_watts_avg": 29.78, "energy_joules_est": 20.85, "sample_count": 5, "duration_seconds": 0.7}, "timestamp": "2026-01-17T17:58:41.795406"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 947.436, "latencies_ms": [947.436], "images_per_second": 1.055, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "horse: 1\nperson: 1\ncart: 1\nnumber: 1\nroad: 1\ngrass: 1\nfence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.08, "peak": 36.24, "min": 22.06}, "VIN": {"avg": 62.66, "peak": 77.54, "min": 55.77}, "VDD_CPU_SOC_MSS": {"avg": 14.74, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 28.08, "energy_joules_est": 26.61, "sample_count": 7, "duration_seconds": 0.948}, "timestamp": "2026-01-17T17:58:42.749409"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 835.327, "latencies_ms": [835.327], "images_per_second": 1.197, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The horse and driver are positioned in the foreground of the image, with the horse and cart moving towards the background. The track is situated near the background, extending beyond the immediate focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.68, "peak": 34.65, "min": 23.25}, "VIN": {"avg": 63.44, "peak": 82.56, "min": 57.41}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.68, "energy_joules_est": 23.97, "sample_count": 6, "duration_seconds": 0.836}, "timestamp": "2026-01-17T17:58:43.591542"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 956.527, "latencies_ms": [956.527], "images_per_second": 1.045, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A jockey and horse are racing on a dirt track. The jockey is wearing a helmet and riding a red carriage with a number 8 displayed. The horse is brown and pulling the carriage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 61.97, "peak": 81.12, "min": 53.54}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 26.6, "sample_count": 7, "duration_seconds": 0.957}, "timestamp": "2026-01-17T17:58:44.554664"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 838.412, "latencies_ms": [838.412], "images_per_second": 1.193, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The horse and driver are brown. The scene is well-lit, likely under bright sunlight. The horse and driver appear to be wearing protective gear. The track appears to be dry and dusty.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.29, "peak": 33.88, "min": 23.23}, "VIN": {"avg": 63.09, "peak": 78.27, "min": 57.98}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 23.73, "sample_count": 6, "duration_seconds": 0.839}, "timestamp": "2026-01-17T17:58:45.399489"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 461.988, "latencies_ms": [461.988], "images_per_second": 2.165, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A brown dog stands alertly on a gray deck in a backyard, gazing at the camera.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 25.46, "peak": 27.56, "min": 23.62}, "VIN": {"avg": 59.16, "peak": 60.72, "min": 56.62}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 25.46, "energy_joules_est": 11.77, "sample_count": 3, "duration_seconds": 0.462}, "timestamp": "2026-01-17T17:58:45.873047"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 938.902, "latencies_ms": [938.902], "images_per_second": 1.065, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "dog: 1\nfence: 1\ntree: 1\nlemon: 2\ngrass: 1\nwooden bench: 1\npaved area: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 23.23, "peak": 27.95, "min": 20.48}, "VIN": {"avg": 61.01, "peak": 63.94, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.23, "energy_joules_est": 21.82, "sample_count": 7, "duration_seconds": 0.939}, "timestamp": "2026-01-17T17:58:46.818473"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 981.105, "latencies_ms": [981.105], "images_per_second": 1.019, "prompt_tokens": 25, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The dog is positioned in the foreground, slightly to the right of the image. The fence and tree are in the background, extending from the left edge to the right edge. The dog is situated on a raised surface, which appears to be a deck or patio, placed between the dog and the tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.34, "peak": 26.0, "min": 20.09}, "VIN": {"avg": 58.62, "peak": 63.26, "min": 54.91}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.34, "energy_joules_est": 21.93, "sample_count": 7, "duration_seconds": 0.981}, "timestamp": "2026-01-17T17:58:47.805748"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 763.862, "latencies_ms": [763.862], "images_per_second": 1.309, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A brown dog, possibly a Labrador Retriever mix, stands on a gray deck in a backyard setting.  The dog is positioned in front of a small citrus tree with several ripe oranges.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.01, "peak": 26.0, "min": 20.89}, "VIN": {"avg": 59.81, "peak": 62.64, "min": 54.37}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.01, "energy_joules_est": 17.59, "sample_count": 5, "duration_seconds": 0.764}, "timestamp": "2026-01-17T17:58:48.575888"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 854.59, "latencies_ms": [854.59], "images_per_second": 1.17, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The dog is light brown with darker brown markings. The lighting is bright, likely from sunlight, and the dog is standing on a gray surface, possibly a deck or patio. The dog appears to be in a backyard setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 22.66, "peak": 26.01, "min": 20.49}, "VIN": {"avg": 59.36, "peak": 63.22, "min": 56.31}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 22.66, "energy_joules_est": 19.37, "sample_count": 6, "duration_seconds": 0.855}, "timestamp": "2026-01-17T17:58:49.436090"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 879.803, "latencies_ms": [879.803], "images_per_second": 1.137, "prompt_tokens": 8, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A person wearing red pants and blue and yellow shoes stands on a wooden bench with a handwritten sign that reads \"WEINER, YOU'VE GOT TO BE SITTING ME!!\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.76, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 64.92, "peak": 85.22, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 27.76, "energy_joules_est": 24.44, "sample_count": 6, "duration_seconds": 0.88}, "timestamp": "2026-01-17T17:58:50.332051"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1042.408, "latencies_ms": [1042.408], "images_per_second": 0.959, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "Bench: 2\nSign: 1\nPerson's legs: 2\nShoes: 2\nBricks: 6\nWood: 6\nGround: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.84, "peak": 34.66, "min": 21.27}, "VIN": {"avg": 63.32, "peak": 83.07, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.84, "energy_joules_est": 27.99, "sample_count": 8, "duration_seconds": 1.043}, "timestamp": "2026-01-17T17:58:51.380845"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 771.265, "latencies_ms": [771.265], "images_per_second": 1.297, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The wooden bench is positioned in the foreground, close to the person's feet. The person's feet are placed on the bench's surface. The background features a concrete wall with graffiti.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.7, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.38, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 64.7, "peak": 82.96, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.38, "energy_joules_est": 22.67, "sample_count": 5, "duration_seconds": 0.772}, "timestamp": "2026-01-17T17:58:52.158839"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1061.642, "latencies_ms": [1061.642], "images_per_second": 0.942, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 1, "output_text": "A person is standing on a wooden bench with a sign that reads \"WEINER. You've got to be sittin' me!!!\" The setting appears to be a public space, possibly a park or outdoor area, with a brick pavement and a concrete wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.7, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.18, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 63.75, "peak": 77.13, "min": 60.18}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 28.86, "sample_count": 8, "duration_seconds": 1.062}, "timestamp": "2026-01-17T17:58:53.226979"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 766.907, "latencies_ms": [766.907], "images_per_second": 1.304, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The bench is made of wood and has a light brown color. The lighting in the image suggests it was taken during daylight hours. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.2, "ram_available_mb": 100226.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.46, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 67.03, "peak": 97.98, "min": 55.31}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.46, "energy_joules_est": 22.61, "sample_count": 5, "duration_seconds": 0.767}, "timestamp": "2026-01-17T17:58:54.004438"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 824.548, "latencies_ms": [824.548], "images_per_second": 1.213, "prompt_tokens": 8, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The living room features a red sofa, a wooden coffee table with tea and pastries, a wooden dresser, a flat-screen TV, and lamps, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25546.1, "ram_available_mb": 100226.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.82, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 65.34, "peak": 82.73, "min": 61.42}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 23.79, "sample_count": 6, "duration_seconds": 0.825}, "timestamp": "2026-01-17T17:58:54.843003"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1274.972, "latencies_ms": [1274.972], "images_per_second": 0.784, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "sofa: 2\ntable: 1\nchairs: 2\nlamp: 2\ntea set: 1\ntelevision: 1\ndresser: 1\npicture frame: 1\ncarpet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.48, "peak": 35.44, "min": 20.88}, "VIN": {"avg": 63.88, "peak": 83.34, "min": 56.55}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.48, "energy_joules_est": 33.77, "sample_count": 9, "duration_seconds": 1.275}, "timestamp": "2026-01-17T17:58:56.127297"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 941.445, "latencies_ms": [941.445], "images_per_second": 1.062, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The orange couch is positioned to the left of the dining table and chairs. The dining table and chairs are situated in the background, slightly further away than the couch. The television is positioned in the background, near the right edge of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.07, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 65.68, "peak": 86.81, "min": 59.67}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.07, "energy_joules_est": 25.49, "sample_count": 7, "duration_seconds": 0.942}, "timestamp": "2026-01-17T17:58:57.076447"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1379.94, "latencies_ms": [1379.94], "images_per_second": 0.725, "prompt_tokens": 19, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The scene depicts a cozy hotel room with a comfortable living area and a dining area. The living area features a red sofa, a wooden coffee table, and a small side table with a lamp and flowers. The dining area has a round table set for tea or coffee, surrounded by chairs. The room is well-lit by natural light coming through a window with curtains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 25.41, "peak": 34.26, "min": 20.1}, "VIN": {"avg": 62.93, "peak": 82.55, "min": 57.38}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.41, "energy_joules_est": 35.07, "sample_count": 10, "duration_seconds": 1.38}, "timestamp": "2026-01-17T17:58:58.462849"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1174.983, "latencies_ms": [1174.983], "images_per_second": 0.851, "prompt_tokens": 18, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The room features a warm color scheme with orange walls and brown curtains. The lighting is soft and warm, creating a cozy atmosphere. The furniture includes a red sofa, a round table with chairs, and a wooden dresser. A large window provides natural light, enhancing the overall ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.3}, "VDD_GPU": {"avg": 26.34, "peak": 33.88, "min": 21.27}, "VIN": {"avg": 64.71, "peak": 87.54, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 30.97, "sample_count": 8, "duration_seconds": 1.176}, "timestamp": "2026-01-17T17:58:59.644698"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 830.329, "latencies_ms": [830.329], "images_per_second": 1.204, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A black frying pan is filled with a vibrant medley of vegetables, including broccoli, carrots, bell peppers, and cubed meat, all coated in a light brown sauce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.3}, "VDD_GPU": {"avg": 27.76, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 68.05, "peak": 99.53, "min": 60.33}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.76, "energy_joules_est": 23.07, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T17:59:00.486983"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1154.327, "latencies_ms": [1154.327], "images_per_second": 0.866, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "broccoli: 8\ncarrots: 8\npeppers: 8\nham: 8\nzucchini: 2\npotatoes: 2\nsalt: 1\nspoon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.66, "peak": 35.45, "min": 20.88}, "VIN": {"avg": 63.63, "peak": 90.82, "min": 55.96}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.66, "energy_joules_est": 30.79, "sample_count": 9, "duration_seconds": 1.155}, "timestamp": "2026-01-17T17:59:01.648698"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 594.672, "latencies_ms": [594.672], "images_per_second": 1.682, "prompt_tokens": 25, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The broccoli, carrots, and meat are positioned in the foreground of the image, with the pan and spoon placed in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.32, "peak": 33.85, "min": 26.38}, "VIN": {"avg": 69.29, "peak": 96.77, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.32, "energy_joules_est": 18.04, "sample_count": 4, "duration_seconds": 0.595}, "timestamp": "2026-01-17T17:59:02.249617"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 948.164, "latencies_ms": [948.164], "images_per_second": 1.055, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "A skillet filled with a colorful mix of vegetables, including broccoli, carrots, and meat, is being prepared on a stovetop. A serving spoon rests in the pan, indicating the meal is ready to be served.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.76, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 62.02, "peak": 80.87, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 27.29, "sample_count": 7, "duration_seconds": 0.949}, "timestamp": "2026-01-17T17:59:03.205337"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1026.167, "latencies_ms": [1026.167], "images_per_second": 0.975, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The vegetables are vibrantly colored, showcasing a range of hues from green to orange. The lighting is bright, illuminating the dish and highlighting the textures of the ingredients. The pan appears to be made of metal, and the vegetables appear to be cooked together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.46, "peak": 34.65, "min": 22.07}, "VIN": {"avg": 64.77, "peak": 91.47, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 28.19, "sample_count": 7, "duration_seconds": 1.027}, "timestamp": "2026-01-17T17:59:04.238121"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 629.992, "latencies_ms": [629.992], "images_per_second": 1.587, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "Three hot dogs, each topped with yellow mustard, are arranged on a dark plate on a countertop, accompanied by a magazine.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.2}, "VDD_GPU": {"avg": 30.33, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 65.41, "peak": 76.59, "min": 61.29}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 19.13, "sample_count": 4, "duration_seconds": 0.631}, "timestamp": "2026-01-17T17:59:04.880594"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 890.858, "latencies_ms": [890.858], "images_per_second": 1.123, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "hot dog: 3\nbun: 2\nmustard: 2\nplate: 1\ntable: 1\nmagazine: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.34, "peak": 35.83, "min": 23.64}, "VIN": {"avg": 66.56, "peak": 98.93, "min": 57.32}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.34, "energy_joules_est": 26.16, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T17:59:05.783517"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 790.769, "latencies_ms": [790.769], "images_per_second": 1.265, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The main objects are positioned close together, with the hot dogs placed in the foreground and the magazine in the background. The hot dogs are situated on the plate, which occupies the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.74, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 61.43, "peak": 78.45, "min": 54.44}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.74, "energy_joules_est": 22.73, "sample_count": 6, "duration_seconds": 0.791}, "timestamp": "2026-01-17T17:59:06.580550"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 630.205, "latencies_ms": [630.205], "images_per_second": 1.587, "prompt_tokens": 19, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Three hot dogs topped with mustard are arranged on a dark plate on a countertop. A magazine is visible in the background.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.2}, "VDD_GPU": {"avg": 31.7, "peak": 36.24, "min": 27.17}, "VIN": {"avg": 69.48, "peak": 95.77, "min": 54.53}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.7, "energy_joules_est": 20.01, "sample_count": 4, "duration_seconds": 0.631}, "timestamp": "2026-01-17T17:59:07.218274"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 832.851, "latencies_ms": [832.851], "images_per_second": 1.201, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The hot dogs are topped with yellow mustard. The plate appears to be dark brown or black. The lighting is bright, likely from overhead lighting. The hot dogs appear to be made of beef.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.86, "peak": 37.39, "min": 23.64}, "VIN": {"avg": 64.2, "peak": 77.11, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 24.88, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T17:59:08.058136"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 574.276, "latencies_ms": [574.276], "images_per_second": 1.741, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A group of people are enjoying a sunny day at the beach, swimming and playing in the water under a green umbrella.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 31.11, "peak": 35.44, "min": 26.39}, "VIN": {"avg": 67.73, "peak": 81.82, "min": 62.21}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.11, "energy_joules_est": 17.88, "sample_count": 4, "duration_seconds": 0.575}, "timestamp": "2026-01-17T17:59:08.644509"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 907.806, "latencies_ms": [907.806], "images_per_second": 1.102, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "Umbrella: 1\nBeach chairs: 2\nSand: 1\nWater: 4\nPeople: 4\nWaves: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 30.27, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 64.0, "peak": 89.14, "min": 53.82}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.27, "energy_joules_est": 27.49, "sample_count": 6, "duration_seconds": 0.908}, "timestamp": "2026-01-17T17:59:09.562923"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 699.229, "latencies_ms": [699.229], "images_per_second": 1.43, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the beach chair and umbrella situated near the middle ground. The water extends to the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.17, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 67.69, "peak": 92.15, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.17, "energy_joules_est": 21.12, "sample_count": 5, "duration_seconds": 0.7}, "timestamp": "2026-01-17T17:59:10.269242"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 950.632, "latencies_ms": [950.632], "images_per_second": 1.052, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "A group of people are enjoying a sunny day at the beach, swimming and playing in the ocean. Two beach chairs with colorful umbrellas are set up on the sandy shore, providing shade and seating for the beachgoers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.47, "peak": 36.24, "min": 22.44}, "VIN": {"avg": 65.34, "peak": 94.27, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.47, "energy_joules_est": 27.08, "sample_count": 7, "duration_seconds": 0.951}, "timestamp": "2026-01-17T17:59:11.226482"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1029.397, "latencies_ms": [1029.397], "images_per_second": 0.971, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The beach is sandy and appears damp, likely due to recent rain or ocean spray. The beach umbrella is green and vibrant, providing shade for the beachgoers. The lighting suggests a sunny day, with bright sunlight illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.5, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.79, "peak": 35.04, "min": 22.05}, "VIN": {"avg": 63.91, "peak": 82.54, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.79, "energy_joules_est": 28.63, "sample_count": 7, "duration_seconds": 1.03}, "timestamp": "2026-01-17T17:59:12.262842"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 732.333, "latencies_ms": [732.333], "images_per_second": 1.365, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The vintage kitchen features a wooden table with a white bowl, various kitchenware, and a white refrigerator, all set against a green and white floral wallpaper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.3, "peak": 33.86, "min": 24.82}, "VIN": {"avg": 63.64, "peak": 76.37, "min": 59.59}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 21.47, "sample_count": 5, "duration_seconds": 0.733}, "timestamp": "2026-01-17T17:59:13.007916"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1571.05, "latencies_ms": [1571.05], "images_per_second": 0.637, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 1, "output_text": "kitchen: 6\ntable: 1\ncabinets: 4\nrefrigerator: 1\nsink: 1\nfaucet: 1\npan: 1\nwooden chair: 1\nwooden bucket: 1\nfan: 1\nwallpaper: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.0}, "VDD_GPU": {"avg": 25.11, "peak": 36.24, "min": 19.7}, "VIN": {"avg": 61.86, "peak": 75.42, "min": 55.24}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.11, "energy_joules_est": 39.46, "sample_count": 12, "duration_seconds": 1.571}, "timestamp": "2026-01-17T17:59:14.585190"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 813.965, "latencies_ms": [813.965], "images_per_second": 1.229, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The kitchen is positioned in the foreground, with the dining table, chairs, and appliances located in the background. The kitchen is situated next to a refrigerator and sink, further back in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.57, "peak": 32.7, "min": 22.84}, "VIN": {"avg": 64.04, "peak": 76.41, "min": 59.07}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.57, "energy_joules_est": 22.45, "sample_count": 6, "duration_seconds": 0.814}, "timestamp": "2026-01-17T17:59:15.406851"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 773.218, "latencies_ms": [773.218], "images_per_second": 1.293, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The scene depicts a vintage kitchen exhibit, featuring antique appliances and furniture. The kitchen is decorated with green and white wallpaper and showcases a retro aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.1, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.85, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 65.17, "peak": 81.37, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.85, "energy_joules_est": 23.09, "sample_count": 5, "duration_seconds": 0.774}, "timestamp": "2026-01-17T17:59:16.188384"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1009.239, "latencies_ms": [1009.239], "images_per_second": 0.991, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The kitchen features green and white color schemes, creating a vintage aesthetic. The lighting is soft and diffused, enhancing the overall ambiance. The materials include wood, metal, and fabric, contributing to the rustic feel of the space.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.02, "peak": 35.83, "min": 22.06}, "VIN": {"avg": 63.6, "peak": 82.36, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.02, "energy_joules_est": 28.29, "sample_count": 7, "duration_seconds": 1.01}, "timestamp": "2026-01-17T17:59:17.204375"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 597.33, "latencies_ms": [597.33], "images_per_second": 1.674, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A black and white dog is playing with a frisbee in a wooded area, surrounded by fallen leaves and tall trees.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.53, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 71.21, "peak": 96.05, "min": 61.08}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.53, "energy_joules_est": 18.25, "sample_count": 4, "duration_seconds": 0.598}, "timestamp": "2026-01-17T17:59:17.814078"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 879.747, "latencies_ms": [879.747], "images_per_second": 1.137, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "tree: 1\nfrisbee: 1\nground: 1\nleaves: 4\ndog: 1\nshadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 29.67, "peak": 37.01, "min": 23.64}, "VIN": {"avg": 63.94, "peak": 87.51, "min": 56.57}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 29.67, "energy_joules_est": 26.12, "sample_count": 6, "duration_seconds": 0.88}, "timestamp": "2026-01-17T17:59:18.700479"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 865.452, "latencies_ms": [865.452], "images_per_second": 1.155, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The dog is positioned to the left of the image, close to the foreground. The tree trunk occupies the background, extending from the left edge towards the center. The dog appears to be interacting with the ground near the tree.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.28, "peak": 35.85, "min": 23.64}, "VIN": {"avg": 64.61, "peak": 83.06, "min": 57.98}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.28, "energy_joules_est": 25.35, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T17:59:19.572600"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 752.553, "latencies_ms": [752.553], "images_per_second": 1.329, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A black and tan dog is playing with a frisbee in a park with fallen leaves on the ground. A large tree trunk with moss is visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.77, "peak": 35.45, "min": 24.42}, "VIN": {"avg": 65.55, "peak": 86.82, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.77, "energy_joules_est": 22.42, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T17:59:20.331874"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 912.165, "latencies_ms": [912.165], "images_per_second": 1.096, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The dog is black and white and appears to be playing with a brown frisbee. The ground is covered with fallen leaves, suggesting it's autumn. The lighting is soft and diffused, likely due to overcast conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.28, "peak": 35.83, "min": 23.64}, "VIN": {"avg": 63.35, "peak": 72.87, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.28, "energy_joules_est": 26.73, "sample_count": 6, "duration_seconds": 0.913}, "timestamp": "2026-01-17T17:59:21.251238"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 708.214, "latencies_ms": [708.214], "images_per_second": 1.412, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A skier in green boots and gray pants is skiing down a snowy slope, following a trail marked by red poles.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.71, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 68.8, "peak": 98.23, "min": 59.39}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.71, "energy_joules_est": 21.06, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T17:59:21.971809"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1219.948, "latencies_ms": [1219.948], "images_per_second": 0.82, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Ski: 2\nSki poles: 2\nSki boots: 2\nBackpack: 1\nSnow: 6\nSki tracks: 4\nTrees: 4\nSky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.65, "peak": 36.23, "min": 20.88}, "VIN": {"avg": 62.55, "peak": 93.6, "min": 50.25}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.65, "energy_joules_est": 32.52, "sample_count": 9, "duration_seconds": 1.22}, "timestamp": "2026-01-17T17:59:23.198027"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1004.712, "latencies_ms": [1004.712], "images_per_second": 0.995, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The skier is positioned in the foreground, moving towards the background. The skis are in the foreground, and the skier is using ski poles. The background features trees and a snowy slope, suggesting the skier is on a relatively distant slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.46, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 63.28, "peak": 77.14, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.46, "energy_joules_est": 27.61, "sample_count": 7, "duration_seconds": 1.005}, "timestamp": "2026-01-17T17:59:24.214029"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 990.46, "latencies_ms": [990.46], "images_per_second": 1.01, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "A person is cross-country skiing on a snowy trail, wearing a white hat and carrying a backpack. They are using ski poles to navigate the terrain. The setting is a forested area with snow-covered ground and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.34, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 62.38, "peak": 75.36, "min": 53.36}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.34, "energy_joules_est": 27.1, "sample_count": 7, "duration_seconds": 0.991}, "timestamp": "2026-01-17T17:59:25.211180"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1025.278, "latencies_ms": [1025.278], "images_per_second": 0.975, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The skier is wearing a light blue and white jacket, dark pants, and bright green ski boots. The snow is bright white, and the lighting suggests a sunny day. The skier is using ski poles for balance and propulsion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.84, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 64.29, "peak": 82.11, "min": 57.64}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.84, "energy_joules_est": 27.54, "sample_count": 8, "duration_seconds": 1.026}, "timestamp": "2026-01-17T17:59:26.243594"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 559.773, "latencies_ms": [559.773], "images_per_second": 1.786, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "An orange and black BNSF freight train travels along railroad tracks, passing through a wooded area with bare trees.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 24.03, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 61.61, "peak": 64.19, "min": 59.08}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 24.03, "energy_joules_est": 13.46, "sample_count": 4, "duration_seconds": 0.56}, "timestamp": "2026-01-17T17:59:26.815285"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1219.943, "latencies_ms": [1219.943], "images_per_second": 0.82, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Train: 2\nTrain car: 2\nTrain engine: 1\nTrain wheels: 4\nTrain tracks: 1\nTrain windows: 2\nTrain cab: 1\nTrain number: 6009", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 13.79}, "VDD_GPU": {"avg": 22.36, "peak": 27.18, "min": 19.7}, "VIN": {"avg": 60.08, "peak": 63.0, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.36, "energy_joules_est": 27.29, "sample_count": 9, "duration_seconds": 1.22}, "timestamp": "2026-01-17T17:59:28.041968"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 798.507, "latencies_ms": [798.507], "images_per_second": 1.252, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The orange and black BNSF train is positioned in the foreground, moving from left to right across the image. The background consists of bare trees and a clear blue sky, creating a contrast with the vibrant colors of the train.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 22.93, "peak": 25.6, "min": 20.89}, "VIN": {"avg": 60.02, "peak": 63.89, "min": 53.5}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 22.93, "energy_joules_est": 18.33, "sample_count": 5, "duration_seconds": 0.799}, "timestamp": "2026-01-17T17:59:28.847387"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 734.951, "latencies_ms": [734.951], "images_per_second": 1.361, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "An orange and black BNSF freight train travels through a rural landscape, passing through a wooded area with bare trees. The clear blue sky indicates a bright, sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.32, "peak": 26.01, "min": 21.27}, "VIN": {"avg": 60.3, "peak": 63.66, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.32, "energy_joules_est": 17.15, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T17:59:29.588735"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 694.324, "latencies_ms": [694.324], "images_per_second": 1.44, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The train is primarily orange with black and yellow accents. The lighting suggests it is likely daytime. The train appears to be made of metal and has a visible engine and multiple cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.39, "peak": 26.38, "min": 21.26}, "VIN": {"avg": 58.73, "peak": 61.12, "min": 54.14}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.39, "energy_joules_est": 16.25, "sample_count": 5, "duration_seconds": 0.695}, "timestamp": "2026-01-17T17:59:30.289548"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 765.9, "latencies_ms": [765.9], "images_per_second": 1.306, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A wooden table holds a white plate with a slice of seeded bread topped with creamy avocado spread, and a small blue bowl filled with steamed broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.3, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 66.64, "peak": 93.58, "min": 56.77}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 22.46, "sample_count": 5, "duration_seconds": 0.766}, "timestamp": "2026-01-17T17:59:31.071827"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1064.397, "latencies_ms": [1064.397], "images_per_second": 0.939, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "toast: 2\navocado spread: 1\ncream cheese: 1\nbroccoli: 6\nbowl: 1\ntable: 1\ntext: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.03, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 65.01, "peak": 95.24, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.03, "energy_joules_est": 28.79, "sample_count": 8, "duration_seconds": 1.065}, "timestamp": "2026-01-17T17:59:32.147267"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1023.286, "latencies_ms": [1023.286], "images_per_second": 0.977, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The avocado spread is positioned to the left of the bowl, closer to the viewer. The broccoli is situated to the right of the bowl, further away from the viewer. The arrangement suggests a balanced composition, with the avocado spread providing a contrasting texture and color to the broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.52, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 63.99, "peak": 80.21, "min": 59.79}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 28.18, "sample_count": 7, "duration_seconds": 1.024}, "timestamp": "2026-01-17T17:59:33.177747"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1007.329, "latencies_ms": [1007.329], "images_per_second": 0.993, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene depicts a simple, healthy meal consisting of a piece of whole grain toast topped with avocado, a small bowl of roasted broccoli, and a small bowl of white cream cheese. The meal is presented on a light blue plate on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.63, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 64.72, "peak": 93.22, "min": 55.0}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 27.84, "sample_count": 7, "duration_seconds": 1.008}, "timestamp": "2026-01-17T17:59:34.192615"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1183.845, "latencies_ms": [1183.845], "images_per_second": 0.845, "prompt_tokens": 18, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The plate features a light blue rim and a white interior. The food items include green guacamole, a creamy white spread, and a vibrant green broccoli floret. The plate rests on a wooden surface, and the lighting is soft and warm, enhancing the colors and textures of the food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.7, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 25.91, "peak": 33.86, "min": 20.48}, "VIN": {"avg": 63.43, "peak": 98.55, "min": 51.5}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.91, "energy_joules_est": 30.68, "sample_count": 9, "duration_seconds": 1.184}, "timestamp": "2026-01-17T17:59:35.383322"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 648.008, "latencies_ms": [648.008], "images_per_second": 1.543, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A person is sleeping on a wooden bench, wrapped in an orange blanket, with a blue backpack placed beside them and a parking meter nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25546.7, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.23, "peak": 33.88, "min": 26.39}, "VIN": {"avg": 65.04, "peak": 81.43, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.23, "energy_joules_est": 19.6, "sample_count": 4, "duration_seconds": 0.648}, "timestamp": "2026-01-17T17:59:36.042608"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1117.253, "latencies_ms": [1117.253], "images_per_second": 0.895, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "park bench: 2\nparking meter: 2\nperson: 1\nblanket: 1\nbackpack: 1\ncar: 1\ngrass: 1\nfence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25547.6, "ram_available_mb": 100224.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.62, "peak": 36.65, "min": 21.66}, "VIN": {"avg": 63.58, "peak": 89.1, "min": 56.1}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.62, "energy_joules_est": 30.88, "sample_count": 8, "duration_seconds": 1.118}, "timestamp": "2026-01-17T17:59:37.166883"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1000.541, "latencies_ms": [1000.541], "images_per_second": 0.999, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The bench is positioned in the foreground, slightly to the right of the image. The park bench is situated in the background, extending from the left edge of the image to the middle. The park setting is further back, providing a backdrop to the bench and parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.6, "ram_available_mb": 100224.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.0, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.41, "peak": 34.28, "min": 22.06}, "VIN": {"avg": 64.31, "peak": 82.43, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.41, "energy_joules_est": 27.44, "sample_count": 7, "duration_seconds": 1.001}, "timestamp": "2026-01-17T17:59:38.173752"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 687.796, "latencies_ms": [687.796], "images_per_second": 1.454, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A person is sleeping on a wooden bench in a park-like setting. The bench is situated near a parking meter and has a backpack placed on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.0, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.54, "peak": 34.65, "min": 24.82}, "VIN": {"avg": 67.83, "peak": 98.65, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 20.33, "sample_count": 5, "duration_seconds": 0.688}, "timestamp": "2026-01-17T17:59:38.869106"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 945.979, "latencies_ms": [945.979], "images_per_second": 1.057, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The bench is light brown and appears to be made of wood. The lighting suggests it's daytime, and the colors are vibrant, reflecting the outdoor environment. The materials appear to be sturdy and weather-resistant, suitable for outdoor use.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25547.0, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.42, "peak": 36.63, "min": 22.46}, "VIN": {"avg": 64.41, "peak": 80.36, "min": 60.93}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 26.9, "sample_count": 7, "duration_seconds": 0.946}, "timestamp": "2026-01-17T17:59:39.821793"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 709.866, "latencies_ms": [709.866], "images_per_second": 1.409, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A vibrant vase with dried flowers and leaves sits on a white pedestal, accompanied by smaller vases and decorative items, against a brown wall.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25547.0, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.62, "peak": 35.45, "min": 24.42}, "VIN": {"avg": 64.02, "peak": 79.36, "min": 59.15}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 21.04, "sample_count": 5, "duration_seconds": 0.71}, "timestamp": "2026-01-17T17:59:40.543673"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1179.819, "latencies_ms": [1179.819], "images_per_second": 0.848, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "vase: 1\nlotus seed pod: 2\nstalk: 2\nbush: 2\nwhisk: 1\nflower: 1\npot: 2\nbowl: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.28, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 65.31, "peak": 100.23, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.28, "energy_joules_est": 32.19, "sample_count": 8, "duration_seconds": 1.18}, "timestamp": "2026-01-17T17:59:41.730246"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 867.089, "latencies_ms": [867.089], "images_per_second": 1.153, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main object, vase and dried flowers, are positioned in the foreground, slightly to the right of the viewer. The background features other ceramic items and wall decorations, creating a sense of depth and context within the display.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.16, "peak": 33.86, "min": 23.24}, "VIN": {"avg": 64.33, "peak": 88.64, "min": 54.64}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.16, "energy_joules_est": 24.43, "sample_count": 6, "duration_seconds": 0.868}, "timestamp": "2026-01-17T17:59:42.602940"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1589.359, "latencies_ms": [1589.359], "images_per_second": 0.629, "prompt_tokens": 19, "response_tokens_est": 81, "n_tiles": 1, "output_text": "The scene depicts a display of handcrafted pottery and dried floral arrangements at a market or exhibition. The arrangement features a variety of dried plants in shades of brown and beige, complemented by decorative elements like dried seed pods and a small bird figurine. The setting is indoors, with white pedestals showcasing the artworks and providing a clean backdrop for the vibrant colors and textures of the pottery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.22, "min": 14.1}, "VDD_GPU": {"avg": 24.68, "peak": 35.03, "min": 19.7}, "VIN": {"avg": 61.98, "peak": 81.4, "min": 56.51}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.68, "energy_joules_est": 39.24, "sample_count": 12, "duration_seconds": 1.59}, "timestamp": "2026-01-17T17:59:44.200067"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1100.016, "latencies_ms": [1100.016], "images_per_second": 0.909, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The vase is primarily dark brown and gold, showcasing a blend of earthy tones. The arrangement includes dried grasses and seed pods, creating a harmonious and textured display. The lighting is soft and diffused, enhancing the colors without harsh shadows.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 25.94, "peak": 33.08, "min": 20.88}, "VIN": {"avg": 62.76, "peak": 73.75, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.94, "energy_joules_est": 28.54, "sample_count": 8, "duration_seconds": 1.1}, "timestamp": "2026-01-17T17:59:45.306351"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 706.063, "latencies_ms": [706.063], "images_per_second": 1.416, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A young man in blue jeans and a white shirt skillfully maneuvers his skateboard on a concrete ramp in a skate park.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.3, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.11, "peak": 79.12, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 20.7, "sample_count": 5, "duration_seconds": 0.707}, "timestamp": "2026-01-17T17:59:46.023121"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1177.089, "latencies_ms": [1177.089], "images_per_second": 0.85, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "helmet: 1\nknee pads: 2\nshin guards: 2\nskateboard: 1\njeans: 1\nshirt: 1\nfence: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.61, "peak": 35.83, "min": 20.88}, "VIN": {"avg": 60.74, "peak": 80.4, "min": 51.19}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.61, "energy_joules_est": 31.34, "sample_count": 9, "duration_seconds": 1.178}, "timestamp": "2026-01-17T17:59:47.207222"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1137.934, "latencies_ms": [1137.934], "images_per_second": 0.879, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground of the image, maneuvering through the concrete bowl. The skateboard is situated near the center of the image, partially obscured by the bowl's curves. The skateboarder is facing the camera, engaging in the activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 26.24, "peak": 33.86, "min": 20.88}, "VIN": {"avg": 65.78, "peak": 92.46, "min": 57.24}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.24, "energy_joules_est": 29.88, "sample_count": 8, "duration_seconds": 1.139}, "timestamp": "2026-01-17T17:59:48.355994"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1169.048, "latencies_ms": [1169.048], "images_per_second": 0.855, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "A young man is skateboarding in a concrete skatepark, performing a trick while wearing a helmet and protective gear. The skatepark is well-maintained and features ramps for skateboarding. Other skateboarders are visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.09, "peak": 33.47, "min": 20.88}, "VIN": {"avg": 62.71, "peak": 79.72, "min": 54.55}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.09, "energy_joules_est": 30.51, "sample_count": 8, "duration_seconds": 1.169}, "timestamp": "2026-01-17T17:59:49.531496"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1065.151, "latencies_ms": [1065.151], "images_per_second": 0.939, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The skateboarder is wearing a white shirt, blue jeans, and a black helmet. The concrete surface is gray, and the lighting suggests it's likely daytime. The skateboarder is performing a trick, showcasing their skill and agility.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.2, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.19, "peak": 33.48, "min": 21.27}, "VIN": {"avg": 65.68, "peak": 97.03, "min": 58.22}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.19, "energy_joules_est": 27.9, "sample_count": 8, "duration_seconds": 1.065}, "timestamp": "2026-01-17T17:59:50.603220"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 704.559, "latencies_ms": [704.559], "images_per_second": 1.419, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A large Christmas tree adorned with twinkling lights stands tall in the heart of a bustling city square, surrounded by people enjoying the festive atmosphere.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25547.2, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.15, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.59, "peak": 87.05, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 20.55, "sample_count": 5, "duration_seconds": 0.705}, "timestamp": "2026-01-17T17:59:51.323828"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1000.137, "latencies_ms": [1000.137], "images_per_second": 1.0, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "clock tower: 2\nchristmas tree: 1\nlights: 10\nstreet: 5\nbuildings: 5\npeople: 2\ncars: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.13, "peak": 35.85, "min": 22.45}, "VIN": {"avg": 62.63, "peak": 70.98, "min": 59.71}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.13, "energy_joules_est": 28.15, "sample_count": 7, "duration_seconds": 1.001}, "timestamp": "2026-01-17T17:59:52.330824"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1172.511, "latencies_ms": [1172.511], "images_per_second": 0.853, "prompt_tokens": 25, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The large clock tower stands prominently in the foreground, positioned between the left side of the image and the Christmas tree. The Christmas tree occupies the central-right portion of the image, extending beyond the frame of the main objects. The street and surrounding buildings are visible in the background, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.03, "peak": 34.65, "min": 20.48}, "VIN": {"avg": 64.79, "peak": 94.73, "min": 59.21}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.03, "energy_joules_est": 30.54, "sample_count": 9, "duration_seconds": 1.173}, "timestamp": "2026-01-17T17:59:53.510450"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1124.737, "latencies_ms": [1124.737], "images_per_second": 0.889, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a city square at night, illuminated by festive string lights. A large Christmas tree adorned with lights stands prominently in the square, surrounded by people enjoying the holiday atmosphere. The surrounding buildings feature modern architecture and retail spaces, creating a lively urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.2, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.04, "peak": 33.08, "min": 20.88}, "VIN": {"avg": 62.86, "peak": 76.0, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.04, "energy_joules_est": 29.3, "sample_count": 8, "duration_seconds": 1.125}, "timestamp": "2026-01-17T17:59:54.641706"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1061.369, "latencies_ms": [1061.369], "images_per_second": 0.942, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The Christmas tree is decorated with blue and white lights, creating a festive atmosphere. The clock tower is illuminated with warm yellow lights, adding to the overall warmth of the scene. The wet street reflects the lights and lights from the clock tower, creating a reflective surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.2, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.7, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.64, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 62.87, "peak": 78.15, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.64, "energy_joules_est": 28.28, "sample_count": 8, "duration_seconds": 1.062}, "timestamp": "2026-01-17T17:59:55.714197"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 737.746, "latencies_ms": [737.746], "images_per_second": 1.355, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A young boy in a blue shirt and black shorts is playing tennis, swinging his racket to hit a yellow tennis ball on a green court.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25546.7, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.7, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.91, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 66.69, "peak": 85.52, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.91, "energy_joules_est": 21.34, "sample_count": 5, "duration_seconds": 0.738}, "timestamp": "2026-01-17T17:59:56.463183"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1293.446, "latencies_ms": [1293.446], "images_per_second": 0.773, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis court: 2\nTennis player: 1\nTennis shoes: 2\nTennis net: 1\nTrees: 2\nFence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.7, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.7, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 25.84, "peak": 35.83, "min": 20.48}, "VIN": {"avg": 63.67, "peak": 90.36, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.84, "energy_joules_est": 33.44, "sample_count": 10, "duration_seconds": 1.294}, "timestamp": "2026-01-17T17:59:57.767199"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 989.254, "latencies_ms": [989.254], "images_per_second": 1.011, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The tennis player is positioned on the right side of the image, facing left. The tennis ball is near the player's racket, implying a dynamic moment of play. The tennis court extends into the background, creating a sense of depth and space.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25546.7, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.06, "peak": 33.48, "min": 22.06}, "VIN": {"avg": 62.9, "peak": 77.56, "min": 51.82}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.06, "energy_joules_est": 26.78, "sample_count": 7, "duration_seconds": 0.99}, "timestamp": "2026-01-17T17:59:58.762940"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1019.476, "latencies_ms": [1019.476], "images_per_second": 0.981, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A young boy is playing tennis on a court. He is wearing a blue shirt and black shorts, swinging his racket to hit a tennis ball. The setting appears to be a park or recreational area with a chain-link fence and trees in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.2, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.34, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 62.55, "peak": 82.81, "min": 54.72}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 27.34, "energy_joules_est": 27.88, "sample_count": 7, "duration_seconds": 1.02}, "timestamp": "2026-01-17T17:59:59.792448"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 858.725, "latencies_ms": [858.725], "images_per_second": 1.165, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The tennis court appears to be made of dark gray or black material. The lighting suggests an outdoor setting, possibly on a sunny day. The tennis racket is yellow and appears to be made of metal.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.2, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.17, "peak": 34.27, "min": 22.86}, "VIN": {"avg": 65.01, "peak": 85.45, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 24.2, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T18:00:00.658209"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 581.153, "latencies_ms": [581.153], "images_per_second": 1.721, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The living room features a white fireplace with green marble surround, flanked by built-in bookshelves filled with books and framed artwork.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25547.0, "ram_available_mb": 100225.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 24.43, "peak": 27.19, "min": 22.07}, "VIN": {"avg": 58.92, "peak": 63.16, "min": 52.62}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 24.43, "energy_joules_est": 14.2, "sample_count": 4, "duration_seconds": 0.581}, "timestamp": "2026-01-17T18:00:01.248432"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 938.261, "latencies_ms": [938.261], "images_per_second": 1.066, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "bookcase: 6\nfireplace: 2\nlamp: 2\narmchair: 1\nsofa: 1\nplant: 2\nside table: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 22.85, "peak": 27.19, "min": 20.09}, "VIN": {"avg": 59.91, "peak": 63.12, "min": 55.31}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 15.35, "min": 13.76}}, "power_watts_avg": 22.85, "energy_joules_est": 21.45, "sample_count": 7, "duration_seconds": 0.939}, "timestamp": "2026-01-17T18:00:02.192657"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 712.309, "latencies_ms": [712.309], "images_per_second": 1.404, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The bookshelves are positioned on the left side of the image, while the armchair is situated in the foreground. The fireplace is situated in the background, slightly further away than the bookshelves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 23.4, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 59.61, "peak": 60.53, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.4, "energy_joules_est": 16.68, "sample_count": 5, "duration_seconds": 0.713}, "timestamp": "2026-01-17T18:00:02.911042"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 888.222, "latencies_ms": [888.222], "images_per_second": 1.126, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The living room features a white fireplace with green marble surround, surrounded by built-in bookshelves filled with books. The room is furnished with beige armchairs and matching couches, complemented by wooden side tables and lamps.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.7, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.18, "peak": 26.8, "min": 20.89}, "VIN": {"avg": 59.09, "peak": 62.64, "min": 56.69}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 23.18, "energy_joules_est": 20.6, "sample_count": 6, "duration_seconds": 0.889}, "timestamp": "2026-01-17T18:00:03.805631"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 732.231, "latencies_ms": [732.231], "images_per_second": 1.366, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The room features a green marble fireplace mantel, white walls, and warm lighting from lamps. The furniture includes beige armchairs and a beige sofa, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.7, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.4, "peak": 26.4, "min": 21.27}, "VIN": {"avg": 60.25, "peak": 62.61, "min": 57.22}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 23.4, "energy_joules_est": 17.15, "sample_count": 5, "duration_seconds": 0.733}, "timestamp": "2026-01-17T18:00:04.545038"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 616.267, "latencies_ms": [616.267], "images_per_second": 1.623, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Three zebras are grazing on grass in a field, with one actively eating while the other two stand nearby.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.32, "peak": 33.86, "min": 26.38}, "VIN": {"avg": 64.3, "peak": 76.38, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.32, "energy_joules_est": 18.7, "sample_count": 4, "duration_seconds": 0.617}, "timestamp": "2026-01-17T18:00:05.172648"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 869.717, "latencies_ms": [869.717], "images_per_second": 1.15, "prompt_tokens": 21, "response_tokens_est": 25, "n_tiles": 1, "output_text": "zebra: 3\ngrass: 2\ndirt: 1\ntree: 1\nbush: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 30.0, "peak": 37.01, "min": 24.03}, "VIN": {"avg": 64.09, "peak": 81.25, "min": 56.17}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.0, "energy_joules_est": 26.11, "sample_count": 6, "duration_seconds": 0.87}, "timestamp": "2026-01-17T18:00:06.048791"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 958.373, "latencies_ms": [958.373], "images_per_second": 1.043, "prompt_tokens": 25, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The main objects are positioned in a relatively close proximity to each other, with the zebra on the left closest to the viewer and the zebra on the right further away. The foreground is dominated by the grassy area, while the background features more sparse vegetation and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.86, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 63.26, "peak": 79.29, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.86, "energy_joules_est": 26.71, "sample_count": 7, "duration_seconds": 0.959}, "timestamp": "2026-01-17T18:00:07.012782"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 852.522, "latencies_ms": [852.522], "images_per_second": 1.173, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Three zebras are grazing on grass in a savanna-like environment. The zebras are standing near a dirt patch and surrounded by dry grass and sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.56, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 64.07, "peak": 78.55, "min": 59.49}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.56, "energy_joules_est": 24.36, "sample_count": 6, "duration_seconds": 0.853}, "timestamp": "2026-01-17T18:00:07.871706"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1186.472, "latencies_ms": [1186.472], "images_per_second": 0.843, "prompt_tokens": 18, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The zebras exhibit distinct black and white stripes, creating a striking visual pattern. The lighting in the image appears to be natural daylight, enhancing the visibility of the zebras' stripes. The scene is set in a grassy area with sparse vegetation, typical of a savanna or grassland habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.25, "peak": 34.64, "min": 20.88}, "VIN": {"avg": 60.75, "peak": 70.02, "min": 54.58}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.25, "energy_joules_est": 31.15, "sample_count": 9, "duration_seconds": 1.187}, "timestamp": "2026-01-17T18:00:09.064170"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 631.597, "latencies_ms": [631.597], "images_per_second": 1.583, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A group of people, mostly older adults, are gathered around a wooden table in a cozy restaurant, enjoying a meal and conversation.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.12, "peak": 33.47, "min": 26.38}, "VIN": {"avg": 68.27, "peak": 88.05, "min": 60.77}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.12, "energy_joules_est": 19.03, "sample_count": 4, "duration_seconds": 0.632}, "timestamp": "2026-01-17T18:00:09.707959"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1146.062, "latencies_ms": [1146.062], "images_per_second": 0.873, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "table: 6\nchairs: 4\npeople: 10\nserving dishes: 2\nserving cups: 2\nbottles: 2\nclocks: 1\nframes: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.09, "peak": 36.62, "min": 20.88}, "VIN": {"avg": 63.24, "peak": 94.74, "min": 54.62}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.09, "energy_joules_est": 31.06, "sample_count": 9, "duration_seconds": 1.146}, "timestamp": "2026-01-17T18:00:10.859791"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1160.143, "latencies_ms": [1160.143], "images_per_second": 0.862, "prompt_tokens": 25, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective in the image. The foreground features the seated people, while the background includes additional tables, chairs, and a counter. The distance between the foreground and the background is relatively clear, allowing the viewer to focus on the people seated at the table.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.49, "peak": 33.86, "min": 21.27}, "VIN": {"avg": 63.65, "peak": 86.07, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.49, "energy_joules_est": 30.74, "sample_count": 8, "duration_seconds": 1.16}, "timestamp": "2026-01-17T18:00:12.026065"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1046.961, "latencies_ms": [1046.961], "images_per_second": 0.955, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A group of people are gathered around a long wooden table in a restaurant, enjoying a meal together. The restaurant has a rustic atmosphere with exposed brick walls and wooden floors. Various items like cups, bottles, and napkins are placed on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.24, "peak": 33.47, "min": 21.27}, "VIN": {"avg": 65.69, "peak": 97.98, "min": 51.84}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.24, "energy_joules_est": 27.48, "sample_count": 8, "duration_seconds": 1.047}, "timestamp": "2026-01-17T18:00:13.078762"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1065.035, "latencies_ms": [1065.035], "images_per_second": 0.939, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The restaurant has a warm and inviting atmosphere with wooden floors and exposed brick walls. The lighting is warm and inviting, complementing the rustic ambiance. The tables are set with plates, cutlery, and napkins, creating a welcoming dining experience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.64, "peak": 33.88, "min": 21.27}, "VIN": {"avg": 62.14, "peak": 82.4, "min": 52.7}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.64, "energy_joules_est": 28.39, "sample_count": 8, "duration_seconds": 1.066}, "timestamp": "2026-01-17T18:00:14.150568"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 732.163, "latencies_ms": [732.163], "images_per_second": 1.366, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A flock of twelve elegant white swans glides gracefully across the dark water, their gleaming bodies contrasting beautifully against the serene backdrop.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.83, "peak": 33.86, "min": 24.03}, "VIN": {"avg": 66.19, "peak": 82.63, "min": 60.26}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.83, "energy_joules_est": 21.12, "sample_count": 5, "duration_seconds": 0.733}, "timestamp": "2026-01-17T18:00:14.895957"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 882.901, "latencies_ms": [882.901], "images_per_second": 1.133, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "swans: 12\nboats: 6\nsailboats: 4\ndock: 1\nshore: 1\nsand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.28, "peak": 35.85, "min": 23.64}, "VIN": {"avg": 66.85, "peak": 97.23, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.28, "energy_joules_est": 25.87, "sample_count": 6, "duration_seconds": 0.884}, "timestamp": "2026-01-17T18:00:15.785431"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 789.787, "latencies_ms": [789.787], "images_per_second": 1.266, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the swans primarily on the right side of the image. The boats are situated in the background, extending from the left edge to the far right of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.1, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.89, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 63.56, "peak": 81.28, "min": 57.09}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 22.83, "sample_count": 6, "duration_seconds": 0.79}, "timestamp": "2026-01-17T18:00:16.581261"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 877.838, "latencies_ms": [877.838], "images_per_second": 1.139, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a serene harbor with a flock of white swans swimming in the dark water. Boats of various sizes and colors are docked nearby, contributing to the picturesque setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.1, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.27, "peak": 36.24, "min": 23.64}, "VIN": {"avg": 65.46, "peak": 92.87, "min": 57.6}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.27, "energy_joules_est": 25.71, "sample_count": 6, "duration_seconds": 0.878}, "timestamp": "2026-01-17T18:00:17.465903"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1145.241, "latencies_ms": [1145.241], "images_per_second": 0.873, "prompt_tokens": 18, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The water is a dark blue-purple color, creating a striking contrast with the white swans. The lighting suggests it might be near sunrise or sunset, casting a warm glow on the scene. The swans appear to be swimming leisurely, and the overall atmosphere is serene and picturesque.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.98, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 64.03, "peak": 92.7, "min": 55.08}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 26.98, "energy_joules_est": 30.91, "sample_count": 8, "duration_seconds": 1.146}, "timestamp": "2026-01-17T18:00:18.617036"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 625.857, "latencies_ms": [625.857], "images_per_second": 1.598, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A newlywed couple cuts their wedding cake together at their reception, surrounded by the festive atmosphere of the event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.12, "peak": 33.86, "min": 26.0}, "VIN": {"avg": 67.64, "peak": 90.08, "min": 57.41}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.12, "energy_joules_est": 18.87, "sample_count": 4, "duration_seconds": 0.627}, "timestamp": "2026-01-17T18:00:19.254448"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1477.74, "latencies_ms": [1477.74], "images_per_second": 0.677, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "Cake: 3\nWedding dress: 1\nBride: 1\nGroom: 1\nTable: 1\nCandles: 1\nSpeaker: 1\nMusical instruments: 2\nFlags: 2\nDance floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 25.79, "peak": 36.63, "min": 20.09}, "VIN": {"avg": 62.53, "peak": 77.27, "min": 56.39}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.79, "energy_joules_est": 38.12, "sample_count": 11, "duration_seconds": 1.478}, "timestamp": "2026-01-17T18:00:20.738158"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 756.882, "latencies_ms": [756.882], "images_per_second": 1.321, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the bride and groom cutting the cake. The background includes additional guests and tables, suggesting the event is taking place in a larger venue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.59, "peak": 33.08, "min": 24.03}, "VIN": {"avg": 64.19, "peak": 85.06, "min": 54.96}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 21.66, "sample_count": 5, "duration_seconds": 0.758}, "timestamp": "2026-01-17T18:00:21.501607"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 796.678, "latencies_ms": [796.678], "images_per_second": 1.255, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A bride and groom are cutting their wedding cake inside a tent decorated with colorful pennants and flags.  A band is playing in the background, and guests are standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.82, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 64.08, "peak": 81.56, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 22.98, "sample_count": 6, "duration_seconds": 0.797}, "timestamp": "2026-01-17T18:00:22.304899"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 944.765, "latencies_ms": [944.765], "images_per_second": 1.058, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The wedding cake is multi-colored and appears to be made of fondant or similar material. The lighting in the tent creates a warm, inviting atmosphere. The tent's fabric is light-colored and appears to be made of a durable material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.02, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 65.21, "peak": 96.03, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.02, "energy_joules_est": 26.48, "sample_count": 7, "duration_seconds": 0.945}, "timestamp": "2026-01-17T18:00:23.255787"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 792.914, "latencies_ms": [792.914], "images_per_second": 1.261, "prompt_tokens": 8, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The living room features a blue sofa, a wooden coffee table, a glass-topped side table with a lamp, a potted plant, a vase, and a framed picture on the wall.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.69, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 65.46, "peak": 86.89, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.69, "energy_joules_est": 22.76, "sample_count": 6, "duration_seconds": 0.793}, "timestamp": "2026-01-17T18:00:24.060698"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1060.41, "latencies_ms": [1060.41], "images_per_second": 0.943, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "plant: 2\nlamp: 1\ntable: 2\ncouch: 2\nrug: 1\nside table: 1\npicture: 2\nbook: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 27.27, "peak": 35.44, "min": 21.27}, "VIN": {"avg": 63.79, "peak": 80.04, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.27, "energy_joules_est": 28.93, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T18:00:25.127168"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1005.532, "latencies_ms": [1005.532], "images_per_second": 0.994, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The sofa is positioned in the foreground, facing the window and lamp. The coffee table is situated near the sofa, partially obscured by the sofa. The window and lamp are located in the background, offering a view and light into the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.46, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 64.15, "peak": 79.09, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 27.62, "sample_count": 7, "duration_seconds": 1.006}, "timestamp": "2026-01-17T18:00:26.138832"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1221.173, "latencies_ms": [1221.173], "images_per_second": 0.819, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The living room features a cozy and vibrant atmosphere with rich red walls, natural light from several windows, and a variety of furniture including a blue sofa, a wooden coffee table, and a small side table with a lamp and plant. The space is decorated with artwork and plants, creating a warm and inviting ambiance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.22, "peak": 34.66, "min": 20.89}, "VIN": {"avg": 62.47, "peak": 79.25, "min": 55.21}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.22, "energy_joules_est": 32.04, "sample_count": 9, "duration_seconds": 1.222}, "timestamp": "2026-01-17T18:00:27.366697"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1123.446, "latencies_ms": [1123.446], "images_per_second": 0.89, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The room features a deep red wall, complemented by white lace curtains. The lighting is soft and warm, creating a cozy atmosphere. The furniture includes dark blue sofas, a wooden coffee table, and several decorative elements like lamps, plants, and artwork.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.34, "peak": 33.48, "min": 21.27}, "VIN": {"avg": 62.15, "peak": 86.66, "min": 55.68}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 29.6, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-17T18:00:28.496293"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 637.522, "latencies_ms": [637.522], "images_per_second": 1.569, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Two dolls with red hair and round eyes are sitting together, holding hands and looking at each other.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 31.31, "peak": 34.26, "min": 27.18}, "VIN": {"avg": 66.81, "peak": 92.33, "min": 55.17}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.31, "energy_joules_est": 19.98, "sample_count": 4, "duration_seconds": 0.638}, "timestamp": "2026-01-17T18:00:29.147670"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1093.027, "latencies_ms": [1093.027], "images_per_second": 0.915, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "clock: 10\nperson: 2\nbook: 5\nhair: 2\nface: 2\neyes: 2\nclock hands: 5\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.8, "peak": 37.41, "min": 22.06}, "VIN": {"avg": 64.11, "peak": 85.79, "min": 57.21}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.8, "energy_joules_est": 31.5, "sample_count": 8, "duration_seconds": 1.094}, "timestamp": "2026-01-17T18:00:30.247311"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 914.447, "latencies_ms": [914.447], "images_per_second": 1.094, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The left object appears to be a clock positioned close to the viewer, while the right object is further away, partially obscured by the clock. The foreground is relatively close, while the background is further away, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 29.47, "peak": 34.65, "min": 23.64}, "VIN": {"avg": 67.68, "peak": 97.06, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.47, "energy_joules_est": 26.97, "sample_count": 6, "duration_seconds": 0.915}, "timestamp": "2026-01-17T18:00:31.168437"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1066.774, "latencies_ms": [1066.774], "images_per_second": 0.937, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts two dolls with clock faces superimposed on their faces, positioned side-by-side. The dolls appear to be sitting together, possibly engaged in a shared activity or conversation. The setting is indoors, with a wooden surface visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.86, "peak": 35.45, "min": 21.66}, "VIN": {"avg": 62.73, "peak": 92.49, "min": 51.48}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.86, "energy_joules_est": 29.73, "sample_count": 8, "duration_seconds": 1.067}, "timestamp": "2026-01-17T18:00:32.241208"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1619.834, "latencies_ms": [1619.834], "images_per_second": 0.617, "prompt_tokens": 18, "response_tokens_est": 90, "n_tiles": 1, "output_text": "The dominant colors are warm hues of orange and red, contrasted by the cool yellowish tones of the clock faces. The lighting creates a soft, dreamy effect, enhancing the surreal and artistic nature of the image. The materials appear to be fabric and possibly paper, giving the composition a slightly textured and layered look. The weather is not explicitly visible in the image, but the overall mood and atmosphere are suggestive of a warm, cozy setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 25.27, "peak": 35.04, "min": 19.7}, "VIN": {"avg": 63.73, "peak": 95.78, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.27, "energy_joules_est": 40.94, "sample_count": 12, "duration_seconds": 1.62}, "timestamp": "2026-01-17T18:00:33.866956"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 528.925, "latencies_ms": [528.925], "images_per_second": 1.891, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A young boy, wearing a black helmet and tan jacket, sits on a black motor scooter, looking to the left.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.6, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 23.77, "peak": 25.21, "min": 22.45}, "VIN": {"avg": 56.85, "peak": 57.43, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 23.77, "energy_joules_est": 12.58, "sample_count": 3, "duration_seconds": 0.529}, "timestamp": "2026-01-17T18:00:34.406579"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1149.615, "latencies_ms": [1149.615], "images_per_second": 0.87, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "helmet: 1\njacket: 1\npants: 1\nscooter: 1\nmirror: 1\nhelmet buckle: 1\nlaces: 1\nshoe: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 22.4, "peak": 26.79, "min": 20.09}, "VIN": {"avg": 60.2, "peak": 64.29, "min": 51.33}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.4, "energy_joules_est": 25.76, "sample_count": 8, "duration_seconds": 1.15}, "timestamp": "2026-01-17T18:00:35.563799"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 637.851, "latencies_ms": [637.851], "images_per_second": 1.568, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The scooter is positioned in the foreground, slightly to the right of the main subject. The background includes other motorcycles and people, suggesting a public setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 26.0, "min": 21.66}, "VIN": {"avg": 60.27, "peak": 61.8, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.63, "energy_joules_est": 15.08, "sample_count": 4, "duration_seconds": 0.638}, "timestamp": "2026-01-17T18:00:36.208216"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 970.14, "latencies_ms": [970.14], "images_per_second": 1.031, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "A young boy is riding a scooter in a busy area, possibly a city street. He wears a tan jacket and beige pants, sitting comfortably on the scooter while looking to his right. Other people are visible in the background, suggesting a public setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.67, "peak": 26.79, "min": 20.09}, "VIN": {"avg": 59.98, "peak": 62.99, "min": 54.71}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.67, "energy_joules_est": 22.0, "sample_count": 7, "duration_seconds": 0.97}, "timestamp": "2026-01-17T18:00:37.184023"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 637.774, "latencies_ms": [637.774], "images_per_second": 1.568, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The scooter is primarily silver and black. The lighting appears to be natural daylight. The materials appear to be durable plastic and metal. The weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 26.0, "min": 21.66}, "VIN": {"avg": 58.84, "peak": 60.16, "min": 56.23}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 15.08, "sample_count": 4, "duration_seconds": 0.638}, "timestamp": "2026-01-17T18:00:37.828691"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 404.971, "latencies_ms": [404.971], "images_per_second": 2.469, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A large cheese pizza is being sliced and served on a wooden table in a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.6, "ram_available_mb": 100223.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.79, "peak": 14.0, "min": 13.59}, "VDD_GPU": {"avg": 25.41, "peak": 26.39, "min": 24.42}, "VIN": {"avg": 60.02, "peak": 60.81, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 14.37, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 25.41, "energy_joules_est": 10.3, "sample_count": 2, "duration_seconds": 0.405}, "timestamp": "2026-01-17T18:00:38.246346"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1421.91, "latencies_ms": [1421.91], "images_per_second": 0.703, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 1, "output_text": "Pizza: 8\nCheese: 8\nSpinach: 8\nTomato sauce: 8\nBasil: 8\nCheese: 8\nTomato sauce: 8\nOlive oil: 8\nSauce: 8\nBaking pan: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.11, "min": 13.39}, "VDD_GPU": {"avg": 22.03, "peak": 27.98, "min": 19.3}, "VIN": {"avg": 59.32, "peak": 63.37, "min": 55.64}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 15.36, "min": 13.78}}, "power_watts_avg": 22.03, "energy_joules_est": 31.34, "sample_count": 11, "duration_seconds": 1.423}, "timestamp": "2026-01-17T18:00:39.675117"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 714.676, "latencies_ms": [714.676], "images_per_second": 1.399, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The main object is a large, freshly baked pizza, positioned prominently in the foreground. The background features kitchen elements and equipment, suggesting the pizza is being prepared or cooked in a professional setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 22.85, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 59.08, "peak": 62.92, "min": 55.07}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.85, "energy_joules_est": 16.34, "sample_count": 5, "duration_seconds": 0.715}, "timestamp": "2026-01-17T18:00:40.396126"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 632.816, "latencies_ms": [632.816], "images_per_second": 1.58, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A large pizza with green toppings is being prepared on a wooden table in a dimly lit kitchen. A person is visible in the background, seemingly involved in the pizza-making process.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 24.13, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 57.81, "peak": 61.53, "min": 51.26}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.13, "energy_joules_est": 15.29, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T18:00:41.035355"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 732.11, "latencies_ms": [732.11], "images_per_second": 1.366, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The cheese pizza has a golden-brown crust and vibrant green toppings, likely spinach. The lighting is dim, creating a cozy atmosphere. The pizza appears to be freshly baked and ready to be served.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25548.3, "ram_available_mb": 100223.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.63, "peak": 26.77, "min": 21.27}, "VIN": {"avg": 60.11, "peak": 61.22, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 17.31, "sample_count": 5, "duration_seconds": 0.733}, "timestamp": "2026-01-17T18:00:41.774710"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 679.176, "latencies_ms": [679.176], "images_per_second": 1.472, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A female tennis player, dressed in white, is captured mid-serve on a grass court, holding a tennis racket and poised to strike the ball.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25548.3, "ram_available_mb": 100223.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25548.2, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 23.39, "peak": 26.38, "min": 21.27}, "VIN": {"avg": 60.1, "peak": 61.43, "min": 57.03}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.39, "energy_joules_est": 15.9, "sample_count": 5, "duration_seconds": 0.68}, "timestamp": "2026-01-17T18:00:42.463983"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1119.941, "latencies_ms": [1119.941], "images_per_second": 0.893, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis skirt: 1\nTennis shoes: 2\nTennis visor: 1\nTennis net: 1\nTennis court: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.2, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.9, "ram_available_mb": 100224.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.3, "peak": 26.38, "min": 20.09}, "VIN": {"avg": 59.72, "peak": 62.71, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.3, "energy_joules_est": 24.98, "sample_count": 8, "duration_seconds": 1.12}, "timestamp": "2026-01-17T18:00:43.591010"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 711.028, "latencies_ms": [711.028], "images_per_second": 1.406, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The tennis player is positioned near the center of the image, reaching up with her right arm to hit the ball. The tennis court extends into the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.9, "ram_available_mb": 100224.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25547.9, "ram_available_mb": 100224.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 23.0, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 60.48, "peak": 65.7, "min": 58.1}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 23.0, "energy_joules_est": 16.36, "sample_count": 5, "duration_seconds": 0.711}, "timestamp": "2026-01-17T18:00:44.308266"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 673.914, "latencies_ms": [673.914], "images_per_second": 1.484, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A woman in a white tennis outfit is playing tennis on a grass court, reaching up to hit the ball with her racket. The setting appears to be a sunny day at the tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.9, "ram_available_mb": 100224.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.48, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 60.78, "peak": 62.18, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 23.48, "energy_joules_est": 15.83, "sample_count": 5, "duration_seconds": 0.674}, "timestamp": "2026-01-17T18:00:44.988214"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 625.632, "latencies_ms": [625.632], "images_per_second": 1.598, "prompt_tokens": 18, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The tennis player is wearing a white outfit. The tennis court is green and appears to be well-maintained. The lighting suggests a sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.6, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 24.23, "peak": 26.79, "min": 22.06}, "VIN": {"avg": 60.47, "peak": 64.48, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.23, "energy_joules_est": 15.17, "sample_count": 4, "duration_seconds": 0.626}, "timestamp": "2026-01-17T18:00:45.619725"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 752.249, "latencies_ms": [752.249], "images_per_second": 1.329, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A white toilet is situated in a small bathroom with yellow walls, brown tile flooring, and a white bathtub partially covered by a yellow shower curtain.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25546.6, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.46, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 63.93, "peak": 77.57, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 22.17, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T18:00:46.381508"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1221.181, "latencies_ms": [1221.181], "images_per_second": 0.819, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "toilet: 1\nshower curtain: 1\nbathroom towel rack: 2\ntoilet paper: 1\nbathtub: 1\ncloset: 3\nbedspread: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.9, "ram_available_mb": 100225.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.6, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.52, "peak": 35.45, "min": 20.88}, "VIN": {"avg": 62.86, "peak": 88.27, "min": 57.66}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.52, "energy_joules_est": 32.4, "sample_count": 9, "duration_seconds": 1.222}, "timestamp": "2026-01-17T18:00:47.608649"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 792.259, "latencies_ms": [792.259], "images_per_second": 1.262, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The toilet is positioned to the left of the shower and bathtub, occupying the foreground. The bathroom is partially visible in the background, extending to the right and slightly behind the toilet.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.6, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.9, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 64.21, "peak": 83.74, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.9, "energy_joules_est": 22.11, "sample_count": 6, "duration_seconds": 0.793}, "timestamp": "2026-01-17T18:00:48.406720"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 868.025, "latencies_ms": [868.025], "images_per_second": 1.152, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The bathroom features a white toilet, a bathtub with a shower curtain, and a closet with wire shelving for storage. The scene suggests a clean, functional, and possibly shared living space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.88, "peak": 35.83, "min": 23.23}, "VIN": {"avg": 64.05, "peak": 81.02, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 25.08, "sample_count": 6, "duration_seconds": 0.868}, "timestamp": "2026-01-17T18:00:49.281064"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1149.872, "latencies_ms": [1149.872], "images_per_second": 0.87, "prompt_tokens": 18, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The bathroom features a pale yellow color scheme, creating a warm and inviting atmosphere. The lighting appears to be soft and diffused, enhancing the overall ambiance. The materials used include white fixtures for the toilet and shower, beige tiles for the floor, and metal shelving for the closet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.68, "peak": 34.65, "min": 21.27}, "VIN": {"avg": 65.12, "peak": 93.05, "min": 56.55}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.68, "energy_joules_est": 30.69, "sample_count": 8, "duration_seconds": 1.15}, "timestamp": "2026-01-17T18:00:50.437073"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 517.776, "latencies_ms": [517.776], "images_per_second": 1.931, "prompt_tokens": 8, "response_tokens_est": 17, "n_tiles": 1, "output_text": "Two people are standing in a room, holding wine glasses and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.1}, "VDD_GPU": {"avg": 31.63, "peak": 33.86, "min": 28.74}, "VIN": {"avg": 65.84, "peak": 82.0, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.63, "energy_joules_est": 16.39, "sample_count": 3, "duration_seconds": 0.518}, "timestamp": "2026-01-17T18:00:50.970499"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1146.978, "latencies_ms": [1146.978], "images_per_second": 0.872, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "Wine glass: 2\nWine bottle: 1\nWine: 2\nTable: 2\nChairs: 2\nWindow: 1\nMan: 2\nWoman: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 28.2, "peak": 37.39, "min": 21.66}, "VIN": {"avg": 61.97, "peak": 79.58, "min": 55.22}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.2, "energy_joules_est": 32.36, "sample_count": 8, "duration_seconds": 1.147}, "timestamp": "2026-01-17T18:00:52.123942"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 754.072, "latencies_ms": [754.072], "images_per_second": 1.326, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with a man and woman standing between them. The background features additional people and a dining table, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 29.53, "peak": 34.65, "min": 24.81}, "VIN": {"avg": 61.33, "peak": 76.52, "min": 52.57}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.53, "energy_joules_est": 22.28, "sample_count": 5, "duration_seconds": 0.755}, "timestamp": "2026-01-17T18:00:52.885588"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 947.658, "latencies_ms": [947.658], "images_per_second": 1.055, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The scene depicts a casual gathering in a home or restaurant, where two people are holding wine glasses and smiling at the camera. The setting appears to be a relaxed social environment, possibly a wine tasting or informal get-together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.42, "peak": 36.63, "min": 22.45}, "VIN": {"avg": 65.41, "peak": 90.64, "min": 55.44}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 26.94, "sample_count": 7, "duration_seconds": 0.948}, "timestamp": "2026-01-17T18:00:53.839273"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1107.902, "latencies_ms": [1107.902], "images_per_second": 0.903, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The wine glasses are clear with red wine inside. The lighting in the room is soft and warm, creating a pleasant atmosphere. The table and chairs appear to be made of a light-colored wood or similar material. The overall ambiance suggests a relaxed and enjoyable setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.73, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 64.42, "peak": 97.42, "min": 53.58}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.73, "energy_joules_est": 29.63, "sample_count": 8, "duration_seconds": 1.109}, "timestamp": "2026-01-17T18:00:54.953938"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 734.982, "latencies_ms": [734.982], "images_per_second": 1.361, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A surfer skillfully rides a wave on a surfboard, showcasing their expertise and balance amidst the powerful blue-green water.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.15, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 67.6, "peak": 89.87, "min": 61.59}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 21.43, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T18:00:55.701004"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 970.922, "latencies_ms": [970.922], "images_per_second": 1.03, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "wave: 1\nsurfer: 1\nsurfboard: 1\nwater: 1\nsand: 1\nsky: 1\npeople: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.91, "peak": 35.85, "min": 22.06}, "VIN": {"avg": 63.0, "peak": 84.68, "min": 54.78}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 27.11, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-17T18:00:56.677780"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1164.938, "latencies_ms": [1164.938], "images_per_second": 0.858, "prompt_tokens": 25, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The surfer is positioned in the foreground of the image, riding a wave that curves behind him. The wave is prominent in the foreground, extending from the left to the right side of the frame. The surfer is relatively close to the wave, suggesting an active and dynamic interaction between the surfer and the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.12, "peak": 34.65, "min": 20.48}, "VIN": {"avg": 64.55, "peak": 95.74, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.12, "energy_joules_est": 30.44, "sample_count": 9, "duration_seconds": 1.165}, "timestamp": "2026-01-17T18:00:57.849443"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 966.271, "latencies_ms": [966.271], "images_per_second": 1.035, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A surfer skillfully rides a wave in the ocean, wearing a wetsuit and riding a light-colored surfboard. The wave is breaking, creating a dramatic scene with clear blue water and a sandy beach in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.17, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 65.71, "peak": 97.74, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.17, "energy_joules_est": 26.27, "sample_count": 7, "duration_seconds": 0.967}, "timestamp": "2026-01-17T18:00:58.822243"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1062.928, "latencies_ms": [1062.928], "images_per_second": 0.941, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The water is a vibrant turquoise color, creating a striking contrast with the white foam of the wave. The lighting is bright and clear, illuminating the surfer and the wave's curves. The surfboard appears to be made of a light-colored wood or composite material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.78, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 65.6, "peak": 95.92, "min": 59.79}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 26.78, "energy_joules_est": 28.47, "sample_count": 8, "duration_seconds": 1.063}, "timestamp": "2026-01-17T18:00:59.891112"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 610.233, "latencies_ms": [610.233], "images_per_second": 1.639, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A wooden table holds six open laptops, some with visible keyboards and screens, and a red and black backpack.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 30.62, "peak": 34.65, "min": 26.38}, "VIN": {"avg": 66.64, "peak": 81.96, "min": 60.6}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.62, "energy_joules_est": 18.7, "sample_count": 4, "duration_seconds": 0.611}, "timestamp": "2026-01-17T18:01:00.512472"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1373.332, "latencies_ms": [1373.332], "images_per_second": 0.728, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "laptop: 5\nbackpack: 1\ncables: 8\ntable: 1\nlaptop: 4\nlaptop: 2\nlaptop: 1\nlaptop: 1\nlaptop: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.51, "peak": 37.01, "min": 20.48}, "VIN": {"avg": 63.56, "peak": 84.4, "min": 56.88}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.51, "energy_joules_est": 36.42, "sample_count": 10, "duration_seconds": 1.374}, "timestamp": "2026-01-17T18:01:01.893007"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 826.731, "latencies_ms": [826.731], "images_per_second": 1.21, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The main objects are positioned in a somewhat haphazard manner, with laptops and bags placed close together in the foreground. The background features additional laptops, suggesting a workspace or gathering area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.97, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 64.85, "peak": 85.78, "min": 59.19}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.97, "energy_joules_est": 23.14, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T18:01:02.726462"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 963.453, "latencies_ms": [963.453], "images_per_second": 1.038, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A group of laptops is arranged on a wooden table, showcasing various sizes and models. A red and black backpack is placed next to the laptops, partially obscuring the view. The setting appears to be an indoor workspace or study area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.14, "peak": 35.45, "min": 22.45}, "VIN": {"avg": 65.34, "peak": 91.23, "min": 59.52}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.14, "energy_joules_est": 27.12, "sample_count": 7, "duration_seconds": 0.964}, "timestamp": "2026-01-17T18:01:03.695957"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 808.717, "latencies_ms": [808.717], "images_per_second": 1.237, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The laptops are primarily black and silver. The lighting in the room appears to be soft and diffused. The laptops appear to be made of metal and plastic. The floor appears to be carpeted.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.49, "peak": 34.26, "min": 23.23}, "VIN": {"avg": 63.02, "peak": 74.02, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 23.05, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-17T18:01:04.510562"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 688.136, "latencies_ms": [688.136], "images_per_second": 1.453, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A skier in an orange and green jacket is performing an impressive jump, crossing their skis in the air while holding ski poles.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.33, "peak": 35.85, "min": 25.21}, "VIN": {"avg": 64.16, "peak": 75.82, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 20.89, "sample_count": 5, "duration_seconds": 0.689}, "timestamp": "2026-01-17T18:01:05.209275"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1216.385, "latencies_ms": [1216.385], "images_per_second": 0.822, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "skier: 2\nskis: 2\nsnowboard: 1\nski poles: 1\nhelmet: 1\ngloves: 1\ngoggles: 1\nclothing: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.96, "peak": 36.63, "min": 20.88}, "VIN": {"avg": 63.07, "peak": 80.95, "min": 54.15}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.96, "energy_joules_est": 32.81, "sample_count": 9, "duration_seconds": 1.217}, "timestamp": "2026-01-17T18:01:06.431674"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 986.344, "latencies_ms": [986.344], "images_per_second": 1.014, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The skier is positioned in the foreground, mid-air, performing a jump. The background features another skier further away, appearing smaller due to distance. The foreground is dominated by the snow, while the background is mostly clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 27.07, "peak": 35.06, "min": 21.66}, "VIN": {"avg": 64.04, "peak": 91.47, "min": 56.5}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.07, "energy_joules_est": 26.71, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T18:01:07.423876"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 817.042, "latencies_ms": [817.042], "images_per_second": 1.224, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A skier is performing a jump in the air, showcasing their skill and athleticism. The scene takes place on a snowy slope, with another skier visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.3, "ram_available_mb": 100224.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 34.27, "min": 23.24}, "VIN": {"avg": 63.85, "peak": 79.17, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 23.19, "sample_count": 6, "duration_seconds": 0.818}, "timestamp": "2026-01-17T18:01:08.247364"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1036.528, "latencies_ms": [1036.528], "images_per_second": 0.965, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The skier is wearing bright orange and green clothing. The lighting is bright and clear, creating a visually striking image. The skis appear to be made of metal and have a sleek design. The snow is bright white, indicative of a sunny day.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.95, "peak": 35.44, "min": 21.66}, "VIN": {"avg": 58.51, "peak": 64.84, "min": 50.87}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.95, "energy_joules_est": 27.96, "sample_count": 7, "duration_seconds": 1.037}, "timestamp": "2026-01-17T18:01:09.290637"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 553.858, "latencies_ms": [553.858], "images_per_second": 1.806, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A small bird perches on the window sill of a boat, gazing out at the calm, dark blue water.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 24.22, "peak": 26.77, "min": 22.06}, "VIN": {"avg": 59.2, "peak": 62.77, "min": 55.41}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 24.22, "energy_joules_est": 13.43, "sample_count": 4, "duration_seconds": 0.554}, "timestamp": "2026-01-17T18:01:09.853808"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 993.266, "latencies_ms": [993.266], "images_per_second": 1.007, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "porthole: 1\nbird: 1\nwater: 1\nmetal: 1\npane: 1\nframe: 1\nhandle: 1\nsign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 23.07, "peak": 27.17, "min": 20.48}, "VIN": {"avg": 57.63, "peak": 63.14, "min": 53.24}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.07, "energy_joules_est": 22.93, "sample_count": 7, "duration_seconds": 0.994}, "timestamp": "2026-01-17T18:01:10.853474"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 831.667, "latencies_ms": [831.667], "images_per_second": 1.202, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The bird is positioned near the window, seemingly observing the water. The window is situated in the foreground, offering a view of the water beyond. The background is relatively dark, suggesting the photo was taken in a dimly lit environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.65, "peak": 25.99, "min": 20.48}, "VIN": {"avg": 59.55, "peak": 64.35, "min": 55.89}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 22.65, "energy_joules_est": 18.85, "sample_count": 6, "duration_seconds": 0.832}, "timestamp": "2026-01-17T18:01:11.691191"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 716.32, "latencies_ms": [716.32], "images_per_second": 1.396, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A bird is perched on a rusty porthole window, gazing out at the calm, greyish-blue water. A rocky coastline is visible in the distance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.32, "peak": 26.0, "min": 21.27}, "VIN": {"avg": 59.22, "peak": 61.97, "min": 54.26}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.32, "energy_joules_est": 16.72, "sample_count": 5, "duration_seconds": 0.717}, "timestamp": "2026-01-17T18:01:12.415247"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 771.325, "latencies_ms": [771.325], "images_per_second": 1.296, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The water appears gray and somewhat murky. The window frame is rusty, suggesting age or exposure to the elements. The overall lighting is dim, contributing to the moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.48, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 60.52, "peak": 62.6, "min": 57.02}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.48, "energy_joules_est": 18.12, "sample_count": 5, "duration_seconds": 0.772}, "timestamp": "2026-01-17T18:01:13.196955"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 556.221, "latencies_ms": [556.221], "images_per_second": 1.798, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A man is cleaning a bathroom stall while another man stands nearby, possibly observing or waiting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.14, "peak": 33.5, "min": 26.0}, "VIN": {"avg": 64.47, "peak": 79.36, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.14, "energy_joules_est": 16.79, "sample_count": 4, "duration_seconds": 0.557}, "timestamp": "2026-01-17T18:01:13.764582"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1874.235, "latencies_ms": [1874.235], "images_per_second": 0.534, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 1, "output_text": "toilet: 1\ntoilet paper: 2\ntoilet brush: 1\ntoilet cleaner: 1\ntoilet cleaner bucket: 1\ntoilet cleaner mop: 1\ntoilet cleaner shelf: 2\ntoilet lid: 1\ntoilet seat: 1\ntoilet seat cover: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.7, "ram_available_mb": 100224.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 13.79}, "VDD_GPU": {"avg": 24.5, "peak": 37.41, "min": 19.3}, "VIN": {"avg": 63.42, "peak": 85.26, "min": 56.87}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.17}}, "power_watts_avg": 24.5, "energy_joules_est": 45.93, "sample_count": 14, "duration_seconds": 1.875}, "timestamp": "2026-01-17T18:01:15.645692"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 792.946, "latencies_ms": [792.946], "images_per_second": 1.261, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The main objects are positioned in a way that suggests a spatial relationship between the foreground, which includes the toilet and bucket, and the background, which includes the shelves and the person cleaning the bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.7, "ram_available_mb": 100224.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.97, "peak": 33.09, "min": 23.24}, "VIN": {"avg": 62.69, "peak": 75.62, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 22.19, "sample_count": 6, "duration_seconds": 0.793}, "timestamp": "2026-01-17T18:01:16.444108"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 808.089, "latencies_ms": [808.089], "images_per_second": 1.237, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Two men are working in a bathroom, one cleaning the floor while the other is climbing into a toilet stall. The bathroom appears cluttered with cleaning supplies and toiletries.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.88, "peak": 35.04, "min": 23.23}, "VIN": {"avg": 64.5, "peak": 83.29, "min": 57.86}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 23.36, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-17T18:01:17.259770"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 792.68, "latencies_ms": [792.68], "images_per_second": 1.262, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The bathroom has a light brown color scheme. The lighting appears to be artificial, likely fluorescent. The floor is covered in teal mop stains, suggesting recent cleaning activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.5, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.69, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 64.03, "peak": 77.25, "min": 60.11}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.69, "energy_joules_est": 23.55, "sample_count": 5, "duration_seconds": 0.793}, "timestamp": "2026-01-17T18:01:18.058489"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 689.515, "latencies_ms": [689.515], "images_per_second": 1.45, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A man is standing in a hallway, holding an umbrella aloft amidst the rain, shielding himself from the downpour.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.16, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 69.43, "peak": 99.7, "min": 60.91}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.16, "energy_joules_est": 20.81, "sample_count": 5, "duration_seconds": 0.69}, "timestamp": "2026-01-17T18:01:18.759074"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1199.87, "latencies_ms": [1199.87], "images_per_second": 0.833, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Umbrella: 1\nPerson: 1\nDoor: 2\nWall: 2\nFramed Picture: 1\nFloor: 1\nRadiator: 1\nHallway: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.2, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 27.0, "peak": 37.01, "min": 20.89}, "VIN": {"avg": 62.23, "peak": 77.6, "min": 54.93}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.0, "energy_joules_est": 32.41, "sample_count": 9, "duration_seconds": 1.2}, "timestamp": "2026-01-17T18:01:19.967084"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1144.329, "latencies_ms": [1144.329], "images_per_second": 0.874, "prompt_tokens": 25, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The man is standing in the foreground of the image, holding an umbrella that partially obscures him. The umbrella is positioned above him, creating a sense of depth and perspective. The background features a hallway with red walls, doors, and framed pictures, adding to the overall setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.2, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.49, "peak": 33.88, "min": 21.26}, "VIN": {"avg": 61.93, "peak": 74.02, "min": 57.69}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.49, "energy_joules_est": 30.33, "sample_count": 8, "duration_seconds": 1.145}, "timestamp": "2026-01-17T18:01:21.117382"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 786.778, "latencies_ms": [786.778], "images_per_second": 1.271, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A man is standing in a hallway, holding an umbrella to shield himself from the rain. The hallway has red walls and features framed pictures and mirrors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 67.18, "peak": 91.08, "min": 59.84}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 23.01, "sample_count": 5, "duration_seconds": 0.787}, "timestamp": "2026-01-17T18:01:21.915105"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1121.929, "latencies_ms": [1121.929], "images_per_second": 0.891, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The walls are painted a reddish-pink color. The lighting is dim, creating a moody atmosphere. The umbrella is black and appears to be made of a sturdy material, likely metal or plastic. The rain is visible, falling from the ceiling and creating the visual effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25547.2, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.32, "peak": 35.45, "min": 21.26}, "VIN": {"avg": 62.2, "peak": 91.83, "min": 50.16}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.32, "energy_joules_est": 30.67, "sample_count": 8, "duration_seconds": 1.123}, "timestamp": "2026-01-17T18:01:23.043842"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 757.463, "latencies_ms": [757.463], "images_per_second": 1.32, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A man wearing a red shirt and carrying a backpack is standing on a rocky path, looking upwards, while another person in a yellow outfit is walking nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25547.2, "ram_available_mb": 100225.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.22, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 64.85, "peak": 75.25, "min": 59.03}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.22, "energy_joules_est": 22.15, "sample_count": 5, "duration_seconds": 0.758}, "timestamp": "2026-01-17T18:01:23.816414"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1082.487, "latencies_ms": [1082.487], "images_per_second": 0.924, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "stairs: 5\nsign: 4\nman: 1\nbackpack: 1\npole: 1\nwater: 1\nrocks: 5\nvegetation: 5", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.33, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 63.68, "peak": 86.16, "min": 56.33}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.33, "energy_joules_est": 29.6, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T18:01:24.905362"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1397.103, "latencies_ms": [1397.103], "images_per_second": 0.716, "prompt_tokens": 25, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The man in the foreground is standing near the stream and looking upwards, possibly contemplating his next step or observing the surroundings. The man in the background is further away, standing on a path near a railing and looking towards the stream. The foreground is relatively close to the stream, while the background is further away, suggesting the stream is relatively shallow and accessible from the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25547.7, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 25.29, "peak": 33.86, "min": 20.09}, "VIN": {"avg": 63.94, "peak": 98.29, "min": 55.18}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.29, "energy_joules_est": 35.34, "sample_count": 10, "duration_seconds": 1.397}, "timestamp": "2026-01-17T18:01:26.308113"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 881.113, "latencies_ms": [881.113], "images_per_second": 1.135, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "Two hikers, one in an orange jacket and the other in a yellow robe, are walking along a trail in a lush, forested area. The trail is partially covered in rocks and surrounded by dense greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.7, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.84, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 63.46, "peak": 74.05, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.84, "energy_joules_est": 24.54, "sample_count": 6, "duration_seconds": 0.882}, "timestamp": "2026-01-17T18:01:27.195160"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 949.395, "latencies_ms": [949.395], "images_per_second": 1.053, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The man is wearing an orange jacket and blue jeans. The lighting is natural, suggesting it's daytime. The scene appears to be in a natural setting with rocks, plants, and a stream. The overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.68, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 62.82, "peak": 94.07, "min": 53.52}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.68, "energy_joules_est": 26.29, "sample_count": 7, "duration_seconds": 0.95}, "timestamp": "2026-01-17T18:01:28.150246"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 576.918, "latencies_ms": [576.918], "images_per_second": 1.733, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "Four men in business attire are standing together in a room, smiling and posing for a photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 31.91, "peak": 35.06, "min": 27.57}, "VIN": {"avg": 66.6, "peak": 90.77, "min": 56.84}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.91, "energy_joules_est": 18.43, "sample_count": 4, "duration_seconds": 0.578}, "timestamp": "2026-01-17T18:01:28.739350"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1030.739, "latencies_ms": [1030.739], "images_per_second": 0.97, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "man: 3\nman: 2\nman: 1\nman: 1\nman: 1\nman: 1\nman: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.4, "ram_available_mb": 100224.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25547.9, "ram_available_mb": 100224.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.93, "peak": 38.19, "min": 22.85}, "VIN": {"avg": 63.13, "peak": 76.93, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 30.87, "sample_count": 7, "duration_seconds": 1.031}, "timestamp": "2026-01-17T18:01:29.776538"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 828.342, "latencies_ms": [828.342], "images_per_second": 1.207, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The four men are standing close together in the foreground, with a bar and chairs visible in the background. The bar is positioned to the left of the men, while the chairs are placed towards the right.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.9, "ram_available_mb": 100224.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.41, "peak": 35.44, "min": 23.64}, "VIN": {"avg": 62.37, "peak": 71.59, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.41, "energy_joules_est": 24.37, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T18:01:30.610732"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 951.095, "latencies_ms": [951.095], "images_per_second": 1.051, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "Four men are standing together in a room, possibly at a social gathering or event. They are dressed in business casual attire and appear to be posing for a photo. The setting includes a bar area with bottles and glasses visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.14, "peak": 36.24, "min": 22.84}, "VIN": {"avg": 63.47, "peak": 91.93, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.14, "energy_joules_est": 27.73, "sample_count": 7, "duration_seconds": 0.952}, "timestamp": "2026-01-17T18:01:31.568227"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 973.42, "latencies_ms": [973.42], "images_per_second": 1.027, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The men are wearing light-colored shirts and suits. The lighting is bright, likely from overhead fixtures. The chairs appear to be made of wood and fabric. The setting suggests an indoor event, possibly a meeting or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.75, "peak": 35.44, "min": 22.86}, "VIN": {"avg": 62.31, "peak": 79.51, "min": 55.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 28.0, "sample_count": 7, "duration_seconds": 0.974}, "timestamp": "2026-01-17T18:01:32.548538"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 656.676, "latencies_ms": [656.676], "images_per_second": 1.523, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A yellow traffic sign is mounted on a black pole, partially broken and leaning, situated on a city street corner.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.46, "peak": 34.65, "min": 24.41}, "VIN": {"avg": 64.03, "peak": 79.11, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 19.37, "sample_count": 5, "duration_seconds": 0.657}, "timestamp": "2026-01-17T18:01:33.217541"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 989.492, "latencies_ms": [989.492], "images_per_second": 1.011, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "pole: 1\nsign: 1\nbus: 1\ncar: 2\nstreet light: 1\nbuildings: 5\nbus stop: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.58, "peak": 36.62, "min": 22.45}, "VIN": {"avg": 62.02, "peak": 80.05, "min": 55.35}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.58, "energy_joules_est": 28.29, "sample_count": 7, "duration_seconds": 0.99}, "timestamp": "2026-01-17T18:01:34.213128"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 812.036, "latencies_ms": [812.036], "images_per_second": 1.231, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The foreground features a damaged traffic signal pole, positioned near the center of the image. The background showcases a typical city street scene with buildings, parked cars, and traffic lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.76, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 64.57, "peak": 80.27, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.76, "energy_joules_est": 23.36, "sample_count": 6, "duration_seconds": 0.812}, "timestamp": "2026-01-17T18:01:35.031092"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 934.665, "latencies_ms": [934.665], "images_per_second": 1.07, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene depicts a city street with buildings lining both sides. A yellow traffic sign is partially obscured by a damaged pole on the left side of the road. Several vehicles, including cars and a bus, are visible traveling down the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.85, "peak": 35.45, "min": 22.05}, "VIN": {"avg": 64.26, "peak": 78.25, "min": 60.36}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 26.04, "sample_count": 7, "duration_seconds": 0.935}, "timestamp": "2026-01-17T18:01:35.971510"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1128.841, "latencies_ms": [1128.841], "images_per_second": 0.886, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The street is paved with asphalt and appears wet, likely due to recent rain. The buildings lining the street are primarily brick and feature various storefronts.  The scene is illuminated by streetlights and traffic signals, contributing to the overall urban atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.83, "peak": 34.65, "min": 21.27}, "VIN": {"avg": 63.39, "peak": 79.95, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.83, "energy_joules_est": 30.3, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T18:01:37.107194"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 731.56, "latencies_ms": [731.56], "images_per_second": 1.367, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A young man in a white shirt and cap is poised to hit a tennis ball with a red racket on a green court, surrounded by fences and advertisements.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 28.99, "peak": 33.88, "min": 24.43}, "VIN": {"avg": 62.08, "peak": 78.53, "min": 53.71}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.99, "energy_joules_est": 21.23, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T18:01:37.850006"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1087.164, "latencies_ms": [1087.164], "images_per_second": 0.92, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis court: 2\nFence: 2\nSign: 2\nPerson: 1\nGrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.37, "peak": 36.24, "min": 21.27}, "VIN": {"avg": 65.45, "peak": 102.31, "min": 55.97}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.37, "energy_joules_est": 29.76, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-17T18:01:38.943263"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 956.414, "latencies_ms": [956.414], "images_per_second": 1.046, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The tennis player is positioned near the center of the image, facing the left side of the court. The tennis racket is held in his right hand, positioned in the foreground. The tennis court extends into the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.51, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 65.54, "peak": 92.77, "min": 57.17}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.51, "energy_joules_est": 26.32, "sample_count": 7, "duration_seconds": 0.957}, "timestamp": "2026-01-17T18:01:39.905165"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 910.389, "latencies_ms": [910.389], "images_per_second": 1.098, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "A young man is playing tennis on a green court at night. He holds a tennis racket and appears to be preparing to hit a ball. The court is enclosed by a fence, and there are signs visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.75, "peak": 35.06, "min": 23.23}, "VIN": {"avg": 65.23, "peak": 97.91, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 26.2, "sample_count": 6, "duration_seconds": 0.911}, "timestamp": "2026-01-17T18:01:40.822334"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 917.49, "latencies_ms": [917.49], "images_per_second": 1.09, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The tennis court is green and appears to be well-lit by artificial lighting. The court's surface is smooth and appears to be made of a durable material. The overall scene suggests an outdoor tennis court at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.8, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.63, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 64.71, "peak": 91.68, "min": 55.03}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 25.36, "sample_count": 7, "duration_seconds": 0.918}, "timestamp": "2026-01-17T18:01:41.746859"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 815.702, "latencies_ms": [815.702], "images_per_second": 1.226, "prompt_tokens": 8, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Two skiers, one in a black jacket and the other in a white and gray outfit, greet each other with a high-five while standing near a blue fence on a snowy slope.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.34, "peak": 35.45, "min": 23.23}, "VIN": {"avg": 63.82, "peak": 81.58, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.34, "energy_joules_est": 23.94, "sample_count": 6, "duration_seconds": 0.816}, "timestamp": "2026-01-17T18:01:42.572706"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1213.471, "latencies_ms": [1213.471], "images_per_second": 0.824, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "helmet: 2\ngloves: 2\nski poles: 4\nskis: 2\nsnow: 6\nbib: 1\ngoggles: 2\nbib: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 27.17, "peak": 36.23, "min": 20.87}, "VIN": {"avg": 65.73, "peak": 96.18, "min": 60.71}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.17, "energy_joules_est": 32.98, "sample_count": 9, "duration_seconds": 1.214}, "timestamp": "2026-01-17T18:01:43.792117"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 690.819, "latencies_ms": [690.819], "images_per_second": 1.448, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the skiers interacting between them. The skiers are located in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 29.77, "peak": 34.26, "min": 24.82}, "VIN": {"avg": 66.96, "peak": 91.15, "min": 59.86}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.77, "energy_joules_est": 20.57, "sample_count": 5, "duration_seconds": 0.691}, "timestamp": "2026-01-17T18:01:44.488647"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 706.316, "latencies_ms": [706.316], "images_per_second": 1.416, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two skiers are congratulating each other after completing a race. They are standing on a snowy slope surrounded by ski equipment and barriers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 31.82, "peak": 37.41, "min": 25.99}, "VIN": {"avg": 64.09, "peak": 84.96, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 31.82, "energy_joules_est": 22.48, "sample_count": 5, "duration_seconds": 0.707}, "timestamp": "2026-01-17T18:01:45.201080"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1263.038, "latencies_ms": [1263.038], "images_per_second": 0.792, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The skiers are wearing bright, colorful gear that stands out against the snowy background. The lighting is bright enough to illuminate their faces and equipment clearly. The skiers are wearing helmets and goggles, indicating safety precautions. The snow appears well-groomed and relatively undisturbed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.66, "peak": 37.41, "min": 20.87}, "VIN": {"avg": 62.67, "peak": 77.25, "min": 55.14}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.66, "energy_joules_est": 34.94, "sample_count": 9, "duration_seconds": 1.263}, "timestamp": "2026-01-17T18:01:46.470231"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 607.254, "latencies_ms": [607.254], "images_per_second": 1.647, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A large blue semi-truck is driving down a wet residential street, passing parked cars and houses.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 29.44, "peak": 33.08, "min": 25.6}, "VIN": {"avg": 67.91, "peak": 91.91, "min": 53.89}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.44, "energy_joules_est": 17.89, "sample_count": 4, "duration_seconds": 0.608}, "timestamp": "2026-01-17T18:01:47.089236"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1287.558, "latencies_ms": [1287.558], "images_per_second": 0.777, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "Truck: 1\nCar: 1\nBus: 1\nTrees: 2\nHouses: 2\nStreet: 2\nSidewalk: 2\nGrass: 2\nLamp post: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.43, "peak": 37.01, "min": 20.49}, "VIN": {"avg": 62.71, "peak": 81.64, "min": 54.88}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.43, "energy_joules_est": 34.04, "sample_count": 10, "duration_seconds": 1.288}, "timestamp": "2026-01-17T18:01:48.383175"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 837.889, "latencies_ms": [837.889], "images_per_second": 1.193, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main object is a blue semi-truck driving on the right side of the road, moving away from the viewer. The background includes residential houses, trees, and other vehicles, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.23, "peak": 34.28, "min": 22.85}, "VIN": {"avg": 61.94, "peak": 81.71, "min": 52.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 23.67, "sample_count": 6, "duration_seconds": 0.839}, "timestamp": "2026-01-17T18:01:49.227391"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 874.289, "latencies_ms": [874.289], "images_per_second": 1.144, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A large blue semi-truck is driving down a wet residential street, passing parked cars and houses. The scene suggests a typical suburban or urban area with residential homes and businesses.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.02, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 67.8, "peak": 97.99, "min": 61.14}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.02, "energy_joules_est": 25.38, "sample_count": 6, "duration_seconds": 0.875}, "timestamp": "2026-01-17T18:01:50.108242"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1077.685, "latencies_ms": [1077.685], "images_per_second": 0.928, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The truck is primarily blue. The scene is lit by natural daylight, indicating a sunny day. The truck's materials appear to be sturdy and durable, suitable for heavy hauling. The weather appears to be pleasant, with no visible signs of rain or strong winds.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.94, "peak": 34.65, "min": 21.28}, "VIN": {"avg": 63.23, "peak": 94.15, "min": 52.04}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 26.94, "energy_joules_est": 29.04, "sample_count": 8, "duration_seconds": 1.078}, "timestamp": "2026-01-17T18:01:51.191897"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 552.992, "latencies_ms": [552.992], "images_per_second": 1.808, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A large commercial airplane is approaching the runway, moving towards the viewer with visible landing gear.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 30.13, "peak": 34.27, "min": 26.0}, "VIN": {"avg": 68.39, "peak": 90.46, "min": 59.22}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.13, "energy_joules_est": 16.68, "sample_count": 4, "duration_seconds": 0.554}, "timestamp": "2026-01-17T18:01:51.756764"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 892.5, "latencies_ms": [892.5], "images_per_second": 1.12, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "airplane: 1\nrunway: 6\nlights: 6\nmountains: 2\ngrass: 2\nfog: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.93, "peak": 37.41, "min": 23.64}, "VIN": {"avg": 63.55, "peak": 75.74, "min": 59.58}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 29.93, "energy_joules_est": 26.72, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T18:01:52.655510"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1007.202, "latencies_ms": [1007.202], "images_per_second": 0.993, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The airplane is positioned in the background, appearing to be approaching the runway. The foreground features several orange and silver lights, possibly for guiding or signaling aircraft. The background includes hazy mountains, suggesting the airport is located in a mountainous region.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.79, "peak": 35.04, "min": 22.07}, "VIN": {"avg": 65.0, "peak": 91.29, "min": 57.4}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.79, "energy_joules_est": 28.0, "sample_count": 7, "duration_seconds": 1.008}, "timestamp": "2026-01-17T18:01:53.669168"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 759.256, "latencies_ms": [759.256], "images_per_second": 1.317, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A large passenger jet is approaching a runway, possibly taxiing. The scene is set in a mountainous area, with hazy skies and distant mountains visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.71, "peak": 34.68, "min": 24.83}, "VIN": {"avg": 64.33, "peak": 77.3, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.71, "energy_joules_est": 22.57, "sample_count": 5, "duration_seconds": 0.76}, "timestamp": "2026-01-17T18:01:54.434392"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 829.839, "latencies_ms": [829.839], "images_per_second": 1.205, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The plane is primarily white and blue. The runway is wet, indicating recent rain or moisture. The scene is illuminated by several orange floodlights, enhancing visibility on the runway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.42, "peak": 36.26, "min": 23.64}, "VIN": {"avg": 65.48, "peak": 93.28, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.42, "energy_joules_est": 24.43, "sample_count": 6, "duration_seconds": 0.83}, "timestamp": "2026-01-17T18:01:55.270352"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 515.184, "latencies_ms": [515.184], "images_per_second": 1.941, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Two women are standing on a sandy beach, one holding a tennis racket high in the air, while the other looks on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25550.0, "ram_available_mb": 100222.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 25.87, "peak": 27.97, "min": 24.03}, "VIN": {"avg": 60.73, "peak": 62.79, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 25.87, "energy_joules_est": 13.35, "sample_count": 3, "duration_seconds": 0.516}, "timestamp": "2026-01-17T18:01:55.795530"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1119.888, "latencies_ms": [1119.888], "images_per_second": 0.893, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "American flag: 1\nLife guard tower: 1\nTennis racket: 1\nTowel: 1\nWater bottle: 1\nPeople: 6\nTrees: 1\nMountains: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25550.0, "ram_available_mb": 100222.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25550.0, "ram_available_mb": 100222.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.9, "peak": 27.97, "min": 20.09}, "VIN": {"avg": 61.27, "peak": 65.53, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.9, "energy_joules_est": 25.66, "sample_count": 8, "duration_seconds": 1.12}, "timestamp": "2026-01-17T18:01:56.921312"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 968.12, "latencies_ms": [968.12], "images_per_second": 1.033, "prompt_tokens": 25, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The woman with the tennis racket is positioned in the foreground, slightly to the right of the main subjects. The lifeguard stand and American flag are in the background, further away from the main subjects. The scene appears to be set on a sandy beach, with mountains visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25550.0, "ram_available_mb": 100222.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25550.2, "ram_available_mb": 100221.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.39, "peak": 25.99, "min": 20.09}, "VIN": {"avg": 58.63, "peak": 62.09, "min": 55.22}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 22.39, "energy_joules_est": 21.68, "sample_count": 7, "duration_seconds": 0.968}, "timestamp": "2026-01-17T18:01:57.895233"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 650.853, "latencies_ms": [650.853], "images_per_second": 1.536, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene takes place on a sandy beach with mountains in the background. Two people are in the foreground, one holding a tennis racket and the other appearing to be in a defensive or aggressive stance.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25550.2, "ram_available_mb": 100221.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25550.2, "ram_available_mb": 100221.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.63, "peak": 25.99, "min": 21.66}, "VIN": {"avg": 62.57, "peak": 65.71, "min": 60.84}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.63, "energy_joules_est": 15.39, "sample_count": 4, "duration_seconds": 0.651}, "timestamp": "2026-01-17T18:01:58.556308"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 927.353, "latencies_ms": [927.353], "images_per_second": 1.078, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The scene is bathed in warm sunlight, creating a vibrant atmosphere. The colors are predominantly earthy tones, complementing the natural surroundings. The lighting is soft and diffused, enhancing the overall ambiance of the beach setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25550.2, "ram_available_mb": 100221.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.74, "peak": 26.79, "min": 20.09}, "VIN": {"avg": 59.91, "peak": 66.67, "min": 55.27}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.74, "energy_joules_est": 21.09, "sample_count": 7, "duration_seconds": 0.928}, "timestamp": "2026-01-17T18:01:59.493748"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 700.534, "latencies_ms": [700.534], "images_per_second": 1.427, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A male tennis player, dressed in blue and white, stands on a blue tennis court, holding a tennis racket and appearing contemplative.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.59, "peak": 33.08, "min": 24.02}, "VIN": {"avg": 68.4, "peak": 97.22, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.59, "energy_joules_est": 20.04, "sample_count": 5, "duration_seconds": 0.701}, "timestamp": "2026-01-17T18:02:00.204554"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1356.287, "latencies_ms": [1356.287], "images_per_second": 0.737, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis shoes: 2\nTennis court: 1\nTennis player: 1\nTennis headband: 1\nTennis shorts: 1\nTennis socks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.11, "peak": 35.85, "min": 20.48}, "VIN": {"avg": 61.14, "peak": 79.06, "min": 53.19}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.11, "energy_joules_est": 35.42, "sample_count": 10, "duration_seconds": 1.357}, "timestamp": "2026-01-17T18:02:01.566928"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 982.34, "latencies_ms": [982.34], "images_per_second": 1.018, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The tennis player is positioned near the center of the image, facing the left side of the court. The tennis ball is held in his right hand, further away from the player. The tennis court extends into the background, creating a sense of depth and space.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 14.91, "min": 14.3}, "VDD_GPU": {"avg": 27.06, "peak": 33.47, "min": 22.06}, "VIN": {"avg": 63.2, "peak": 80.52, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.06, "energy_joules_est": 26.59, "sample_count": 7, "duration_seconds": 0.983}, "timestamp": "2026-01-17T18:02:02.555101"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 946.556, "latencies_ms": [946.556], "images_per_second": 1.056, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A tennis player, dressed in blue and white, is seen on a blue tennis court, appearing somewhat dejected or contemplative. He holds a tennis racket in his right hand and looks downwards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.34, "peak": 34.24, "min": 22.06}, "VIN": {"avg": 64.84, "peak": 90.88, "min": 59.06}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.34, "energy_joules_est": 25.89, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T18:02:03.507435"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1128.577, "latencies_ms": [1128.577], "images_per_second": 0.886, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The tennis court is painted a light blue color. The lighting in the image is bright and creates a strong contrast with the blue surface. The tennis player is wearing white shorts and a light blue shirt. The player's shadow is cast on the court, indicating the sun's position.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.63, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 62.17, "peak": 72.45, "min": 57.33}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 26.63, "energy_joules_est": 30.06, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T18:02:04.641800"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 614.201, "latencies_ms": [614.201], "images_per_second": 1.628, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A woman in a pink shirt and khaki shorts is standing in a kitchen, preparing food on a black stove.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.61, "min": 14.2}, "VDD_GPU": {"avg": 30.04, "peak": 33.89, "min": 26.0}, "VIN": {"avg": 69.66, "peak": 96.63, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 30.04, "energy_joules_est": 18.47, "sample_count": 4, "duration_seconds": 0.615}, "timestamp": "2026-01-17T18:02:05.266880"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1418.657, "latencies_ms": [1418.657], "images_per_second": 0.705, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "oven: 2\npan: 1\nkettle: 1\nsaucepan: 1\nwooden spoon: 1\nwooden spoon rest: 1\nwooden spoon: 1\nwooden spoon: 1\nwooden spoon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.22, "min": 14.1}, "VDD_GPU": {"avg": 25.85, "peak": 37.03, "min": 20.09}, "VIN": {"avg": 62.88, "peak": 82.92, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.85, "energy_joules_est": 36.69, "sample_count": 11, "duration_seconds": 1.419}, "timestamp": "2026-01-17T18:02:06.692162"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1021.177, "latencies_ms": [1021.177], "images_per_second": 0.979, "prompt_tokens": 25, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The woman is positioned to the left of the stove, seemingly preparing food. The stove occupies the central foreground, extending from the left edge to the bottom right of the image. The kitchen area extends from the left side to the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.3}, "VDD_GPU": {"avg": 26.44, "peak": 33.48, "min": 21.27}, "VIN": {"avg": 64.13, "peak": 89.8, "min": 56.7}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.44, "energy_joules_est": 27.01, "sample_count": 8, "duration_seconds": 1.021}, "timestamp": "2026-01-17T18:02:07.719229"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 772.511, "latencies_ms": [772.511], "images_per_second": 1.294, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A woman is cooking on a large, black wood-burning stove in a cozy kitchen. The kitchen features a large black range hood and various kitchen items and utensils.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25548.3, "ram_available_mb": 100223.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.54, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 65.16, "peak": 81.88, "min": 57.33}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 22.84, "sample_count": 5, "duration_seconds": 0.773}, "timestamp": "2026-01-17T18:02:08.498021"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 988.132, "latencies_ms": [988.132], "images_per_second": 1.012, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The kitchen features a black wood-burning stove with exposed brick foundations. The lighting is warm and inviting, creating a cozy atmosphere. The materials include dark wood, metal, and brick, contributing to the rustic charm of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.3, "ram_available_mb": 100223.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25548.1, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.91, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 63.09, "peak": 82.34, "min": 55.48}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 27.59, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T18:02:09.492416"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 662.576, "latencies_ms": [662.576], "images_per_second": 1.509, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two giraffes stand tall and majestic in a zoo enclosure, surrounded by rocks and trees, with a tan building in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25548.1, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25548.1, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 30.64, "peak": 34.66, "min": 25.6}, "VIN": {"avg": 63.27, "peak": 79.7, "min": 55.17}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.64, "energy_joules_est": 20.32, "sample_count": 5, "duration_seconds": 0.663}, "timestamp": "2026-01-17T18:02:10.167027"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 913.215, "latencies_ms": [913.215], "images_per_second": 1.095, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "giraffe: 2\nbuilding: 1\nrocks: 10\ntrees: 5\ngrass: 2\nfence: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25548.1, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.65, "peak": 37.41, "min": 22.85}, "VIN": {"avg": 64.21, "peak": 90.8, "min": 52.89}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.65, "energy_joules_est": 27.09, "sample_count": 7, "duration_seconds": 0.914}, "timestamp": "2026-01-17T18:02:11.087205"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 886.276, "latencies_ms": [886.276], "images_per_second": 1.128, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with the giraffes standing relatively close to the building. The background features trees and a grassy area, creating a natural setting for the giraffes within the enclosure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 30.08, "peak": 35.85, "min": 24.04}, "VIN": {"avg": 64.68, "peak": 85.52, "min": 57.24}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.08, "energy_joules_est": 26.68, "sample_count": 6, "duration_seconds": 0.887}, "timestamp": "2026-01-17T18:02:11.984214"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 647.416, "latencies_ms": [647.416], "images_per_second": 1.545, "prompt_tokens": 19, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Two giraffes stand in a zoo enclosure, surrounded by rocks and trees. A tan building is visible in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 32.39, "peak": 35.83, "min": 27.97}, "VIN": {"avg": 63.59, "peak": 84.41, "min": 51.25}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.39, "energy_joules_est": 20.98, "sample_count": 4, "duration_seconds": 0.648}, "timestamp": "2026-01-17T18:02:12.637607"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1182.469, "latencies_ms": [1182.469], "images_per_second": 0.846, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The giraffes exhibit a rich brown coloration that stands out against the natural backdrop. Their long necks and legs are illuminated by natural lighting, creating a warm and inviting atmosphere. The giraffes are standing on a rocky terrain, which adds texture and depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.0, "peak": 37.41, "min": 21.27}, "VIN": {"avg": 64.23, "peak": 95.49, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.0, "energy_joules_est": 33.12, "sample_count": 9, "duration_seconds": 1.183}, "timestamp": "2026-01-17T18:02:13.826692"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 653.472, "latencies_ms": [653.472], "images_per_second": 1.53, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A young baseball player in a green and yellow uniform swings a blue bat, anticipating the incoming ball as it flies towards him.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.2}, "VDD_GPU": {"avg": 30.63, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 68.37, "peak": 96.19, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 20.03, "sample_count": 4, "duration_seconds": 0.654}, "timestamp": "2026-01-17T18:02:14.493733"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1502.916, "latencies_ms": [1502.916], "images_per_second": 0.665, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 1, "output_text": "baseball bat: 1\nbaseball: 1\nbaseball glove: 1\nbaseball: 1\nbaseball field: 1\nbaseball: 1\nbaseball diamond: 1\nchain link fence: 2\ngrass: 2\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 25.71, "peak": 36.62, "min": 20.1}, "VIN": {"avg": 63.74, "peak": 97.45, "min": 51.07}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.71, "energy_joules_est": 38.65, "sample_count": 11, "duration_seconds": 1.503}, "timestamp": "2026-01-17T18:02:16.007529"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 831.924, "latencies_ms": [831.924], "images_per_second": 1.202, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The batter is positioned in the foreground, facing the incoming baseball. The baseball is in the air, near the batter. The background features a chain-link fence, grass, and a parked vehicle.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 27.83, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 67.18, "peak": 92.37, "min": 59.97}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.83, "energy_joules_est": 23.16, "sample_count": 6, "duration_seconds": 0.832}, "timestamp": "2026-01-17T18:02:16.845766"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 734.181, "latencies_ms": [734.181], "images_per_second": 1.362, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A young baseball player is swinging a bat at a ball during a game. The setting is a baseball field with a chain-link fence and grassy areas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 29.93, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 68.42, "peak": 96.77, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 21.98, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T18:02:17.585680"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 798.145, "latencies_ms": [798.145], "images_per_second": 1.253, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The batter is wearing a green and gold uniform. The lighting appears to be natural sunlight, creating a bright and clear atmosphere. The baseball bat is blue, and the ground is dirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.1, "ram_available_mb": 100224.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.21, "peak": 35.44, "min": 23.64}, "VIN": {"avg": 63.98, "peak": 79.17, "min": 56.59}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.21, "energy_joules_est": 23.33, "sample_count": 6, "duration_seconds": 0.799}, "timestamp": "2026-01-17T18:02:18.389727"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 649.858, "latencies_ms": [649.858], "images_per_second": 1.539, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A vintage car is parked next to a red and white bus on a cobblestone street, with several people walking nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.24, "peak": 35.44, "min": 25.21}, "VIN": {"avg": 64.31, "peak": 77.74, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.24, "energy_joules_est": 19.67, "sample_count": 5, "duration_seconds": 0.651}, "timestamp": "2026-01-17T18:02:19.051081"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1116.918, "latencies_ms": [1116.918], "images_per_second": 0.895, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "car: 3\nmotorcycle: 4\ntank: 1\nbus: 1\nhistorical vehicle: 1\nhistorical building: 1\ntrees: 4\nflags: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.72, "peak": 36.62, "min": 21.66}, "VIN": {"avg": 62.11, "peak": 77.64, "min": 53.12}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.72, "energy_joules_est": 30.98, "sample_count": 8, "duration_seconds": 1.118}, "timestamp": "2026-01-17T18:02:20.175212"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 805.845, "latencies_ms": [805.845], "images_per_second": 1.241, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The black vintage cars are positioned in the foreground, slightly to the left of the center. The cobblestone street extends into the background, separating the vehicles from the people and the buildings.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 34.27, "min": 23.24}, "VIN": {"avg": 63.55, "peak": 83.63, "min": 55.93}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 22.88, "sample_count": 6, "duration_seconds": 0.807}, "timestamp": "2026-01-17T18:02:20.987641"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1023.507, "latencies_ms": [1023.507], "images_per_second": 0.977, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a vintage car show or gathering, featuring various antique vehicles parked on a cobblestone street. A red double-decker bus and several vintage motorcycles are also present, alongside people admiring the vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.85, "peak": 35.06, "min": 22.05}, "VIN": {"avg": 64.53, "peak": 89.76, "min": 57.76}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 28.52, "sample_count": 7, "duration_seconds": 1.024}, "timestamp": "2026-01-17T18:02:22.017229"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 844.365, "latencies_ms": [844.365], "images_per_second": 1.184, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The cobblestone street is illuminated by sunlight, creating a warm and inviting atmosphere. The vehicles, including vintage cars and buses, are parked in a designated area, suggesting an event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25548.8, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.3}, "VDD_GPU": {"avg": 28.49, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 64.74, "peak": 86.56, "min": 56.88}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.49, "energy_joules_est": 24.07, "sample_count": 6, "duration_seconds": 0.845}, "timestamp": "2026-01-17T18:02:22.867477"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 632.211, "latencies_ms": [632.211], "images_per_second": 1.582, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "The setting sun casts a warm glow over the city street, illuminating the blurred lights and creating a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25548.8, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 31.11, "peak": 35.04, "min": 26.79}, "VIN": {"avg": 69.96, "peak": 95.75, "min": 59.7}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.11, "energy_joules_est": 19.69, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T18:02:23.508848"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1041.865, "latencies_ms": [1041.865], "images_per_second": 0.96, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Parking meter: 2\nSunset: 1\nBuildings: 2\nLights: 4\nCar: 1\nRoad: 1\nSky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.67, "peak": 36.62, "min": 21.66}, "VIN": {"avg": 63.81, "peak": 81.57, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.67, "energy_joules_est": 28.84, "sample_count": 8, "duration_seconds": 1.042}, "timestamp": "2026-01-17T18:02:24.556798"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 839.325, "latencies_ms": [839.325], "images_per_second": 1.191, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The parking meters are positioned in the foreground, slightly to the right of the viewer. The setting sun in the background creates a warm glow and soft light, enhancing the overall ambiance of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.29, "peak": 33.86, "min": 23.23}, "VIN": {"avg": 64.97, "peak": 82.57, "min": 59.01}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 23.76, "sample_count": 6, "duration_seconds": 0.84}, "timestamp": "2026-01-17T18:02:25.401788"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 716.52, "latencies_ms": [716.52], "images_per_second": 1.396, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The scene is set at sunset, with two parking meters in the foreground. The sun is setting in the background, casting a warm glow over the urban landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.8, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.93, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 65.07, "peak": 83.35, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 21.46, "sample_count": 5, "duration_seconds": 0.717}, "timestamp": "2026-01-17T18:02:26.124906"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 883.539, "latencies_ms": [883.539], "images_per_second": 1.132, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The parking meters are dark brown or black. The lighting suggests it's either sunrise or sunset, creating a warm, golden glow. The scene appears to be outdoors, possibly on a street or in a parking lot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.8, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 29.21, "peak": 35.85, "min": 23.64}, "VIN": {"avg": 66.62, "peak": 97.18, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.21, "energy_joules_est": 25.82, "sample_count": 6, "duration_seconds": 0.884}, "timestamp": "2026-01-17T18:02:27.018741"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 656.348, "latencies_ms": [656.348], "images_per_second": 1.524, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A couple stands next to a large brown suitcase with travel stickers, positioned in front of a Fidelity Investments building.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.8, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.82, "peak": 34.66, "min": 26.79}, "VIN": {"avg": 67.11, "peak": 84.54, "min": 59.29}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.82, "energy_joules_est": 20.24, "sample_count": 4, "duration_seconds": 0.657}, "timestamp": "2026-01-17T18:02:27.686384"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1141.0, "latencies_ms": [1141.0], "images_per_second": 0.876, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "suitcase: 1\nstatue: 1\nman: 2\nwoman: 2\nman: 1\nwoman: 1\nman: 1\nwoman: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.8, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.86, "peak": 37.03, "min": 21.66}, "VIN": {"avg": 65.78, "peak": 91.24, "min": 60.24}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.86, "energy_joules_est": 31.8, "sample_count": 8, "duration_seconds": 1.141}, "timestamp": "2026-01-17T18:02:28.837458"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 946.604, "latencies_ms": [946.604], "images_per_second": 1.056, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The large suitcase is positioned in the foreground, slightly to the left of the couple. The couple stands in the background, slightly to the right of the suitcase. The suitcase and couple are situated on a raised platform or plinth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.35, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 63.09, "peak": 77.16, "min": 59.49}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.35, "energy_joules_est": 25.9, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T18:02:29.790804"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1084.6, "latencies_ms": [1084.6], "images_per_second": 0.922, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A large brown suitcase adorned with various travel stickers sits on a pedestal in a public area, possibly a plaza or square. A couple stands nearby, seemingly admiring the suitcase. A Fidelity Investments sign is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.78, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 63.78, "peak": 97.91, "min": 54.53}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.78, "energy_joules_est": 29.06, "sample_count": 8, "duration_seconds": 1.085}, "timestamp": "2026-01-17T18:02:30.881964"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1147.565, "latencies_ms": [1147.565], "images_per_second": 0.871, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The suitcase is brown and adorned with various stickers, showcasing a mix of colors. The lighting in the image is bright, highlighting the details of the suitcase and the stickers. The suitcase appears to be made of leather or a similar material. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 25.91, "peak": 33.86, "min": 20.88}, "VIN": {"avg": 65.39, "peak": 97.3, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.91, "energy_joules_est": 29.74, "sample_count": 9, "duration_seconds": 1.148}, "timestamp": "2026-01-17T18:02:32.039593"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 867.728, "latencies_ms": [867.728], "images_per_second": 1.152, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The dish features a medley of grilled chicken, saut\u00e9ed mushrooms, and fresh broccoli, all topped with herbs and spices, creating a visually appealing and flavorful meal.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.69, "peak": 33.08, "min": 22.84}, "VIN": {"avg": 63.73, "peak": 81.69, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.69, "energy_joules_est": 24.05, "sample_count": 6, "duration_seconds": 0.869}, "timestamp": "2026-01-17T18:02:32.917983"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1064.133, "latencies_ms": [1064.133], "images_per_second": 0.94, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Mushrooms: 8\nBroccoli: 2\nChicken: 2\nParsley: 2\nBlack pepper: 2\nWhite wine: 1\nWhite sauce: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.28, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 62.95, "peak": 83.16, "min": 56.43}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.28, "energy_joules_est": 29.04, "sample_count": 8, "duration_seconds": 1.065}, "timestamp": "2026-01-17T18:02:33.988550"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 825.215, "latencies_ms": [825.215], "images_per_second": 1.212, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the mushrooms and broccoli slightly behind and to the right. The mushrooms are located primarily on the left side of the image, while the broccoli is situated near the top right corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 28.36, "peak": 34.65, "min": 23.23}, "VIN": {"avg": 63.44, "peak": 81.66, "min": 58.04}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 23.42, "sample_count": 6, "duration_seconds": 0.826}, "timestamp": "2026-01-17T18:02:34.820781"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 862.547, "latencies_ms": [862.547], "images_per_second": 1.159, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The scene features a plate of grilled fish with mushrooms and broccoli, garnished with fresh parsley. The dish is presented on a dark surface, possibly a table or countertop.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.82, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 65.93, "peak": 90.29, "min": 59.22}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.82, "energy_joules_est": 24.88, "sample_count": 6, "duration_seconds": 0.863}, "timestamp": "2026-01-17T18:02:35.690003"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1057.157, "latencies_ms": [1057.157], "images_per_second": 0.946, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The dish features a mix of vibrant colors, including green broccoli, white fish, and brown mushrooms. The lighting is warm and inviting, enhancing the visual appeal of the meal. The fish appears to be seasoned with herbs and spices, and the mushrooms have a slightly glossy appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.97, "peak": 35.44, "min": 21.26}, "VIN": {"avg": 65.6, "peak": 97.19, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.97, "energy_joules_est": 28.52, "sample_count": 8, "duration_seconds": 1.057}, "timestamp": "2026-01-17T18:02:36.753024"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 553.133, "latencies_ms": [553.133], "images_per_second": 1.808, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A wooden crate brimming with fresh, vibrant vegetables is displayed, showcasing a colorful array of produce for sale.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 24.23, "peak": 26.79, "min": 22.06}, "VIN": {"avg": 59.93, "peak": 60.87, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.23, "energy_joules_est": 13.41, "sample_count": 4, "duration_seconds": 0.553}, "timestamp": "2026-01-17T18:02:37.319996"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 965.798, "latencies_ms": [965.798], "images_per_second": 1.035, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "carrots: 10\nbroccoli: 2\ncabbage: 1\nlettuce: 1\nasparagus: 1\nonions: 4\nspinach: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.3, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 22.79, "peak": 26.79, "min": 20.48}, "VIN": {"avg": 61.32, "peak": 63.69, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.79, "energy_joules_est": 22.02, "sample_count": 7, "duration_seconds": 0.966}, "timestamp": "2026-01-17T18:02:38.291682"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 591.445, "latencies_ms": [591.445], "images_per_second": 1.691, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The carrots are positioned in the foreground, slightly to the right of the image. The broccoli and cabbage are placed in the background, further back from the carrots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.82, "peak": 26.0, "min": 22.05}, "VIN": {"avg": 61.34, "peak": 62.81, "min": 59.58}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.82, "energy_joules_est": 14.09, "sample_count": 4, "duration_seconds": 0.592}, "timestamp": "2026-01-17T18:02:38.890308"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 941.206, "latencies_ms": [941.206], "images_per_second": 1.062, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The scene depicts a vibrant display of fresh produce at a market or grocery store. Various vegetables, including carrots, leafy greens, and cabbage, are neatly arranged in baskets and crates, showcasing a wide selection of healthy options.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25549.2, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 22.78, "peak": 26.77, "min": 20.48}, "VIN": {"avg": 61.2, "peak": 63.77, "min": 57.17}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.78, "energy_joules_est": 21.45, "sample_count": 7, "duration_seconds": 0.942}, "timestamp": "2026-01-17T18:02:39.839694"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 534.647, "latencies_ms": [534.647], "images_per_second": 1.87, "prompt_tokens": 18, "response_tokens_est": 24, "n_tiles": 1, "output_text": "The carrots are bright orange and appear to be freshly picked. The vegetables are arranged in baskets with natural light illuminating them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.2, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.2, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 24.29, "peak": 26.0, "min": 22.85}, "VIN": {"avg": 62.61, "peak": 63.3, "min": 61.27}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.29, "energy_joules_est": 12.99, "sample_count": 3, "duration_seconds": 0.535}, "timestamp": "2026-01-17T18:02:40.384112"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 513.299, "latencies_ms": [513.299], "images_per_second": 1.948, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A conveyor belt is transporting freshly made glazed doughnuts through a bustling bakery shop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.2, "ram_available_mb": 100222.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 24.94, "peak": 26.77, "min": 23.23}, "VIN": {"avg": 62.28, "peak": 64.83, "min": 60.12}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.57, "min": 14.17}}, "power_watts_avg": 24.94, "energy_joules_est": 12.82, "sample_count": 3, "duration_seconds": 0.514}, "timestamp": "2026-01-17T18:02:40.907817"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 832.402, "latencies_ms": [832.402], "images_per_second": 1.201, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "donuts: 10\nconveyor belt: 5\noven: 1\nshelves: 1\npeople: 4\nsigns: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 23.7, "peak": 27.57, "min": 20.89}, "VIN": {"avg": 59.67, "peak": 63.42, "min": 53.25}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 23.7, "energy_joules_est": 19.73, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T18:02:41.746086"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 768.689, "latencies_ms": [768.689], "images_per_second": 1.301, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the conveyor belt and doughnut-making equipment extending into the background. The doughnuts are moving along the conveyor belt, illustrating the process of production.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.7, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 23.47, "peak": 26.38, "min": 21.27}, "VIN": {"avg": 60.6, "peak": 63.13, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.47, "energy_joules_est": 18.05, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T18:02:42.520761"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 870.661, "latencies_ms": [870.661], "images_per_second": 1.149, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The scene depicts a busy bakery or doughnut factory, with multiple conveyor belts transporting freshly made doughnuts. The environment is bustling with activity, with people working and customers browsing the offerings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.7, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25548.7, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.98, "peak": 26.39, "min": 20.48}, "VIN": {"avg": 59.43, "peak": 61.78, "min": 53.89}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 22.98, "energy_joules_est": 20.02, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T18:02:43.397341"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 775.753, "latencies_ms": [775.753], "images_per_second": 1.289, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The donuts on the conveyor belt are golden-brown in color. The lighting in the bakery is bright, illuminating the donuts and creating reflections in the glass window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.7, "ram_available_mb": 100223.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.78, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 58.53, "peak": 63.41, "min": 52.11}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 22.78, "energy_joules_est": 17.68, "sample_count": 6, "duration_seconds": 0.776}, "timestamp": "2026-01-17T18:02:44.178895"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 650.524, "latencies_ms": [650.524], "images_per_second": 1.537, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man in green is preparing to throw an orange frisbee in a wooded area, surrounded by tall trees and fallen leaves.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.23, "peak": 34.27, "min": 26.0}, "VIN": {"avg": 67.29, "peak": 84.44, "min": 60.66}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.23, "energy_joules_est": 19.68, "sample_count": 4, "duration_seconds": 0.651}, "timestamp": "2026-01-17T18:02:44.841217"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1098.665, "latencies_ms": [1098.665], "images_per_second": 0.91, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "tree: 1\nthrowing disc: 1\nperson: 1\nfrisbee: 1\nground: 1\nleaves: 1\nbranches: 1\nleaves: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.86, "peak": 36.62, "min": 21.66}, "VIN": {"avg": 62.06, "peak": 73.0, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.86, "energy_joules_est": 30.62, "sample_count": 8, "duration_seconds": 1.099}, "timestamp": "2026-01-17T18:02:45.945935"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 826.769, "latencies_ms": [826.769], "images_per_second": 1.21, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The man is positioned near the center of the image, slightly to the right of the fallen tree trunk. The ground is mostly covered in fallen leaves and twigs, creating a natural, wooded setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.15, "peak": 34.24, "min": 22.85}, "VIN": {"avg": 66.65, "peak": 94.68, "min": 59.39}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.15, "energy_joules_est": 23.28, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T18:02:46.777998"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 884.097, "latencies_ms": [884.097], "images_per_second": 1.131, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A man is playing disc golf in a wooded area. He's positioned near a fallen tree, preparing to throw a disc. The setting is natural, featuring pine trees and leaf-covered ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.0, "ram_available_mb": 100223.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25547.7, "ram_available_mb": 100224.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.08, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 63.61, "peak": 79.67, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 25.72, "sample_count": 6, "duration_seconds": 0.885}, "timestamp": "2026-01-17T18:02:47.669728"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1065.649, "latencies_ms": [1065.649], "images_per_second": 0.938, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene is bathed in natural daylight, creating a warm and inviting atmosphere. The ground is covered with fallen leaves, hinting at a recent rainfall or dampness. The presence of a fallen tree trunk adds a touch of rustic charm to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.7, "ram_available_mb": 100224.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.72, "peak": 34.65, "min": 21.27}, "VIN": {"avg": 62.88, "peak": 79.61, "min": 56.91}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.72, "energy_joules_est": 28.49, "sample_count": 8, "duration_seconds": 1.066}, "timestamp": "2026-01-17T18:02:48.741937"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 638.005, "latencies_ms": [638.005], "images_per_second": 1.567, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The bathroom features a white sink, toilet, and shower with striped curtains, complemented by beige tiles on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.23, "peak": 33.86, "min": 26.39}, "VIN": {"avg": 63.82, "peak": 76.1, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.23, "energy_joules_est": 19.3, "sample_count": 4, "duration_seconds": 0.638}, "timestamp": "2026-01-17T18:02:49.394477"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1632.06, "latencies_ms": [1632.06], "images_per_second": 0.613, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 1, "output_text": "sink: 1\ntoilet: 1\nshower: 1\nbathroom mirror: 1\nshower curtain: 1\ntoilet paper holder: 1\nshampoo bottle: 1\ncleaning solution: 1\ntowel rack: 1\ndoor handle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.22, "min": 14.0}, "VDD_GPU": {"avg": 25.24, "peak": 36.62, "min": 19.7}, "VIN": {"avg": 61.59, "peak": 84.18, "min": 53.24}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 25.24, "energy_joules_est": 41.21, "sample_count": 12, "duration_seconds": 1.633}, "timestamp": "2026-01-17T18:02:51.037495"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 709.284, "latencies_ms": [709.284], "images_per_second": 1.41, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The sink is located on the left side of the image, while the toilet is situated in the background. The bathroom is relatively narrow and positioned close to the shower and bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.59, "peak": 33.08, "min": 24.02}, "VIN": {"avg": 64.43, "peak": 83.67, "min": 57.44}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 20.29, "sample_count": 5, "duration_seconds": 0.71}, "timestamp": "2026-01-17T18:02:51.752567"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 978.079, "latencies_ms": [978.079], "images_per_second": 1.022, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The bathroom features a white sink, toilet, and bathtub with striped shower curtains. The walls are painted a light yellow, and the floor is tiled in a beige color. Various cleaning supplies are scattered across the countertop.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25548.6, "ram_available_mb": 100223.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25548.5, "ram_available_mb": 100223.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.47, "peak": 36.23, "min": 22.44}, "VIN": {"avg": 63.63, "peak": 84.14, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.47, "energy_joules_est": 27.86, "sample_count": 7, "duration_seconds": 0.979}, "timestamp": "2026-01-17T18:02:52.737801"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 896.876, "latencies_ms": [896.876], "images_per_second": 1.115, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The bathroom features a beige color scheme with blue accents. The lighting is soft and diffused, creating a calm atmosphere. The materials appear to be standard bathroom fixtures. The floor is tiled in a light beige color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.5, "ram_available_mb": 100223.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 28.82, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 65.48, "peak": 87.82, "min": 57.85}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 25.86, "sample_count": 6, "duration_seconds": 0.897}, "timestamp": "2026-01-17T18:02:53.644788"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 848.45, "latencies_ms": [848.45], "images_per_second": 1.179, "prompt_tokens": 8, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The kitchen features a wooden island with a sink, silver faucet, and black countertop, complemented by wooden cabinets, a window, and a dining area with wooden chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.55, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 64.03, "peak": 85.4, "min": 55.47}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.55, "energy_joules_est": 24.24, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T18:02:54.503388"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1301.187, "latencies_ms": [1301.187], "images_per_second": 0.769, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "kitchen sink: 1\nwooden cabinets: 4\ncountertop: 2\nwooden island: 1\nfaucet: 1\nwindow: 2\ntable: 1\nchairs: 2\ntoaster: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.22, "min": 14.1}, "VDD_GPU": {"avg": 25.68, "peak": 35.04, "min": 20.48}, "VIN": {"avg": 62.67, "peak": 83.41, "min": 56.79}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.68, "energy_joules_est": 33.42, "sample_count": 10, "duration_seconds": 1.302}, "timestamp": "2026-01-17T18:02:55.814778"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1060.345, "latencies_ms": [1060.345], "images_per_second": 0.943, "prompt_tokens": 25, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The sink is located in the foreground, positioned between the left and central sides of the image. The wooden counter extends from the sink towards the background, occupying a significant portion of the foreground. The kitchen area extends beyond the counter and sink area, with chairs and a dining table visible in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.14, "peak": 32.68, "min": 21.27}, "VIN": {"avg": 65.09, "peak": 98.32, "min": 54.8}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.14, "energy_joules_est": 27.73, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T18:02:56.880926"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1083.527, "latencies_ms": [1083.527], "images_per_second": 0.923, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The kitchen features a modern design with light wood cabinetry, a dark countertop, and a large island with a sink and faucet. A dining area with wooden chairs and a table is visible in the background, adjacent to windows that offer natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25548.3, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.59, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 62.64, "peak": 78.5, "min": 56.14}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.59, "energy_joules_est": 28.82, "sample_count": 8, "duration_seconds": 1.084}, "timestamp": "2026-01-17T18:02:57.971172"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 980.562, "latencies_ms": [980.562], "images_per_second": 1.02, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The kitchen features a light wood countertop with a dark granite top. The lighting is bright, likely from overhead fixtures, creating a well-lit space. The materials appear to be natural wood and granite, contributing to the warm and inviting ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.3, "ram_available_mb": 100223.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25548.3, "ram_available_mb": 100223.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.28, "peak": 33.86, "min": 22.05}, "VIN": {"avg": 62.95, "peak": 78.73, "min": 58.04}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.28, "energy_joules_est": 26.76, "sample_count": 7, "duration_seconds": 0.981}, "timestamp": "2026-01-17T18:02:58.957899"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 559.062, "latencies_ms": [559.062], "images_per_second": 1.789, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A child is sleeping peacefully on a bed covered with a black blanket with white daisy patterns.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.3, "ram_available_mb": 100223.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.23, "peak": 34.26, "min": 26.0}, "VIN": {"avg": 63.11, "peak": 73.33, "min": 55.69}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.23, "energy_joules_est": 16.91, "sample_count": 4, "duration_seconds": 0.559}, "timestamp": "2026-01-17T18:02:59.527447"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 893.669, "latencies_ms": [893.669], "images_per_second": 1.119, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "Bed: 2\nPillow: 1\nBlanket: 2\nBedspread: 2\nFlower pattern: 8", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25549.1, "ram_available_mb": 100223.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 30.0, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.32, "peak": 95.17, "min": 55.76}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.0, "energy_joules_est": 26.82, "sample_count": 6, "duration_seconds": 0.894}, "timestamp": "2026-01-17T18:03:00.427062"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 672.211, "latencies_ms": [672.211], "images_per_second": 1.488, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The child is lying in bed near the foot of the bed. The bed occupies the foreground, while the child's face is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25545.6, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.84, "peak": 35.04, "min": 24.81}, "VIN": {"avg": 63.06, "peak": 79.45, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.84, "energy_joules_est": 20.08, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T18:03:01.105877"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 769.106, "latencies_ms": [769.106], "images_per_second": 1.3, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A young child is sleeping peacefully on a bed covered with a dark blanket adorned with white daisy-like flowers. The room is dimly lit, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 31.12, "peak": 37.03, "min": 25.6}, "VIN": {"avg": 62.72, "peak": 72.68, "min": 56.2}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.12, "energy_joules_est": 23.95, "sample_count": 5, "duration_seconds": 0.77}, "timestamp": "2026-01-17T18:03:01.881016"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 731.176, "latencies_ms": [731.176], "images_per_second": 1.368, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The bedspread is dark blue with white daisy patterns. The lighting is dim, creating a cozy atmosphere. The material appears to be soft and possibly cotton.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.6, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.25, "peak": 35.83, "min": 24.82}, "VIN": {"avg": 68.19, "peak": 95.83, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.25, "energy_joules_est": 22.13, "sample_count": 5, "duration_seconds": 0.731}, "timestamp": "2026-01-17T18:03:02.618070"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 709.617, "latencies_ms": [709.617], "images_per_second": 1.409, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A young skateboarder in white shoes and cargo shorts is captured mid-trick on a concrete ramp in a skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.6, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.39, "peak": 35.82, "min": 25.2}, "VIN": {"avg": 62.14, "peak": 71.49, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.39, "energy_joules_est": 21.59, "sample_count": 5, "duration_seconds": 0.71}, "timestamp": "2026-01-17T18:03:03.337867"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1118.966, "latencies_ms": [1118.966], "images_per_second": 0.894, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "shoe: 1\nskateboard: 1\nskater: 1\nshorts: 1\nsocks: 1\nbuilding: 1\ntrees: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 27.72, "peak": 36.62, "min": 21.66}, "VIN": {"avg": 65.89, "peak": 96.94, "min": 60.31}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.72, "energy_joules_est": 31.02, "sample_count": 8, "duration_seconds": 1.119}, "timestamp": "2026-01-17T18:03:04.462564"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1068.773, "latencies_ms": [1068.773], "images_per_second": 0.936, "prompt_tokens": 25, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The skateboarder is positioned in the foreground, moving towards the left side of the image. The background features buildings and a clear sky, suggesting an urban setting. The skateboarder's shadow is cast on the ground, indicating a relatively bright and sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.53, "peak": 34.26, "min": 21.27}, "VIN": {"avg": 64.0, "peak": 87.16, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.53, "energy_joules_est": 28.37, "sample_count": 8, "duration_seconds": 1.069}, "timestamp": "2026-01-17T18:03:05.537608"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 918.132, "latencies_ms": [918.132], "images_per_second": 1.089, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a skateboarder performing a trick on a concrete ramp in a skate park. Another person is visible in the background, possibly watching the skateboarder or waiting for their turn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.16, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 63.62, "peak": 82.39, "min": 54.38}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.16, "energy_joules_est": 25.87, "sample_count": 6, "duration_seconds": 0.919}, "timestamp": "2026-01-17T18:03:06.461882"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 735.74, "latencies_ms": [735.74], "images_per_second": 1.359, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The skateboarder is wearing light-colored shoes. The lighting is bright, likely from natural sunlight, creating a clear contrast against the black and white image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.46, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 66.79, "peak": 89.89, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 21.7, "sample_count": 5, "duration_seconds": 0.736}, "timestamp": "2026-01-17T18:03:07.205485"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 644.582, "latencies_ms": [644.582], "images_per_second": 1.551, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man is lying on the floor surrounded by various objects, including a laptop, camera, cell phone, book, and sports equipment.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 31.8, "peak": 36.24, "min": 27.17}, "VIN": {"avg": 66.38, "peak": 86.34, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 31.8, "energy_joules_est": 20.51, "sample_count": 4, "duration_seconds": 0.645}, "timestamp": "2026-01-17T18:03:07.862155"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1285.928, "latencies_ms": [1285.928], "images_per_second": 0.778, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "laptop: 2\ncamera: 2\ncell phone: 1\nlaptop: 1\nracket: 1\nkey: 1\nbook: 1\nbag: 1\nwater bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.55, "peak": 37.01, "min": 20.49}, "VIN": {"avg": 62.96, "peak": 87.17, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 26.55, "energy_joules_est": 34.16, "sample_count": 10, "duration_seconds": 1.287}, "timestamp": "2026-01-17T18:03:09.154556"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 858.048, "latencies_ms": [858.048], "images_per_second": 1.165, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The man is positioned on the left side of the image, lying down with his head resting on the ground. The objects surrounding him are arranged more towards the center and background, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.77, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 64.43, "peak": 84.3, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.77, "energy_joules_est": 23.84, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T18:03:10.018749"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1109.405, "latencies_ms": [1109.405], "images_per_second": 0.901, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "A man is lying on the floor surrounded by various items, including a laptop displaying a plant, a camera, a cell phone, a book, a tennis racket, a water bottle, a bag, and a calendar. The scene suggests a casual, relaxed setting, possibly a home or office.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.98, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 63.44, "peak": 91.48, "min": 52.81}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.98, "energy_joules_est": 29.95, "sample_count": 8, "duration_seconds": 1.11}, "timestamp": "2026-01-17T18:03:11.134540"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 805.538, "latencies_ms": [805.538], "images_per_second": 1.241, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene features a beige carpet with soft lighting, creating a warm and inviting atmosphere. The objects are primarily black, white, and red, giving the impression of a casual, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 28.17, "peak": 34.66, "min": 22.85}, "VIN": {"avg": 65.24, "peak": 92.06, "min": 56.03}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 22.7, "sample_count": 6, "duration_seconds": 0.806}, "timestamp": "2026-01-17T18:03:11.947026"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 768.632, "latencies_ms": [768.632], "images_per_second": 1.301, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A black electric stovetop with four burners sits on a black countertop, accompanied by a spice rack, a utensil holder, and a towel rack.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 30.01, "peak": 35.85, "min": 24.82}, "VIN": {"avg": 64.94, "peak": 83.18, "min": 56.84}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.01, "energy_joules_est": 23.07, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T18:03:12.727242"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1436.925, "latencies_ms": [1436.925], "images_per_second": 0.696, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "spice rack: 5\noven: 4\ncountertop: 4\noven handle: 1\noven door: 1\noven knob: 1\noven temperature dial: 1\noven handle: 1\noven handle: 1\noven handle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 25.45, "peak": 35.44, "min": 20.1}, "VIN": {"avg": 62.89, "peak": 96.49, "min": 52.81}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.45, "energy_joules_est": 36.58, "sample_count": 11, "duration_seconds": 1.437}, "timestamp": "2026-01-17T18:03:14.174243"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 734.215, "latencies_ms": [734.215], "images_per_second": 1.362, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The spice rack is positioned to the left of the stove, partially obscured by the countertop. The oven is situated in the background, further away from the spice rack.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.6, "peak": 33.1, "min": 24.04}, "VIN": {"avg": 64.74, "peak": 81.49, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.6, "energy_joules_est": 21.01, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T18:03:14.914422"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 935.862, "latencies_ms": [935.862], "images_per_second": 1.069, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The scene depicts a modern kitchen with a black electric cooktop, a spice rack, and a hanging oven mitt. The kitchen features a marble backsplash and light wood cabinetry, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.74, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 62.77, "peak": 75.94, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.74, "energy_joules_est": 25.97, "sample_count": 7, "duration_seconds": 0.936}, "timestamp": "2026-01-17T18:03:15.856778"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1217.88, "latencies_ms": [1217.88], "images_per_second": 0.821, "prompt_tokens": 18, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The kitchen features a light-colored, marble-like backsplash with shades of yellow, gray, and blue. The lighting is bright and evenly distributed, creating a warm and inviting atmosphere. The countertop appears to be made of dark material, providing a contrast to the lighter tones of the backsplash and countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.21, "peak": 35.06, "min": 20.88}, "VIN": {"avg": 64.07, "peak": 90.18, "min": 57.54}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.21, "energy_joules_est": 31.93, "sample_count": 9, "duration_seconds": 1.218}, "timestamp": "2026-01-17T18:03:17.081679"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 833.814, "latencies_ms": [833.814], "images_per_second": 1.199, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A young man is sitting at a table in a bakery, enjoying a plate of assorted donuts, including chocolate and sprinkled varieties, while sipping coffee.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.56, "peak": 33.47, "min": 22.84}, "VIN": {"avg": 64.22, "peak": 76.55, "min": 60.04}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.56, "energy_joules_est": 23.0, "sample_count": 6, "duration_seconds": 0.834}, "timestamp": "2026-01-17T18:03:17.926698"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1541.027, "latencies_ms": [1541.027], "images_per_second": 0.649, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 1, "output_text": "Donut: 2\nChocolate glazed donut: 1\nChocolate frosted donut: 1\nChocolate sprinkles: 1\nCoffee cup: 1\nSoda: 1\nBrown paper bag: 1\nWooden chair: 1\nTable: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.22, "min": 14.1}, "VDD_GPU": {"avg": 25.1, "peak": 35.04, "min": 19.7}, "VIN": {"avg": 63.35, "peak": 90.18, "min": 56.25}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.1, "energy_joules_est": 38.69, "sample_count": 11, "duration_seconds": 1.542}, "timestamp": "2026-01-17T18:03:19.474666"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 790.892, "latencies_ms": [790.892], "images_per_second": 1.264, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The donuts are placed in the foreground of the image, while the person eating them is in the background. The table and chairs are positioned near the donuts, creating a casual dining setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.75, "peak": 33.09, "min": 24.02}, "VIN": {"avg": 66.53, "peak": 88.55, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.75, "energy_joules_est": 22.75, "sample_count": 5, "duration_seconds": 0.791}, "timestamp": "2026-01-17T18:03:20.275649"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1161.6, "latencies_ms": [1161.6], "images_per_second": 0.861, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The scene takes place in a brightly lit cafe or bakery, where a person is enjoying a selection of donuts on trays and plates. A man is seated at a table, partially visible in the frame, eating a donut and holding a piece of paper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.47, "peak": 35.03, "min": 20.88}, "VIN": {"avg": 64.09, "peak": 97.18, "min": 54.77}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.47, "energy_joules_est": 30.76, "sample_count": 9, "duration_seconds": 1.162}, "timestamp": "2026-01-17T18:03:21.443730"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1025.731, "latencies_ms": [1025.731], "images_per_second": 0.975, "prompt_tokens": 18, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The donuts are primarily in shades of brown and dark chocolate. The lighting in the image is warm and slightly dim, creating a cozy atmosphere. The donuts appear to be made of dough and have a glossy glaze, suggesting they are freshly baked.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.53, "peak": 33.86, "min": 21.26}, "VIN": {"avg": 64.77, "peak": 80.95, "min": 59.23}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.53, "energy_joules_est": 27.22, "sample_count": 8, "duration_seconds": 1.026}, "timestamp": "2026-01-17T18:03:22.475537"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 620.215, "latencies_ms": [620.215], "images_per_second": 1.612, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The bathroom features a unique sink design with two basins, one larger and one smaller, placed on a glass countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.03, "peak": 33.86, "min": 26.0}, "VIN": {"avg": 71.22, "peak": 94.86, "min": 61.47}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.03, "energy_joules_est": 18.64, "sample_count": 4, "duration_seconds": 0.621}, "timestamp": "2026-01-17T18:03:23.106711"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1041.996, "latencies_ms": [1041.996], "images_per_second": 0.96, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "sink: 2\ntoilet: 1\nchair: 1\nglass: 1\ndecorative plate: 1\nfloor tiles: 2\nwall tiles: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.0, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.82, "peak": 36.62, "min": 21.66}, "VIN": {"avg": 62.28, "peak": 78.06, "min": 56.89}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.82, "energy_joules_est": 29.0, "sample_count": 8, "duration_seconds": 1.042}, "timestamp": "2026-01-17T18:03:24.154762"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 970.641, "latencies_ms": [970.641], "images_per_second": 1.03, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The sink and toilet are placed close together, while the cow-print toilet seat is positioned further away, contributing to the overall arrangement of the bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.12, "peak": 33.86, "min": 21.66}, "VIN": {"avg": 61.57, "peak": 77.62, "min": 55.46}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.12, "energy_joules_est": 26.34, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-17T18:03:25.131912"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 893.665, "latencies_ms": [893.665], "images_per_second": 1.119, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a bathroom with a modern design, featuring white sinks, a glass-topped counter, and a cow-print toilet seat. The bathroom is situated on a black and white checkered floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.0, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.09, "peak": 34.65, "min": 22.85}, "VIN": {"avg": 64.98, "peak": 83.24, "min": 60.45}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.09, "energy_joules_est": 25.11, "sample_count": 6, "duration_seconds": 0.894}, "timestamp": "2026-01-17T18:03:26.031688"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1201.579, "latencies_ms": [1201.579], "images_per_second": 0.832, "prompt_tokens": 18, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The bathroom features a predominantly white color scheme with black accents. The lighting is bright and evenly distributed, creating a clean and welcoming atmosphere. The sink is made of clear glass, enhancing its modern aesthetic. The floor is tiled in a black and white checkered pattern, adding a classic touch to the space.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25546.0, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.34, "peak": 35.04, "min": 20.88}, "VIN": {"avg": 63.45, "peak": 85.75, "min": 56.55}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.34, "energy_joules_est": 31.66, "sample_count": 9, "duration_seconds": 1.202}, "timestamp": "2026-01-17T18:03:27.239786"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 829.241, "latencies_ms": [829.241], "images_per_second": 1.206, "prompt_tokens": 8, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Three stuffed animals are perched atop a weathered wooden cross, adorned with a white beaded rosary, situated in a grassy area near a brick border and a tree.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.97, "peak": 33.88, "min": 22.85}, "VIN": {"avg": 64.69, "peak": 85.69, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.97, "energy_joules_est": 23.2, "sample_count": 6, "duration_seconds": 0.83}, "timestamp": "2026-01-17T18:03:28.083061"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1046.083, "latencies_ms": [1046.083], "images_per_second": 0.956, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "cross: 3\nbears: 3\nstone: 1\nsign: 2\ngrass: 2\npath: 1\ntree: 1\nbush: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.5, "ram_available_mb": 100225.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 64.89, "peak": 91.83, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 29.09, "sample_count": 7, "duration_seconds": 1.047}, "timestamp": "2026-01-17T18:03:29.136256"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 748.923, "latencies_ms": [748.923], "images_per_second": 1.335, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The stuffed animals are positioned prominently in the foreground, contrasting with the more distant background elements. The cross stands prominently in the background, partially obscured by the stuffed animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.38, "peak": 34.66, "min": 24.41}, "VIN": {"avg": 68.06, "peak": 96.69, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.38, "energy_joules_est": 22.01, "sample_count": 5, "duration_seconds": 0.749}, "timestamp": "2026-01-17T18:03:29.895187"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 966.207, "latencies_ms": [966.207], "images_per_second": 1.035, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a grave with a wooden cross adorned with stuffed animals. The cross bears inscriptions identifying Joseph Panis and Mystie Malulant. The setting appears to be a residential area with trees and bushes nearby.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.01, "peak": 35.83, "min": 22.45}, "VIN": {"avg": 63.1, "peak": 77.48, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.01, "energy_joules_est": 27.07, "sample_count": 7, "duration_seconds": 0.966}, "timestamp": "2026-01-17T18:03:30.867460"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 984.711, "latencies_ms": [984.711], "images_per_second": 1.016, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The cross appears to be made of light brown or tan stone. The lighting in the image is soft and diffused, suggesting an overcast day. The material of the cross appears to be sturdy and weathered, potentially made of wood or stone.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.74, "peak": 34.65, "min": 22.45}, "VIN": {"avg": 63.07, "peak": 80.38, "min": 51.71}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.74, "energy_joules_est": 27.33, "sample_count": 7, "duration_seconds": 0.985}, "timestamp": "2026-01-17T18:03:31.859507"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 612.976, "latencies_ms": [612.976], "images_per_second": 1.631, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A large group of people are gathered in a spacious, modern restaurant, enjoying a meal and engaging in lively conversation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25546.2, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.62, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 65.28, "peak": 81.69, "min": 57.65}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 18.79, "sample_count": 4, "duration_seconds": 0.614}, "timestamp": "2026-01-17T18:03:32.484390"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1100.584, "latencies_ms": [1100.584], "images_per_second": 0.909, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "large clock: 8\nmetal beams: 8\nwooden chairs: 8\ntable: 8\npeople: 8\ncounter: 8\nmenu: 1\nlight fixture: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.92, "peak": 37.42, "min": 21.67}, "VIN": {"avg": 63.21, "peak": 81.2, "min": 56.05}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.92, "energy_joules_est": 30.74, "sample_count": 8, "duration_seconds": 1.101}, "timestamp": "2026-01-17T18:03:33.590770"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 983.768, "latencies_ms": [983.768], "images_per_second": 1.017, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The large clock dominates the background, positioned near the center and slightly to the right of the main seating area. The foreground features tables and chairs arranged around the clock, providing a space for patrons to sit and enjoy their meals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.7, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.52, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 65.66, "peak": 97.8, "min": 56.63}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 27.08, "sample_count": 7, "duration_seconds": 0.984}, "timestamp": "2026-01-17T18:03:34.580759"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 884.852, "latencies_ms": [884.852], "images_per_second": 1.13, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a bustling restaurant with a large clock on the wall, offering a unique atmosphere for patrons. The restaurant is filled with people seated at tables, enjoying their meals and engaging in conversations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.7, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25545.7, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.48, "peak": 35.06, "min": 23.23}, "VIN": {"avg": 65.69, "peak": 93.33, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.48, "energy_joules_est": 25.21, "sample_count": 6, "duration_seconds": 0.885}, "timestamp": "2026-01-17T18:03:35.472061"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1003.734, "latencies_ms": [1003.734], "images_per_second": 0.996, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The interior of the caf\u00e9 is illuminated by warm lighting, creating a cozy atmosphere. The walls are painted in a light beige color, and the ceiling features exposed metal beams. The large clock on the wall adds a unique architectural element to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.7, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.62, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 64.39, "peak": 86.27, "min": 57.59}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.62, "energy_joules_est": 27.73, "sample_count": 7, "duration_seconds": 1.004}, "timestamp": "2026-01-17T18:03:36.481861"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 554.504, "latencies_ms": [554.504], "images_per_second": 1.803, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A man and a child are enjoying a day of snowboarding down a snowy hill, surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.32, "peak": 33.86, "min": 26.38}, "VIN": {"avg": 70.21, "peak": 97.34, "min": 59.04}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.32, "energy_joules_est": 16.82, "sample_count": 4, "duration_seconds": 0.555}, "timestamp": "2026-01-17T18:03:37.047734"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1275.758, "latencies_ms": [1275.758], "images_per_second": 0.784, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "snowboard: 2\nperson: 2\nsnow: 8\ntree: 4\nrocks: 4\nsnowboard: 2\nskis: 2\ngoggles: 1\nhat: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.44, "peak": 37.8, "min": 20.88}, "VIN": {"avg": 63.29, "peak": 82.98, "min": 57.74}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.44, "energy_joules_est": 35.01, "sample_count": 9, "duration_seconds": 1.276}, "timestamp": "2026-01-17T18:03:38.329504"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 649.446, "latencies_ms": [649.446], "images_per_second": 1.54, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The child is positioned to the left of the man, who is standing in the foreground. The child is further in the background, near the edge of the snowy slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 30.13, "peak": 33.86, "min": 26.0}, "VIN": {"avg": 64.08, "peak": 74.51, "min": 60.27}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 30.13, "energy_joules_est": 19.58, "sample_count": 4, "duration_seconds": 0.65}, "timestamp": "2026-01-17T18:03:38.985735"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 728.922, "latencies_ms": [728.922], "images_per_second": 1.372, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Two people are snowboarding down a snowy slope in a wooded area. The scene is bright and sunny, with snow-covered trees and a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.8, "peak": 36.62, "min": 25.21}, "VIN": {"avg": 64.04, "peak": 73.48, "min": 60.83}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.8, "energy_joules_est": 22.46, "sample_count": 5, "duration_seconds": 0.729}, "timestamp": "2026-01-17T18:03:39.720717"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 671.1, "latencies_ms": [671.1], "images_per_second": 1.49, "prompt_tokens": 18, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The snow is bright white, indicating good lighting conditions. The snow appears smooth and undisturbed, suggesting good weather conditions for skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.48, "peak": 36.23, "min": 25.21}, "VIN": {"avg": 63.75, "peak": 77.57, "min": 59.67}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.48, "energy_joules_est": 20.46, "sample_count": 5, "duration_seconds": 0.671}, "timestamp": "2026-01-17T18:03:40.397598"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 657.689, "latencies_ms": [657.689], "images_per_second": 1.52, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A pair of feet wearing black flip-flops is standing on a wooden floor, accompanied by three old cell phones and a black flip-flop case.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25542.6, "ram_available_mb": 100229.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 25.7, "peak": 29.15, "min": 22.85}, "VIN": {"avg": 56.87, "peak": 59.54, "min": 53.13}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 25.7, "energy_joules_est": 16.91, "sample_count": 4, "duration_seconds": 0.658}, "timestamp": "2026-01-17T18:03:41.064918"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1427.067, "latencies_ms": [1427.067], "images_per_second": 0.701, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 1, "output_text": "Cell phone: 2\nFlip phone: 1\nCell phone cover: 1\nCell phone body: 1\nFlip phone: 1\nFlip phone screen: 1\nFlip phone buttons: 4\nFlip phone base: 1\nFlip phone flip: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.6, "ram_available_mb": 100229.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 13.79}, "VDD_GPU": {"avg": 21.73, "peak": 27.17, "min": 19.3}, "VIN": {"avg": 60.23, "peak": 63.41, "min": 57.14}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 21.73, "energy_joules_est": 31.01, "sample_count": 11, "duration_seconds": 1.427}, "timestamp": "2026-01-17T18:03:42.497801"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 677.506, "latencies_ms": [677.506], "images_per_second": 1.476, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The person's feet are positioned in the foreground, partially obscuring the background. The flip-flops are placed near the center and slightly to the right of the feet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 22.85, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 60.42, "peak": 64.96, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 22.85, "energy_joules_est": 15.49, "sample_count": 5, "duration_seconds": 0.678}, "timestamp": "2026-01-17T18:03:43.181435"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 895.554, "latencies_ms": [895.554], "images_per_second": 1.117, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a person's feet wearing flip-flops on a wooden floor, with three cell phones and a broken flip-flop nearby. The phones are scattered, indicating a possible disassembly or repair process.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.97, "peak": 26.38, "min": 20.48}, "VIN": {"avg": 60.35, "peak": 64.49, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.97, "energy_joules_est": 20.58, "sample_count": 6, "duration_seconds": 0.896}, "timestamp": "2026-01-17T18:03:44.087407"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 873.526, "latencies_ms": [873.526], "images_per_second": 1.145, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The wooden floor is brown and appears worn. The lighting is soft and diffused, suggesting natural light. The cell phones are black and appear to be made of plastic or metal. The phone's internal components are visible.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.71, "peak": 25.99, "min": 20.48}, "VIN": {"avg": 57.93, "peak": 61.1, "min": 54.95}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.71, "energy_joules_est": 19.85, "sample_count": 6, "duration_seconds": 0.874}, "timestamp": "2026-01-17T18:03:44.967238"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 660.253, "latencies_ms": [660.253], "images_per_second": 1.515, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The Palace of Westminster, home to the British Parliament, is illuminated at dusk and features the iconic clock tower, Big Ben, and Westminster Bridge.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 23.53, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 60.26, "peak": 65.11, "min": 57.13}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.53, "energy_joules_est": 15.54, "sample_count": 4, "duration_seconds": 0.661}, "timestamp": "2026-01-17T18:03:45.638349"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 814.988, "latencies_ms": [814.988], "images_per_second": 1.227, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "building: 8\ntower: 1\nclock: 2\nflag: 1\nboat: 3\nwater: 4\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.04, "peak": 26.39, "min": 20.48}, "VIN": {"avg": 61.44, "peak": 65.41, "min": 56.91}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.04, "energy_joules_est": 18.78, "sample_count": 6, "duration_seconds": 0.815}, "timestamp": "2026-01-17T18:03:46.459915"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 778.383, "latencies_ms": [778.383], "images_per_second": 1.285, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main objects are positioned along a large body of water, with the Palace of Westminster situated in the background. The foreground features several boats moored near the water, while the background includes additional boats and structures along the shoreline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.24, "peak": 26.39, "min": 20.88}, "VIN": {"avg": 59.45, "peak": 61.32, "min": 56.14}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.24, "energy_joules_est": 18.1, "sample_count": 5, "duration_seconds": 0.779}, "timestamp": "2026-01-17T18:03:47.248164"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 798.342, "latencies_ms": [798.342], "images_per_second": 1.253, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The scene depicts the Palace of Westminster in London, illuminated at night. Boats are moored along the riverbank, and Big Ben is visible in the background. The atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.85, "peak": 26.39, "min": 20.48}, "VIN": {"avg": 60.95, "peak": 64.9, "min": 55.79}, "VDD_CPU_SOC_MSS": {"avg": 14.63, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 22.85, "energy_joules_est": 18.25, "sample_count": 6, "duration_seconds": 0.799}, "timestamp": "2026-01-17T18:03:48.056729"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 759.332, "latencies_ms": [759.332], "images_per_second": 1.317, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The buildings are primarily illuminated in warm yellow light, creating a striking contrast against the muted, cloudy sky. The river reflects the soft light, giving the scene a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.0, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.16, "peak": 25.99, "min": 20.88}, "VIN": {"avg": 59.89, "peak": 63.8, "min": 55.19}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.16, "energy_joules_est": 17.59, "sample_count": 5, "duration_seconds": 0.76}, "timestamp": "2026-01-17T18:03:48.822094"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 733.56, "latencies_ms": [733.56], "images_per_second": 1.363, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The living room features a wooden floor, a ceiling fan, a green recliner, a gray couch, a coffee table, a television, and various decorative items.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25543.0, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.0, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.9, "peak": 33.85, "min": 24.42}, "VIN": {"avg": 66.5, "peak": 79.73, "min": 60.86}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.9, "energy_joules_est": 21.21, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T18:03:49.565741"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1478.069, "latencies_ms": [1478.069], "images_per_second": 0.677, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 1, "output_text": "chair: 2\ncouch: 2\ntelevision: 1\ncoffee table: 1\nmirror: 1\ntv stand: 1\nshelving unit: 1\nplant: 2\nbicycle: 1\nceiling fan: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25543.0, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 25.34, "peak": 36.23, "min": 20.08}, "VIN": {"avg": 63.61, "peak": 84.58, "min": 56.37}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.76, "min": 14.56}}, "power_watts_avg": 25.34, "energy_joules_est": 37.47, "sample_count": 11, "duration_seconds": 1.479}, "timestamp": "2026-01-17T18:03:51.051488"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1129.524, "latencies_ms": [1129.524], "images_per_second": 0.885, "prompt_tokens": 25, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The main objects are positioned in a somewhat haphazard manner, with some items in the foreground and others in the background. The foreground includes a red chair, a bicycle, and a coffee table. The background features two windows, a ceiling fan, and a television set. The television is situated near the center of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.05, "peak": 33.08, "min": 20.88}, "VIN": {"avg": 62.96, "peak": 81.21, "min": 56.7}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.05, "energy_joules_est": 29.43, "sample_count": 8, "duration_seconds": 1.13}, "timestamp": "2026-01-17T18:03:52.187123"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1245.993, "latencies_ms": [1245.993], "images_per_second": 0.803, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 1, "output_text": "The living room features a wooden floor, a ceiling fan, and several windows that let in natural light. The room is furnished with a green recliner, a gray armchair, a coffee table, and a television set.  A bicycle is leaning against a wall, and various plants add a touch of greenery to the space.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 25.86, "peak": 34.27, "min": 20.48}, "VIN": {"avg": 62.98, "peak": 80.3, "min": 57.71}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.86, "energy_joules_est": 32.23, "sample_count": 9, "duration_seconds": 1.246}, "timestamp": "2026-01-17T18:03:53.439303"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1139.126, "latencies_ms": [1139.126], "images_per_second": 0.878, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The room features a warm color palette, with light-colored walls and hardwood flooring. The lighting is natural and diffused, creating a cozy atmosphere. Various materials like wood, metal, and fabric are visible, contributing to the room's overall aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 25.75, "peak": 33.08, "min": 20.88}, "VIN": {"avg": 62.43, "peak": 72.89, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 25.75, "energy_joules_est": 29.34, "sample_count": 8, "duration_seconds": 1.14}, "timestamp": "2026-01-17T18:03:54.584417"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 573.687, "latencies_ms": [573.687], "images_per_second": 1.743, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A red metal parking meter post with two attached parking meters stands on the sidewalk in front of a brick building.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 29.74, "peak": 33.08, "min": 26.0}, "VIN": {"avg": 64.93, "peak": 83.85, "min": 57.12}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.74, "energy_joules_est": 17.07, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T18:03:55.168320"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1256.872, "latencies_ms": [1256.872], "images_per_second": 0.796, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "Parking meter: 2\nBicycle rack: 1\nStreet sign: 1\nBuilding: 1\nSidewalk: 1\nBushes: 2\nStreet: 1\nCrack in pavement: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.5, "ram_available_mb": 100228.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 26.7, "peak": 37.03, "min": 20.89}, "VIN": {"avg": 62.91, "peak": 88.88, "min": 53.63}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 26.7, "energy_joules_est": 33.57, "sample_count": 9, "duration_seconds": 1.257}, "timestamp": "2026-01-17T18:03:56.432071"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1001.051, "latencies_ms": [1001.051], "images_per_second": 0.999, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The parking meters are positioned in the foreground, slightly to the left of the red post. The building in the background occupies the upper portion of the image, extending from the left edge to the center. The parking meters are located near the building, closer to the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.96, "peak": 33.89, "min": 21.66}, "VIN": {"avg": 63.01, "peak": 80.31, "min": 57.97}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.96, "energy_joules_est": 27.0, "sample_count": 7, "duration_seconds": 1.002}, "timestamp": "2026-01-17T18:03:57.439531"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 644.697, "latencies_ms": [644.697], "images_per_second": 1.551, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A red parking meter stands on the sidewalk in front of a brick building. A large advertisement for the building's anniversary is visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.03, "peak": 33.86, "min": 26.0}, "VIN": {"avg": 65.07, "peak": 75.67, "min": 59.07}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.03, "energy_joules_est": 19.37, "sample_count": 4, "duration_seconds": 0.645}, "timestamp": "2026-01-17T18:03:58.090431"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 729.162, "latencies_ms": [729.162], "images_per_second": 1.371, "prompt_tokens": 18, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The parking meters are black and silver. The red metal post stands out against the brick sidewalk and street. The lighting suggests it's daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.72, "peak": 36.63, "min": 25.21}, "VIN": {"avg": 65.29, "peak": 92.71, "min": 54.96}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.72, "energy_joules_est": 22.41, "sample_count": 5, "duration_seconds": 0.73}, "timestamp": "2026-01-17T18:03:58.825891"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 746.032, "latencies_ms": [746.032], "images_per_second": 1.34, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man and woman are sitting together on a couch, playing a video game on a Nintendo Wii console, while enjoying snacks and drinks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 30.48, "peak": 36.23, "min": 25.21}, "VIN": {"avg": 64.7, "peak": 80.2, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.48, "energy_joules_est": 22.75, "sample_count": 5, "duration_seconds": 0.746}, "timestamp": "2026-01-17T18:03:59.582228"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1650.816, "latencies_ms": [1650.816], "images_per_second": 0.606, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 1, "output_text": "television: 1\ncouch: 2\nman: 1\nwoman: 1\npotted poinsettia: 1\ncandles: 2\nsoda cans: 1\nsoda bottle: 1\nsoda glass: 1\nchips: 1\nsunglasses: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 24.42, "peak": 36.24, "min": 19.3}, "VIN": {"avg": 63.27, "peak": 80.56, "min": 59.71}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.55}}, "power_watts_avg": 24.42, "energy_joules_est": 40.32, "sample_count": 13, "duration_seconds": 1.651}, "timestamp": "2026-01-17T18:04:01.238982"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 728.86, "latencies_ms": [728.86], "images_per_second": 1.372, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The television is positioned to the left of the couch, near the foreground. The man and woman are seated on the couch in the background, slightly further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.7, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.51, "peak": 32.68, "min": 24.03}, "VIN": {"avg": 62.97, "peak": 73.7, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.51, "energy_joules_est": 20.79, "sample_count": 5, "duration_seconds": 0.729}, "timestamp": "2026-01-17T18:04:01.973935"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 904.6, "latencies_ms": [904.6], "images_per_second": 1.105, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "A man and a woman are sitting on a couch in a living room, watching television together. They are surrounded by various items, including a poinsettia, snacks, drinks, and a lit candle, creating a cozy and relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 29.08, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 64.22, "peak": 77.35, "min": 59.18}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.08, "energy_joules_est": 26.31, "sample_count": 6, "duration_seconds": 0.905}, "timestamp": "2026-01-17T18:04:02.884448"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 962.548, "latencies_ms": [962.548], "images_per_second": 1.039, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The room is lit with warm lighting, creating a cozy atmosphere. The walls are painted a light color, and the overall decor includes various colors and textures. The furniture includes a couch, a coffee table, and a television set.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 65.37, "peak": 92.15, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 26.77, "sample_count": 7, "duration_seconds": 0.963}, "timestamp": "2026-01-17T18:04:03.853026"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 582.574, "latencies_ms": [582.574], "images_per_second": 1.717, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A hand is holding a silver control panel with six circular buttons, likely for a modern toilet control system.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.62, "peak": 34.66, "min": 26.38}, "VIN": {"avg": 66.07, "peak": 80.74, "min": 59.13}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 17.86, "sample_count": 4, "duration_seconds": 0.583}, "timestamp": "2026-01-17T18:04:04.446384"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1854.332, "latencies_ms": [1854.332], "images_per_second": 0.539, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 1, "output_text": "toilet: 1\ntoilet seat: 1\ntoilet lid: 1\ntoilet seat cover: 1\ntoilet seat: 1\ntoilet seat cover: 1\ntoilet seat: 1\ntoilet seat cover: 1\ntoilet seat: 1\ntoilet seat cover: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25541.6, "ram_available_mb": 100230.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 24.44, "peak": 37.01, "min": 19.3}, "VIN": {"avg": 64.11, "peak": 99.3, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.44, "energy_joules_est": 45.33, "sample_count": 14, "duration_seconds": 1.855}, "timestamp": "2026-01-17T18:04:06.306709"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 872.353, "latencies_ms": [872.353], "images_per_second": 1.146, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The toilet is positioned in the foreground, with the hand holding the control panel behind it. The control panel is situated near the toilet, slightly out of focus. The background consists of white walls and a wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.6, "ram_available_mb": 100230.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25541.3, "ram_available_mb": 100230.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.7, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 67.61, "peak": 99.69, "min": 59.65}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.7, "energy_joules_est": 24.17, "sample_count": 6, "duration_seconds": 0.873}, "timestamp": "2026-01-17T18:04:07.186436"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 775.479, "latencies_ms": [775.479], "images_per_second": 1.29, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A person is holding a control panel with buttons, likely for a toilet or bidet. The control panel is mounted on a stand and positioned next to a toilet lid.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.3, "ram_available_mb": 100230.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25541.6, "ram_available_mb": 100230.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.94, "peak": 35.45, "min": 24.83}, "VIN": {"avg": 66.82, "peak": 86.56, "min": 60.4}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 23.23, "sample_count": 5, "duration_seconds": 0.776}, "timestamp": "2026-01-17T18:04:07.967927"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 968.862, "latencies_ms": [968.862], "images_per_second": 1.032, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The bathroom features a white color scheme, creating a clean and minimalist aesthetic. The lighting appears to be soft and diffused, contributing to a calm atmosphere. The materials appear to be modern and sleek, enhancing the overall design of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.6, "ram_available_mb": 100230.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 36.24, "min": 22.45}, "VIN": {"avg": 64.74, "peak": 82.28, "min": 60.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 27.49, "sample_count": 7, "duration_seconds": 0.969}, "timestamp": "2026-01-17T18:04:08.942966"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 653.262, "latencies_ms": [653.262], "images_per_second": 1.531, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "Two snowboarders are captured mid-air, performing a thrilling trick against a backdrop of snowy mountains and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25541.6, "ram_available_mb": 100230.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.51, "min": 14.1}, "VDD_GPU": {"avg": 31.02, "peak": 35.45, "min": 26.79}, "VIN": {"avg": 65.4, "peak": 78.57, "min": 59.95}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.02, "energy_joules_est": 20.27, "sample_count": 4, "duration_seconds": 0.654}, "timestamp": "2026-01-17T18:04:09.611424"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1245.436, "latencies_ms": [1245.436], "images_per_second": 0.803, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "Snowboard: 2\nSnowboarder: 2\nSnowboard: 1\nSnowboard ramp: 2\nSnowboard: 1\nSnowboarders: 2\nSpectators: 10", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25541.6, "ram_available_mb": 100230.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 26.82, "peak": 36.62, "min": 20.89}, "VIN": {"avg": 63.88, "peak": 85.07, "min": 55.0}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.82, "energy_joules_est": 33.41, "sample_count": 9, "duration_seconds": 1.246}, "timestamp": "2026-01-17T18:04:10.863134"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1004.481, "latencies_ms": [1004.481], "images_per_second": 0.996, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The snowboarder is positioned in the foreground, performing a trick above a snow ramp. The spectators are located in the background, watching the snowboarders. The snow ramp is situated near the spectators, creating a clear separation between the two.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.12, "peak": 33.88, "min": 22.06}, "VIN": {"avg": 64.7, "peak": 94.86, "min": 56.6}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 27.12, "energy_joules_est": 27.26, "sample_count": 7, "duration_seconds": 1.005}, "timestamp": "2026-01-17T18:04:11.875237"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 906.545, "latencies_ms": [906.545], "images_per_second": 1.103, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "Two snowboarders are performing aerial tricks on a snow-covered slope, drawing the attention of a crowd of spectators. The scene takes place outdoors on a sunny day, with snow-covered terrain and clear skies.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.51, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 65.27, "peak": 84.88, "min": 57.09}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.51, "energy_joules_est": 24.95, "sample_count": 7, "duration_seconds": 0.907}, "timestamp": "2026-01-17T18:04:12.790035"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1165.767, "latencies_ms": [1165.767], "images_per_second": 0.858, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The snowboarders' attire is predominantly red, creating a vibrant contrast against the white snow. The lighting is bright and clear, illuminating the scene and highlighting the snowboarders' movements. The snowboarders are performing tricks on a snow-covered slope under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.17, "peak": 34.65, "min": 20.89}, "VIN": {"avg": 62.95, "peak": 85.66, "min": 54.27}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.17, "energy_joules_est": 30.52, "sample_count": 9, "duration_seconds": 1.166}, "timestamp": "2026-01-17T18:04:13.961693"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 725.972, "latencies_ms": [725.972], "images_per_second": 1.377, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A small home office features a wooden desk with a computer setup, a black chair, and a lamp, creating a functional workspace in the corner of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.76, "peak": 33.48, "min": 24.04}, "VIN": {"avg": 68.54, "peak": 89.79, "min": 61.24}, "VDD_CPU_SOC_MSS": {"avg": 14.71, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 28.76, "energy_joules_est": 20.9, "sample_count": 5, "duration_seconds": 0.727}, "timestamp": "2026-01-17T18:04:14.698432"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1366.389, "latencies_ms": [1366.389], "images_per_second": 0.732, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 1, "output_text": "desk: 2\nlaptop: 1\nchair: 1\nlamp: 1\ncomputer monitor: 1\nkeyboard: 1\nmouse: 1\nplant: 1\nbookshelf: 1\ncouch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.3, "ram_available_mb": 100230.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 26.0, "peak": 35.85, "min": 20.09}, "VIN": {"avg": 61.22, "peak": 69.94, "min": 54.58}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.0, "energy_joules_est": 35.54, "sample_count": 10, "duration_seconds": 1.367}, "timestamp": "2026-01-17T18:04:16.073605"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 892.014, "latencies_ms": [892.014], "images_per_second": 1.121, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main objects are positioned in a corner of the room, with the desk and chair placed near the corner of the room. The desk and chair are placed in the foreground, while the couch and bookshelf are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 27.71, "peak": 33.09, "min": 22.85}, "VIN": {"avg": 65.82, "peak": 97.62, "min": 57.74}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.71, "energy_joules_est": 24.72, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T18:04:16.972376"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1191.756, "latencies_ms": [1191.756], "images_per_second": 0.839, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The scene depicts a home office setup with a desk, computer equipment, a lamp, and a chair. A laptop, keyboard, and mouse are positioned on the desk, along with a laptop computer. A potted plant adds a touch of greenery to the space, and a white couch is partially visible in the corner.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25541.1, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25540.8, "ram_available_mb": 100231.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.12, "peak": 35.06, "min": 20.48}, "VIN": {"avg": 63.46, "peak": 87.42, "min": 54.5}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.12, "energy_joules_est": 31.15, "sample_count": 9, "duration_seconds": 1.192}, "timestamp": "2026-01-17T18:04:18.170788"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1095.185, "latencies_ms": [1095.185], "images_per_second": 0.913, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The room features a light wood desk with a black chair and a black lamp. The desk is illuminated by a warm lamp, creating a cozy atmosphere. The carpet is light-colored and appears clean and freshly vacuumed. A white couch is partially visible in the corner.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25540.8, "ram_available_mb": 100231.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25541.0, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.54, "peak": 33.86, "min": 21.27}, "VIN": {"avg": 64.91, "peak": 94.91, "min": 57.37}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.54, "energy_joules_est": 29.08, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T18:04:19.272102"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 551.139, "latencies_ms": [551.139], "images_per_second": 1.814, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "Two motorcyclists are riding down a dirt road surrounded by rolling hills and lush vegetation.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25541.0, "ram_available_mb": 100231.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25540.8, "ram_available_mb": 100231.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.42, "peak": 34.24, "min": 26.39}, "VIN": {"avg": 65.53, "peak": 81.17, "min": 55.94}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.42, "energy_joules_est": 16.78, "sample_count": 4, "duration_seconds": 0.552}, "timestamp": "2026-01-17T18:04:19.837427"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 972.698, "latencies_ms": [972.698], "images_per_second": 1.028, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "motorcycle: 2\nperson: 1\nroad: 1\nhill: 2\ntrees: 2\nsky: 1\nclouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.8, "ram_available_mb": 100231.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25540.5, "ram_available_mb": 100231.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 28.64, "peak": 37.01, "min": 22.45}, "VIN": {"avg": 63.06, "peak": 73.15, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.64, "energy_joules_est": 27.87, "sample_count": 7, "duration_seconds": 0.973}, "timestamp": "2026-01-17T18:04:20.816236"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 734.86, "latencies_ms": [734.86], "images_per_second": 1.361, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The main object is a motorcycle positioned near the foreground of the image. The motorcycle is situated on a dirt road that extends into the background, surrounding the mountainous terrain.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25540.5, "ram_available_mb": 100231.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25540.5, "ram_available_mb": 100231.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.61, "peak": 34.26, "min": 24.82}, "VIN": {"avg": 64.1, "peak": 73.46, "min": 59.59}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.61, "energy_joules_est": 21.77, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T18:04:21.556954"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1114.661, "latencies_ms": [1114.661], "images_per_second": 0.897, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "Two motorcyclists are riding on a dirt road that winds through a mountainous landscape. The riders are positioned on separate motorcycles, one closer to the left side of the road and the other towards the right. The surrounding terrain is rugged and features rolling hills and patches of greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.5, "ram_available_mb": 100231.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.5, "ram_available_mb": 100231.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.37, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 62.77, "peak": 86.72, "min": 52.04}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.37, "energy_joules_est": 30.53, "sample_count": 8, "duration_seconds": 1.115}, "timestamp": "2026-01-17T18:04:22.678213"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 921.255, "latencies_ms": [921.255], "images_per_second": 1.085, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The sky is a bright blue with scattered white clouds. The lighting suggests it's likely daytime. The terrain is rocky and uneven, composed of gravel and dirt. The overall atmosphere appears serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.5, "ram_available_mb": 100231.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.34, "peak": 34.26, "min": 22.07}, "VIN": {"avg": 64.84, "peak": 87.4, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.34, "energy_joules_est": 25.19, "sample_count": 7, "duration_seconds": 0.922}, "timestamp": "2026-01-17T18:04:23.605311"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 494.343, "latencies_ms": [494.343], "images_per_second": 2.023, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A wooden dining table with a bowl of fresh fruit, including oranges and bananas, is situated in the center of the kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 25.35, "peak": 27.19, "min": 23.64}, "VIN": {"avg": 61.13, "peak": 64.22, "min": 59.04}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 25.35, "energy_joules_est": 12.54, "sample_count": 3, "duration_seconds": 0.495}, "timestamp": "2026-01-17T18:04:24.107507"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1298.739, "latencies_ms": [1298.739], "images_per_second": 0.77, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 1, "output_text": "oven: 1\nstove: 1\nrange hood: 1\ncabinets: 6\ncountertops: 2\nsink: 1\nfridge: 1\ntable: 1\nchairs: 2\nfruit bowl: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.0, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 13.69}, "VDD_GPU": {"avg": 22.26, "peak": 27.97, "min": 19.7}, "VIN": {"avg": 60.77, "peak": 64.64, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.26, "energy_joules_est": 28.92, "sample_count": 10, "duration_seconds": 1.299}, "timestamp": "2026-01-17T18:04:25.412385"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 591.499, "latencies_ms": [591.499], "images_per_second": 1.691, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The kitchen is positioned to the right of the dining table. The dining table occupies the foreground, while the kitchen counters and appliances extend into the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25540.0, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 23.44, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 61.08, "peak": 63.59, "min": 58.04}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.44, "energy_joules_est": 13.87, "sample_count": 4, "duration_seconds": 0.592}, "timestamp": "2026-01-17T18:04:26.009995"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 708.655, "latencies_ms": [708.655], "images_per_second": 1.411, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The kitchen and dining area are well-lit and clean, creating a bright and welcoming atmosphere.  A bowl of fresh fruit sits on the dining table, ready to be enjoyed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 26.77, "min": 21.27}, "VIN": {"avg": 62.08, "peak": 65.12, "min": 60.48}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 16.76, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T18:04:26.725560"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 774.622, "latencies_ms": [774.622], "images_per_second": 1.291, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The kitchen features light brown wooden cabinets, white appliances, and light beige walls. The lighting is bright and evenly distributed, creating a welcoming atmosphere. The kitchen also has a clean and organized appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.48, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 62.02, "peak": 65.71, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.95, "min": 14.17}}, "power_watts_avg": 23.48, "energy_joules_est": 18.19, "sample_count": 5, "duration_seconds": 0.775}, "timestamp": "2026-01-17T18:04:27.506198"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 753.305, "latencies_ms": [753.305], "images_per_second": 1.327, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A female tennis player, dressed in black and white, is poised to hit a yellow tennis ball with her racket on a blue and green tennis court.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.15, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.73, "peak": 83.06, "min": 61.19}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 21.97, "sample_count": 5, "duration_seconds": 0.754}, "timestamp": "2026-01-17T18:04:28.269704"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1493.606, "latencies_ms": [1493.606], "images_per_second": 0.67, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 1, "output_text": "Tennis ball: 1\nTennis racket: 1\nTennis skirt: 1\nTennis player: 1\nTennis court: 1\nTennis ball: 1\nTennis ball: 1\nTennis player: 1\nTennis player: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 13.79}, "VDD_GPU": {"avg": 25.24, "peak": 35.85, "min": 20.09}, "VIN": {"avg": 63.27, "peak": 90.58, "min": 59.01}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 25.24, "energy_joules_est": 37.71, "sample_count": 11, "duration_seconds": 1.494}, "timestamp": "2026-01-17T18:04:29.769206"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 887.489, "latencies_ms": [887.489], "images_per_second": 1.127, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The tennis player is positioned in the foreground, reaching up towards the tennis ball. The tennis court extends into the background, creating a sense of depth and space. The player's position suggests they are actively engaged in the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 27.84, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 65.73, "peak": 93.79, "min": 54.28}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.84, "energy_joules_est": 24.72, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T18:04:30.662819"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 976.665, "latencies_ms": [976.665], "images_per_second": 1.024, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "A female tennis player is serving a tennis ball on a green court. She is wearing a white top and black skirt, and holds a tennis racket in her right hand. A polo logo sign is visible in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.45, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 63.72, "peak": 73.53, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.45, "energy_joules_est": 26.82, "sample_count": 7, "duration_seconds": 0.977}, "timestamp": "2026-01-17T18:04:31.646465"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 720.827, "latencies_ms": [720.827], "images_per_second": 1.387, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The tennis court is green. The lighting appears to be natural, likely from the sun. The tennis racket is red and white. The weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.3, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 65.7, "peak": 84.32, "min": 59.53}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 21.14, "sample_count": 5, "duration_seconds": 0.721}, "timestamp": "2026-01-17T18:04:32.373698"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 515.796, "latencies_ms": [515.796], "images_per_second": 1.939, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A red fire hydrant with a white cap is situated on a sidewalk next to a yellow pedestrian crossing sign.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.86, "peak": 14.2, "min": 13.49}, "VDD_GPU": {"avg": 25.87, "peak": 27.97, "min": 24.03}, "VIN": {"avg": 59.7, "peak": 64.65, "min": 54.12}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 25.87, "energy_joules_est": 13.36, "sample_count": 3, "duration_seconds": 0.516}, "timestamp": "2026-01-17T18:04:32.899446"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1103.448, "latencies_ms": [1103.448], "images_per_second": 0.906, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "fire hydrant: 1\ncrosswalk sign: 1\nstreet sign: 1\nchild crossing sign: 1\nperson walking: 1\nbuildings: 2\ntrees: 2\nsidewalk: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 15.01, "min": 13.49}, "VDD_GPU": {"avg": 22.85, "peak": 27.57, "min": 20.09}, "VIN": {"avg": 61.11, "peak": 63.02, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.85, "energy_joules_est": 25.22, "sample_count": 8, "duration_seconds": 1.104}, "timestamp": "2026-01-17T18:04:34.009337"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 986.209, "latencies_ms": [986.209], "images_per_second": 1.014, "prompt_tokens": 25, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The red fire hydrant is positioned in the foreground, slightly to the right of the viewer. The sidewalk extends into the background, separating the viewer from the street and buildings. A pedestrian crossing sign is visible near the fire hydrant, further emphasizing the urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.4, "peak": 26.0, "min": 20.1}, "VIN": {"avg": 60.24, "peak": 63.8, "min": 55.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.4, "energy_joules_est": 22.1, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T18:04:35.001900"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 751.801, "latencies_ms": [751.801], "images_per_second": 1.33, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A red fire hydrant is situated on a sidewalk next to a tree. A pedestrian crossing sign is visible nearby, indicating a pedestrian crossing area. A person can be seen walking on the sidewalk in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.09, "peak": 26.01, "min": 20.88}, "VIN": {"avg": 60.57, "peak": 62.34, "min": 57.75}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 23.09, "energy_joules_est": 17.37, "sample_count": 5, "duration_seconds": 0.752}, "timestamp": "2026-01-17T18:04:35.760617"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 563.707, "latencies_ms": [563.707], "images_per_second": 1.774, "prompt_tokens": 18, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The fire hydrant is red and appears to be made of metal. The surrounding area is well-lit, suggesting sunny weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 24.04, "peak": 26.39, "min": 22.07}, "VIN": {"avg": 59.93, "peak": 63.53, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.04, "energy_joules_est": 13.56, "sample_count": 4, "duration_seconds": 0.564}, "timestamp": "2026-01-17T18:04:36.330236"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 591.9, "latencies_ms": [591.9], "images_per_second": 1.689, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A white toilet with a closed lid is situated in a small, narrow bathroom, accompanied by a white toilet paper dispenser and a white trash can.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 24.62, "peak": 27.18, "min": 22.45}, "VIN": {"avg": 60.92, "peak": 62.66, "min": 59.46}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.62, "energy_joules_est": 14.58, "sample_count": 4, "duration_seconds": 0.592}, "timestamp": "2026-01-17T18:04:36.931023"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1592.378, "latencies_ms": [1592.378], "images_per_second": 0.628, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 1, "output_text": "toilet: 1\ntoilet paper dispenser: 1\ntoilet brush: 1\ntoilet seat: 1\ntoilet tank: 1\ntoilet lid: 1\ntoilet seat cover: 1\ntoilet paper: 1\ntoilet paper holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.22, "min": 13.69}, "VDD_GPU": {"avg": 21.6, "peak": 27.18, "min": 19.29}, "VIN": {"avg": 58.98, "peak": 63.83, "min": 52.11}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 21.6, "energy_joules_est": 34.4, "sample_count": 12, "duration_seconds": 1.593}, "timestamp": "2026-01-17T18:04:38.529860"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 573.38, "latencies_ms": [573.38], "images_per_second": 1.744, "prompt_tokens": 25, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The toilet is positioned to the right of the toilet paper dispenser. The toilet is situated further back in the bathroom, near the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.44, "peak": 25.6, "min": 21.66}, "VIN": {"avg": 60.16, "peak": 61.11, "min": 57.67}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 23.44, "energy_joules_est": 13.45, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T18:04:39.109279"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 887.792, "latencies_ms": [887.792], "images_per_second": 1.126, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene depicts a small bathroom with a white toilet, toilet brush, and a turquoise wastebasket. The toilet is positioned centrally, contrasting with the white walls and brown carpet. The toilet brush is visible near the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 23.1, "peak": 26.77, "min": 20.48}, "VIN": {"avg": 60.77, "peak": 62.89, "min": 58.93}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.1, "energy_joules_est": 20.52, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T18:04:40.007497"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 887.967, "latencies_ms": [887.967], "images_per_second": 1.126, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The toilet is white and appears to be made of plastic or ceramic. The lighting in the bathroom is soft and diffused, creating a calm atmosphere. The walls are painted a light color, and the floor is carpeted in a neutral tone.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.84, "peak": 25.99, "min": 20.48}, "VIN": {"avg": 59.64, "peak": 63.89, "min": 52.73}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 22.84, "energy_joules_est": 20.29, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T18:04:40.905645"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 592.036, "latencies_ms": [592.036], "images_per_second": 1.689, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A person dressed in a bright red snowsuit is skiing down a snowy mountain, leaving a trail behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25539.5, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.12, "peak": 33.47, "min": 26.39}, "VIN": {"avg": 64.3, "peak": 77.13, "min": 56.71}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.12, "energy_joules_est": 17.85, "sample_count": 4, "duration_seconds": 0.593}, "timestamp": "2026-01-17T18:04:41.507591"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1099.783, "latencies_ms": [1099.783], "images_per_second": 0.909, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "person: 1\nski: 1\nsnow: 2\nmountains: 1\nsky: 1\nclouds: 0\ntrees: 0\nhills: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.5, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.62, "peak": 36.62, "min": 21.27}, "VIN": {"avg": 63.99, "peak": 82.59, "min": 57.2}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.62, "energy_joules_est": 30.38, "sample_count": 8, "duration_seconds": 1.1}, "timestamp": "2026-01-17T18:04:42.613728"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 669.992, "latencies_ms": [669.992], "images_per_second": 1.493, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The skier is positioned in the foreground, moving towards the background. The snow-covered mountain rises in the background, creating a sense of distance and scale.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.79, "peak": 34.66, "min": 24.83}, "VIN": {"avg": 62.38, "peak": 78.31, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.79, "energy_joules_est": 19.97, "sample_count": 5, "duration_seconds": 0.67}, "timestamp": "2026-01-17T18:04:43.289804"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 813.531, "latencies_ms": [813.531], "images_per_second": 1.229, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A person is skiing down a snow-covered mountain, wearing a bright red jacket and using ski poles. The mountain is covered in snow and has a distinct peak in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.61, "peak": 36.63, "min": 23.64}, "VIN": {"avg": 65.75, "peak": 82.98, "min": 59.95}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.61, "energy_joules_est": 24.1, "sample_count": 6, "duration_seconds": 0.814}, "timestamp": "2026-01-17T18:04:44.109215"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1383.304, "latencies_ms": [1383.304], "images_per_second": 0.723, "prompt_tokens": 18, "response_tokens_est": 72, "n_tiles": 1, "output_text": "The skier is wearing a bright red jacket, which stands out against the white snow. The lighting is bright and clear, illuminating the scene and highlighting the snow-covered mountain. The materials appear to be typical skiing gear, including skis, poles, and a backpack. The weather is sunny and clear, contributing to the overall bright and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 25.92, "peak": 35.85, "min": 20.09}, "VIN": {"avg": 63.53, "peak": 89.02, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.92, "energy_joules_est": 35.87, "sample_count": 10, "duration_seconds": 1.384}, "timestamp": "2026-01-17T18:04:45.499453"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 792.332, "latencies_ms": [792.332], "images_per_second": 1.262, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A man wearing a red and white top and black pants is cross-country skiing through a snowy forest, skillfully maneuvering his poles and gliding over the snow.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.4, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.51, "peak": 33.08, "min": 24.03}, "VIN": {"avg": 64.03, "peak": 78.45, "min": 56.55}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.51, "energy_joules_est": 22.61, "sample_count": 5, "duration_seconds": 0.793}, "timestamp": "2026-01-17T18:04:46.302942"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1136.7, "latencies_ms": [1136.7], "images_per_second": 0.88, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "man: 2\nskis: 2\nsnow: 8\ntrees: 8\nsnow covered ground: 8\ngloves: 2\nhat: 1\nbib: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.4, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.9, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.02, "peak": 35.42, "min": 21.27}, "VIN": {"avg": 63.89, "peak": 82.56, "min": 59.13}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.02, "energy_joules_est": 30.73, "sample_count": 8, "duration_seconds": 1.137}, "timestamp": "2026-01-17T18:04:47.446162"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1158.988, "latencies_ms": [1158.988], "images_per_second": 0.863, "prompt_tokens": 25, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The main object is a skier positioned in the foreground of the image, moving towards the left side of the frame. The background features snow-covered trees and a snowy landscape, suggesting the setting is near a mountain or ski slope. The skier is relatively close to the viewer, indicating a relatively close distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25539.9, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.09, "peak": 33.89, "min": 20.88}, "VIN": {"avg": 63.59, "peak": 83.5, "min": 56.37}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.09, "energy_joules_est": 30.25, "sample_count": 9, "duration_seconds": 1.159}, "timestamp": "2026-01-17T18:04:48.611068"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 931.445, "latencies_ms": [931.445], "images_per_second": 1.074, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A man is cross-country skiing through a snowy forest, wearing a red and white top and dark pants. Another person is visible in the distance, also skiing. The scene is set in a mountainous area with snow-covered trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.23, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 63.33, "peak": 74.53, "min": 59.44}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 25.38, "sample_count": 7, "duration_seconds": 0.932}, "timestamp": "2026-01-17T18:04:49.548439"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1002.202, "latencies_ms": [1002.202], "images_per_second": 0.998, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The snow is white and appears quite fresh and undisturbed. The lighting suggests an overcast day, with diffused light that doesn't cast harsh shadows. The snow appears to be made of fresh, powdery material, suitable for skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.9, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.63, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 64.52, "peak": 81.33, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 27.71, "sample_count": 7, "duration_seconds": 1.003}, "timestamp": "2026-01-17T18:04:50.558368"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 670.639, "latencies_ms": [670.639], "images_per_second": 1.491, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A computer setup is displayed, featuring a monitor, keyboard, and mouse on a desk, with the word \"WORKPLACE\" written on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.38, "peak": 34.26, "min": 24.82}, "VIN": {"avg": 65.1, "peak": 81.73, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.38, "energy_joules_est": 19.72, "sample_count": 5, "duration_seconds": 0.671}, "timestamp": "2026-01-17T18:04:51.238476"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1159.354, "latencies_ms": [1159.354], "images_per_second": 0.863, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Keyboard: 2\nMouse: 1\nComputer monitor: 1\nDesk: 2\nComputer mouse: 1\nDesk surface: 1\nComputer tower: 1\nComputer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.96, "peak": 36.23, "min": 20.89}, "VIN": {"avg": 64.24, "peak": 83.88, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.96, "energy_joules_est": 31.27, "sample_count": 9, "duration_seconds": 1.16}, "timestamp": "2026-01-17T18:04:52.403897"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 672.282, "latencies_ms": [672.282], "images_per_second": 1.487, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The keyboard and mouse are positioned in the foreground, closer to the viewer. The monitor is situated in the background, slightly elevated and closer to the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.43}, "VIN": {"avg": 62.71, "peak": 76.87, "min": 57.36}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 19.66, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T18:04:53.082253"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1131.231, "latencies_ms": [1131.231], "images_per_second": 0.884, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The scene depicts a workspace with a computer setup on a desk. A monitor displays an image of a desk with a computer and keyboard, while a mouse sits in front of the keyboard. The word \"WORKPLACE\" is prominently displayed, suggesting the setting is designed for productivity and computer use.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.53, "peak": 36.26, "min": 21.67}, "VIN": {"avg": 61.64, "peak": 78.89, "min": 53.58}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.53, "energy_joules_est": 31.16, "sample_count": 8, "duration_seconds": 1.132}, "timestamp": "2026-01-17T18:04:54.220166"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1032.049, "latencies_ms": [1032.049], "images_per_second": 0.969, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The computer setup is primarily in grayscale, creating a monochrome aesthetic. The lighting appears to be soft and diffused, contributing to a calm and focused atmosphere. The materials appear to be sleek and modern, complementing the minimalist design of the workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.59, "peak": 33.88, "min": 21.27}, "VIN": {"avg": 65.58, "peak": 98.91, "min": 57.96}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.59, "energy_joules_est": 27.45, "sample_count": 8, "duration_seconds": 1.033}, "timestamp": "2026-01-17T18:04:55.259083"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 631.535, "latencies_ms": [631.535], "images_per_second": 1.583, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A woman in a red shirt is sitting at a table in a train, holding a bagel and smiling while enjoying her meal.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.53, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 66.29, "peak": 82.21, "min": 59.19}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.53, "energy_joules_est": 19.3, "sample_count": 4, "duration_seconds": 0.632}, "timestamp": "2026-01-17T18:04:55.902498"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1043.261, "latencies_ms": [1043.261], "images_per_second": 0.959, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "bagel: 1\nwoman: 1\ntable: 1\ncoffee cup: 1\nbox: 1\nwindow: 1\nman: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.97, "peak": 37.01, "min": 21.66}, "VIN": {"avg": 61.62, "peak": 72.16, "min": 54.64}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 29.2, "sample_count": 8, "duration_seconds": 1.044}, "timestamp": "2026-01-17T18:04:56.952294"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1045.97, "latencies_ms": [1045.97], "images_per_second": 0.956, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The woman is positioned in the foreground, holding the bagel and smiling. The train car is situated behind her, extending into the background. The bagel is held in her right hand, while the coffee cup and box are placed in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 26.64, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 63.15, "peak": 95.35, "min": 53.4}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.64, "energy_joules_est": 27.88, "sample_count": 8, "duration_seconds": 1.046}, "timestamp": "2026-01-17T18:04:58.004389"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 902.136, "latencies_ms": [902.136], "images_per_second": 1.108, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "A woman is sitting in a train car, holding a bagel and smiling. She is wearing a maroon shirt and appears to be enjoying her breakfast. Behind her, another person is standing and observing the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.5, "ram_available_mb": 100232.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.24, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 64.19, "peak": 89.24, "min": 55.94}, "VDD_CPU_SOC_MSS": {"avg": 14.78, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.24, "energy_joules_est": 24.59, "sample_count": 7, "duration_seconds": 0.903}, "timestamp": "2026-01-17T18:04:58.912633"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 770.216, "latencies_ms": [770.216], "images_per_second": 1.298, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The woman is wearing a maroon shirt. The bagel is light brown. The train interior has warm lighting. The bagel appears to have a cream filling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.5, "ram_available_mb": 100232.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.45, "peak": 35.04, "min": 24.42}, "VIN": {"avg": 67.5, "peak": 92.68, "min": 59.95}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.45, "energy_joules_est": 22.7, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T18:04:59.689078"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 596.658, "latencies_ms": [596.658], "images_per_second": 1.676, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Two zebras are grazing on green grass in a field, displaying their distinctive black and white stripes.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.41, "peak": 35.85, "min": 26.79}, "VIN": {"avg": 68.74, "peak": 93.92, "min": 53.64}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 31.41, "energy_joules_est": 18.75, "sample_count": 4, "duration_seconds": 0.597}, "timestamp": "2026-01-17T18:05:00.297500"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1143.47, "latencies_ms": [1143.47], "images_per_second": 0.875, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "zebra: 2\ngrass: 2\nfence: 1\nzebra: 1\nzebra: 1\nzebra: 1\nzebra: 1\nzebra: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.02, "peak": 37.82, "min": 21.66}, "VIN": {"avg": 63.04, "peak": 78.01, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.55}}, "power_watts_avg": 28.02, "energy_joules_est": 32.05, "sample_count": 8, "duration_seconds": 1.144}, "timestamp": "2026-01-17T18:05:01.451264"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 790.307, "latencies_ms": [790.307], "images_per_second": 1.265, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The zebras are positioned close together in the foreground, creating a sense of proximity and interaction. The grassy field extends in the background, providing a natural setting for the zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.31, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 64.17, "peak": 81.26, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.31, "energy_joules_est": 23.18, "sample_count": 5, "duration_seconds": 0.791}, "timestamp": "2026-01-17T18:05:02.248831"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 791.886, "latencies_ms": [791.886], "images_per_second": 1.263, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Two zebras are grazing on green grass in a grassy field. The zebras are facing opposite directions, appearing to be enjoying their meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.4, "ram_available_mb": 100232.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.1, "ram_available_mb": 100233.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.15, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 65.84, "peak": 87.15, "min": 59.66}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 23.1, "sample_count": 6, "duration_seconds": 0.792}, "timestamp": "2026-01-17T18:05:03.046961"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 834.579, "latencies_ms": [834.579], "images_per_second": 1.198, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The zebras have black and white stripes. The lighting appears to be natural, possibly sunlight, giving the scene a bright and lively atmosphere. The grass appears to be short and green, suggesting a healthy environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25539.1, "ram_available_mb": 100233.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.69, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 63.93, "peak": 84.83, "min": 56.69}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 23.95, "sample_count": 6, "duration_seconds": 0.835}, "timestamp": "2026-01-17T18:05:03.887622"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 745.706, "latencies_ms": [745.706], "images_per_second": 1.341, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two young men in white shirts and dark pants are riding a green bicycle down a busy city street, surrounded by other motorcycles and pedestrians.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25539.1, "ram_available_mb": 100233.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.54, "peak": 35.44, "min": 24.42}, "VIN": {"avg": 63.78, "peak": 76.85, "min": 57.9}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.54, "energy_joules_est": 22.04, "sample_count": 5, "duration_seconds": 0.746}, "timestamp": "2026-01-17T18:05:04.646438"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1254.061, "latencies_ms": [1254.061], "images_per_second": 0.797, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "bicycle: 2\nmotorcycle: 3\nscooter: 2\nman: 2\nwoman: 1\nman on bike: 2\nstorefront: 2\nsign: 1\nplants: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 26.34, "peak": 35.44, "min": 20.88}, "VIN": {"avg": 63.48, "peak": 82.69, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.34, "energy_joules_est": 33.04, "sample_count": 9, "duration_seconds": 1.254}, "timestamp": "2026-01-17T18:05:05.906496"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 793.446, "latencies_ms": [793.446], "images_per_second": 1.26, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the bicycle and motorbikes positioned behind them.  The background includes other vehicles and buildings, indicating a busy urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.91, "peak": 33.86, "min": 24.03}, "VIN": {"avg": 66.44, "peak": 98.63, "min": 55.85}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.91, "energy_joules_est": 22.95, "sample_count": 5, "duration_seconds": 0.794}, "timestamp": "2026-01-17T18:05:06.706091"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 653.593, "latencies_ms": [653.593], "images_per_second": 1.53, "prompt_tokens": 19, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Two young men in uniform are riding a green bicycle on a busy city street, surrounded by other motorcycles and pedestrians.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.21, "peak": 35.44, "min": 26.77}, "VIN": {"avg": 66.23, "peak": 77.22, "min": 60.59}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.21, "energy_joules_est": 20.41, "sample_count": 4, "duration_seconds": 0.654}, "timestamp": "2026-01-17T18:05:07.365725"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1041.021, "latencies_ms": [1041.021], "images_per_second": 0.961, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene is brightly lit, likely by natural or artificial light. The colors are vibrant and varied, contributing to the lively atmosphere. The materials appear to be primarily metal and plastic, typical of urban transportation. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.63, "peak": 37.01, "min": 22.45}, "VIN": {"avg": 61.78, "peak": 73.15, "min": 51.86}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.63, "energy_joules_est": 29.81, "sample_count": 7, "duration_seconds": 1.041}, "timestamp": "2026-01-17T18:05:08.413025"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 553.841, "latencies_ms": [553.841], "images_per_second": 1.806, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Two tennis players are engaged in a match on a grass court, with spectators watching from the stands.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 30.33, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 66.36, "peak": 87.01, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 16.81, "sample_count": 4, "duration_seconds": 0.554}, "timestamp": "2026-01-17T18:05:08.978823"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1096.924, "latencies_ms": [1096.924], "images_per_second": 0.912, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "chair: 2\nnet: 1\ntennis ball: 1\ntennis racket: 1\ntennis player: 2\nspectators: 10\ntennis court: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 28.26, "peak": 37.8, "min": 21.66}, "VIN": {"avg": 62.64, "peak": 83.2, "min": 57.2}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.26, "energy_joules_est": 31.01, "sample_count": 8, "duration_seconds": 1.097}, "timestamp": "2026-01-17T18:05:10.081766"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 688.031, "latencies_ms": [688.031], "images_per_second": 1.453, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The tennis court is positioned in the foreground, with the players positioned near the center. The spectators are seated in the background, occupying the upper portion of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.54, "peak": 34.26, "min": 24.82}, "VIN": {"avg": 65.59, "peak": 82.33, "min": 59.55}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.54, "energy_joules_est": 20.34, "sample_count": 5, "duration_seconds": 0.688}, "timestamp": "2026-01-17T18:05:10.776272"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 865.719, "latencies_ms": [865.719], "images_per_second": 1.155, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The scene depicts a tennis match taking place on a grass court, with two players actively engaged in a rally. Spectators are seated in stands surrounding the court, watching the match unfold.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25538.9, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 7.2, "ram_used_mb": 25538.1, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.34, "peak": 37.01, "min": 23.64}, "VIN": {"avg": 63.55, "peak": 76.15, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.34, "energy_joules_est": 25.41, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T18:05:11.648001"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 882.979, "latencies_ms": [882.979], "images_per_second": 1.133, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The tennis court is green and appears to be well-maintained. The lighting is bright, illuminating the players and the court surface. The materials used are likely durable and suitable for the outdoor conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.1, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 28.88, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 66.98, "peak": 84.6, "min": 61.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 25.51, "sample_count": 6, "duration_seconds": 0.883}, "timestamp": "2026-01-17T18:05:12.537359"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 742.779, "latencies_ms": [742.779], "images_per_second": 1.346, "prompt_tokens": 8, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The living room features a brown sofa, a wooden coffee table with a sewing machine, a flat-screen TV, and several potted plants, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25538.0, "ram_available_mb": 100234.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 29.86, "peak": 35.85, "min": 24.82}, "VIN": {"avg": 68.25, "peak": 99.87, "min": 56.16}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 22.19, "sample_count": 5, "duration_seconds": 0.743}, "timestamp": "2026-01-17T18:05:13.291912"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1418.025, "latencies_ms": [1418.025], "images_per_second": 0.705, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "couch: 1\ntelevision: 1\ncurtains: 2\nwindow: 1\ntable: 1\nsofa: 1\npillow: 1\nplants: 2\nsewing machine: 1\nfloor cushions: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.9, "ram_available_mb": 100234.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.22, "min": 14.1}, "VDD_GPU": {"avg": 25.24, "peak": 36.23, "min": 20.09}, "VIN": {"avg": 59.88, "peak": 65.06, "min": 51.04}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 25.24, "energy_joules_est": 35.8, "sample_count": 10, "duration_seconds": 1.418}, "timestamp": "2026-01-17T18:05:14.715899"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 909.299, "latencies_ms": [909.299], "images_per_second": 1.1, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The sofa and coffee table are positioned close together, creating a cozy and intimate seating area. The TV and sewing machine are placed further back in the room, providing a clear separation from the seating area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.76, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 64.9, "peak": 86.25, "min": 56.65}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.76, "energy_joules_est": 25.25, "sample_count": 6, "duration_seconds": 0.91}, "timestamp": "2026-01-17T18:05:15.635349"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 863.15, "latencies_ms": [863.15], "images_per_second": 1.159, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The living room features a brown sofa, a wooden coffee table with a sewing machine, and a flat-screen TV. Various plants are scattered throughout the space, adding a touch of greenery and color.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.23, "peak": 35.06, "min": 22.85}, "VIN": {"avg": 66.94, "peak": 83.47, "min": 62.07}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 24.38, "sample_count": 6, "duration_seconds": 0.864}, "timestamp": "2026-01-17T18:05:16.504672"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 812.705, "latencies_ms": [812.705], "images_per_second": 1.23, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The room features a warm color scheme, with brown furniture, white curtains, and natural light coming in through the window. The lighting appears soft and diffused, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.88, "peak": 35.85, "min": 23.24}, "VIN": {"avg": 65.34, "peak": 90.53, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.88, "energy_joules_est": 23.48, "sample_count": 6, "duration_seconds": 0.813}, "timestamp": "2026-01-17T18:05:17.323499"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 704.367, "latencies_ms": [704.367], "images_per_second": 1.42, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A female tennis player in a red dress and white visor stands on a clay court, holding a tennis racket and appearing to celebrate a point.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25538.7, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25539.0, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.4, "peak": 35.85, "min": 25.2}, "VIN": {"avg": 68.3, "peak": 97.03, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.4, "energy_joules_est": 21.43, "sample_count": 5, "duration_seconds": 0.705}, "timestamp": "2026-01-17T18:05:18.037775"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1123.894, "latencies_ms": [1123.894], "images_per_second": 0.89, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "woman: 1\ntennis racket: 1\ntennis dress: 1\ntennis visor: 1\ntennis court: 1\nclothing: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25538.7, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 27.52, "peak": 36.24, "min": 21.66}, "VIN": {"avg": 66.24, "peak": 93.19, "min": 60.09}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 30.94, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-17T18:05:19.167655"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 713.54, "latencies_ms": [713.54], "images_per_second": 1.401, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The tennis player is positioned in the foreground of the image, facing the viewer. The tennis court extends into the background, creating a sense of depth and space.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25538.7, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.7, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.3, "peak": 34.26, "min": 24.41}, "VIN": {"avg": 68.55, "peak": 100.73, "min": 59.05}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 20.92, "sample_count": 5, "duration_seconds": 0.714}, "timestamp": "2026-01-17T18:05:19.891502"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 747.447, "latencies_ms": [747.447], "images_per_second": 1.338, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A woman in a red tennis dress and visor is standing on a clay tennis court, holding a tennis racket and appearing to be in the middle of a tennis match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.7, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.49, "peak": 36.23, "min": 25.21}, "VIN": {"avg": 67.54, "peak": 97.92, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.49, "energy_joules_est": 22.8, "sample_count": 5, "duration_seconds": 0.748}, "timestamp": "2026-01-17T18:05:20.645138"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 709.209, "latencies_ms": [709.209], "images_per_second": 1.41, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The tennis player is wearing a bright red dress and a white visor. The clay court is reddish-brown, and the lighting appears to be natural sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.0, "peak": 35.83, "min": 24.81}, "VIN": {"avg": 68.75, "peak": 94.94, "min": 61.23}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.0, "energy_joules_est": 21.28, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T18:05:21.359963"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 819.368, "latencies_ms": [819.368], "images_per_second": 1.22, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A bustling city street lined with brick buildings, cars, and pedestrians, features a prominent OmniFest billboard and several businesses, including a restaurant and a movie theater.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.47, "peak": 36.23, "min": 23.64}, "VIN": {"avg": 64.53, "peak": 90.06, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.47, "energy_joules_est": 24.16, "sample_count": 6, "duration_seconds": 0.82}, "timestamp": "2026-01-17T18:05:22.190176"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1070.858, "latencies_ms": [1070.858], "images_per_second": 0.934, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "billboard: 1\nstreet light: 2\ncar: 3\nbuilding: 6\nrestaurant: 2\npeople: 2\ntable: 1\nchairs: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 27.37, "peak": 35.85, "min": 21.67}, "VIN": {"avg": 66.07, "peak": 99.0, "min": 57.75}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.37, "energy_joules_est": 29.32, "sample_count": 8, "duration_seconds": 1.071}, "timestamp": "2026-01-17T18:05:23.268224"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 831.755, "latencies_ms": [831.755], "images_per_second": 1.202, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The cars are parked or moving along the street, occupying the foreground. The buildings, billboards, and streetlights are positioned in the background, creating a layered perspective of the street scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 28.3, "peak": 34.66, "min": 22.85}, "VIN": {"avg": 61.51, "peak": 72.9, "min": 50.87}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.3, "energy_joules_est": 23.55, "sample_count": 6, "duration_seconds": 0.832}, "timestamp": "2026-01-17T18:05:24.106365"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 843.372, "latencies_ms": [843.372], "images_per_second": 1.186, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The scene depicts a bustling city street lined with colorful buildings, busy with cars and pedestrians. The atmosphere suggests a lively urban environment, possibly during a special event or festival.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.76, "peak": 34.66, "min": 23.64}, "VIN": {"avg": 63.39, "peak": 80.34, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 24.27, "sample_count": 6, "duration_seconds": 0.844}, "timestamp": "2026-01-17T18:05:24.956171"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 973.18, "latencies_ms": [973.18], "images_per_second": 1.028, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The buildings exhibit a mix of warm colors, including shades of red, orange, and brown. The street is well-lit by streetlights, creating a bright and inviting atmosphere. The overall scene suggests a bustling urban environment on a cloudy day.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.91, "peak": 35.45, "min": 22.45}, "VIN": {"avg": 62.84, "peak": 80.46, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 27.17, "sample_count": 7, "duration_seconds": 0.974}, "timestamp": "2026-01-17T18:05:25.935541"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 702.055, "latencies_ms": [702.055], "images_per_second": 1.424, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A female tennis player in a red outfit and white visor is poised to hit a yellow tennis ball on a blue and green court.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25538.2, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.46, "peak": 34.65, "min": 24.42}, "VIN": {"avg": 64.06, "peak": 81.38, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 20.69, "sample_count": 5, "duration_seconds": 0.702}, "timestamp": "2026-01-17T18:05:26.648698"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1086.821, "latencies_ms": [1086.821], "images_per_second": 0.92, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis court: 2\nTennis player: 1\nTennis shoes: 2\nTennis visor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.47, "peak": 35.83, "min": 21.66}, "VIN": {"avg": 64.92, "peak": 94.65, "min": 57.91}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.47, "energy_joules_est": 29.87, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-17T18:05:27.742826"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 907.472, "latencies_ms": [907.472], "images_per_second": 1.102, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The tennis player is positioned on the left side of the image, facing towards the right side. The tennis ball is in the background, near the player's feet. The tennis court is visible in the background, extending beyond the player's feet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.52, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 63.52, "peak": 74.13, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 24.99, "sample_count": 7, "duration_seconds": 0.908}, "timestamp": "2026-01-17T18:05:28.656562"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 874.025, "latencies_ms": [874.025], "images_per_second": 1.144, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A woman in a red tennis outfit is playing tennis on a blue and green court, poised to hit a yellow tennis ball. She is wearing a visor and holding a tennis racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.48, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 62.95, "peak": 75.66, "min": 59.15}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.48, "energy_joules_est": 24.91, "sample_count": 6, "duration_seconds": 0.875}, "timestamp": "2026-01-17T18:05:29.537328"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 799.411, "latencies_ms": [799.411], "images_per_second": 1.251, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The tennis player is wearing a vibrant red outfit and white shoes. The green tennis court is well-lit, and the bright yellow tennis ball is visible in mid-air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.95, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 66.85, "peak": 90.19, "min": 60.74}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 23.16, "sample_count": 6, "duration_seconds": 0.8}, "timestamp": "2026-01-17T18:05:30.343096"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 577.258, "latencies_ms": [577.258], "images_per_second": 1.732, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A blue and silver train travels along a curving track, passing through a lush green forest.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 32.39, "peak": 35.83, "min": 27.96}, "VIN": {"avg": 64.46, "peak": 73.97, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.39, "energy_joules_est": 18.71, "sample_count": 4, "duration_seconds": 0.578}, "timestamp": "2026-01-17T18:05:30.936294"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1341.939, "latencies_ms": [1341.939], "images_per_second": 0.745, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Train: 2\nTrain car: 2\nTrain tracks: 2\nTrain windows: 6\nTrain headlights: 2\nTrain door: 2\nTrain engine: 1\nTrain conductor: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.61, "peak": 38.19, "min": 20.48}, "VIN": {"avg": 62.94, "peak": 80.28, "min": 54.6}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.61, "energy_joules_est": 37.06, "sample_count": 10, "duration_seconds": 1.342}, "timestamp": "2026-01-17T18:05:32.285273"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 755.85, "latencies_ms": [755.85], "images_per_second": 1.323, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The blue and silver train is positioned in the foreground, moving towards the viewer. The train tracks are visible in the background, curving gently through the vegetation.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 30.34, "peak": 34.66, "min": 25.22}, "VIN": {"avg": 63.8, "peak": 75.41, "min": 57.21}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.34, "energy_joules_est": 22.94, "sample_count": 5, "duration_seconds": 0.756}, "timestamp": "2026-01-17T18:05:33.047431"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 739.745, "latencies_ms": [739.745], "images_per_second": 1.352, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A blue and silver train travels along a curving track through a lush, wooded area. The train appears to be moving through a natural, undisturbed environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 31.51, "peak": 36.63, "min": 25.6}, "VIN": {"avg": 62.71, "peak": 76.45, "min": 52.92}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 31.51, "energy_joules_est": 23.32, "sample_count": 5, "duration_seconds": 0.74}, "timestamp": "2026-01-17T18:05:33.793552"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 757.805, "latencies_ms": [757.805], "images_per_second": 1.32, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The train is primarily silver and blue with orange accents. The lighting suggests it might be daytime. The train appears to be made of metal and has a modern design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 31.9, "peak": 37.01, "min": 26.0}, "VIN": {"avg": 64.96, "peak": 78.09, "min": 59.25}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.34, "min": 14.55}}, "power_watts_avg": 31.9, "energy_joules_est": 24.19, "sample_count": 5, "duration_seconds": 0.758}, "timestamp": "2026-01-17T18:05:34.557227"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 675.17, "latencies_ms": [675.17], "images_per_second": 1.481, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two cats, one tabby and one striped, sleep peacefully on a pink blanket atop a red couch, accompanied by two remote controls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 31.8, "peak": 36.63, "min": 27.18}, "VIN": {"avg": 69.8, "peak": 95.79, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 31.8, "energy_joules_est": 21.49, "sample_count": 4, "duration_seconds": 0.676}, "timestamp": "2026-01-17T18:05:35.243935"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 890.993, "latencies_ms": [890.993], "images_per_second": 1.122, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "remote: 2\ncouch: 2\nblanket: 2\ncat: 2\npaws: 2\nfur: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.54, "peak": 36.63, "min": 23.64}, "VIN": {"avg": 65.52, "peak": 88.19, "min": 57.42}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 26.33, "sample_count": 6, "duration_seconds": 0.891}, "timestamp": "2026-01-17T18:05:36.145201"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 834.811, "latencies_ms": [834.811], "images_per_second": 1.198, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The cats are positioned close together on the pink blanket, with the cat on the left partially covering the remote control. The cats are lying on the couch, which occupies the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.68, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 66.02, "peak": 95.04, "min": 59.06}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.68, "energy_joules_est": 23.96, "sample_count": 6, "duration_seconds": 0.835}, "timestamp": "2026-01-17T18:05:36.986626"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 962.456, "latencies_ms": [962.456], "images_per_second": 1.039, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "Two tabby cats are sleeping on a pink blanket on a red couch. One cat is curled up on the left, while the other is curled up on the right. A remote control is visible on the couch near the cats.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.57, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 65.03, "peak": 80.85, "min": 59.48}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.57, "energy_joules_est": 26.55, "sample_count": 7, "duration_seconds": 0.963}, "timestamp": "2026-01-17T18:05:37.955380"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 632.766, "latencies_ms": [632.766], "images_per_second": 1.58, "prompt_tokens": 18, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The cats are resting on a bright pink blanket. The lighting in the image appears to be soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.71, "peak": 34.65, "min": 26.38}, "VIN": {"avg": 64.68, "peak": 79.67, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.71, "energy_joules_est": 19.45, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T18:05:38.594761"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 610.555, "latencies_ms": [610.555], "images_per_second": 1.638, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Two surfers navigate rapids in a river, surrounded by lush greenery and trees, with one skillfully riding a wave and the other holding a blue surfboard.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 25.9, "peak": 29.14, "min": 23.24}, "VIN": {"avg": 61.59, "peak": 63.37, "min": 60.77}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 25.9, "energy_joules_est": 15.83, "sample_count": 4, "duration_seconds": 0.611}, "timestamp": "2026-01-17T18:05:39.216409"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 967.047, "latencies_ms": [967.047], "images_per_second": 1.034, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "river: 2\nbridge: 1\nbench: 1\nperson: 1\nsurfboard: 1\ntree: 1\nground: 1\nleaves: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.18, "peak": 27.56, "min": 20.48}, "VIN": {"avg": 61.67, "peak": 64.91, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.18, "energy_joules_est": 22.42, "sample_count": 7, "duration_seconds": 0.967}, "timestamp": "2026-01-17T18:05:40.190894"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 847.209, "latencies_ms": [847.209], "images_per_second": 1.18, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The main object is a person surfing in a river, positioned in the foreground. The river flows in the background, separating the foreground from the background. Additionally, a bridge is visible in the background, further separating the foreground from the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 22.84, "peak": 25.99, "min": 20.48}, "VIN": {"avg": 59.66, "peak": 62.92, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.84, "energy_joules_est": 19.36, "sample_count": 6, "duration_seconds": 0.847}, "timestamp": "2026-01-17T18:05:41.044897"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 937.551, "latencies_ms": [937.551], "images_per_second": 1.067, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The scene depicts a river with rapids, where two people are surfing. One person is actively riding a wave, while another person stands nearby holding a blue surfboard. The setting is outdoors, surrounded by trees and greenery, with a stone bridge in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.28, "peak": 25.99, "min": 20.09}, "VIN": {"avg": 60.14, "peak": 63.16, "min": 57.12}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.28, "energy_joules_est": 20.9, "sample_count": 7, "duration_seconds": 0.938}, "timestamp": "2026-01-17T18:05:41.989279"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 739.029, "latencies_ms": [739.029], "images_per_second": 1.353, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The water is a murky greenish-brown color. The lighting suggests an overcast day. The scene appears to be set in a park-like environment with lush greenery and a stone bridge in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.08, "peak": 26.0, "min": 20.88}, "VIN": {"avg": 59.19, "peak": 62.35, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.08, "energy_joules_est": 17.06, "sample_count": 5, "duration_seconds": 0.739}, "timestamp": "2026-01-17T18:05:42.734404"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 649.174, "latencies_ms": [649.174], "images_per_second": 1.54, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A woman in a black jacket and blue jeans is holding a colorful kite while standing in a grassy field with a young girl.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 30.33, "peak": 33.88, "min": 26.39}, "VIN": {"avg": 66.11, "peak": 81.56, "min": 59.27}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 30.33, "energy_joules_est": 19.7, "sample_count": 4, "duration_seconds": 0.649}, "timestamp": "2026-01-17T18:05:43.394689"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 953.068, "latencies_ms": [953.068], "images_per_second": 1.049, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "woman: 2\nkite: 1\nchild: 1\ngrass: 1\ntrees: 4\nfence: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.1, "ram_available_mb": 100226.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 28.36, "peak": 35.85, "min": 22.44}, "VIN": {"avg": 62.44, "peak": 80.17, "min": 55.18}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 27.04, "sample_count": 7, "duration_seconds": 0.953}, "timestamp": "2026-01-17T18:05:44.354268"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 722.184, "latencies_ms": [722.184], "images_per_second": 1.385, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The woman and child are standing in the foreground of the image. The kite is positioned in the background, slightly further away than the woman and child.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.77, "peak": 34.65, "min": 24.82}, "VIN": {"avg": 64.42, "peak": 75.76, "min": 60.71}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.77, "energy_joules_est": 21.51, "sample_count": 5, "duration_seconds": 0.722}, "timestamp": "2026-01-17T18:05:45.083297"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 717.029, "latencies_ms": [717.029], "images_per_second": 1.395, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A woman and a child are flying a colorful butterfly kite in a park. The park is surrounded by trees, and several other people are visible in the distance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.25, "peak": 35.85, "min": 24.82}, "VIN": {"avg": 65.12, "peak": 80.66, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.25, "energy_joules_est": 21.71, "sample_count": 5, "duration_seconds": 0.718}, "timestamp": "2026-01-17T18:05:45.807522"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 771.59, "latencies_ms": [771.59], "images_per_second": 1.296, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The kite is colorful with pink, orange, and blue patterns. The woman and child are standing on a grassy field under a clear, sunny sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.49, "peak": 36.24, "min": 25.21}, "VIN": {"avg": 67.15, "peak": 96.59, "min": 57.67}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.49, "energy_joules_est": 23.53, "sample_count": 5, "duration_seconds": 0.772}, "timestamp": "2026-01-17T18:05:46.585832"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 873.513, "latencies_ms": [873.513], "images_per_second": 1.145, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A young male tennis player, wearing a white shirt and maroon cap, is crouched low and poised to hit a tennis ball with his yellow racket on a blue court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.94, "peak": 35.82, "min": 23.23}, "VIN": {"avg": 67.15, "peak": 95.64, "min": 57.89}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.94, "energy_joules_est": 25.29, "sample_count": 6, "duration_seconds": 0.874}, "timestamp": "2026-01-17T18:05:47.469079"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1459.764, "latencies_ms": [1459.764], "images_per_second": 0.685, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis court: 1\nNet: 1\nTennis player: 1\nTennis racket: 1\nTennis ball: 1\nTennis ball: 1\nTennis player: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 25.17, "peak": 35.04, "min": 20.09}, "VIN": {"avg": 61.94, "peak": 77.0, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 25.17, "energy_joules_est": 36.75, "sample_count": 11, "duration_seconds": 1.46}, "timestamp": "2026-01-17T18:05:48.935148"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 952.061, "latencies_ms": [952.061], "images_per_second": 1.05, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The tennis player is positioned in the foreground, near the net, preparing to hit the ball. The tennis ball is in the air, near the player's racket. The background features a green curtain and a banner advertising \"National Masters.\"", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.19, "peak": 33.48, "min": 22.06}, "VIN": {"avg": 63.0, "peak": 84.41, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.19, "energy_joules_est": 25.89, "sample_count": 7, "duration_seconds": 0.952}, "timestamp": "2026-01-17T18:05:49.894146"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1186.531, "latencies_ms": [1186.531], "images_per_second": 0.843, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "A young male tennis player is crouching low and preparing to hit a tennis ball during a match on a blue court. He wears a white shirt and a maroon cap. Behind him, a green wall displays a banner reading \"Are you next? National Masters.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.17, "peak": 34.65, "min": 20.88}, "VIN": {"avg": 63.87, "peak": 95.26, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.17, "energy_joules_est": 31.06, "sample_count": 9, "duration_seconds": 1.187}, "timestamp": "2026-01-17T18:05:51.090903"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 900.006, "latencies_ms": [900.006], "images_per_second": 1.111, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The tennis court is painted blue. The lighting appears to be natural sunlight, creating a bright and clear atmosphere. The materials appear to be standard tennis court surface and netting. The weather appears to be sunny and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.76, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 64.79, "peak": 88.61, "min": 53.73}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.76, "energy_joules_est": 24.99, "sample_count": 6, "duration_seconds": 0.9}, "timestamp": "2026-01-17T18:05:52.004472"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 604.929, "latencies_ms": [604.929], "images_per_second": 1.653, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "The room is filled with boxes, furniture, and personal belongings, indicating a recent move or reorganization.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.3, "min": 13.79}, "VDD_GPU": {"avg": 30.61, "peak": 34.26, "min": 26.38}, "VIN": {"avg": 69.75, "peak": 94.17, "min": 55.25}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.61, "energy_joules_est": 18.53, "sample_count": 4, "duration_seconds": 0.605}, "timestamp": "2026-01-17T18:05:52.619667"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1453.01, "latencies_ms": [1453.01], "images_per_second": 0.688, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 1, "output_text": "bed: 2\npillows: 2\nbrown comforter: 1\nwooden door: 1\nbrick wall: 1\nbox: 2\ncardboard: 1\nchair: 1\nmirror: 1\nwooden floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 25.77, "peak": 37.0, "min": 20.09}, "VIN": {"avg": 61.14, "peak": 83.64, "min": 54.12}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 25.77, "energy_joules_est": 37.45, "sample_count": 11, "duration_seconds": 1.453}, "timestamp": "2026-01-17T18:05:54.079062"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 959.018, "latencies_ms": [959.018], "images_per_second": 1.043, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The bed occupies the foreground, positioned to the left of the image. The chair is situated in the background, near a doorway. The bed and chair are separated by a small gap, further emphasizing the spatial relationship between the objects.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.79, "peak": 33.08, "min": 21.66}, "VIN": {"avg": 63.14, "peak": 78.39, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.79, "energy_joules_est": 25.7, "sample_count": 7, "duration_seconds": 0.959}, "timestamp": "2026-01-17T18:05:55.044500"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1028.272, "latencies_ms": [1028.272], "images_per_second": 0.973, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The room appears to be in the midst of moving or renovation, evidenced by the presence of boxes, furniture, and debris scattered across the floor. The room features a rustic brick wall and a wooden door, suggesting an older or industrial-style living space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.4, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.29, "peak": 34.66, "min": 21.66}, "VIN": {"avg": 62.8, "peak": 78.1, "min": 50.72}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.29, "energy_joules_est": 28.07, "sample_count": 7, "duration_seconds": 1.029}, "timestamp": "2026-01-17T18:05:56.079148"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 869.058, "latencies_ms": [869.058], "images_per_second": 1.151, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The room features a rustic brick wall and a wooden floor. The lighting is dim, creating a cozy atmosphere. The furniture includes a bed, a chair, and various boxes and furniture items scattered around the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.76, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 62.22, "peak": 74.8, "min": 55.39}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.76, "energy_joules_est": 24.14, "sample_count": 6, "duration_seconds": 0.87}, "timestamp": "2026-01-17T18:05:56.954750"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 655.792, "latencies_ms": [655.792], "images_per_second": 1.525, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A rider wearing a red and green shirt and helmet is guiding a brown horse over a wooden obstacle in a grassy field.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 30.32, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 71.15, "peak": 97.86, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.32, "energy_joules_est": 19.9, "sample_count": 4, "duration_seconds": 0.656}, "timestamp": "2026-01-17T18:05:57.625902"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1082.39, "latencies_ms": [1082.39], "images_per_second": 0.924, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "horse: 1\njockey: 1\nfence: 1\ngate: 1\nflowers: 1\nlog: 1\nground: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 27.13, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 62.94, "peak": 85.79, "min": 54.52}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 27.13, "energy_joules_est": 29.37, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T18:05:58.714494"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 880.652, "latencies_ms": [880.652], "images_per_second": 1.136, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The horse is positioned in the foreground, jumping over a wooden obstacle. The rider is positioned near the horse, guiding it through the jump. The background features trees and greenery, creating a natural setting for the event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.17, "peak": 34.26, "min": 22.86}, "VIN": {"avg": 68.21, "peak": 101.35, "min": 60.81}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 24.82, "sample_count": 6, "duration_seconds": 0.881}, "timestamp": "2026-01-17T18:05:59.602687"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 859.296, "latencies_ms": [859.296], "images_per_second": 1.164, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A rider in a red and green jacket is guiding a brown horse over a wooden obstacle in a wooded area. The horse is mid-jump, showcasing its athleticism and skill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.76, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 62.21, "peak": 82.45, "min": 54.51}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 24.72, "sample_count": 6, "duration_seconds": 0.86}, "timestamp": "2026-01-17T18:06:00.469525"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 635.748, "latencies_ms": [635.748], "images_per_second": 1.573, "prompt_tokens": 18, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The horse is brown. The rider is wearing a red and green jacket. The scene is brightly lit, suggesting sunny weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.51, "min": 14.2}, "VDD_GPU": {"avg": 31.51, "peak": 35.45, "min": 27.18}, "VIN": {"avg": 69.32, "peak": 89.74, "min": 62.19}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.51, "energy_joules_est": 20.04, "sample_count": 4, "duration_seconds": 0.636}, "timestamp": "2026-01-17T18:06:01.112437"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 563.159, "latencies_ms": [563.159], "images_per_second": 1.776, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "Two men are seated under a white umbrella, engaged in conversation, while a cart filled with tools sits nearby on a city sidewalk.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 25.8, "peak": 29.15, "min": 23.24}, "VIN": {"avg": 60.57, "peak": 64.55, "min": 57.13}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.8, "energy_joules_est": 14.54, "sample_count": 4, "duration_seconds": 0.564}, "timestamp": "2026-01-17T18:06:01.684920"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1016.455, "latencies_ms": [1016.455], "images_per_second": 0.984, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Umbrella: 2\nMen: 2\nChair: 1\nCar: 1\nBicycle: 1\nStreet: 1\nSign: 1\nBox: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.24, "peak": 27.96, "min": 20.48}, "VIN": {"avg": 61.3, "peak": 64.34, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.24, "energy_joules_est": 23.63, "sample_count": 7, "duration_seconds": 1.017}, "timestamp": "2026-01-17T18:06:02.707649"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 678.461, "latencies_ms": [678.461], "images_per_second": 1.474, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the vendor's cart and umbrella providing shade in the background. The vendor is situated near the edge of the street, further back from the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.09, "peak": 26.0, "min": 20.89}, "VIN": {"avg": 60.62, "peak": 63.72, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.09, "energy_joules_est": 15.67, "sample_count": 5, "duration_seconds": 0.679}, "timestamp": "2026-01-17T18:06:03.392359"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 852.504, "latencies_ms": [852.504], "images_per_second": 1.173, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Two men are seated under a large umbrella on a city sidewalk, seemingly engaged in conversation or observing something. Various items are displayed on tables and crates nearby, suggesting they are selling goods or providing services.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 23.18, "peak": 26.8, "min": 20.89}, "VIN": {"avg": 58.31, "peak": 61.66, "min": 54.68}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.18, "energy_joules_est": 19.77, "sample_count": 6, "duration_seconds": 0.853}, "timestamp": "2026-01-17T18:06:04.251995"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 894.405, "latencies_ms": [894.405], "images_per_second": 1.118, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene is black and white, creating a vintage feel. The lighting appears to be natural daylight, casting shadows and highlighting the textures of the objects and people. The materials appear to be simple, possibly wood or metal, contributing to the overall rustic aesthetic.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25546.3, "ram_available_mb": 100225.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 22.85, "peak": 26.01, "min": 20.48}, "VIN": {"avg": 60.33, "peak": 62.31, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 22.85, "energy_joules_est": 20.45, "sample_count": 6, "duration_seconds": 0.895}, "timestamp": "2026-01-17T18:06:05.152720"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 811.41, "latencies_ms": [811.41], "images_per_second": 1.232, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The kitchen features a white refrigerator, stove, dishwasher, and cabinets, complemented by a black ceiling fan, tiled floor, and yellow backsplash.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.03, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 61.36, "peak": 77.26, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.03, "energy_joules_est": 22.76, "sample_count": 6, "duration_seconds": 0.812}, "timestamp": "2026-01-17T18:06:05.975716"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1329.224, "latencies_ms": [1329.224], "images_per_second": 0.752, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "kitchen: 8\ncountertop: 2\ncabinets: 8\nrefrigerator: 1\ntoaster: 1\noven: 2\ndishwasher: 1\nwindow: 2\nceiling fan: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.1, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 25.44, "peak": 34.65, "min": 20.09}, "VIN": {"avg": 62.31, "peak": 82.73, "min": 54.3}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 25.44, "energy_joules_est": 33.83, "sample_count": 10, "duration_seconds": 1.33}, "timestamp": "2026-01-17T18:06:07.311216"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1011.196, "latencies_ms": [1011.196], "images_per_second": 0.989, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The main objects are positioned in a kitchen with a dark tiled floor. The kitchen is relatively small, with white cabinets and appliances. The foreground is dominated by the kitchen appliances and countertops, while the background features windows and a ceiling fan.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.1, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.1, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.18, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 65.3, "peak": 84.1, "min": 59.8}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 27.5, "sample_count": 7, "duration_seconds": 1.012}, "timestamp": "2026-01-17T18:06:08.329555"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 974.459, "latencies_ms": [974.459], "images_per_second": 1.026, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The kitchen is clean, well-lit, and features white cabinets, a black countertop, and various appliances, including a refrigerator, oven, and dishwasher. The ceiling fan adds a touch of elegance to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.1, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.8, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.18, "peak": 34.26, "min": 21.67}, "VIN": {"avg": 65.03, "peak": 82.5, "min": 58.98}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 26.49, "sample_count": 7, "duration_seconds": 0.975}, "timestamp": "2026-01-17T18:06:09.311457"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 960.429, "latencies_ms": [960.429], "images_per_second": 1.041, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The kitchen features white cabinets and dark gray tile flooring. The lighting is primarily from overhead fixtures, creating a warm ambiance. The kitchen is equipped with modern appliances, including a refrigerator and a dishwasher.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25544.8, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.8, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.41, "peak": 34.68, "min": 22.06}, "VIN": {"avg": 66.76, "peak": 98.02, "min": 57.47}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.41, "energy_joules_est": 26.34, "sample_count": 7, "duration_seconds": 0.961}, "timestamp": "2026-01-17T18:06:10.280573"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 719.21, "latencies_ms": [719.21], "images_per_second": 1.39, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A young child sleeps peacefully on a bed with a blue and white floral comforter, wearing a white tank top and holding a pacifier.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25544.8, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.22, "peak": 33.86, "min": 24.42}, "VIN": {"avg": 64.99, "peak": 86.63, "min": 55.61}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.22, "energy_joules_est": 21.03, "sample_count": 5, "duration_seconds": 0.72}, "timestamp": "2026-01-17T18:06:11.010562"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1185.441, "latencies_ms": [1185.441], "images_per_second": 0.844, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "bed: 2\nblanket: 2\npillow: 1\ntoddler: 1\npacifier: 1\ntelevision: 1\nwall: 1\ncord: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.7, "peak": 36.24, "min": 20.88}, "VIN": {"avg": 65.57, "peak": 97.3, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.7, "energy_joules_est": 31.66, "sample_count": 9, "duration_seconds": 1.186}, "timestamp": "2026-01-17T18:06:12.202152"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 851.466, "latencies_ms": [851.466], "images_per_second": 1.174, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The main object is a child lying in bed near a wall. The bed occupies the foreground, while the wall is in the background. The child is positioned close to the wall, suggesting they are close by.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.3, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.97, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 64.24, "peak": 81.2, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 23.83, "sample_count": 6, "duration_seconds": 0.852}, "timestamp": "2026-01-17T18:06:13.060035"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 660.84, "latencies_ms": [660.84], "images_per_second": 1.513, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A young child is sleeping peacefully on a bed with a train-themed blanket and pillow. The room is dimly lit, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.3, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.85, "peak": 35.06, "min": 24.81}, "VIN": {"avg": 67.4, "peak": 88.34, "min": 61.3}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.85, "energy_joules_est": 19.73, "sample_count": 5, "duration_seconds": 0.661}, "timestamp": "2026-01-17T18:06:13.727172"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 894.998, "latencies_ms": [894.998], "images_per_second": 1.117, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The child's bed is covered with a blue and white floral blanket. The lighting in the room is dim, creating a cozy atmosphere. The child is wearing a white tank top and has a pacifier in their mouth.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.86, "peak": 37.01, "min": 23.62}, "VIN": {"avg": 64.14, "peak": 77.93, "min": 56.22}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.86, "energy_joules_est": 26.73, "sample_count": 6, "duration_seconds": 0.895}, "timestamp": "2026-01-17T18:06:14.628398"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 753.488, "latencies_ms": [753.488], "images_per_second": 1.327, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A green highway sign with graffiti reading \"Quesens Bronx\" and a \"No Trucks\" sign above it indicates no trucks are allowed in the area.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 23.78, "peak": 27.17, "min": 21.27}, "VIN": {"avg": 59.68, "peak": 62.82, "min": 52.35}, "VDD_CPU_SOC_MSS": {"avg": 14.41, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 23.78, "energy_joules_est": 17.93, "sample_count": 5, "duration_seconds": 0.754}, "timestamp": "2026-01-17T18:06:15.391575"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 952.967, "latencies_ms": [952.967], "images_per_second": 1.049, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "No trucks: 1\nEast: 1\nQueens Bronx: 1\n278: 1\nSign: 1\nGraffiti: 1\nMetal structure: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.56, "peak": 26.38, "min": 20.09}, "VIN": {"avg": 60.52, "peak": 66.83, "min": 55.4}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 22.56, "energy_joules_est": 21.5, "sample_count": 7, "duration_seconds": 0.953}, "timestamp": "2026-01-17T18:06:16.351155"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 893.938, "latencies_ms": [893.938], "images_per_second": 1.119, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The green sign is positioned in the foreground, slightly to the right of the main sign. The background is mostly clear, offering a view of the sky. The sign is situated near the center of the image, suggesting it's placed at a significant point along the highway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.72, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 61.62, "peak": 64.3, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 22.72, "energy_joules_est": 20.32, "sample_count": 6, "duration_seconds": 0.894}, "timestamp": "2026-01-17T18:06:17.255269"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 838.338, "latencies_ms": [838.338], "images_per_second": 1.193, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The scene depicts a highway sign displaying \"No Trucks\" above a green sign indicating the direction to Queens Bronx. A smaller sign above the main one reads \"East 278.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.3, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.72, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 58.51, "peak": 60.58, "min": 54.24}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.72, "energy_joules_est": 19.06, "sample_count": 6, "duration_seconds": 0.839}, "timestamp": "2026-01-17T18:06:18.100291"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 644.347, "latencies_ms": [644.347], "images_per_second": 1.552, "prompt_tokens": 18, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The green highway sign is brightly lit, contrasting against the overcast sky. The sign appears to be made of metal and shows signs of wear and graffiti.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 23.83, "peak": 26.39, "min": 21.66}, "VIN": {"avg": 61.25, "peak": 63.11, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.83, "energy_joules_est": 15.36, "sample_count": 4, "duration_seconds": 0.645}, "timestamp": "2026-01-17T18:06:18.755577"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 648.014, "latencies_ms": [648.014], "images_per_second": 1.543, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A red Chevrolet pickup truck is parked in a lot with other vehicles, showcasing its classic design and shiny paint.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 30.02, "peak": 33.86, "min": 25.99}, "VIN": {"avg": 65.14, "peak": 80.7, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.97, "min": 14.18}}, "power_watts_avg": 30.02, "energy_joules_est": 19.47, "sample_count": 4, "duration_seconds": 0.648}, "timestamp": "2026-01-17T18:06:19.414030"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1351.069, "latencies_ms": [1351.069], "images_per_second": 0.74, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "Truck: 2\nTires: 3\nWheels: 2\nTailgate: 1\nChrome: 1\nLicense plate: 1\nStreet light: 1\nTent: 1\nFlag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.18, "peak": 36.21, "min": 20.09}, "VIN": {"avg": 63.3, "peak": 86.05, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 26.18, "energy_joules_est": 35.38, "sample_count": 10, "duration_seconds": 1.351}, "timestamp": "2026-01-17T18:06:20.771807"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 914.599, "latencies_ms": [914.599], "images_per_second": 1.093, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The main object is a red Chevrolet pickup truck parked in the foreground, positioned slightly to the right of the viewer. The background features other vehicles, including a blue car and a tent, suggesting an outdoor setting or event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.78, "peak": 33.08, "min": 21.66}, "VIN": {"avg": 65.34, "peak": 87.5, "min": 59.87}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.78, "energy_joules_est": 24.5, "sample_count": 7, "duration_seconds": 0.915}, "timestamp": "2026-01-17T18:06:21.692712"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 933.806, "latencies_ms": [933.806], "images_per_second": 1.071, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a vintage red Chevrolet pickup truck parked in a parking lot, possibly at a car show or similar event. Other vehicles, including a blue car and a green tent, are also visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.41, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 62.72, "peak": 75.76, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.41, "energy_joules_est": 25.61, "sample_count": 7, "duration_seconds": 0.934}, "timestamp": "2026-01-17T18:06:22.632680"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 977.084, "latencies_ms": [977.084], "images_per_second": 1.023, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The red pickup truck stands out against the light blue sky. Its shiny paint reflects the sunlight, creating a vibrant contrast. The truck is parked in a parking lot, suggesting an outdoor setting, possibly during a car show or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.4, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 64.88, "peak": 93.91, "min": 56.93}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.4, "energy_joules_est": 26.78, "sample_count": 7, "duration_seconds": 0.977}, "timestamp": "2026-01-17T18:06:23.616884"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 599.393, "latencies_ms": [599.393], "images_per_second": 1.668, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Three cows are visible behind a barbed wire fence, gazing at the camera in a rural setting.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.42, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 64.7, "peak": 79.51, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.42, "energy_joules_est": 18.25, "sample_count": 4, "duration_seconds": 0.6}, "timestamp": "2026-01-17T18:06:24.227799"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1284.108, "latencies_ms": [1284.108], "images_per_second": 0.779, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Barbed wire fence: 5\nCow ears: 4\nCow head: 3\nCow body: 2\nCow tail: 1\nGrass: 6\nTrees: 1\nSky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.13, "peak": 37.41, "min": 20.88}, "VIN": {"avg": 63.5, "peak": 80.69, "min": 55.39}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.13, "energy_joules_est": 34.84, "sample_count": 9, "duration_seconds": 1.284}, "timestamp": "2026-01-17T18:06:25.522013"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 734.087, "latencies_ms": [734.087], "images_per_second": 1.362, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the barbed wire fence separating them from the background. The cows are situated near the fence, partially obscured by it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.92, "peak": 33.89, "min": 24.43}, "VIN": {"avg": 67.47, "peak": 97.5, "min": 54.04}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 28.92, "energy_joules_est": 21.24, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T18:06:26.262111"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 709.371, "latencies_ms": [709.371], "images_per_second": 1.41, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Three cows are seen behind a barbed wire fence in a field. The black and white image captures a pastoral scene with grazing cattle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 30.01, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 64.25, "peak": 82.67, "min": 57.61}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.01, "energy_joules_est": 21.31, "sample_count": 5, "duration_seconds": 0.71}, "timestamp": "2026-01-17T18:06:26.978091"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 687.952, "latencies_ms": [687.952], "images_per_second": 1.454, "prompt_tokens": 18, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The cows are primarily black and white. The lighting suggests a sunny day. The cows are seen behind a barbed wire fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 30.26, "peak": 35.85, "min": 24.83}, "VIN": {"avg": 66.2, "peak": 85.28, "min": 58.82}, "VDD_CPU_SOC_MSS": {"avg": 14.71, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 30.26, "energy_joules_est": 20.82, "sample_count": 5, "duration_seconds": 0.688}, "timestamp": "2026-01-17T18:06:27.672915"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 653.299, "latencies_ms": [653.299], "images_per_second": 1.531, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The room features a cozy and inviting atmosphere with a warm color scheme, a comfortable bed, a fireplace, a television, and two chairs.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.97, "peak": 14.3, "min": 13.59}, "VDD_GPU": {"avg": 32.22, "peak": 36.65, "min": 27.59}, "VIN": {"avg": 65.28, "peak": 81.52, "min": 56.49}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 32.22, "energy_joules_est": 21.06, "sample_count": 4, "duration_seconds": 0.654}, "timestamp": "2026-01-17T18:06:28.336253"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1113.287, "latencies_ms": [1113.287], "images_per_second": 0.898, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "bed: 6\nchair: 2\nwindow: 2\nfireplace: 1\ntelevision: 1\nclock: 1\nlamp: 1\nsofa: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.0, "ram_available_mb": 100226.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.92, "peak": 37.03, "min": 21.67}, "VIN": {"avg": 64.33, "peak": 79.27, "min": 60.67}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.92, "energy_joules_est": 31.09, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T18:06:29.455633"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 900.197, "latencies_ms": [900.197], "images_per_second": 1.111, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The main objects are positioned in a balanced and harmonious manner, with the bed dominating the left side of the image and the seating area on the right. The seating area is situated closer to the viewer, creating a sense of proximity and comfort.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.9, "peak": 34.27, "min": 22.84}, "VIN": {"avg": 63.49, "peak": 80.98, "min": 53.28}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.9, "energy_joules_est": 25.12, "sample_count": 6, "duration_seconds": 0.9}, "timestamp": "2026-01-17T18:06:30.361855"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 707.087, "latencies_ms": [707.087], "images_per_second": 1.414, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The scene depicts a cozy bedroom with a large bed, wooden furniture, and a fireplace. The room is lit by warm lighting, creating a comfortable and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.46, "peak": 35.04, "min": 24.42}, "VIN": {"avg": 63.4, "peak": 79.97, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.46, "energy_joules_est": 20.84, "sample_count": 5, "duration_seconds": 0.707}, "timestamp": "2026-01-17T18:06:31.075001"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1071.931, "latencies_ms": [1071.931], "images_per_second": 0.933, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The room features a warm color scheme with brown and beige tones. The lighting is soft and warm, creating a cozy atmosphere. The furniture includes wooden elements and a stone fireplace, adding a rustic touch. The overall ambiance suggests a comfortable and inviting space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.61, "peak": 36.23, "min": 21.66}, "VIN": {"avg": 63.43, "peak": 86.66, "min": 54.59}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.61, "energy_joules_est": 29.6, "sample_count": 8, "duration_seconds": 1.072}, "timestamp": "2026-01-17T18:06:32.153237"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 737.941, "latencies_ms": [737.941], "images_per_second": 1.355, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "Three black birds with white speckled feathers and distinct blue-gray heads and necks are gathered on a dry, grassy hillside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.7, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.3, "peak": 34.65, "min": 24.41}, "VIN": {"avg": 65.87, "peak": 82.88, "min": 61.0}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 21.63, "sample_count": 5, "duration_seconds": 0.738}, "timestamp": "2026-01-17T18:06:32.904167"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 756.396, "latencies_ms": [756.396], "images_per_second": 1.322, "prompt_tokens": 21, "response_tokens_est": 19, "n_tiles": 1, "output_text": "bird: 3\ngrass: 2\ntree: 1\nbush: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.7, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 30.33, "peak": 35.83, "min": 24.83}, "VIN": {"avg": 64.57, "peak": 84.52, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.33, "energy_joules_est": 22.95, "sample_count": 5, "duration_seconds": 0.757}, "timestamp": "2026-01-17T18:06:33.666566"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 659.091, "latencies_ms": [659.091], "images_per_second": 1.517, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the background slightly blurred. The birds are located near the center, slightly to the right of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.72, "peak": 36.24, "min": 25.21}, "VIN": {"avg": 64.83, "peak": 83.21, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.72, "energy_joules_est": 20.26, "sample_count": 5, "duration_seconds": 0.659}, "timestamp": "2026-01-17T18:06:34.332296"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 819.105, "latencies_ms": [819.105], "images_per_second": 1.221, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Three black speckled birds with blue heads and necks are walking across a dry, grassy hillside.  The birds appear to be foraging for food amidst sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.48, "peak": 36.24, "min": 23.64}, "VIN": {"avg": 64.26, "peak": 80.19, "min": 56.15}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.48, "energy_joules_est": 24.16, "sample_count": 6, "duration_seconds": 0.82}, "timestamp": "2026-01-17T18:06:35.158048"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 911.032, "latencies_ms": [911.032], "images_per_second": 1.098, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The birds have dark gray plumage with speckled patterns. The lighting is soft and diffused, suggesting an overcast sky. The ground is covered with dry, light brown grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 27.96, "peak": 35.83, "min": 22.06}, "VIN": {"avg": 64.78, "peak": 91.12, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 14.74, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 27.96, "energy_joules_est": 25.48, "sample_count": 7, "duration_seconds": 0.911}, "timestamp": "2026-01-17T18:06:36.075228"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 615.385, "latencies_ms": [615.385], "images_per_second": 1.625, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Three people are posing for a photo on a snowy mountain slope, wearing winter gear and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.83, "peak": 35.06, "min": 26.4}, "VIN": {"avg": 70.4, "peak": 91.72, "min": 59.81}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.83, "energy_joules_est": 18.98, "sample_count": 4, "duration_seconds": 0.616}, "timestamp": "2026-01-17T18:06:36.702236"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1265.908, "latencies_ms": [1265.908], "images_per_second": 0.79, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Ski poles: 3\nSnowshoes: 2\nGoggles: 1\nJacket: 2\nPants: 2\nHat: 1\nBackpack: 1\nSnow: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 26.39, "peak": 37.04, "min": 20.48}, "VIN": {"avg": 62.87, "peak": 76.8, "min": 56.59}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 26.39, "energy_joules_est": 33.41, "sample_count": 10, "duration_seconds": 1.266}, "timestamp": "2026-01-17T18:06:37.974431"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 938.207, "latencies_ms": [938.207], "images_per_second": 1.066, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The three people are positioned in a relatively close grouping, with the person on the left closest to the camera and the person on the right furthest away. The foreground is dominated by the snow-covered ground, while the background features snow-covered trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.0, "peak": 33.47, "min": 21.66}, "VIN": {"avg": 62.78, "peak": 75.44, "min": 55.51}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.0, "energy_joules_est": 25.36, "sample_count": 7, "duration_seconds": 0.939}, "timestamp": "2026-01-17T18:06:38.919670"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 840.95, "latencies_ms": [840.95], "images_per_second": 1.189, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "Three people are snowshoeing on a snowy hill. They are dressed in winter gear and holding ski poles. The scene is set in a forested area with snow-covered trees in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.15, "peak": 34.24, "min": 22.85}, "VIN": {"avg": 62.92, "peak": 72.13, "min": 60.05}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.15, "energy_joules_est": 23.69, "sample_count": 6, "duration_seconds": 0.842}, "timestamp": "2026-01-17T18:06:39.767542"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 875.57, "latencies_ms": [875.57], "images_per_second": 1.142, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The snow is white and appears well-groomed. The lighting is bright and evenly distributed, illuminating the scene. The snow appears to be fresh and undisturbed, suggesting good weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.89, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 65.14, "peak": 80.8, "min": 59.99}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 25.31, "sample_count": 6, "duration_seconds": 0.876}, "timestamp": "2026-01-17T18:06:40.650050"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 734.731, "latencies_ms": [734.731], "images_per_second": 1.361, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A white and blue city bus numbered 51 is parked on the side of the road, displaying \"Crosstown\" on its electronic sign.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 30.49, "peak": 35.44, "min": 25.21}, "VIN": {"avg": 64.27, "peak": 86.72, "min": 54.2}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 30.49, "energy_joules_est": 22.42, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T18:06:41.397514"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1985.941, "latencies_ms": [1985.941], "images_per_second": 0.504, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 1, "output_text": "Bus: 2\nNumber plate: 1\nLicense plate: 1\nBus number: 51\nBus route: Crosstown\nBus front: 2\nBus windows: 2\nBus headlights: 2\nBus front bumper: 2\nBus side mirrors: 1\nBus front windshield wipers: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 24.35, "peak": 36.63, "min": 19.3}, "VIN": {"avg": 60.53, "peak": 74.35, "min": 50.96}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.35, "energy_joules_est": 48.37, "sample_count": 15, "duration_seconds": 1.986}, "timestamp": "2026-01-17T18:06:43.389967"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 915.963, "latencies_ms": [915.963], "images_per_second": 1.092, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The bus is positioned in the foreground, facing the viewer. The building is partially visible in the background, situated to the right of the bus. The bus is parked on the street, further back from the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.17, "peak": 33.48, "min": 23.25}, "VIN": {"avg": 63.12, "peak": 75.49, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 25.81, "sample_count": 6, "duration_seconds": 0.916}, "timestamp": "2026-01-17T18:06:44.316143"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1199.806, "latencies_ms": [1199.806], "images_per_second": 0.833, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 1, "output_text": "A white and blue bus is parked on the side of a road, displaying the number 51 and the destination \"Crosstown.\" A person can be seen standing near the bus, possibly waiting to board or observing the bus. The setting appears to be an urban environment with a brick building visible in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.74, "peak": 35.06, "min": 20.88}, "VIN": {"avg": 63.22, "peak": 94.2, "min": 52.47}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.74, "energy_joules_est": 32.09, "sample_count": 9, "duration_seconds": 1.2}, "timestamp": "2026-01-17T18:06:45.522710"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1086.26, "latencies_ms": [1086.26], "images_per_second": 0.921, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The bus is primarily white with blue and green accents. The lighting appears to be bright and sunny, illuminating the bus and its surroundings. The materials appear to be standard bus construction, with clear glass windows and metal framing. The weather appears to be sunny and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.18, "peak": 34.66, "min": 21.26}, "VIN": {"avg": 62.21, "peak": 84.8, "min": 54.57}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 29.53, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-17T18:06:46.619164"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 827.917, "latencies_ms": [827.917], "images_per_second": 1.208, "prompt_tokens": 8, "response_tokens_est": 41, "n_tiles": 1, "output_text": "A man dressed in a blue blazer, white collared shirt, striped tie, gray pleated skirt, black tights, and black shoes is standing against a white wall, holding a black purse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 22.91, "peak": 26.38, "min": 20.48}, "VIN": {"avg": 58.66, "peak": 61.04, "min": 56.6}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.91, "energy_joules_est": 18.98, "sample_count": 6, "duration_seconds": 0.828}, "timestamp": "2026-01-17T18:06:47.457717"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1080.107, "latencies_ms": [1080.107], "images_per_second": 0.926, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Jacket: 2\nSkirt: 1\nTie: 1\nShirt: 1\nPants: 1\nTights: 1\nHandbag: 1\nShoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 22.01, "peak": 26.0, "min": 19.7}, "VIN": {"avg": 61.0, "peak": 64.02, "min": 54.88}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.01, "energy_joules_est": 23.79, "sample_count": 8, "duration_seconds": 1.081}, "timestamp": "2026-01-17T18:06:48.544527"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 793.94, "latencies_ms": [793.94], "images_per_second": 1.26, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The man is positioned in the foreground, wearing a school uniform and holding a handbag. The handbag is situated near his feet. The background features a white wall with a red stripe, providing a neutral backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.52, "peak": 25.61, "min": 20.48}, "VIN": {"avg": 59.58, "peak": 61.18, "min": 57.98}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.52, "energy_joules_est": 17.89, "sample_count": 6, "duration_seconds": 0.794}, "timestamp": "2026-01-17T18:06:49.344754"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 834.265, "latencies_ms": [834.265], "images_per_second": 1.199, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "A young man is standing in a school uniform, wearing a blue blazer, white shirt, striped tie, and a gray pleated skirt. He is holding a black purse and posing in front of a white wall with a red stripe.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.85, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 59.76, "peak": 61.98, "min": 53.14}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.85, "energy_joules_est": 19.07, "sample_count": 6, "duration_seconds": 0.835}, "timestamp": "2026-01-17T18:06:50.185620"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1134.278, "latencies_ms": [1134.278], "images_per_second": 0.882, "prompt_tokens": 18, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The man's attire is predominantly blue and gray, accented with red stripes. The lighting appears to be artificial, casting a soft glow on his outfit. The materials appear to be standard school uniforms, composed of fabric and possibly some synthetic elements. The weather appears to be overcast, contributing to the subdued lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.11, "peak": 26.0, "min": 19.7}, "VIN": {"avg": 61.0, "peak": 64.63, "min": 55.16}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.11, "energy_joules_est": 25.08, "sample_count": 8, "duration_seconds": 1.135}, "timestamp": "2026-01-17T18:06:51.325861"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 675.658, "latencies_ms": [675.658], "images_per_second": 1.48, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A train yard filled with multiple train cars and tracks is visible in the hazy, sepia-toned image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.36, "peak": 32.3, "min": 24.03}, "VIN": {"avg": 66.76, "peak": 85.98, "min": 60.08}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 19.17, "sample_count": 5, "duration_seconds": 0.676}, "timestamp": "2026-01-17T18:06:52.015130"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1108.873, "latencies_ms": [1108.873], "images_per_second": 0.902, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Train car: 6\nTrain cars: 5\nTrain tracks: 4\nPower lines: 6\nTrees: 2\nBuilding: 1\nFog: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 27.18, "peak": 36.23, "min": 21.27}, "VIN": {"avg": 61.76, "peak": 77.48, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.18, "energy_joules_est": 30.15, "sample_count": 8, "duration_seconds": 1.109}, "timestamp": "2026-01-17T18:06:53.130383"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 734.437, "latencies_ms": [734.437], "images_per_second": 1.362, "prompt_tokens": 25, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the train cars extending into the background. The train tracks run parallel to the train cars, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.14, "peak": 33.86, "min": 24.42}, "VIN": {"avg": 65.55, "peak": 85.92, "min": 59.07}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.14, "energy_joules_est": 21.41, "sample_count": 5, "duration_seconds": 0.735}, "timestamp": "2026-01-17T18:06:53.871302"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1143.419, "latencies_ms": [1143.419], "images_per_second": 0.875, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene depicts a train yard or rail yard with multiple train cars and tracks. The image is captured in sepia tones, giving it a vintage or nostalgic feel. The hazy atmosphere and limited visibility suggest it might be early morning or late evening.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25544.9, "ram_available_mb": 100227.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.22, "peak": 35.83, "min": 21.26}, "VIN": {"avg": 64.69, "peak": 89.42, "min": 59.27}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.22, "energy_joules_est": 31.14, "sample_count": 8, "duration_seconds": 1.144}, "timestamp": "2026-01-17T18:06:55.021186"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 964.921, "latencies_ms": [964.921], "images_per_second": 1.036, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The scene is dominated by muted, brownish-gold hues, creating a somber atmosphere. The lighting suggests an early morning or late evening setting, casting a hazy glow over the train yard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.22, "peak": 33.86, "min": 22.05}, "VIN": {"avg": 63.15, "peak": 90.99, "min": 54.98}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 27.22, "energy_joules_est": 26.28, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T18:06:55.992373"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 567.926, "latencies_ms": [567.926], "images_per_second": 1.761, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A bathroom floor is cluttered with various items, including a toilet, potted plant, skis, and multiple pairs of gloves.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 24.12, "peak": 26.77, "min": 22.05}, "VIN": {"avg": 59.95, "peak": 62.76, "min": 56.21}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.12, "energy_joules_est": 13.71, "sample_count": 4, "duration_seconds": 0.569}, "timestamp": "2026-01-17T18:06:56.571024"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1101.848, "latencies_ms": [1101.848], "images_per_second": 0.908, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "toilet: 1\nplant: 1\nsnowboard: 1\ngloves: 2\nskis: 2\nhelmet: 1\nshoes: 4\nclothes: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 22.6, "peak": 27.17, "min": 20.09}, "VIN": {"avg": 61.48, "peak": 64.77, "min": 53.93}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.6, "energy_joules_est": 24.91, "sample_count": 8, "duration_seconds": 1.102}, "timestamp": "2026-01-17T18:06:57.680367"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 616.051, "latencies_ms": [616.051], "images_per_second": 1.623, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The toilet is positioned to the left of the image, near the foreground. The bathroom floor extends into the background, partially obscured by the toilet and shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.53, "peak": 25.59, "min": 21.66}, "VIN": {"avg": 60.56, "peak": 61.45, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.53, "energy_joules_est": 14.5, "sample_count": 4, "duration_seconds": 0.616}, "timestamp": "2026-01-17T18:06:58.304797"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 773.386, "latencies_ms": [773.386], "images_per_second": 1.293, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a bathroom with a toilet, sink, and various personal items scattered on the floor, including skis, gloves, and a helmet. The overall setting suggests a casual or possibly disorganized environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.1, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.63, "peak": 26.77, "min": 21.27}, "VIN": {"avg": 59.73, "peak": 63.73, "min": 56.88}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 18.29, "sample_count": 5, "duration_seconds": 0.774}, "timestamp": "2026-01-17T18:06:59.088578"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 832.571, "latencies_ms": [832.571], "images_per_second": 1.201, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The bathroom features a white toilet and a small plant on a stand. The floor is covered in skis and ski boots, indicating skiing activities. The lighting is dim, contributing to the overall subdued atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 22.91, "peak": 26.39, "min": 20.48}, "VIN": {"avg": 59.88, "peak": 63.47, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.91, "energy_joules_est": 19.08, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T18:06:59.927320"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 622.151, "latencies_ms": [622.151], "images_per_second": 1.607, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A polar bear is playing with colorful balls in a pool of water, appearing to be enjoying its time outdoors.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 29.83, "peak": 33.48, "min": 26.0}, "VIN": {"avg": 65.68, "peak": 82.39, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.83, "energy_joules_est": 18.58, "sample_count": 4, "duration_seconds": 0.623}, "timestamp": "2026-01-17T18:07:00.562051"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1021.767, "latencies_ms": [1021.767], "images_per_second": 0.979, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "ball: 3\npaw: 2\nball: 2\nball: 2\nball: 2\nball: 2\nball: 2\nball: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.64, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 62.43, "peak": 78.3, "min": 53.94}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.64, "energy_joules_est": 29.28, "sample_count": 7, "duration_seconds": 1.022}, "timestamp": "2026-01-17T18:07:01.591108"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 773.117, "latencies_ms": [773.117], "images_per_second": 1.293, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The green and yellow balls are positioned in the foreground, close to the polar bear. The background is primarily a sandy area, suggesting the polar bear is in a zoo enclosure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.3, "peak": 34.66, "min": 24.41}, "VIN": {"avg": 64.19, "peak": 83.73, "min": 54.49}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 22.67, "sample_count": 5, "duration_seconds": 0.774}, "timestamp": "2026-01-17T18:07:02.370427"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 667.859, "latencies_ms": [667.859], "images_per_second": 1.497, "prompt_tokens": 19, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A polar bear is playing with colorful balls in a pool of water. The setting appears to be a zoo enclosure with rocks and sandy ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.6, "ram_available_mb": 100226.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.08, "peak": 35.82, "min": 24.82}, "VIN": {"avg": 70.3, "peak": 102.23, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.08, "energy_joules_est": 20.11, "sample_count": 5, "duration_seconds": 0.669}, "timestamp": "2026-01-17T18:07:03.047254"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 650.414, "latencies_ms": [650.414], "images_per_second": 1.537, "prompt_tokens": 18, "response_tokens_est": 22, "n_tiles": 1, "output_text": "The polar bear is predominantly white. The water appears dark and murky. The bear is playing with colorful balls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.9, "ram_available_mb": 100226.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 31.89, "peak": 36.23, "min": 27.17}, "VIN": {"avg": 66.47, "peak": 89.53, "min": 54.63}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 31.89, "energy_joules_est": 20.75, "sample_count": 4, "duration_seconds": 0.651}, "timestamp": "2026-01-17T18:07:03.704408"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 674.32, "latencies_ms": [674.32], "images_per_second": 1.483, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A person is sitting on a wooden chair wearing blue slippers and jeans, while holding a Samsung cell phone in their right hand.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25546.1, "ram_available_mb": 100226.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.7, "peak": 36.21, "min": 26.79}, "VIN": {"avg": 64.87, "peak": 75.12, "min": 59.4}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 31.7, "energy_joules_est": 21.38, "sample_count": 4, "duration_seconds": 0.675}, "timestamp": "2026-01-17T18:07:04.389171"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1016.82, "latencies_ms": [1016.82], "images_per_second": 0.983, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "chair: 2\nslippers: 2\nsocks: 2\nphone: 1\nwindow: 1\nwooden floor: 2\nperson: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.25, "peak": 36.63, "min": 22.06}, "VIN": {"avg": 64.81, "peak": 83.0, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 28.25, "energy_joules_est": 28.73, "sample_count": 7, "duration_seconds": 1.017}, "timestamp": "2026-01-17T18:07:05.413584"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 698.642, "latencies_ms": [698.642], "images_per_second": 1.431, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The chair is positioned in the foreground, while the cell phone is held in the background. The chair and cell phone are located near each other, suggesting proximity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.07, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 62.01, "peak": 71.42, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.07, "energy_joules_est": 20.32, "sample_count": 5, "duration_seconds": 0.699}, "timestamp": "2026-01-17T18:07:06.118476"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 894.803, "latencies_ms": [894.803], "images_per_second": 1.118, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The scene depicts a person sitting on a wooden chair, wearing blue slippers and jeans. The right side shows a Samsung cell phone being held by a hand, displaying the time and date.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.3, "ram_available_mb": 100225.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.88, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 62.16, "peak": 73.64, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 25.85, "sample_count": 6, "duration_seconds": 0.895}, "timestamp": "2026-01-17T18:07:07.019644"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 886.013, "latencies_ms": [886.013], "images_per_second": 1.129, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The chair is light brown and appears to be made of wood. The floor is made of wood planks. The lighting in the image is soft and diffused, suggesting natural light from a nearby window.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.49, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 67.66, "peak": 97.68, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 25.25, "sample_count": 6, "duration_seconds": 0.886}, "timestamp": "2026-01-17T18:07:07.911817"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 630.633, "latencies_ms": [630.633], "images_per_second": 1.586, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A yellow and black train travels down a snowy track, passing through a forest of snow-covered trees and bushes.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25546.6, "ram_available_mb": 100225.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 32.11, "peak": 35.06, "min": 27.98}, "VIN": {"avg": 65.33, "peak": 80.57, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 32.11, "energy_joules_est": 20.27, "sample_count": 4, "duration_seconds": 0.631}, "timestamp": "2026-01-17T18:07:08.559367"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1302.358, "latencies_ms": [1302.358], "images_per_second": 0.768, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Train: 1\nTrees: 6\nSnow: 6\nSky: 1\nTrain tracks: 2\nPower lines: 2\nTraffic light: 1\nSnow-covered ground: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 27.71, "peak": 37.82, "min": 20.89}, "VIN": {"avg": 62.71, "peak": 77.12, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.71, "energy_joules_est": 36.1, "sample_count": 9, "duration_seconds": 1.303}, "timestamp": "2026-01-17T18:07:09.868001"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 762.694, "latencies_ms": [762.694], "images_per_second": 1.311, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The train is moving from left to right, occupying the foreground of the image. The snowy landscape and trees in the background create a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25547.1, "ram_available_mb": 100225.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.3}, "VDD_GPU": {"avg": 29.78, "peak": 34.27, "min": 24.81}, "VIN": {"avg": 60.06, "peak": 70.71, "min": 54.41}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 22.72, "sample_count": 5, "duration_seconds": 0.763}, "timestamp": "2026-01-17T18:07:10.636894"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 855.549, "latencies_ms": [855.549], "images_per_second": 1.169, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A yellow train travels through a snowy landscape, passing through a forest of snow-covered trees. The scene is captured during winter, with snow falling and covering the ground and vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.99, "peak": 36.62, "min": 24.02}, "VIN": {"avg": 63.09, "peak": 76.38, "min": 55.69}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.99, "energy_joules_est": 25.67, "sample_count": 6, "duration_seconds": 0.856}, "timestamp": "2026-01-17T18:07:11.499031"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1029.42, "latencies_ms": [1029.42], "images_per_second": 0.971, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The train is yellow and appears to be moving through the snowy landscape. The lighting is soft and diffused, typical of overcast conditions. The scene is dominated by shades of white and gray, reflecting the snow and sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.15, "peak": 36.23, "min": 21.66}, "VIN": {"avg": 63.23, "peak": 83.47, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.15, "energy_joules_est": 28.99, "sample_count": 8, "duration_seconds": 1.03}, "timestamp": "2026-01-17T18:07:12.538920"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 588.706, "latencies_ms": [588.706], "images_per_second": 1.699, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "People are walking through a large pile of snow on a city street, navigating the snow-covered path.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 31.11, "peak": 35.06, "min": 26.79}, "VIN": {"avg": 63.56, "peak": 76.57, "min": 57.49}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.11, "energy_joules_est": 18.34, "sample_count": 4, "duration_seconds": 0.59}, "timestamp": "2026-01-17T18:07:13.139637"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 946.106, "latencies_ms": [946.106], "images_per_second": 1.057, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "snow: 10\nfire hydrant: 1\npiles of snow: 5\npeople walking: 4\ncar: 1\nbuilding: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.98, "peak": 37.8, "min": 22.45}, "VIN": {"avg": 66.87, "peak": 99.27, "min": 60.18}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.98, "energy_joules_est": 27.43, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T18:07:14.092839"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 853.271, "latencies_ms": [853.271], "images_per_second": 1.172, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The snow pile is positioned to the left of the foreground, partially obscuring the view of the street and sidewalk.  In the background, several people can be seen walking along the snow-covered sidewalk and road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.88, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 64.36, "peak": 85.11, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 24.65, "sample_count": 6, "duration_seconds": 0.854}, "timestamp": "2026-01-17T18:07:14.953069"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 787.971, "latencies_ms": [787.971], "images_per_second": 1.269, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A group of people are walking through a snowy city street, navigating around large piles of snow. A fire hydrant is partially buried in the snow, indicating recent snowfall.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.01, "peak": 35.04, "min": 23.64}, "VIN": {"avg": 67.65, "peak": 96.91, "min": 59.1}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.01, "energy_joules_est": 22.88, "sample_count": 6, "duration_seconds": 0.789}, "timestamp": "2026-01-17T18:07:15.748047"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1183.498, "latencies_ms": [1183.498], "images_per_second": 0.845, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The snow is white and appears thick and heavy. The lighting suggests an overcast day, with diffused light. The snow appears to be made of compressed snow and ice, potentially a mixture of snow and ice. The scene depicts a winter day with people walking through deep snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25546.8, "ram_available_mb": 100225.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.35, "peak": 35.45, "min": 20.88}, "VIN": {"avg": 63.95, "peak": 81.26, "min": 57.42}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.35, "energy_joules_est": 31.19, "sample_count": 9, "duration_seconds": 1.184}, "timestamp": "2026-01-17T18:07:16.937738"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 554.298, "latencies_ms": [554.298], "images_per_second": 1.804, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A yellow and black sign with a cartoonish mouth is attached to a metal pole near a tree in a park-like setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 12.9, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 24.03, "peak": 26.39, "min": 22.07}, "VIN": {"avg": 62.92, "peak": 65.0, "min": 61.8}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 24.03, "energy_joules_est": 13.33, "sample_count": 4, "duration_seconds": 0.555}, "timestamp": "2026-01-17T18:07:17.504838"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 892.47, "latencies_ms": [892.47], "images_per_second": 1.12, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "Tow Zone: 1\nNo Parking sign: 1\nYellow sign: 1\nStreet light: 1\nBuilding: 1\nTrees: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 12.8, "ram_used_mb": 25617.2, "ram_available_mb": 100154.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.22, "min": 13.9}, "VDD_GPU": {"avg": 23.38, "peak": 27.18, "min": 20.89}, "VIN": {"avg": 63.25, "peak": 67.01, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 17.06, "peak": 17.32, "min": 16.14}}, "power_watts_avg": 23.38, "energy_joules_est": 20.87, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T18:07:18.403463"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 673.563, "latencies_ms": [673.563], "images_per_second": 1.485, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The \"Tow Zone\" sign is positioned in the foreground, slightly to the right of the central pole. The tree on the left partially obscures the view of the building in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25617.2, "ram_available_mb": 100154.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 6.9, "ram_used_mb": 25617.9, "ram_available_mb": 100154.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 23.32, "peak": 26.0, "min": 21.27}, "VIN": {"avg": 61.1, "peak": 64.42, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.14, "min": 15.74}}, "power_watts_avg": 23.32, "energy_joules_est": 15.71, "sample_count": 5, "duration_seconds": 0.674}, "timestamp": "2026-01-17T18:07:19.084079"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 793.498, "latencies_ms": [793.498], "images_per_second": 1.26, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene is set in a park-like area with lush green trees and a brick building in the background. A tow zone sign is mounted on a pole, and a yellow sign with a cartoon face is attached to the same pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25617.9, "ram_available_mb": 100154.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25618.0, "ram_available_mb": 100154.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 23.1, "peak": 26.38, "min": 20.87}, "VIN": {"avg": 60.44, "peak": 62.9, "min": 57.06}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 23.1, "energy_joules_est": 18.35, "sample_count": 6, "duration_seconds": 0.794}, "timestamp": "2026-01-17T18:07:19.884658"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 633.696, "latencies_ms": [633.696], "images_per_second": 1.578, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The sign is primarily white with red lettering. The lighting suggests a sunny day. The sign appears to be made of metal and has a weathered appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25618.0, "ram_available_mb": 100154.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25618.0, "ram_available_mb": 100154.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 23.92, "peak": 26.38, "min": 22.05}, "VIN": {"avg": 58.49, "peak": 60.29, "min": 56.05}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.92, "energy_joules_est": 15.16, "sample_count": 4, "duration_seconds": 0.634}, "timestamp": "2026-01-17T18:07:20.524989"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 558.744, "latencies_ms": [558.744], "images_per_second": 1.79, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A beige teddy bear wearing glasses sits on a red surface with a keyboard, mouse, microphone, and iPod.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25618.0, "ram_available_mb": 100154.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25618.0, "ram_available_mb": 100154.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 24.22, "peak": 26.77, "min": 22.05}, "VIN": {"avg": 60.59, "peak": 62.08, "min": 59.1}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.22, "energy_joules_est": 13.54, "sample_count": 4, "duration_seconds": 0.559}, "timestamp": "2026-01-17T18:07:21.093218"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 927.26, "latencies_ms": [927.26], "images_per_second": 1.078, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "bear: 1\nglasses: 1\niPod: 1\nmicrophone: 1\nkeyboard: 1\nmouse: 1\nred surface: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25618.0, "ram_available_mb": 100154.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25618.2, "ram_available_mb": 100154.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.02, "peak": 27.18, "min": 20.48}, "VIN": {"avg": 59.99, "peak": 63.22, "min": 57.7}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.02, "energy_joules_est": 21.35, "sample_count": 7, "duration_seconds": 0.927}, "timestamp": "2026-01-17T18:07:22.026736"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 661.162, "latencies_ms": [661.162], "images_per_second": 1.512, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The teddy bear is positioned in the foreground, slightly to the right of the keyboard. The keyboard and teddy bear are placed close together, creating a sense of proximity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25618.2, "ram_available_mb": 100154.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 25617.7, "ram_available_mb": 100154.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 23.64, "peak": 26.0, "min": 21.67}, "VIN": {"avg": 60.11, "peak": 62.5, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.64, "energy_joules_est": 15.64, "sample_count": 4, "duration_seconds": 0.661}, "timestamp": "2026-01-17T18:07:22.696187"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 757.775, "latencies_ms": [757.775], "images_per_second": 1.32, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A teddy bear wearing glasses sits on a red surface, connected to a computer via cables. A microphone and a small device, possibly a music player, are also present on the table.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25617.7, "ram_available_mb": 100154.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25617.5, "ram_available_mb": 100154.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 23.56, "peak": 26.79, "min": 21.27}, "VIN": {"avg": 60.77, "peak": 62.31, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.56, "energy_joules_est": 17.86, "sample_count": 5, "duration_seconds": 0.758}, "timestamp": "2026-01-17T18:07:23.460513"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 731.826, "latencies_ms": [731.826], "images_per_second": 1.366, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The teddy bear is light brown and wears glasses. The lighting is dim, creating a moody atmosphere. The teddy bear appears to be made of a soft, plush material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25617.5, "ram_available_mb": 100154.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25617.2, "ram_available_mb": 100155.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.47, "peak": 26.38, "min": 21.26}, "VIN": {"avg": 60.01, "peak": 61.81, "min": 58.22}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.47, "energy_joules_est": 17.18, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T18:07:24.200409"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 631.776, "latencies_ms": [631.776], "images_per_second": 1.583, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A skier in an orange suit is captured mid-air, performing a trick on a snowy slope with trees and a rocky hillside in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25617.2, "ram_available_mb": 100155.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25617.2, "ram_available_mb": 100155.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 23.92, "peak": 26.37, "min": 22.05}, "VIN": {"avg": 60.53, "peak": 65.05, "min": 54.4}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.92, "energy_joules_est": 15.12, "sample_count": 4, "duration_seconds": 0.632}, "timestamp": "2026-01-17T18:07:24.842712"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1140.42, "latencies_ms": [1140.42], "images_per_second": 0.877, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "skier: 2\nskis: 2\nsnow: 2\nhill: 1\ntree: 1\nlog: 1\nsnowboard: 0\ngloves: 2\nhelmet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25617.2, "ram_available_mb": 100155.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25617.5, "ram_available_mb": 100154.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.5, "peak": 26.77, "min": 20.09}, "VIN": {"avg": 60.07, "peak": 65.12, "min": 51.44}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.5, "energy_joules_est": 25.67, "sample_count": 8, "duration_seconds": 1.141}, "timestamp": "2026-01-17T18:07:25.990171"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 730.699, "latencies_ms": [730.699], "images_per_second": 1.369, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The skier is positioned in the foreground, mid-jump, with the snowy mountain and trees in the background. The image captures the dynamic motion of the skier against the serene, snowy landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25617.5, "ram_available_mb": 100154.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25609.3, "ram_available_mb": 100162.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 23.0, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 59.61, "peak": 62.52, "min": 57.08}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.0, "energy_joules_est": 16.81, "sample_count": 5, "duration_seconds": 0.731}, "timestamp": "2026-01-17T18:07:26.727435"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 729.299, "latencies_ms": [729.299], "images_per_second": 1.371, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A skier in an orange suit is captured mid-air, performing a trick on a snowy slope. The scene is set against a backdrop of a snow-covered mountain under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25609.3, "ram_available_mb": 100162.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25601.4, "ram_available_mb": 100170.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.71, "min": 13.59}, "VDD_GPU": {"avg": 23.08, "peak": 26.0, "min": 20.88}, "VIN": {"avg": 59.71, "peak": 62.16, "min": 55.94}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.08, "energy_joules_est": 16.84, "sample_count": 5, "duration_seconds": 0.73}, "timestamp": "2026-01-17T18:07:27.465080"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 989.008, "latencies_ms": [989.008], "images_per_second": 1.011, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The skier is wearing bright orange clothing. The lighting is bright and sunny, creating a contrast against the white snow. The skier is captured mid-air, showcasing their skill and athleticism. The snowy mountain landscape adds to the picturesque setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25601.4, "ram_available_mb": 100170.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25601.2, "ram_available_mb": 100171.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.57, "peak": 26.39, "min": 20.09}, "VIN": {"avg": 60.98, "peak": 63.85, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.57, "energy_joules_est": 22.33, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T18:07:28.460291"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 739.937, "latencies_ms": [739.937], "images_per_second": 1.351, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "Two surfers in wetsuits ride waves in the ocean, one skillfully carving a path and the other paddling to catch a wave.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25601.2, "ram_available_mb": 100171.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25602.1, "ram_available_mb": 100170.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.61, "peak": 33.86, "min": 24.81}, "VIN": {"avg": 65.09, "peak": 81.98, "min": 59.67}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.61, "energy_joules_est": 21.93, "sample_count": 5, "duration_seconds": 0.741}, "timestamp": "2026-01-17T18:07:29.214406"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 842.354, "latencies_ms": [842.354], "images_per_second": 1.187, "prompt_tokens": 21, "response_tokens_est": 23, "n_tiles": 1, "output_text": "waves: 2\nsurfers: 2\nsurfboards: 2\nwater: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25602.1, "ram_available_mb": 100170.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.66, "peak": 36.23, "min": 23.62}, "VIN": {"avg": 62.43, "peak": 83.92, "min": 54.46}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.66, "energy_joules_est": 25.0, "sample_count": 6, "duration_seconds": 0.843}, "timestamp": "2026-01-17T18:07:30.063226"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1098.51, "latencies_ms": [1098.51], "images_per_second": 0.91, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The main objects are positioned relatively close to the viewer, creating a sense of proximity and depth. The foreground features the surfers, while the background showcases the vast expanse of the ocean. The image conveys a sense of vastness and distance, emphasizing the ocean's expansive nature.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25578.5, "ram_available_mb": 100193.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.02, "peak": 35.83, "min": 21.67}, "VIN": {"avg": 61.52, "peak": 82.39, "min": 52.7}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.02, "energy_joules_est": 30.8, "sample_count": 8, "duration_seconds": 1.099}, "timestamp": "2026-01-17T18:07:31.168424"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 804.035, "latencies_ms": [804.035], "images_per_second": 1.244, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "Two surfers are riding waves in the ocean. The scene is dynamic and energetic, capturing the thrill of surfing amidst the vastness of the sea.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25578.8, "ram_available_mb": 100193.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25578.6, "ram_available_mb": 100193.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 29.28, "peak": 34.65, "min": 23.64}, "VIN": {"avg": 65.89, "peak": 96.19, "min": 55.59}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.28, "energy_joules_est": 23.55, "sample_count": 6, "duration_seconds": 0.804}, "timestamp": "2026-01-17T18:07:31.979266"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 878.746, "latencies_ms": [878.746], "images_per_second": 1.138, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The ocean is a deep blue, suggesting potentially cold or stormy weather. The lighting is soft and diffused, possibly indicating overcast conditions or early morning/late evening light.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25578.6, "ram_available_mb": 100193.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25573.0, "ram_available_mb": 100199.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.01, "peak": 36.24, "min": 24.03}, "VIN": {"avg": 62.42, "peak": 82.75, "min": 54.31}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.01, "energy_joules_est": 26.38, "sample_count": 6, "duration_seconds": 0.879}, "timestamp": "2026-01-17T18:07:32.864307"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 879.565, "latencies_ms": [879.565], "images_per_second": 1.137, "prompt_tokens": 8, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A freshly baked pizza with cheese, tomato sauce, onions, and pineapple chunks sits on a metal tray, accompanied by a fork, knife, and salt shaker on a blue table.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25573.0, "ram_available_mb": 100199.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25567.3, "ram_available_mb": 100204.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 63.95, "peak": 82.88, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 25.3, "sample_count": 6, "duration_seconds": 0.88}, "timestamp": "2026-01-17T18:07:33.753861"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1558.491, "latencies_ms": [1558.491], "images_per_second": 0.642, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 1, "output_text": "Pizza: 8\nPepper shaker: 1\nSalt shaker: 1\nFork: 1\nKnife: 1\nNapkin: 1\nCheese: 1\nTomato sauce: 1\nSpinach: 1\nPineapple: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25567.3, "ram_available_mb": 100204.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25567.5, "ram_available_mb": 100204.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 24.55, "peak": 35.04, "min": 19.7}, "VIN": {"avg": 61.69, "peak": 88.63, "min": 51.78}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.55, "energy_joules_est": 38.27, "sample_count": 12, "duration_seconds": 1.559}, "timestamp": "2026-01-17T18:07:35.318964"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 762.106, "latencies_ms": [762.106], "images_per_second": 1.312, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The pizza is positioned in the foreground, slightly to the right of the table. The table occupies the middle ground, and the background is blurred, suggesting the setting is indoors.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25567.5, "ram_available_mb": 100204.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25563.1, "ram_available_mb": 100209.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 28.68, "peak": 33.48, "min": 24.03}, "VIN": {"avg": 66.27, "peak": 90.01, "min": 57.01}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.68, "energy_joules_est": 21.87, "sample_count": 5, "duration_seconds": 0.763}, "timestamp": "2026-01-17T18:07:36.091807"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 905.681, "latencies_ms": [905.681], "images_per_second": 1.104, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene is set in a dimly lit restaurant or bar, showcasing a freshly made pizza on a metal tray, accompanied by a fork, knife, and salt shaker. The atmosphere is cozy and inviting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25563.1, "ram_available_mb": 100209.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25557.7, "ram_available_mb": 100214.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 28.89, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 65.47, "peak": 85.95, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.89, "energy_joules_est": 26.17, "sample_count": 6, "duration_seconds": 0.906}, "timestamp": "2026-01-17T18:07:37.008696"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 803.5, "latencies_ms": [803.5], "images_per_second": 1.245, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The pizza is topped with vibrant red sauce, melted cheese, and chunks of pineapple. The table is dark blue, and the lighting creates a warm, inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25557.7, "ram_available_mb": 100214.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25556.7, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.29, "peak": 34.65, "min": 22.85}, "VIN": {"avg": 65.5, "peak": 82.32, "min": 60.28}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.29, "energy_joules_est": 22.74, "sample_count": 6, "duration_seconds": 0.804}, "timestamp": "2026-01-17T18:07:37.818502"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 832.515, "latencies_ms": [832.515], "images_per_second": 1.201, "prompt_tokens": 8, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A black metal clock with Roman numerals is mounted on a pole, displaying the time as approximately 2:55, against a backdrop of snow-covered brick buildings and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25556.7, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25555.9, "ram_available_mb": 100216.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 28.69, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 67.35, "peak": 95.59, "min": 59.42}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.69, "energy_joules_est": 23.9, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T18:07:38.662045"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 996.253, "latencies_ms": [996.253], "images_per_second": 1.004, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "clock: 1\nstreet light: 1\nbuildings: 4\ncars: 2\ntrees: 1\nsnow: 2\npots: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25555.9, "ram_available_mb": 100216.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25554.2, "ram_available_mb": 100217.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.75, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 66.24, "peak": 93.84, "min": 59.94}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.75, "energy_joules_est": 27.65, "sample_count": 7, "duration_seconds": 0.997}, "timestamp": "2026-01-17T18:07:39.664572"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 786.135, "latencies_ms": [786.135], "images_per_second": 1.272, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The clock is positioned in the foreground on the left side of the image, dominating the left side of the frame. The street and buildings extend into the background, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25554.2, "ram_available_mb": 100217.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25554.5, "ram_available_mb": 100217.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.23, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 63.17, "peak": 83.78, "min": 54.27}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 22.99, "sample_count": 5, "duration_seconds": 0.786}, "timestamp": "2026-01-17T18:07:40.460932"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1232.817, "latencies_ms": [1232.817], "images_per_second": 0.811, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The scene depicts a snowy street in a small town or village, lined with brick buildings and a clock displaying the time. The street is relatively empty, with only a few parked cars and a few pedestrians visible. The overall atmosphere is peaceful and quiet, typical of a quiet winter day in a small town.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25554.5, "ram_available_mb": 100217.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25553.4, "ram_available_mb": 100218.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.26, "peak": 35.83, "min": 20.48}, "VIN": {"avg": 62.84, "peak": 72.76, "min": 60.46}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.26, "energy_joules_est": 32.39, "sample_count": 9, "duration_seconds": 1.233}, "timestamp": "2026-01-17T18:07:41.701066"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 936.903, "latencies_ms": [936.903], "images_per_second": 1.067, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The buildings are primarily brick, exhibiting a reddish-brown hue. The street is lit by streetlights, creating a warm glow against the cold, snowy backdrop. The scene evokes a serene winter atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25553.2, "ram_available_mb": 100219.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25552.7, "ram_available_mb": 100219.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.84, "peak": 33.48, "min": 21.66}, "VIN": {"avg": 64.61, "peak": 85.15, "min": 57.48}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.84, "energy_joules_est": 25.15, "sample_count": 7, "duration_seconds": 0.937}, "timestamp": "2026-01-17T18:07:42.644110"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 660.948, "latencies_ms": [660.948], "images_per_second": 1.513, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A baseball player in a white uniform with blue accents is swinging his bat, attempting to hit a pitched ball during a game.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25552.7, "ram_available_mb": 100219.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25549.5, "ram_available_mb": 100222.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.29, "peak": 34.24, "min": 24.41}, "VIN": {"avg": 65.4, "peak": 91.64, "min": 57.43}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.29, "energy_joules_est": 19.37, "sample_count": 5, "duration_seconds": 0.661}, "timestamp": "2026-01-17T18:07:43.315859"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1424.146, "latencies_ms": [1424.146], "images_per_second": 0.702, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 1, "output_text": "baseball bat: 1\nbaseball: 1\nbaseball glove: 1\nbaseball: 1\nbaseball field: 1\nbaseball player: 1\nbaseball umpire: 1\nbaseball umpire's mask: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25549.5, "ram_available_mb": 100222.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 25.31, "peak": 36.6, "min": 19.7}, "VIN": {"avg": 63.04, "peak": 87.25, "min": 57.29}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.31, "energy_joules_est": 36.06, "sample_count": 11, "duration_seconds": 1.425}, "timestamp": "2026-01-17T18:07:44.746299"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 632.427, "latencies_ms": [632.427], "images_per_second": 1.581, "prompt_tokens": 25, "response_tokens_est": 24, "n_tiles": 1, "output_text": "The batter is positioned in the foreground, facing the pitcher. The catcher is positioned in the background, behind the batter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 29.73, "peak": 33.09, "min": 25.99}, "VIN": {"avg": 63.42, "peak": 74.37, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.73, "energy_joules_est": 18.82, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T18:07:45.385113"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1010.984, "latencies_ms": [1010.984], "images_per_second": 0.989, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A baseball game is in progress on a sunny day. A batter in a white uniform with blue accents is swinging at a pitched ball, while a catcher in a red uniform is crouched behind home plate, ready to receive the pitch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25547.6, "ram_available_mb": 100224.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.58, "peak": 36.62, "min": 22.45}, "VIN": {"avg": 62.86, "peak": 82.85, "min": 54.1}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.58, "energy_joules_est": 28.91, "sample_count": 7, "duration_seconds": 1.011}, "timestamp": "2026-01-17T18:07:46.402377"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 956.577, "latencies_ms": [956.577], "images_per_second": 1.045, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The baseball game is taking place under bright sunlight. The batter is wearing a blue and white uniform, and the catcher is wearing a red uniform. The field is well-maintained, and the overall atmosphere suggests a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.8, "ram_available_mb": 100226.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.52, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 64.31, "peak": 87.91, "min": 53.76}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 26.34, "sample_count": 7, "duration_seconds": 0.957}, "timestamp": "2026-01-17T18:07:47.365314"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 718.626, "latencies_ms": [718.626], "images_per_second": 1.392, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A brown teddy bear with a red bow tie sits comfortably on a wicker chair with a dark red cushion, positioned against a striped curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.55, "peak": 35.06, "min": 24.43}, "VIN": {"avg": 65.94, "peak": 82.96, "min": 59.97}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.55, "energy_joules_est": 21.24, "sample_count": 5, "duration_seconds": 0.719}, "timestamp": "2026-01-17T18:07:48.094578"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 963.63, "latencies_ms": [963.63], "images_per_second": 1.038, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "chair: 2\nteddy bear: 1\nwicker: 2\nred cushion: 1\nbow tie: 1\ncurtains: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25541.7, "ram_available_mb": 100230.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.08, "peak": 36.24, "min": 22.07}, "VIN": {"avg": 62.6, "peak": 83.12, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.08, "energy_joules_est": 27.07, "sample_count": 7, "duration_seconds": 0.964}, "timestamp": "2026-01-17T18:07:49.064415"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 803.881, "latencies_ms": [803.881], "images_per_second": 1.244, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The brown teddy bear is positioned in the foreground, slightly to the right of the wicker chair. The chair is situated in the background, partially obscured by the teddy bear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25541.7, "ram_available_mb": 100230.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.63, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 65.61, "peak": 86.27, "min": 59.63}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.63, "energy_joules_est": 23.03, "sample_count": 6, "duration_seconds": 0.804}, "timestamp": "2026-01-17T18:07:49.874770"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 841.925, "latencies_ms": [841.925], "images_per_second": 1.188, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A brown teddy bear with a red bow sits on a wicker chair with a dark wooden backrest and armrests. A blue and white striped curtain is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.7, "ram_available_mb": 100229.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.95, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 64.77, "peak": 85.01, "min": 57.56}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 24.39, "sample_count": 6, "duration_seconds": 0.842}, "timestamp": "2026-01-17T18:07:50.723047"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 788.551, "latencies_ms": [788.551], "images_per_second": 1.268, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The chair features a combination of brown and red colors. The lighting appears to be soft and warm, creating a cozy atmosphere. The chair is made of wicker and has a plush cushion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 29.94, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 64.45, "peak": 81.15, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 29.94, "energy_joules_est": 23.62, "sample_count": 5, "duration_seconds": 0.789}, "timestamp": "2026-01-17T18:07:51.519568"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 582.347, "latencies_ms": [582.347], "images_per_second": 1.717, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Two snowboarders, dressed in red and black, prepare to descend a snowy mountain slope under a bright, sunny sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.7, "ram_available_mb": 100232.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.61, "min": 13.59}, "VDD_GPU": {"avg": 24.71, "peak": 27.56, "min": 22.45}, "VIN": {"avg": 59.32, "peak": 63.09, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.71, "energy_joules_est": 14.4, "sample_count": 4, "duration_seconds": 0.583}, "timestamp": "2026-01-17T18:07:52.111921"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1151.636, "latencies_ms": [1151.636], "images_per_second": 0.868, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "person: 2\nsnowboard: 1\nhelmet: 1\ngloves: 1\nboots: 1\nsnow: 1\nmountain: 1\nrock: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25537.9, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 22.6, "peak": 27.18, "min": 20.09}, "VIN": {"avg": 59.6, "peak": 63.31, "min": 54.95}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.6, "energy_joules_est": 26.04, "sample_count": 8, "duration_seconds": 1.152}, "timestamp": "2026-01-17T18:07:53.269942"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 731.282, "latencies_ms": [731.282], "images_per_second": 1.367, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the snowy mountain range in the background. The man and woman are standing relatively close to the foreground, seemingly preparing to snowboard or ski down the mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.9, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 23.0, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 60.52, "peak": 62.48, "min": 57.73}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 23.0, "energy_joules_est": 16.84, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T18:07:54.007816"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 535.796, "latencies_ms": [535.796], "images_per_second": 1.866, "prompt_tokens": 19, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Two snowboarders are standing on a snow-covered mountain peak, enjoying the bright sunlight and breathtaking mountain views.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25537.9, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 24.56, "peak": 26.39, "min": 22.86}, "VIN": {"avg": 61.46, "peak": 62.47, "min": 60.25}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.56, "energy_joules_est": 13.17, "sample_count": 3, "duration_seconds": 0.536}, "timestamp": "2026-01-17T18:07:54.549405"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 820.655, "latencies_ms": [820.655], "images_per_second": 1.219, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The snowboarders are wearing bright red jackets. The sun is shining brightly, creating a lens flare effect. The snowboard is black and appears to be made of metal. The weather is sunny and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.9, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 23.37, "peak": 27.18, "min": 20.87}, "VIN": {"avg": 59.62, "peak": 62.3, "min": 55.38}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.37, "energy_joules_est": 19.18, "sample_count": 6, "duration_seconds": 0.821}, "timestamp": "2026-01-17T18:07:55.376938"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 676.901, "latencies_ms": [676.901], "images_per_second": 1.477, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A cluster of ripe, red apples hangs from a tree branch, surrounded by green leaves and partially obscured by a weathered, textured trunk.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.67, "peak": 33.46, "min": 24.03}, "VIN": {"avg": 64.49, "peak": 81.65, "min": 55.04}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.67, "energy_joules_est": 19.42, "sample_count": 5, "duration_seconds": 0.677}, "timestamp": "2026-01-17T18:07:56.066464"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 858.725, "latencies_ms": [858.725], "images_per_second": 1.165, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "apples: 5\ntree: 1\nleaves: 4\nbranches: 6\nfruit: 4\ntree trunk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.7, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25537.6, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.94, "peak": 35.83, "min": 23.23}, "VIN": {"avg": 66.67, "peak": 97.13, "min": 56.36}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 28.94, "energy_joules_est": 24.87, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T18:07:56.932229"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 662.052, "latencies_ms": [662.052], "images_per_second": 1.51, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The apples are positioned in the foreground, slightly to the left of the viewer. The background features more trees, creating a sense of depth and space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.85, "peak": 35.03, "min": 24.82}, "VIN": {"avg": 63.68, "peak": 82.62, "min": 56.39}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.85, "energy_joules_est": 19.78, "sample_count": 5, "duration_seconds": 0.662}, "timestamp": "2026-01-17T18:07:57.600923"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 743.313, "latencies_ms": [743.313], "images_per_second": 1.345, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The scene depicts an apple tree with ripe, red apples hanging from its branches. The background features other trees and foliage, creating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 30.25, "peak": 36.63, "min": 24.82}, "VIN": {"avg": 64.07, "peak": 70.75, "min": 56.98}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 30.25, "energy_joules_est": 22.49, "sample_count": 5, "duration_seconds": 0.744}, "timestamp": "2026-01-17T18:07:58.349955"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1015.549, "latencies_ms": [1015.549], "images_per_second": 0.985, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The apples are predominantly red, contrasting with the brown and gray tones of the tree bark and leaves. The lighting suggests an outdoor setting, possibly during late afternoon or early evening. The apples appear to be ripe and ready for picking.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25537.9, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.13, "peak": 35.83, "min": 22.45}, "VIN": {"avg": 64.37, "peak": 83.16, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 28.13, "energy_joules_est": 28.58, "sample_count": 7, "duration_seconds": 1.016}, "timestamp": "2026-01-17T18:07:59.376866"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 656.622, "latencies_ms": [656.622], "images_per_second": 1.523, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Two chefs are working diligently in a commercial kitchen, preparing food using various cooking utensils and equipment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.9, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 30.04, "peak": 33.88, "min": 26.01}, "VIN": {"avg": 66.15, "peak": 79.0, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.04, "energy_joules_est": 19.74, "sample_count": 4, "duration_seconds": 0.657}, "timestamp": "2026-01-17T18:08:00.048935"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1199.775, "latencies_ms": [1199.775], "images_per_second": 0.833, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "pan: 2\ntongs: 2\ncutting board: 1\nmetal pot: 1\nserving dish: 1\ncontainer: 1\nshelves: 2\noven hood: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25537.8, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 26.62, "peak": 36.63, "min": 20.48}, "VIN": {"avg": 64.34, "peak": 95.71, "min": 57.54}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 26.62, "energy_joules_est": 31.95, "sample_count": 9, "duration_seconds": 1.2}, "timestamp": "2026-01-17T18:08:01.255563"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 869.41, "latencies_ms": [869.41], "images_per_second": 1.15, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective in the image. The foreground features the chef preparing food, while the background showcases the kitchen's layout with various utensils and equipment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.8, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.23, "peak": 34.27, "min": 22.86}, "VIN": {"avg": 63.38, "peak": 83.58, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 24.55, "sample_count": 6, "duration_seconds": 0.87}, "timestamp": "2026-01-17T18:08:02.131714"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1131.298, "latencies_ms": [1131.298], "images_per_second": 0.884, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The scene depicts a busy commercial kitchen where two chefs are actively working, preparing food. The kitchen is equipped with various cooking equipment, including a large grill, pots, and pans. The chefs are wearing white shirts and appear to be focused on their tasks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.99, "peak": 35.06, "min": 21.28}, "VIN": {"avg": 61.41, "peak": 74.82, "min": 55.81}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.99, "energy_joules_est": 30.55, "sample_count": 8, "duration_seconds": 1.132}, "timestamp": "2026-01-17T18:08:03.272023"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1042.934, "latencies_ms": [1042.934], "images_per_second": 0.959, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The kitchen is brightly lit with yellow fluorescent lights, creating a warm and inviting atmosphere. The stainless steel surfaces and utensils suggest a modern and efficient design. The overall appearance is clean and organized, reflecting a well-maintained professional kitchen.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.53, "peak": 33.86, "min": 21.27}, "VIN": {"avg": 62.77, "peak": 79.07, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.53, "energy_joules_est": 27.68, "sample_count": 8, "duration_seconds": 1.043}, "timestamp": "2026-01-17T18:08:04.321339"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 540.919, "latencies_ms": [540.919], "images_per_second": 1.849, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A group of motorcyclists has gathered on the side of a road, sitting and standing near their parked motorcycles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 24.68, "peak": 26.38, "min": 23.24}, "VIN": {"avg": 60.58, "peak": 61.81, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 24.68, "energy_joules_est": 13.37, "sample_count": 3, "duration_seconds": 0.542}, "timestamp": "2026-01-17T18:08:04.872691"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 984.016, "latencies_ms": [984.016], "images_per_second": 1.016, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "motorcycle: 8\nbike: 8\nhelmet: 2\nperson: 2\nroad: 8\nsky: 8\nclouds: 8\nflag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 23.07, "peak": 27.17, "min": 20.48}, "VIN": {"avg": 60.23, "peak": 64.5, "min": 56.71}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.07, "energy_joules_est": 22.72, "sample_count": 7, "duration_seconds": 0.985}, "timestamp": "2026-01-17T18:08:05.863638"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 674.534, "latencies_ms": [674.534], "images_per_second": 1.483, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned on the left side of the image, separated from the foreground by a paved road. The motorcycles are parked along the roadside, extending into the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 23.24, "peak": 26.0, "min": 21.26}, "VIN": {"avg": 59.85, "peak": 63.34, "min": 57.03}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 23.24, "energy_joules_est": 15.69, "sample_count": 5, "duration_seconds": 0.675}, "timestamp": "2026-01-17T18:08:06.544469"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 864.862, "latencies_ms": [864.862], "images_per_second": 1.156, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "A group of motorcyclists is gathered on a roadside near a body of water, enjoying a ride together.  The setting appears to be rural or coastal, with some motorcycles parked on the side of the road and others parked further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 23.17, "peak": 26.77, "min": 20.88}, "VIN": {"avg": 60.19, "peak": 62.04, "min": 55.58}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.17, "energy_joules_est": 20.05, "sample_count": 6, "duration_seconds": 0.865}, "timestamp": "2026-01-17T18:08:07.416035"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 594.33, "latencies_ms": [594.33], "images_per_second": 1.683, "prompt_tokens": 18, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The sky is cloudy and gray. The motorcycles are parked in a row on the side of the road. The road appears to be paved with asphalt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.92, "peak": 26.38, "min": 22.05}, "VIN": {"avg": 61.29, "peak": 65.02, "min": 59.09}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 23.92, "energy_joules_est": 14.23, "sample_count": 4, "duration_seconds": 0.595}, "timestamp": "2026-01-17T18:08:08.017006"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 702.21, "latencies_ms": [702.21], "images_per_second": 1.424, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A small, single-engine propeller plane is captured in mid-flight, leaving a trail of smoke as it soars through the cloudy sky.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.3, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 64.4, "peak": 75.8, "min": 60.0}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 20.59, "sample_count": 5, "duration_seconds": 0.703}, "timestamp": "2026-01-17T18:08:08.729589"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 883.817, "latencies_ms": [883.817], "images_per_second": 1.131, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "plane: 1\nclouds: 2\nsmoke: 1\nperson: 1\nwings: 2\npropeller: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 28.96, "peak": 35.85, "min": 23.24}, "VIN": {"avg": 62.26, "peak": 89.48, "min": 54.09}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.96, "energy_joules_est": 25.61, "sample_count": 6, "duration_seconds": 0.884}, "timestamp": "2026-01-17T18:08:09.619676"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 757.453, "latencies_ms": [757.453], "images_per_second": 1.32, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The small propeller plane is positioned in the foreground, moving from left to right across the image. The vast sky occupies the background, creating a sense of space and distance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.46, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 65.11, "peak": 86.51, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 22.33, "sample_count": 5, "duration_seconds": 0.758}, "timestamp": "2026-01-17T18:08:10.383975"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 897.622, "latencies_ms": [897.622], "images_per_second": 1.114, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A small, single-engine propeller plane is captured in mid-flight, moving from left to right across a cloudy sky. The image is in sepia tones, giving it a vintage or timeless feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.08, "peak": 36.23, "min": 23.23}, "VIN": {"avg": 65.03, "peak": 83.9, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 29.08, "energy_joules_est": 26.11, "sample_count": 6, "duration_seconds": 0.898}, "timestamp": "2026-01-17T18:08:11.291827"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 814.59, "latencies_ms": [814.59], "images_per_second": 1.228, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The plane is primarily white with black accents. The sky is cloudy, with varying shades of gray. The image appears to be taken in sepia tones, giving it a vintage feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 67.38, "peak": 95.89, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 23.44, "sample_count": 6, "duration_seconds": 0.815}, "timestamp": "2026-01-17T18:08:12.113666"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 719.89, "latencies_ms": [719.89], "images_per_second": 1.389, "prompt_tokens": 8, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A group of four sheep, including two adult sheep and two younger ones, are standing on a grassy hill overlooking a serene blue lake surrounded by majestic mountains.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25538.5, "ram_available_mb": 100233.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 29.47, "peak": 34.66, "min": 24.43}, "VIN": {"avg": 61.5, "peak": 76.12, "min": 52.23}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.47, "energy_joules_est": 21.23, "sample_count": 5, "duration_seconds": 0.72}, "timestamp": "2026-01-17T18:08:12.846651"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 866.571, "latencies_ms": [866.571], "images_per_second": 1.154, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "mountains: 5\nwater: 1\nsheep: 4\ngrass: 6\nrocks: 6\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.48, "peak": 36.24, "min": 23.64}, "VIN": {"avg": 65.12, "peak": 87.34, "min": 57.8}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.48, "energy_joules_est": 25.57, "sample_count": 6, "duration_seconds": 0.867}, "timestamp": "2026-01-17T18:08:13.720313"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 651.663, "latencies_ms": [651.663], "images_per_second": 1.535, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the mountains and lake in the background. The sheep are situated near the edge of the foreground, closer to the viewer.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25538.3, "ram_available_mb": 100233.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.11, "peak": 35.04, "min": 26.77}, "VIN": {"avg": 67.66, "peak": 87.36, "min": 56.7}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.11, "energy_joules_est": 20.29, "sample_count": 4, "duration_seconds": 0.652}, "timestamp": "2026-01-17T18:08:14.378297"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 707.418, "latencies_ms": [707.418], "images_per_second": 1.414, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A group of sheep is grazing on a grassy hillside overlooking a turquoise lake. The landscape features rolling hills and mountains, creating a picturesque and serene setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.3, "ram_available_mb": 100233.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.3, "ram_available_mb": 100233.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 31.11, "peak": 37.01, "min": 25.6}, "VIN": {"avg": 62.8, "peak": 79.59, "min": 57.03}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.11, "energy_joules_est": 22.03, "sample_count": 5, "duration_seconds": 0.708}, "timestamp": "2026-01-17T18:08:15.091708"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 826.765, "latencies_ms": [826.765], "images_per_second": 1.21, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The mountains are predominantly brown and appear dry. The sky is clear and blue, suggesting a sunny day. The scene is peaceful and picturesque, with sheep grazing on the grassy hillside.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25538.3, "ram_available_mb": 100233.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.1, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.6, "peak": 36.62, "min": 23.62}, "VIN": {"avg": 65.71, "peak": 89.35, "min": 56.76}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.6, "energy_joules_est": 24.48, "sample_count": 6, "duration_seconds": 0.827}, "timestamp": "2026-01-17T18:08:15.925198"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 490.219, "latencies_ms": [490.219], "images_per_second": 2.04, "prompt_tokens": 8, "response_tokens_est": 16, "n_tiles": 1, "output_text": "A woman in a wheelchair is preparing to serve a tennis ball during a game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.1, "ram_available_mb": 100234.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 32.82, "peak": 35.85, "min": 29.54}, "VIN": {"avg": 67.18, "peak": 81.49, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 32.82, "energy_joules_est": 16.1, "sample_count": 3, "duration_seconds": 0.491}, "timestamp": "2026-01-17T18:08:16.426529"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1138.7, "latencies_ms": [1138.7], "images_per_second": 0.878, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "woman: 2\ntennis racket: 1\nwheelchair: 2\nbasketball hoop: 1\nbasketball: 1\nwoman: 1\nshorts: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 28.41, "peak": 39.0, "min": 21.66}, "VIN": {"avg": 64.42, "peak": 96.54, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 28.41, "energy_joules_est": 32.36, "sample_count": 8, "duration_seconds": 1.139}, "timestamp": "2026-01-17T18:08:17.571714"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1057.411, "latencies_ms": [1057.411], "images_per_second": 0.946, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The woman is positioned in the foreground of the image, holding a tennis racket and positioned slightly to the right of the center. The background features another person in a wheelchair, also holding a tennis racket. The setting appears to be a sports facility with a basketball hoop visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.69, "peak": 34.66, "min": 21.28}, "VIN": {"avg": 62.32, "peak": 79.48, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.69, "energy_joules_est": 28.23, "sample_count": 8, "duration_seconds": 1.058}, "timestamp": "2026-01-17T18:08:18.635593"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 798.738, "latencies_ms": [798.738], "images_per_second": 1.252, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Two women are playing wheelchair tennis in a gymnasium. One woman is seated in a wheelchair holding a tennis racket, while the other woman is standing and watching.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.23, "peak": 34.28, "min": 24.42}, "VIN": {"avg": 66.49, "peak": 95.89, "min": 55.79}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 23.36, "sample_count": 5, "duration_seconds": 0.799}, "timestamp": "2026-01-17T18:08:19.440762"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 752.903, "latencies_ms": [752.903], "images_per_second": 1.328, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The woman is wearing a gray t-shirt. The tennis racket she's holding is red and black. The setting appears to be a gymnasium with natural lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.25, "peak": 35.45, "min": 25.21}, "VIN": {"avg": 63.51, "peak": 76.2, "min": 59.51}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 30.25, "energy_joules_est": 22.79, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T18:08:20.200351"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 733.56, "latencies_ms": [733.56], "images_per_second": 1.363, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A young girl, dressed in a pink plaid shirt and blue jeans, sits confidently on a brown leather saddle atop a horse wearing a purple blanket.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 31.27, "peak": 36.24, "min": 25.59}, "VIN": {"avg": 60.79, "peak": 72.13, "min": 51.1}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.27, "energy_joules_est": 22.96, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T18:08:20.947754"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1255.355, "latencies_ms": [1255.355], "images_per_second": 0.797, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "helmet: 1\npony saddle: 1\nsaddle: 1\njockey: 1\nboot: 1\njeans: 1\nbracelet: 1\nhair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.53, "peak": 37.01, "min": 20.88}, "VIN": {"avg": 63.52, "peak": 96.27, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.53, "energy_joules_est": 34.57, "sample_count": 9, "duration_seconds": 1.256}, "timestamp": "2026-01-17T18:08:22.209443"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 844.405, "latencies_ms": [844.405], "images_per_second": 1.184, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The child is positioned near the center of the image, sitting on the saddle and facing the left side of the image. The horse and saddle are in the foreground, while the background consists of trees and greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.82, "peak": 33.88, "min": 23.24}, "VIN": {"avg": 61.37, "peak": 71.16, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.82, "energy_joules_est": 24.34, "sample_count": 6, "duration_seconds": 0.845}, "timestamp": "2026-01-17T18:08:23.060014"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 744.634, "latencies_ms": [744.634], "images_per_second": 1.343, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A young child is sitting on a horse's saddle, wearing a helmet and pink plaid shirt. The setting appears to be a wooded area with a purple saddle blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.8, "peak": 35.83, "min": 25.21}, "VIN": {"avg": 63.59, "peak": 84.83, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.8, "energy_joules_est": 22.95, "sample_count": 5, "duration_seconds": 0.745}, "timestamp": "2026-01-17T18:08:23.811083"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 851.518, "latencies_ms": [851.518], "images_per_second": 1.174, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The child is wearing a pink and white plaid shirt and blue jeans. The child is sitting on a brown leather saddle. The saddle is on a purple blanket. The child is wearing brown boots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 30.46, "peak": 37.42, "min": 24.03}, "VIN": {"avg": 65.13, "peak": 87.48, "min": 53.83}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.55}}, "power_watts_avg": 30.46, "energy_joules_est": 25.95, "sample_count": 6, "duration_seconds": 0.852}, "timestamp": "2026-01-17T18:08:24.669766"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 540.029, "latencies_ms": [540.029], "images_per_second": 1.852, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Three surfers in wetsuits ride waves in the deep blue ocean of Raglan, New Zealand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 25.73, "peak": 27.97, "min": 23.62}, "VIN": {"avg": 60.29, "peak": 64.76, "min": 54.61}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.73, "energy_joules_est": 13.9, "sample_count": 3, "duration_seconds": 0.54}, "timestamp": "2026-01-17T18:08:25.218849"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 799.756, "latencies_ms": [799.756], "images_per_second": 1.25, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "waves: 2\nsurfers: 3\nsurfboards: 2\nwater: 1\nsky: 1\nocean: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.57, "peak": 27.56, "min": 20.88}, "VIN": {"avg": 58.7, "peak": 63.12, "min": 54.25}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.57, "energy_joules_est": 18.86, "sample_count": 6, "duration_seconds": 0.8}, "timestamp": "2026-01-17T18:08:26.026398"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 561.109, "latencies_ms": [561.109], "images_per_second": 1.782, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the ocean extending behind them. Surfers can be seen in the background, further away from the main focus.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 24.03, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 58.6, "peak": 59.97, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.03, "energy_joules_est": 13.49, "sample_count": 4, "duration_seconds": 0.562}, "timestamp": "2026-01-17T18:08:26.594275"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 907.682, "latencies_ms": [907.682], "images_per_second": 1.102, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "Two surfers are riding waves in the ocean, wearing wetsuits and skillfully maneuvering their surfboards. The scene is set in Raglan, New Zealand, showcasing the sport's popularity in this coastal location.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.31, "peak": 27.18, "min": 20.88}, "VIN": {"avg": 61.23, "peak": 62.88, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.31, "energy_joules_est": 21.16, "sample_count": 6, "duration_seconds": 0.908}, "timestamp": "2026-01-17T18:08:27.508389"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 945.775, "latencies_ms": [945.775], "images_per_second": 1.057, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The ocean is a deep blue color. The lighting suggests a sunny day with bright sunlight. The surfers are wearing wetsuits, indicating the presence of cold water. The overall scene conveys a dynamic and energetic atmosphere typical of New Zealand surfing.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.57, "peak": 26.39, "min": 20.09}, "VIN": {"avg": 60.87, "peak": 64.01, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.57, "energy_joules_est": 21.36, "sample_count": 7, "duration_seconds": 0.946}, "timestamp": "2026-01-17T18:08:28.460374"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 462.619, "latencies_ms": [462.619], "images_per_second": 2.162, "prompt_tokens": 8, "response_tokens_est": 14, "n_tiles": 1, "output_text": "A kitchen with an open window reveals a view of a building outside.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 31.25, "peak": 33.48, "min": 28.36}, "VIN": {"avg": 71.36, "peak": 88.21, "min": 61.82}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.25, "energy_joules_est": 14.48, "sample_count": 3, "duration_seconds": 0.463}, "timestamp": "2026-01-17T18:08:28.934692"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1060.587, "latencies_ms": [1060.587], "images_per_second": 0.943, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "window: 4\ncupboard: 2\nshelf: 2\npot: 1\nplant: 1\ntable: 1\noven: 1\ncounter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 28.31, "peak": 38.21, "min": 21.66}, "VIN": {"avg": 63.29, "peak": 83.23, "min": 52.99}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 28.31, "energy_joules_est": 30.05, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T18:08:30.003730"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1026.235, "latencies_ms": [1026.235], "images_per_second": 0.974, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The kitchen is positioned to the right of the window, offering a view of the outside. The window occupies the central portion of the image, partially obscuring the view of the kitchen area. The kitchen itself is situated to the left of the window, occupying the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.56, "peak": 34.24, "min": 22.06}, "VIN": {"avg": 64.4, "peak": 82.66, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.56, "energy_joules_est": 28.3, "sample_count": 7, "duration_seconds": 1.027}, "timestamp": "2026-01-17T18:08:31.036860"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1009.16, "latencies_ms": [1009.16], "images_per_second": 0.991, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The scene depicts a cozy kitchen with warm lighting and a large window offering a view of an adjacent building. The kitchen is filled with various items, including plants, a wooden cabinet, and a stove, indicating a lived-in and functional space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.35, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 63.52, "peak": 85.31, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.35, "energy_joules_est": 27.61, "sample_count": 7, "duration_seconds": 1.009}, "timestamp": "2026-01-17T18:08:32.052553"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 875.068, "latencies_ms": [875.068], "images_per_second": 1.143, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The room is dimly lit, creating a warm and cozy atmosphere. The window is frosted, suggesting recent rain or mist. The wooden cabinets and appliances add a rustic charm to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.29, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 63.14, "peak": 75.16, "min": 59.76}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.29, "energy_joules_est": 24.77, "sample_count": 6, "duration_seconds": 0.876}, "timestamp": "2026-01-17T18:08:32.934290"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 733.772, "latencies_ms": [733.772], "images_per_second": 1.363, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A pineapple, oranges, incense sticks, and red cups are arranged on a table, symbolizing a traditional Chinese religious setting.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.46, "peak": 35.04, "min": 24.41}, "VIN": {"avg": 66.83, "peak": 90.04, "min": 59.61}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.46, "energy_joules_est": 21.63, "sample_count": 5, "duration_seconds": 0.734}, "timestamp": "2026-01-17T18:08:33.681490"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 928.564, "latencies_ms": [928.564], "images_per_second": 1.077, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "pineapple: 1\nincense sticks: 8\norange: 4\nred cups: 4\nred plate: 1\nred background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.27, "peak": 36.62, "min": 23.23}, "VIN": {"avg": 64.75, "peak": 77.99, "min": 59.58}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.27, "energy_joules_est": 27.19, "sample_count": 6, "duration_seconds": 0.929}, "timestamp": "2026-01-17T18:08:34.617588"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 830.399, "latencies_ms": [830.399], "images_per_second": 1.204, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The pineapple is positioned to the left of the incense sticks and oranges. The pineapple and oranges are placed in the foreground, while the incense sticks and container are situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25539.3, "ram_available_mb": 100232.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.49, "peak": 35.44, "min": 22.85}, "VIN": {"avg": 67.35, "peak": 90.21, "min": 59.42}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.49, "energy_joules_est": 23.67, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T18:08:35.456622"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1273.381, "latencies_ms": [1273.381], "images_per_second": 0.785, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The scene depicts a traditional Chinese altar or shrine, adorned with a pineapple, oranges, incense sticks, and red cups. The pineapple serves as a decorative element, while the oranges are likely intended for offering or consumption. The presence of incense sticks suggests a ritualistic or spiritual context.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.3, "ram_available_mb": 100232.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25539.0, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.07, "peak": 35.04, "min": 20.48}, "VIN": {"avg": 65.35, "peak": 86.6, "min": 59.88}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.07, "energy_joules_est": 33.21, "sample_count": 9, "duration_seconds": 1.274}, "timestamp": "2026-01-17T18:08:36.739791"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 871.62, "latencies_ms": [871.62], "images_per_second": 1.147, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene features a vibrant red background, contrasted by the green pineapple and bright orange fruits. The lighting creates a warm, inviting atmosphere, and the pineapple and oranges appear to be fresh and whole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.84, "peak": 33.88, "min": 22.85}, "VIN": {"avg": 63.25, "peak": 80.92, "min": 52.98}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.84, "energy_joules_est": 24.27, "sample_count": 6, "duration_seconds": 0.872}, "timestamp": "2026-01-17T18:08:37.617914"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 656.928, "latencies_ms": [656.928], "images_per_second": 1.522, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A man is holding a plate with a sandwich, fries, and a small cup of sauce, smiling at the camera in a casual restaurant setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.92, "peak": 35.06, "min": 26.8}, "VIN": {"avg": 65.66, "peak": 84.8, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.92, "energy_joules_est": 20.33, "sample_count": 4, "duration_seconds": 0.657}, "timestamp": "2026-01-17T18:08:38.285472"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1177.173, "latencies_ms": [1177.173], "images_per_second": 0.849, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "man: 2\nfries: 1\ntostada: 1\nketchup: 1\ntartar sauce: 1\nmenu: 1\ncounter: 1\npeople: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.01, "peak": 37.04, "min": 20.88}, "VIN": {"avg": 63.87, "peak": 97.4, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 27.01, "energy_joules_est": 31.8, "sample_count": 9, "duration_seconds": 1.177}, "timestamp": "2026-01-17T18:08:39.469182"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 947.061, "latencies_ms": [947.061], "images_per_second": 1.056, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The man is holding the plate of food in his right hand. The plate of food is in the foreground, while the background features other patrons and possibly a menu board. The man is positioned in the foreground, close to the plate of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.23, "peak": 33.86, "min": 22.05}, "VIN": {"avg": 67.11, "peak": 102.87, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 25.8, "sample_count": 7, "duration_seconds": 0.948}, "timestamp": "2026-01-17T18:08:40.423816"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 740.06, "latencies_ms": [740.06], "images_per_second": 1.351, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A man is taking a selfie while holding a plate with a sandwich, fries, and a small cup of sauce. The setting appears to be a casual restaurant or diner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.22, "peak": 34.26, "min": 24.42}, "VIN": {"avg": 63.84, "peak": 79.78, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.22, "energy_joules_est": 21.64, "sample_count": 5, "duration_seconds": 0.74}, "timestamp": "2026-01-17T18:08:41.170288"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 954.972, "latencies_ms": [954.972], "images_per_second": 1.047, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The man is wearing glasses and has a beard. The food on his plate includes golden-brown fries and a fried cheese ball. The lighting in the image is bright, likely from overhead lighting, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 36.23, "min": 22.45}, "VIN": {"avg": 62.39, "peak": 80.26, "min": 55.04}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 27.09, "sample_count": 7, "duration_seconds": 0.955}, "timestamp": "2026-01-17T18:08:42.131555"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 602.341, "latencies_ms": [602.341], "images_per_second": 1.66, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Two individuals are walking on a wet sidewalk, holding umbrellas to shield themselves from the rain.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.42, "peak": 34.26, "min": 26.38}, "VIN": {"avg": 65.11, "peak": 84.31, "min": 54.57}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.42, "energy_joules_est": 18.33, "sample_count": 4, "duration_seconds": 0.603}, "timestamp": "2026-01-17T18:08:42.744455"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1113.472, "latencies_ms": [1113.472], "images_per_second": 0.898, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "building: 5\nmetal structure: 2\nbicycles: 3\nperson: 2\nperson with umbrella: 1\ngrass: 1\ntree: 1\nstreet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.71, "peak": 37.01, "min": 21.26}, "VIN": {"avg": 62.87, "peak": 80.21, "min": 57.06}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.71, "energy_joules_est": 30.87, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T18:08:43.864200"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 910.747, "latencies_ms": [910.747], "images_per_second": 1.098, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The foreground features the metal structure and the bicycles parked near it. The background includes other buildings, trees, and possibly a parking area. The structure is positioned in the foreground, slightly to the right of the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.46, "peak": 34.26, "min": 22.06}, "VIN": {"avg": 64.55, "peak": 78.73, "min": 60.71}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.46, "energy_joules_est": 25.02, "sample_count": 7, "duration_seconds": 0.911}, "timestamp": "2026-01-17T18:08:44.782642"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 970.472, "latencies_ms": [970.472], "images_per_second": 1.03, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a modern, gray building complex on a rainy day. Two individuals are walking on the wet pavement while holding umbrellas.  Bicycles are parked near a metal railing in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.7, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.8, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 64.39, "peak": 83.77, "min": 59.18}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 27.0, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-17T18:08:45.759813"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 795.159, "latencies_ms": [795.159], "images_per_second": 1.258, "prompt_tokens": 18, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The building exterior is primarily gray, with some sections appearing lighter in color. The scene is illuminated by natural light, suggesting an overcast or rainy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.29, "peak": 34.27, "min": 22.84}, "VIN": {"avg": 65.76, "peak": 95.29, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 22.51, "sample_count": 6, "duration_seconds": 0.796}, "timestamp": "2026-01-17T18:08:46.561499"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 654.883, "latencies_ms": [654.883], "images_per_second": 1.527, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A generous amount of creamy white mayonnaise is spread across a piece of bread, accompanied by a fork and knife on the side.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.51, "min": 14.1}, "VDD_GPU": {"avg": 31.2, "peak": 35.42, "min": 26.77}, "VIN": {"avg": 67.02, "peak": 89.41, "min": 54.84}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.2, "energy_joules_est": 20.45, "sample_count": 4, "duration_seconds": 0.655}, "timestamp": "2026-01-17T18:08:47.227598"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 694.604, "latencies_ms": [694.604], "images_per_second": 1.44, "prompt_tokens": 21, "response_tokens_est": 17, "n_tiles": 1, "output_text": "fork: 3\nknife: 1\nbread: 1\nmayonnaise: 8", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.8, "peak": 36.62, "min": 25.21}, "VIN": {"avg": 63.75, "peak": 80.89, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.8, "energy_joules_est": 21.41, "sample_count": 5, "duration_seconds": 0.695}, "timestamp": "2026-01-17T18:08:47.928734"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 833.966, "latencies_ms": [833.966], "images_per_second": 1.199, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The left foreground is dominated by the creamy substance, likely mayonnaise. The background is blurred, drawing focus to the foreground object. The object is positioned near the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.54, "peak": 36.23, "min": 23.64}, "VIN": {"avg": 62.7, "peak": 78.94, "min": 53.21}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 24.65, "sample_count": 6, "duration_seconds": 0.834}, "timestamp": "2026-01-17T18:08:48.769440"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 715.4, "latencies_ms": [715.4], "images_per_second": 1.398, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The scene is set on a plate with a fork and knife nearby. The focus is on a generous amount of mayonnaise spread on a piece of bread.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 30.09, "peak": 35.83, "min": 24.81}, "VIN": {"avg": 65.16, "peak": 79.86, "min": 60.06}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.09, "energy_joules_est": 21.54, "sample_count": 5, "duration_seconds": 0.716}, "timestamp": "2026-01-17T18:08:49.490840"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1318.286, "latencies_ms": [1318.286], "images_per_second": 0.759, "prompt_tokens": 18, "response_tokens_est": 70, "n_tiles": 1, "output_text": "The mayonnaise is a pale yellow color. The lighting in the image is soft and diffused, creating a gentle glow on the food. The mayonnaise appears to be made of a smooth, creamy texture, possibly from a dairy product like mayonnaise or sour cream. The plate is white, and the fork and knife are metallic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.2, "peak": 36.24, "min": 20.48}, "VIN": {"avg": 64.79, "peak": 96.87, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.2, "energy_joules_est": 34.55, "sample_count": 10, "duration_seconds": 1.319}, "timestamp": "2026-01-17T18:08:50.814972"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 665.721, "latencies_ms": [665.721], "images_per_second": 1.502, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A silver laptop with a green leaf background is open on a white desk, accompanied by a black mouse, a black speaker, and a cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 29.93, "peak": 33.86, "min": 26.0}, "VIN": {"avg": 66.56, "peak": 90.58, "min": 56.29}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 19.94, "sample_count": 4, "duration_seconds": 0.666}, "timestamp": "2026-01-17T18:08:51.492780"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1117.752, "latencies_ms": [1117.752], "images_per_second": 0.895, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "laptop: 1\nkeyboard: 1\nmouse: 1\nmousepad: 1\nspeaker: 1\nphone: 1\ncomputer monitor: 2\nwindow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.62, "peak": 36.24, "min": 21.66}, "VIN": {"avg": 65.06, "peak": 95.14, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.62, "energy_joules_est": 30.88, "sample_count": 8, "duration_seconds": 1.118}, "timestamp": "2026-01-17T18:08:52.616807"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 867.258, "latencies_ms": [867.258], "images_per_second": 1.153, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The laptop is in the foreground, while the monitor and keyboard are placed in the background. The mouse is situated near the laptop, further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.23, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 64.56, "peak": 80.74, "min": 57.63}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 24.49, "sample_count": 6, "duration_seconds": 0.868}, "timestamp": "2026-01-17T18:08:53.493962"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 946.229, "latencies_ms": [946.229], "images_per_second": 1.057, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a workspace with a laptop displaying a green leaf background, a desktop computer monitor, a keyboard, a mouse, and a speaker. The laptop is open and positioned on a desk with a window nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.56, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 64.37, "peak": 79.93, "min": 60.07}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.56, "energy_joules_est": 26.09, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T18:08:54.446926"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 870.176, "latencies_ms": [870.176], "images_per_second": 1.149, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The laptop is silver and has a green leaf-themed wallpaper. The desk is white and appears to be made of wood. The lighting is bright, likely from natural light coming in through a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 63.85, "peak": 78.05, "min": 57.87}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 24.69, "sample_count": 6, "duration_seconds": 0.871}, "timestamp": "2026-01-17T18:08:55.323070"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 517.087, "latencies_ms": [517.087], "images_per_second": 1.934, "prompt_tokens": 8, "response_tokens_est": 17, "n_tiles": 1, "output_text": "A young girl is sitting on a bed in a cozy bedroom, playing with toys.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 31.62, "peak": 34.26, "min": 28.73}, "VIN": {"avg": 65.94, "peak": 77.47, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 31.62, "energy_joules_est": 16.37, "sample_count": 3, "duration_seconds": 0.518}, "timestamp": "2026-01-17T18:08:55.850859"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1101.429, "latencies_ms": [1101.429], "images_per_second": 0.908, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "bed: 2\npillows: 2\nnightstand: 1\nlamp: 1\ntoys: 2\ntable: 1\nchair: 1\nfloor: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 28.02, "peak": 37.39, "min": 21.67}, "VIN": {"avg": 65.45, "peak": 96.68, "min": 59.87}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 28.02, "energy_joules_est": 30.88, "sample_count": 8, "duration_seconds": 1.102}, "timestamp": "2026-01-17T18:08:56.958390"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 655.219, "latencies_ms": [655.219], "images_per_second": 1.526, "prompt_tokens": 25, "response_tokens_est": 28, "n_tiles": 1, "output_text": "The bed occupies the foreground, while the girl is seated on the bed in the background. The table and lamp are positioned near the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.63, "peak": 34.27, "min": 26.4}, "VIN": {"avg": 64.13, "peak": 78.52, "min": 57.65}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 20.08, "sample_count": 4, "duration_seconds": 0.656}, "timestamp": "2026-01-17T18:08:57.619321"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 754.065, "latencies_ms": [754.065], "images_per_second": 1.326, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A young girl is sitting on a bed in a room with orange walls. The room contains a bed, a nightstand, a table, a lamp, and several toys scattered around.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.8, "peak": 36.63, "min": 25.2}, "VIN": {"avg": 63.97, "peak": 82.02, "min": 55.93}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.8, "energy_joules_est": 23.23, "sample_count": 5, "duration_seconds": 0.754}, "timestamp": "2026-01-17T18:08:58.379525"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 855.039, "latencies_ms": [855.039], "images_per_second": 1.17, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The room features warm orange walls and warm lighting, creating a cozy atmosphere. The bed is adorned with a brown and red patterned bedspread, and there are toys scattered around, suggesting recent play.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.15, "peak": 35.83, "min": 23.24}, "VIN": {"avg": 63.42, "peak": 82.37, "min": 55.41}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 24.94, "sample_count": 6, "duration_seconds": 0.855}, "timestamp": "2026-01-17T18:08:59.240596"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 769.579, "latencies_ms": [769.579], "images_per_second": 1.299, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A batter in a red uniform is poised to swing at a pitch, while a catcher and an umpire stand nearby, observing the play.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25542.9, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.93, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 68.55, "peak": 97.68, "min": 59.15}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 23.06, "sample_count": 5, "duration_seconds": 0.77}, "timestamp": "2026-01-17T18:09:00.027826"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1219.818, "latencies_ms": [1219.818], "images_per_second": 0.82, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "batter: 1\ncatcher: 1\numpire: 1\nbaseball bat: 1\nbaseball glove: 1\nbaseball field: 1\nbase: 1\nhome plate: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25543.2, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.65, "peak": 35.44, "min": 20.89}, "VIN": {"avg": 61.95, "peak": 79.95, "min": 52.44}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.65, "energy_joules_est": 32.52, "sample_count": 9, "duration_seconds": 1.22}, "timestamp": "2026-01-17T18:09:01.253771"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 739.603, "latencies_ms": [739.603], "images_per_second": 1.352, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The batter is positioned to the left of the catcher, close to the foreground. The umpire is situated to the right of the catcher, further in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.99, "peak": 33.47, "min": 24.42}, "VIN": {"avg": 64.31, "peak": 79.86, "min": 56.48}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.99, "energy_joules_est": 21.45, "sample_count": 5, "duration_seconds": 0.74}, "timestamp": "2026-01-17T18:09:01.998961"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 887.515, "latencies_ms": [887.515], "images_per_second": 1.127, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A baseball game is in progress on a sunny day. A batter in a red uniform is swinging at a pitch, while a catcher in gray and an umpire in black observe the play.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.95, "peak": 35.83, "min": 23.23}, "VIN": {"avg": 65.68, "peak": 82.65, "min": 61.43}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 25.71, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T18:09:02.893887"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1082.743, "latencies_ms": [1082.743], "images_per_second": 0.924, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The baseball game is played under bright sunlight, creating a vibrant and energetic atmosphere. The field is well-maintained and appears to be made of dirt or clay. The players' uniforms are predominantly red and white, contributing to the overall visual appeal of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.68, "peak": 35.06, "min": 21.27}, "VIN": {"avg": 61.91, "peak": 76.05, "min": 57.29}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.68, "energy_joules_est": 28.9, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T18:09:03.982773"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 427.382, "latencies_ms": [427.382], "images_per_second": 2.34, "prompt_tokens": 8, "response_tokens_est": 16, "n_tiles": 1, "output_text": "A white and brown cat is eating a dead small bird on a concrete surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 24.68, "peak": 26.39, "min": 23.23}, "VIN": {"avg": 58.5, "peak": 61.48, "min": 52.89}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.57, "min": 14.17}}, "power_watts_avg": 24.68, "energy_joules_est": 10.56, "sample_count": 3, "duration_seconds": 0.428}, "timestamp": "2026-01-17T18:09:04.420131"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 855.87, "latencies_ms": [855.87], "images_per_second": 1.168, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "cat: 1\nbird: 1\nfeather: 1\ntape: 1\nground: 1\nnet: 1\nleaves: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.81, "min": 13.49}, "VDD_GPU": {"avg": 23.63, "peak": 27.56, "min": 20.88}, "VIN": {"avg": 61.12, "peak": 63.7, "min": 57.63}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.63, "energy_joules_est": 20.23, "sample_count": 6, "duration_seconds": 0.856}, "timestamp": "2026-01-17T18:09:05.284068"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 693.394, "latencies_ms": [693.394], "images_per_second": 1.442, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The cat is positioned in the foreground, close to the bird and rope. The bird is situated in the background, near the cat. The scene appears to be set outdoors, with sunlight illuminating the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.48, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 60.49, "peak": 61.38, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 23.48, "energy_joules_est": 16.29, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T18:09:05.983348"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 555.661, "latencies_ms": [555.661], "images_per_second": 1.8, "prompt_tokens": 19, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A white and gray cat is eating a small bird on a concrete surface. The bird is positioned near the cat's paws.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25543.1, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 24.23, "peak": 26.79, "min": 22.06}, "VIN": {"avg": 61.81, "peak": 66.15, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.23, "energy_joules_est": 13.48, "sample_count": 4, "duration_seconds": 0.556}, "timestamp": "2026-01-17T18:09:06.546314"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 749.985, "latencies_ms": [749.985], "images_per_second": 1.333, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The cat's fur is predominantly white and gray. The lighting suggests a sunny outdoor setting. The cat is eating a dead bird, which appears to be brown and gray in color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.1, "ram_available_mb": 100229.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 24.03, "peak": 27.19, "min": 21.66}, "VIN": {"avg": 60.3, "peak": 64.24, "min": 57.29}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 24.03, "energy_joules_est": 18.04, "sample_count": 5, "duration_seconds": 0.751}, "timestamp": "2026-01-17T18:09:07.302879"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 588.836, "latencies_ms": [588.836], "images_per_second": 1.698, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A hand holds a sandwich with fresh ingredients, including tomato, spinach, and cheese, on a white plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.32, "peak": 33.85, "min": 26.39}, "VIN": {"avg": 64.05, "peak": 74.11, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.32, "energy_joules_est": 17.87, "sample_count": 4, "duration_seconds": 0.589}, "timestamp": "2026-01-17T18:09:07.901777"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1079.032, "latencies_ms": [1079.032], "images_per_second": 0.927, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "bread: 2\ntomato: 1\nbasil: 1\nmozzarella: 1\nmayonnaise: 1\nwhite cheese: 1\ngreen lettuce: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.9, "ram_available_mb": 100229.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.4, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.06, "peak": 37.41, "min": 21.66}, "VIN": {"avg": 62.75, "peak": 88.16, "min": 55.29}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.06, "energy_joules_est": 30.29, "sample_count": 8, "duration_seconds": 1.079}, "timestamp": "2026-01-17T18:09:08.986906"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 806.951, "latencies_ms": [806.951], "images_per_second": 1.239, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The hand is holding the sandwich in the foreground, while the background features a stove and possibly a cutting board. The sandwich is positioned near the stove, suggesting it is being prepared or consumed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.4, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25543.4, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.56, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 62.65, "peak": 81.69, "min": 53.44}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.56, "energy_joules_est": 23.07, "sample_count": 6, "duration_seconds": 0.808}, "timestamp": "2026-01-17T18:09:09.800102"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 712.368, "latencies_ms": [712.368], "images_per_second": 1.404, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A hand is holding a sandwich with tomato, basil, and cheese on a white cutting board. The sandwich is being prepared or eaten in a kitchen setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.4, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.69, "peak": 35.06, "min": 24.41}, "VIN": {"avg": 63.55, "peak": 77.15, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.69, "energy_joules_est": 21.17, "sample_count": 5, "duration_seconds": 0.713}, "timestamp": "2026-01-17T18:09:10.519472"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 945.156, "latencies_ms": [945.156], "images_per_second": 1.058, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The sandwich features a vibrant mix of colors: the red tomato, green basil, and white cheese contrast beautifully against the light-colored bread. The lighting is soft and somewhat dim, enhancing the visual appeal of the ingredients.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.47, "peak": 36.23, "min": 22.45}, "VIN": {"avg": 63.16, "peak": 78.0, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.47, "energy_joules_est": 26.92, "sample_count": 7, "duration_seconds": 0.946}, "timestamp": "2026-01-17T18:09:11.471845"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 500.39, "latencies_ms": [500.39], "images_per_second": 1.998, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "Two young girls are sitting on the edge of a boat, enjoying the blue ocean and wearing hats.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 25.34, "peak": 27.18, "min": 23.64}, "VIN": {"avg": 58.39, "peak": 61.83, "min": 55.96}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 25.34, "energy_joules_est": 12.69, "sample_count": 3, "duration_seconds": 0.501}, "timestamp": "2026-01-17T18:09:11.981084"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1070.42, "latencies_ms": [1070.42], "images_per_second": 0.934, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Hat: 1\nJeans: 2\nShirt: 1\nPants: 1\nLife preserver: 1\nRope: 2\nBoat: 1\nWater: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 22.75, "peak": 27.59, "min": 20.1}, "VIN": {"avg": 60.94, "peak": 63.15, "min": 54.42}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 22.75, "energy_joules_est": 24.36, "sample_count": 8, "duration_seconds": 1.071}, "timestamp": "2026-01-17T18:09:13.057457"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 790.295, "latencies_ms": [790.295], "images_per_second": 1.265, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main objects are positioned close together on the boat, with the woman on the left leaning over the railing and the girl on the right sitting further back. The boat is situated in the background, and the ocean extends beyond the boat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.65, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 60.82, "peak": 61.47, "min": 59.48}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 22.65, "energy_joules_est": 17.91, "sample_count": 6, "duration_seconds": 0.791}, "timestamp": "2026-01-17T18:09:13.853925"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 849.196, "latencies_ms": [849.196], "images_per_second": 1.178, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "Two young girls are sitting on the edge of a white boat, enjoying the blue ocean. They are wearing casual summer clothing and hats. The scene suggests a leisurely boat ride or outing on the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.65, "peak": 25.59, "min": 20.48}, "VIN": {"avg": 61.41, "peak": 66.77, "min": 54.98}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.65, "energy_joules_est": 19.25, "sample_count": 6, "duration_seconds": 0.85}, "timestamp": "2026-01-17T18:09:14.709429"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 690.794, "latencies_ms": [690.794], "images_per_second": 1.448, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The boat is primarily white, contrasting with the deep blue of the ocean. The lighting suggests a sunny day, and the materials appear to be sturdy and weather-resistant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.08, "peak": 26.0, "min": 20.88}, "VIN": {"avg": 58.35, "peak": 64.54, "min": 52.4}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.08, "energy_joules_est": 15.95, "sample_count": 5, "duration_seconds": 0.691}, "timestamp": "2026-01-17T18:09:15.407099"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 532.608, "latencies_ms": [532.608], "images_per_second": 1.878, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A white sheep with pink ears stands on a grassy hill, gazing directly at the camera.", "error": null, "sys_before": {"cpu_percent": 13.3, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 31.64, "peak": 34.27, "min": 28.76}, "VIN": {"avg": 66.3, "peak": 83.31, "min": 54.33}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 31.64, "energy_joules_est": 16.86, "sample_count": 3, "duration_seconds": 0.533}, "timestamp": "2026-01-17T18:09:15.956937"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 981.744, "latencies_ms": [981.744], "images_per_second": 1.019, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "Sheep: 1\nStone wall: 2\nGrass: 4\nGround: 4\nBushes: 2\nYellow lichen: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 28.8, "peak": 37.41, "min": 22.45}, "VIN": {"avg": 62.95, "peak": 79.38, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.8, "energy_joules_est": 28.29, "sample_count": 7, "duration_seconds": 0.982}, "timestamp": "2026-01-17T18:09:16.944823"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 877.001, "latencies_ms": [877.001], "images_per_second": 1.14, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main object, a white sheep, stands in the foreground, positioned slightly to the right of the viewer. The sheep is situated near a stone wall, which extends into the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.35, "peak": 34.65, "min": 22.85}, "VIN": {"avg": 65.83, "peak": 91.57, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.35, "energy_joules_est": 24.87, "sample_count": 6, "duration_seconds": 0.877}, "timestamp": "2026-01-17T18:09:17.828067"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 636.957, "latencies_ms": [636.957], "images_per_second": 1.57, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A white sheep stands in a grassy field, facing the camera. A stone wall with yellow lichen is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.03, "peak": 35.44, "min": 25.6}, "VIN": {"avg": 55.54, "peak": 60.2, "min": 50.71}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.55}}, "power_watts_avg": 30.03, "energy_joules_est": 19.14, "sample_count": 4, "duration_seconds": 0.637}, "timestamp": "2026-01-17T18:09:18.470864"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 951.441, "latencies_ms": [951.441], "images_per_second": 1.051, "prompt_tokens": 18, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The sheep is predominantly white. Its wool appears soft and fluffy. The lighting suggests a sunny day, with bright sunlight illuminating the scene. The sheep is standing on a grassy area, which appears to be well-maintained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.27, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 62.93, "peak": 78.8, "min": 56.12}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.27, "energy_joules_est": 27.86, "sample_count": 6, "duration_seconds": 0.952}, "timestamp": "2026-01-17T18:09:19.429320"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 531.48, "latencies_ms": [531.48], "images_per_second": 1.882, "prompt_tokens": 8, "response_tokens_est": 16, "n_tiles": 1, "output_text": "Two men are loading a large black pipe onto a truck in a parking lot.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 31.89, "peak": 34.64, "min": 28.74}, "VIN": {"avg": 67.72, "peak": 83.75, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.89, "energy_joules_est": 16.96, "sample_count": 3, "duration_seconds": 0.532}, "timestamp": "2026-01-17T18:09:19.973425"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1148.499, "latencies_ms": [1148.499], "images_per_second": 0.871, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "Pipe: 2\nCrates: 2\nTruck: 1\nMan: 1\nTrees: 1\nBuilding: 1\nCars: 1\nDrain cover: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.6, "ram_available_mb": 100227.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.27, "peak": 38.18, "min": 21.27}, "VIN": {"avg": 59.54, "peak": 66.52, "min": 47.07}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.27, "energy_joules_est": 31.33, "sample_count": 8, "duration_seconds": 1.149}, "timestamp": "2026-01-17T18:09:21.128143"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 817.721, "latencies_ms": [817.721], "images_per_second": 1.223, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the truck transporting them further back. The truck is parked in a parking lot, indicating a relatively close proximity between the objects and the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 27.77, "peak": 33.86, "min": 22.86}, "VIN": {"avg": 62.26, "peak": 80.07, "min": 54.47}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 27.77, "energy_joules_est": 22.71, "sample_count": 6, "duration_seconds": 0.818}, "timestamp": "2026-01-17T18:09:21.951712"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 828.996, "latencies_ms": [828.996], "images_per_second": 1.206, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Two men are loading a large pipe onto a flatbed truck in a parking lot. The truck is carrying several gray cases and has a yellow platform underneath the pipe.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.49, "peak": 35.44, "min": 22.85}, "VIN": {"avg": 61.53, "peak": 78.92, "min": 56.2}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.49, "energy_joules_est": 23.63, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T18:09:22.786822"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 809.505, "latencies_ms": [809.505], "images_per_second": 1.235, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The truck is primarily black and yellow. The lighting appears to be natural daylight. The materials include black pipes, gray plastic cases, and yellow supports. The weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.82, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 64.72, "peak": 88.7, "min": 54.55}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 23.35, "sample_count": 6, "duration_seconds": 0.81}, "timestamp": "2026-01-17T18:09:23.606435"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 647.231, "latencies_ms": [647.231], "images_per_second": 1.545, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "Three giraffes walk along a dirt path near a pond in a lush, wooded area, accompanied by a resting deer.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.73, "peak": 35.06, "min": 26.39}, "VIN": {"avg": 64.35, "peak": 78.95, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 30.73, "energy_joules_est": 19.91, "sample_count": 4, "duration_seconds": 0.648}, "timestamp": "2026-01-17T18:09:24.267128"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1025.819, "latencies_ms": [1025.819], "images_per_second": 0.975, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "giraffe: 3\ntree: 2\npond: 1\nrocks: 1\ndeer: 1\nfence: 1\npath: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.59, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 65.36, "peak": 82.08, "min": 59.17}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 29.34, "sample_count": 7, "duration_seconds": 1.026}, "timestamp": "2026-01-17T18:09:25.302932"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 771.473, "latencies_ms": [771.473], "images_per_second": 1.296, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The giraffes are positioned in the foreground, moving towards the left side of the image. The pond and rocks are located in the background, providing a natural setting for the animals.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.54, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 64.24, "peak": 80.49, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 22.8, "sample_count": 5, "duration_seconds": 0.772}, "timestamp": "2026-01-17T18:09:26.084269"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 911.858, "latencies_ms": [911.858], "images_per_second": 1.097, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Three giraffes are walking along a dirt path near a pond in a wooded area. A deer is resting nearby, observing the giraffes. The setting appears to be a zoo or wildlife park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.55, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 65.44, "peak": 86.85, "min": 56.98}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.55, "energy_joules_est": 26.05, "sample_count": 6, "duration_seconds": 0.912}, "timestamp": "2026-01-17T18:09:27.003026"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1149.986, "latencies_ms": [1149.986], "images_per_second": 0.87, "prompt_tokens": 18, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The giraffes have distinctive brown and white patterns. The lighting appears to be natural, possibly overcast, creating a soft, diffused effect. The giraffes are walking on a dirt path near a body of water, which reflects the surroundings. The overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.73, "peak": 34.65, "min": 21.27}, "VIN": {"avg": 63.47, "peak": 81.46, "min": 53.25}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.73, "energy_joules_est": 30.75, "sample_count": 8, "duration_seconds": 1.15}, "timestamp": "2026-01-17T18:09:28.163276"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 724.187, "latencies_ms": [724.187], "images_per_second": 1.381, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A partially eaten pizza with mushrooms, artichokes, and ham sits on a white plate, accompanied by two glasses of beer, in a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.75, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 63.45, "peak": 72.35, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 20.83, "sample_count": 5, "duration_seconds": 0.724}, "timestamp": "2026-01-17T18:09:28.899706"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1217.819, "latencies_ms": [1217.819], "images_per_second": 0.821, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Pizza: 2\nWine glass: 2\nBeer glass: 1\nTablecloth: 2\nChairs: 2\nCutlery: 1\nBowl: 1\nPerson: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 26.39, "peak": 35.45, "min": 20.88}, "VIN": {"avg": 63.18, "peak": 75.41, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.39, "energy_joules_est": 32.15, "sample_count": 9, "duration_seconds": 1.218}, "timestamp": "2026-01-17T18:09:30.123708"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 717.823, "latencies_ms": [717.823], "images_per_second": 1.393, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the pizza and wine glass placed close together. The background features additional tables and chairs, suggesting a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.15, "peak": 33.88, "min": 24.42}, "VIN": {"avg": 67.3, "peak": 91.51, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 20.94, "sample_count": 5, "duration_seconds": 0.718}, "timestamp": "2026-01-17T18:09:30.848037"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 889.637, "latencies_ms": [889.637], "images_per_second": 1.124, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The scene depicts a restaurant setting with a table set for a meal, featuring a pizza with various toppings and two glasses of beer. The restaurant appears well-stocked with wine bottles and has a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 29.08, "peak": 35.83, "min": 23.64}, "VIN": {"avg": 64.51, "peak": 85.41, "min": 57.86}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 25.88, "sample_count": 6, "duration_seconds": 0.89}, "timestamp": "2026-01-17T18:09:31.743899"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1114.028, "latencies_ms": [1114.028], "images_per_second": 0.898, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The pizza is topped with vibrant red sauce, creamy white cheese, and colorful vegetables like artichokes and mushrooms. The table setting includes blue tablecloths, wine glasses, and a beer glass. The lighting is warm and inviting, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 26.79, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 63.01, "peak": 79.68, "min": 56.39}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.79, "energy_joules_est": 29.85, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T18:09:32.863911"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 639.042, "latencies_ms": [639.042], "images_per_second": 1.565, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A black cat with white paws is drinking water from a running faucet in a white sink, accompanied by a bottle of soap.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 29.75, "peak": 33.88, "min": 25.61}, "VIN": {"avg": 63.83, "peak": 74.05, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.75, "energy_joules_est": 19.03, "sample_count": 4, "duration_seconds": 0.64}, "timestamp": "2026-01-17T18:09:33.513669"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1077.162, "latencies_ms": [1077.162], "images_per_second": 0.928, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "sink: 2\nfaucet: 1\nbottle: 1\nsoap dispenser: 1\nbowl: 1\ncat: 1\nwater: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.28, "peak": 36.26, "min": 21.27}, "VIN": {"avg": 62.64, "peak": 78.14, "min": 54.93}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.28, "energy_joules_est": 29.4, "sample_count": 8, "duration_seconds": 1.078}, "timestamp": "2026-01-17T18:09:34.596687"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 792.779, "latencies_ms": [792.779], "images_per_second": 1.261, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The black cat is positioned in the foreground, close to the water flowing from the faucet. The sink and soap dispenser are placed in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.99, "peak": 34.27, "min": 24.03}, "VIN": {"avg": 65.63, "peak": 95.48, "min": 52.91}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.99, "energy_joules_est": 22.99, "sample_count": 5, "duration_seconds": 0.793}, "timestamp": "2026-01-17T18:09:35.395428"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 718.837, "latencies_ms": [718.837], "images_per_second": 1.391, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A black cat is drinking water from a faucet in a bathroom sink. A bottle of soap and a small white dish are also present on the counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.62, "peak": 35.04, "min": 24.42}, "VIN": {"avg": 66.12, "peak": 93.32, "min": 57.13}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 21.3, "sample_count": 5, "duration_seconds": 0.719}, "timestamp": "2026-01-17T18:09:36.122390"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1005.224, "latencies_ms": [1005.224], "images_per_second": 0.995, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The sink is white and appears to be made of porcelain or ceramic. The lighting in the bathroom is soft and diffused, creating a calm atmosphere. The cat is drinking from the faucet, which appears to be chrome or silver in color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.25, "peak": 36.63, "min": 22.06}, "VIN": {"avg": 65.17, "peak": 98.92, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.25, "energy_joules_est": 28.41, "sample_count": 7, "duration_seconds": 1.006}, "timestamp": "2026-01-17T18:09:37.133674"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 617.688, "latencies_ms": [617.688], "images_per_second": 1.619, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Two people are riding in a horse-drawn carriage through a muddy field, guided by a person holding a whip.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 31.5, "peak": 35.06, "min": 27.17}, "VIN": {"avg": 63.63, "peak": 74.75, "min": 57.89}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.5, "energy_joules_est": 19.47, "sample_count": 4, "duration_seconds": 0.618}, "timestamp": "2026-01-17T18:09:37.765823"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1125.246, "latencies_ms": [1125.246], "images_per_second": 0.889, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "carriage: 2\nhorse: 1\nperson: 2\nhelmet: 1\nfence: 1\npuddle: 1\nbuilding: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.65, "peak": 38.19, "min": 21.66}, "VIN": {"avg": 63.03, "peak": 77.94, "min": 58.14}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.65, "energy_joules_est": 32.25, "sample_count": 8, "duration_seconds": 1.126}, "timestamp": "2026-01-17T18:09:38.897102"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 813.252, "latencies_ms": [813.252], "images_per_second": 1.23, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The horse and carriage are positioned in the foreground, moving towards the background. The horses and carriage are situated in a muddy area, which suggests a rural or agricultural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 29.54, "peak": 34.65, "min": 24.03}, "VIN": {"avg": 62.68, "peak": 77.84, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 24.03, "sample_count": 6, "duration_seconds": 0.814}, "timestamp": "2026-01-17T18:09:39.720408"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1009.345, "latencies_ms": [1009.345], "images_per_second": 0.991, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "Two people are riding in a horse-drawn carriage through a muddy field. The carriage is pulled by a brown horse, and there are puddles reflecting the scene. In the background, a barn and trees are visible under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.75, "peak": 36.23, "min": 22.45}, "VIN": {"avg": 61.84, "peak": 74.51, "min": 57.33}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 29.03, "sample_count": 7, "duration_seconds": 1.01}, "timestamp": "2026-01-17T18:09:40.735417"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 892.118, "latencies_ms": [892.118], "images_per_second": 1.121, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The horse and carriage are brown, appearing to be made of wood. The scene is lit by natural sunlight, creating a warm, inviting atmosphere. The overall setting suggests a rural or agricultural setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 29.28, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 64.64, "peak": 79.01, "min": 60.73}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.28, "energy_joules_est": 26.13, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T18:09:41.633766"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 710.229, "latencies_ms": [710.229], "images_per_second": 1.408, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A newlywed couple stands under a black and white umbrella, smiling and holding each other's hands, amidst a green lawn with guests in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.94, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 64.83, "peak": 82.29, "min": 56.67}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 21.28, "sample_count": 5, "duration_seconds": 0.711}, "timestamp": "2026-01-17T18:09:42.354853"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1086.211, "latencies_ms": [1086.211], "images_per_second": 0.921, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "bride: 1\ngroom: 1\numbrella: 1\ndress: 1\nflowers: 1\ngrass: 1\nhouse: 1\npeople: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.32, "peak": 36.62, "min": 21.27}, "VIN": {"avg": 62.86, "peak": 88.27, "min": 53.76}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.32, "energy_joules_est": 29.69, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-17T18:09:43.447965"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1059.98, "latencies_ms": [1059.98], "images_per_second": 0.943, "prompt_tokens": 25, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The bride is positioned to the left of the groom, holding the umbrella and walking towards him. The umbrella is held by the groom, who stands slightly behind and to the right of the bride. The scene takes place outdoors in a grassy area, with a stone building visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 26.93, "peak": 34.66, "min": 21.27}, "VIN": {"avg": 62.88, "peak": 80.94, "min": 57.79}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.93, "energy_joules_est": 28.55, "sample_count": 8, "duration_seconds": 1.06}, "timestamp": "2026-01-17T18:09:44.513606"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 707.078, "latencies_ms": [707.078], "images_per_second": 1.414, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A bride and groom are walking on a grassy area under a black and white umbrella, likely during a wedding ceremony. The bride is holding a bouquet of orange and yellow flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.21, "peak": 34.26, "min": 24.41}, "VIN": {"avg": 63.9, "peak": 86.26, "min": 52.67}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.21, "energy_joules_est": 20.66, "sample_count": 5, "duration_seconds": 0.707}, "timestamp": "2026-01-17T18:09:45.226943"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 962.74, "latencies_ms": [962.74], "images_per_second": 1.039, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The bride is wearing a white dress and holding a bouquet of orange and yellow flowers. The groom is wearing a dark suit and holding a black and white umbrella. The scene appears to be outdoors on a sunny day, with grass and a stone building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.3, "peak": 35.82, "min": 22.46}, "VIN": {"avg": 62.04, "peak": 82.76, "min": 53.24}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.3, "energy_joules_est": 27.25, "sample_count": 7, "duration_seconds": 0.963}, "timestamp": "2026-01-17T18:09:46.195767"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 513.719, "latencies_ms": [513.719], "images_per_second": 1.947, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "Two people are enjoying a day at the beach, one lying on the sand and the other sitting up playing with a colorful kite.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 25.08, "peak": 27.18, "min": 23.24}, "VIN": {"avg": 60.25, "peak": 61.13, "min": 59.45}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 25.08, "energy_joules_est": 12.89, "sample_count": 3, "duration_seconds": 0.514}, "timestamp": "2026-01-17T18:09:46.720586"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 848.012, "latencies_ms": [848.012], "images_per_second": 1.179, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "kite: 2\nperson: 2\nwatch: 1\nshorts: 1\nsand: 6\nocean: 2\nwaves: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.58, "peak": 27.57, "min": 20.88}, "VIN": {"avg": 58.62, "peak": 62.29, "min": 53.59}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 23.58, "energy_joules_est": 20.0, "sample_count": 6, "duration_seconds": 0.848}, "timestamp": "2026-01-17T18:09:47.575400"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 573.652, "latencies_ms": [573.652], "images_per_second": 1.743, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The man is lying on the sand in the foreground, close to the water. The kite is positioned in the background, slightly to the right of the man.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 24.03, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 61.29, "peak": 63.89, "min": 57.77}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.03, "energy_joules_est": 13.79, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T18:09:48.155056"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 908.813, "latencies_ms": [908.813], "images_per_second": 1.1, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The scene takes place on a sandy beach near the ocean, where two people are enjoying a kite-flying activity. One person is lying down on the sand, while another is crouched down, seemingly preparing to launch the kite.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 23.44, "peak": 27.18, "min": 20.88}, "VIN": {"avg": 61.12, "peak": 63.95, "min": 59.97}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.44, "energy_joules_est": 21.31, "sample_count": 6, "duration_seconds": 0.909}, "timestamp": "2026-01-17T18:09:49.074031"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 690.787, "latencies_ms": [690.787], "images_per_second": 1.448, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The kite is multicolored with blue, red, and green ribbons. The lighting suggests a sunny day, and the sand appears to be dry and light brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 23.4, "peak": 26.4, "min": 21.26}, "VIN": {"avg": 59.65, "peak": 62.39, "min": 55.41}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.4, "energy_joules_est": 16.17, "sample_count": 5, "duration_seconds": 0.691}, "timestamp": "2026-01-17T18:09:49.772699"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 611.416, "latencies_ms": [611.416], "images_per_second": 1.636, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The living room features a brown sofa, a red armchair, a black coffee table, a lamp, a television, and two windows with wooden shutters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 24.03, "peak": 26.38, "min": 22.07}, "VIN": {"avg": 58.52, "peak": 61.48, "min": 53.68}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.03, "energy_joules_est": 14.7, "sample_count": 4, "duration_seconds": 0.612}, "timestamp": "2026-01-17T18:09:50.392984"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1043.93, "latencies_ms": [1043.93], "images_per_second": 0.958, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "sofa: 2\nlamp: 5\ntable: 1\nchair: 1\nstools: 2\nwindow shutters: 2\ntelevision: 1\nfloor lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 22.46, "peak": 27.19, "min": 20.09}, "VIN": {"avg": 60.74, "peak": 61.99, "min": 57.04}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.46, "energy_joules_est": 23.45, "sample_count": 8, "duration_seconds": 1.044}, "timestamp": "2026-01-17T18:09:51.443827"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 594.027, "latencies_ms": [594.027], "images_per_second": 1.683, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The brown couch is positioned to the left of the image, occupying the foreground. The television is situated in the background, near the right edge of the image.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.64, "peak": 26.0, "min": 21.67}, "VIN": {"avg": 61.01, "peak": 62.08, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 23.64, "energy_joules_est": 14.05, "sample_count": 4, "duration_seconds": 0.594}, "timestamp": "2026-01-17T18:09:52.044178"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 710.587, "latencies_ms": [710.587], "images_per_second": 1.407, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The living room features a cozy atmosphere with a brown sofa, red armchair, and small table with a lamp. The room also has a television, windows with shutters, and a floor lamp.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.72, "peak": 27.18, "min": 21.27}, "VIN": {"avg": 59.4, "peak": 61.91, "min": 56.49}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.72, "energy_joules_est": 16.86, "sample_count": 5, "duration_seconds": 0.711}, "timestamp": "2026-01-17T18:09:52.760919"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 768.71, "latencies_ms": [768.71], "images_per_second": 1.301, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The room features light beige walls and wooden shutters. The lighting is warm and inviting, illuminating the space. A brown sofa, red armchair, and black table provide seating options.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.64, "peak": 26.79, "min": 21.28}, "VIN": {"avg": 59.99, "peak": 62.26, "min": 57.89}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.64, "energy_joules_est": 18.18, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T18:09:53.535623"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 540.115, "latencies_ms": [540.115], "images_per_second": 1.851, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A man is enjoying a slice of cake while sitting in a park, surrounded by lush greenery.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 32.43, "peak": 34.27, "min": 29.54}, "VIN": {"avg": 64.25, "peak": 71.97, "min": 60.22}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.43, "energy_joules_est": 17.53, "sample_count": 3, "duration_seconds": 0.541}, "timestamp": "2026-01-17T18:09:54.090870"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 990.36, "latencies_ms": [990.36], "images_per_second": 1.01, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "cake: 1\nspoon: 1\nplate: 1\nt-shirt: 1\ngrass: 1\ntrees: 2\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100226.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 29.99, "peak": 38.6, "min": 22.84}, "VIN": {"avg": 64.03, "peak": 90.95, "min": 55.88}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.99, "energy_joules_est": 29.72, "sample_count": 7, "duration_seconds": 0.991}, "timestamp": "2026-01-17T18:09:55.087847"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1016.461, "latencies_ms": [1016.461], "images_per_second": 0.984, "prompt_tokens": 25, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The man is positioned in the foreground, eating a piece of cake with a spoon. The cake is situated near the center of the image, slightly to the right of the man. The background consists of green trees and a grassy area, creating a natural setting for the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.2, "ram_available_mb": 100227.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.46, "peak": 35.44, "min": 22.44}, "VIN": {"avg": 64.62, "peak": 91.67, "min": 53.67}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.46, "energy_joules_est": 28.95, "sample_count": 7, "duration_seconds": 1.017}, "timestamp": "2026-01-17T18:09:56.110840"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 857.713, "latencies_ms": [857.713], "images_per_second": 1.166, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A man is enjoying a slice of cake outdoors in a park-like setting with green grass and trees. He's using a spoon to eat the dessert while seated on a lawn chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 29.27, "peak": 35.06, "min": 23.62}, "VIN": {"avg": 62.57, "peak": 80.58, "min": 55.9}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.27, "energy_joules_est": 25.12, "sample_count": 6, "duration_seconds": 0.858}, "timestamp": "2026-01-17T18:09:56.974848"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 937.5, "latencies_ms": [937.5], "images_per_second": 1.067, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The man is wearing a blue shirt. The cake is topped with white frosting and appears to have a fruit topping. The setting is outdoors with green grass and trees in the background. The lighting suggests a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.5, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.75, "peak": 36.23, "min": 22.45}, "VIN": {"avg": 64.49, "peak": 88.46, "min": 57.77}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.75, "energy_joules_est": 26.96, "sample_count": 7, "duration_seconds": 0.938}, "timestamp": "2026-01-17T18:09:57.918478"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 501.045, "latencies_ms": [501.045], "images_per_second": 1.996, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A man stands next to a brown donkey carrying a woven basket filled with various bags and luggage.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25545.7, "ram_available_mb": 100226.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 25.34, "peak": 27.18, "min": 23.64}, "VIN": {"avg": 58.6, "peak": 61.44, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 25.34, "energy_joules_est": 12.7, "sample_count": 3, "duration_seconds": 0.501}, "timestamp": "2026-01-17T18:09:58.434946"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 905.771, "latencies_ms": [905.771], "images_per_second": 1.104, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "donkey: 3\nbags: 2\nblanket: 2\nhiking boots: 1\nman: 1\nground: 1\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.91, "min": 13.29}, "VDD_GPU": {"avg": 23.13, "peak": 27.59, "min": 20.48}, "VIN": {"avg": 59.45, "peak": 62.06, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 14.62, "peak": 15.35, "min": 13.78}}, "power_watts_avg": 23.13, "energy_joules_est": 20.96, "sample_count": 7, "duration_seconds": 0.906}, "timestamp": "2026-01-17T18:09:59.346722"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 731.139, "latencies_ms": [731.139], "images_per_second": 1.368, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The man is standing to the left of the donkey. The donkey is positioned in the foreground, partially obscuring the man's legs. The background consists of bare trees and a dirt path.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25545.4, "ram_available_mb": 100226.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 23.48, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 60.38, "peak": 62.03, "min": 58.36}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.48, "energy_joules_est": 17.18, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T18:10:00.084767"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 592.255, "latencies_ms": [592.255], "images_per_second": 1.688, "prompt_tokens": 19, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man stands next to a donkey carrying various bags and luggage. The scene takes place outdoors in a natural setting with trees and rocks.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 24.23, "peak": 26.79, "min": 22.06}, "VIN": {"avg": 60.51, "peak": 62.55, "min": 59.29}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.23, "energy_joules_est": 14.37, "sample_count": 4, "duration_seconds": 0.593}, "timestamp": "2026-01-17T18:10:00.682777"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 672.903, "latencies_ms": [672.903], "images_per_second": 1.486, "prompt_tokens": 18, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The donkey is brown and has a woven basket on its back. The man is wearing purple and gray clothing. The scene is outdoors in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.94, "peak": 27.17, "min": 21.26}, "VIN": {"avg": 58.95, "peak": 59.93, "min": 56.31}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.94, "energy_joules_est": 16.12, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T18:10:01.365764"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 532.898, "latencies_ms": [532.898], "images_per_second": 1.877, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A brightly lit blue bridge spans across a calm river at night, reflecting the vibrant lights and creating a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 24.94, "peak": 26.77, "min": 23.24}, "VIN": {"avg": 61.25, "peak": 62.59, "min": 59.61}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.94, "energy_joules_est": 13.3, "sample_count": 3, "duration_seconds": 0.533}, "timestamp": "2026-01-17T18:10:01.908843"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 808.204, "latencies_ms": [808.204], "images_per_second": 1.237, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "bridge: 4\nriver: 1\nboat: 1\npeople: 6\nlights: 4\ntrees: 1\nroad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.57, "peak": 27.56, "min": 20.88}, "VIN": {"avg": 61.58, "peak": 64.14, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.57, "energy_joules_est": 19.06, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-17T18:10:02.722698"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 657.071, "latencies_ms": [657.071], "images_per_second": 1.522, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The foreground features a paved walkway with people walking along it. In the background, the blue-lit bridge spans across the river, connecting the two sides.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 24.03, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 61.28, "peak": 64.75, "min": 59.45}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 24.03, "energy_joules_est": 15.81, "sample_count": 4, "duration_seconds": 0.658}, "timestamp": "2026-01-17T18:10:03.391367"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 851.936, "latencies_ms": [851.936], "images_per_second": 1.174, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene is set at night, showcasing a brightly lit blue bridge spanning a river. A boat is docked alongside the riverbank, and several people are gathered on the walkway, enjoying the illuminated view.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25545.0, "ram_available_mb": 100227.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.91, "peak": 26.38, "min": 20.48}, "VIN": {"avg": 61.66, "peak": 63.2, "min": 60.64}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.91, "energy_joules_est": 19.53, "sample_count": 6, "duration_seconds": 0.853}, "timestamp": "2026-01-17T18:10:04.249620"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1026.159, "latencies_ms": [1026.159], "images_per_second": 0.975, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The bridge is illuminated with blue and white lights, creating a striking visual effect against the dark night sky. The water reflects the vibrant colors of the lights, adding to the captivating scene. People are gathered on the walkway near the water's edge, enjoying the illuminated view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.8, "ram_available_mb": 100227.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 22.21, "peak": 26.39, "min": 19.7}, "VIN": {"avg": 61.57, "peak": 64.55, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 22.21, "energy_joules_est": 22.8, "sample_count": 8, "duration_seconds": 1.026}, "timestamp": "2026-01-17T18:10:05.281513"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 519.865, "latencies_ms": [519.865], "images_per_second": 1.924, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A person's foot is visible wearing a pink shoe with a bow on top, resting on a weathered blue wooden bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 24.16, "peak": 25.6, "min": 22.85}, "VIN": {"avg": 59.57, "peak": 63.02, "min": 54.45}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 24.16, "energy_joules_est": 12.58, "sample_count": 3, "duration_seconds": 0.521}, "timestamp": "2026-01-17T18:10:05.812561"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 794.522, "latencies_ms": [794.522], "images_per_second": 1.259, "prompt_tokens": 21, "response_tokens_est": 25, "n_tiles": 1, "output_text": "shoe: 1\nbow: 1\npants: 1\nbench: 4\nwood: 4\npaint: 4", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.91, "min": 13.59}, "VDD_GPU": {"avg": 23.37, "peak": 27.17, "min": 20.88}, "VIN": {"avg": 59.47, "peak": 62.96, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 14.63, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.37, "energy_joules_est": 18.58, "sample_count": 6, "duration_seconds": 0.795}, "timestamp": "2026-01-17T18:10:06.613503"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 515.455, "latencies_ms": [515.455], "images_per_second": 1.94, "prompt_tokens": 25, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The pink shoe is positioned in the foreground, while the blue wooden surface is further back, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 24.68, "peak": 26.38, "min": 23.24}, "VIN": {"avg": 60.56, "peak": 63.91, "min": 57.41}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.56, "min": 14.17}}, "power_watts_avg": 24.68, "energy_joules_est": 12.73, "sample_count": 3, "duration_seconds": 0.516}, "timestamp": "2026-01-17T18:10:07.135536"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 810.545, "latencies_ms": [810.545], "images_per_second": 1.234, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A person is sitting on worn wooden benches with faded paint, wearing bright pink flats and blue jeans. The scene suggests a casual, outdoor setting, possibly a park or public space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.57, "peak": 27.57, "min": 20.87}, "VIN": {"avg": 60.93, "peak": 63.59, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 23.57, "energy_joules_est": 19.12, "sample_count": 6, "duration_seconds": 0.811}, "timestamp": "2026-01-17T18:10:07.952347"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 653.766, "latencies_ms": [653.766], "images_per_second": 1.53, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The pink shoe stands out against the blue and green painted wooden surface. The lighting appears to be natural, possibly sunlight, giving the scene a vibrant and lively feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 24.03, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 59.29, "peak": 65.4, "min": 50.62}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 24.03, "energy_joules_est": 15.72, "sample_count": 4, "duration_seconds": 0.654}, "timestamp": "2026-01-17T18:10:08.612200"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 647.978, "latencies_ms": [647.978], "images_per_second": 1.543, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A woman in a green sweater is holding a knife and standing next to a young boy in a plaid shirt at a dining table.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.5, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.69}, "VDD_GPU": {"avg": 29.93, "peak": 33.86, "min": 26.0}, "VIN": {"avg": 62.96, "peak": 74.05, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 19.4, "sample_count": 4, "duration_seconds": 0.648}, "timestamp": "2026-01-17T18:10:09.271028"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 930.964, "latencies_ms": [930.964], "images_per_second": 1.074, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "woman: 2\nknife: 1\ntable: 1\ncake: 1\ncup: 1\nboy: 1\nchair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.5, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.42, "peak": 35.85, "min": 22.45}, "VIN": {"avg": 65.1, "peak": 95.54, "min": 52.93}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 26.47, "sample_count": 7, "duration_seconds": 0.931}, "timestamp": "2026-01-17T18:10:10.208080"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 811.41, "latencies_ms": [811.41], "images_per_second": 1.232, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The woman is positioned in the foreground, holding the knife. The boy stands in the background, slightly further away. The table occupies the foreground, with a cup and a cake placed on it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.02, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 64.77, "peak": 82.9, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.02, "energy_joules_est": 23.56, "sample_count": 6, "duration_seconds": 0.812}, "timestamp": "2026-01-17T18:10:11.025696"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 719.816, "latencies_ms": [719.816], "images_per_second": 1.389, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A woman is holding a knife and appears to be preparing to cut a cake in a home setting. A young boy stands nearby, observing the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.53, "peak": 35.04, "min": 24.41}, "VIN": {"avg": 66.35, "peak": 80.56, "min": 61.16}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.53, "energy_joules_est": 21.26, "sample_count": 5, "duration_seconds": 0.72}, "timestamp": "2026-01-17T18:10:11.752603"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1060.126, "latencies_ms": [1060.126], "images_per_second": 0.943, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The room has a red wall and soft lighting, creating a warm and inviting atmosphere. The woman is wearing a green cardigan and patterned pants, while the boy is wearing a blue plaid shirt. A cake is visible on the table, contributing to the festive ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.22, "peak": 35.82, "min": 21.27}, "VIN": {"avg": 64.06, "peak": 87.0, "min": 59.51}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.22, "energy_joules_est": 28.86, "sample_count": 8, "duration_seconds": 1.06}, "timestamp": "2026-01-17T18:10:12.818982"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 609.0, "latencies_ms": [609.0], "images_per_second": 1.642, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Two elephants stand side by side in an indoor arena, facing the camera, while a trainer stands nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 30.52, "peak": 34.66, "min": 26.39}, "VIN": {"avg": 65.63, "peak": 80.48, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.52, "energy_joules_est": 18.6, "sample_count": 4, "duration_seconds": 0.609}, "timestamp": "2026-01-17T18:10:13.439238"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1215.403, "latencies_ms": [1215.403], "images_per_second": 0.823, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "Elephant: 2\nCircus ring: 2\nBleachers: 4\nLighting equipment: 2\nPerson in red vest: 1\nPerson in black shirt: 1\nPerson in black pants: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 27.13, "peak": 37.41, "min": 20.88}, "VIN": {"avg": 65.52, "peak": 96.37, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.13, "energy_joules_est": 32.98, "sample_count": 9, "duration_seconds": 1.216}, "timestamp": "2026-01-17T18:10:14.660953"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 790.399, "latencies_ms": [790.399], "images_per_second": 1.265, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The elephants are positioned in the foreground of the image, with the circus ring and bleachers in the background. The elephants are relatively close together, suggesting they are in a relatively confined space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.83, "peak": 33.48, "min": 24.03}, "VIN": {"avg": 63.97, "peak": 73.79, "min": 60.85}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.83, "energy_joules_est": 22.8, "sample_count": 5, "duration_seconds": 0.791}, "timestamp": "2026-01-17T18:10:15.457369"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 833.553, "latencies_ms": [833.553], "images_per_second": 1.2, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Two elephants are performing in a circus ring. A person is assisting one of the elephants, possibly performing a routine or interacting with the animal. The setting is indoors, with bleachers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 65.5, "peak": 82.15, "min": 59.72}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 23.97, "sample_count": 6, "duration_seconds": 0.834}, "timestamp": "2026-01-17T18:10:16.297944"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 942.707, "latencies_ms": [942.707], "images_per_second": 1.061, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The elephants are brown and gray in color. The lighting in the arena is bright and focused, illuminating the elephants and the surrounding space. The arena appears to be constructed of concrete and metal, with bleachers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.85, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 63.7, "peak": 72.37, "min": 61.29}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 26.26, "sample_count": 7, "duration_seconds": 0.943}, "timestamp": "2026-01-17T18:10:17.248622"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 795.186, "latencies_ms": [795.186], "images_per_second": 1.258, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Three jockeys on horseback are galloping across the wet sand of a beach, their horses' hooves kicking up small waves as they race towards the horizon.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25544.3, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.21, "peak": 34.26, "min": 24.41}, "VIN": {"avg": 65.33, "peak": 78.86, "min": 59.3}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.21, "energy_joules_est": 23.25, "sample_count": 5, "duration_seconds": 0.796}, "timestamp": "2026-01-17T18:10:18.058023"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1027.869, "latencies_ms": [1027.869], "images_per_second": 0.973, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Horses: 3\nJockeys: 2\nBeach: 2\nSand: 2\nWater: 2\nSky: 1\nClouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.62, "peak": 35.03, "min": 22.06}, "VIN": {"avg": 61.54, "peak": 73.08, "min": 57.24}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.62, "energy_joules_est": 28.4, "sample_count": 7, "duration_seconds": 1.028}, "timestamp": "2026-01-17T18:10:19.096169"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 808.456, "latencies_ms": [808.456], "images_per_second": 1.237, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the horses running towards the background. The horses are relatively close to the viewer, while the background is further away, suggesting the scene is captured from a distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.0, "ram_available_mb": 100228.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.2, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.3, "peak": 34.27, "min": 23.25}, "VIN": {"avg": 65.53, "peak": 95.45, "min": 54.75}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.3, "energy_joules_est": 22.89, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-17T18:10:19.912560"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 807.677, "latencies_ms": [807.677], "images_per_second": 1.238, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Two jockeys on horseback are galloping across a sandy beach, their horses kicking up sand as they race. The setting appears to be a coastal area with a calm ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100227.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.15, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 62.75, "peak": 81.9, "min": 56.46}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 23.55, "sample_count": 6, "duration_seconds": 0.808}, "timestamp": "2026-01-17T18:10:20.726640"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 905.251, "latencies_ms": [905.251], "images_per_second": 1.105, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The horses and riders are wearing light-colored clothing. The scene is illuminated by soft, diffused light, likely from the overcast sky. The sandy beach and ocean suggest a coastal setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.54, "peak": 35.42, "min": 22.84}, "VIN": {"avg": 66.65, "peak": 97.43, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.54, "energy_joules_est": 25.86, "sample_count": 6, "duration_seconds": 0.906}, "timestamp": "2026-01-17T18:10:21.638719"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 695.165, "latencies_ms": [695.165], "images_per_second": 1.439, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A young man in a black snowsuit and goggles is talking on his cellphone while standing in a snowy area surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.53, "peak": 35.04, "min": 24.41}, "VIN": {"avg": 65.32, "peak": 80.29, "min": 59.13}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.53, "energy_joules_est": 20.54, "sample_count": 5, "duration_seconds": 0.696}, "timestamp": "2026-01-17T18:10:22.344027"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1361.045, "latencies_ms": [1361.045], "images_per_second": 0.735, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "Goggles: 1\nJacket: 1\nT-shirt: 1\nSnowboard: 1\nPhone: 1\nPinstripe: 1\nBrooch: 1\nTrees: 1\nSnow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 25.91, "peak": 36.23, "min": 20.08}, "VIN": {"avg": 63.42, "peak": 78.85, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 25.91, "energy_joules_est": 35.27, "sample_count": 10, "duration_seconds": 1.361}, "timestamp": "2026-01-17T18:10:23.710898"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 963.012, "latencies_ms": [963.012], "images_per_second": 1.038, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The main object is a person wearing a winter jacket, standing in the foreground of the image. The background consists of snow and trees, indicating a snowy outdoor setting. The person appears to be engaged in a phone conversation, further emphasizing the foreground placement.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 26.94, "peak": 33.85, "min": 21.66}, "VIN": {"avg": 63.98, "peak": 82.4, "min": 56.63}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 26.94, "energy_joules_est": 25.96, "sample_count": 7, "duration_seconds": 0.964}, "timestamp": "2026-01-17T18:10:24.680197"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 833.421, "latencies_ms": [833.421], "images_per_second": 1.2, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A young man is outdoors in a snowy setting, wearing a black winter jacket and ski goggles. He is talking on a cell phone while standing amidst snow-covered ground and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.42, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 63.9, "peak": 82.79, "min": 54.33}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.42, "energy_joules_est": 23.69, "sample_count": 6, "duration_seconds": 0.834}, "timestamp": "2026-01-17T18:10:25.519607"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 752.429, "latencies_ms": [752.429], "images_per_second": 1.329, "prompt_tokens": 18, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The person is wearing a black jacket with vertical pinstripes. The lighting suggests it's daytime, and the snowy ground indicates a cold, wintery environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.93, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 66.0, "peak": 83.43, "min": 58.05}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 22.53, "sample_count": 5, "duration_seconds": 0.753}, "timestamp": "2026-01-17T18:10:26.278116"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 613.848, "latencies_ms": [613.848], "images_per_second": 1.629, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A white dirt bike with black saddlebags is parked next to a green tent in a grassy field at sunset.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 31.21, "peak": 35.44, "min": 26.79}, "VIN": {"avg": 70.81, "peak": 94.4, "min": 62.41}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.21, "energy_joules_est": 19.17, "sample_count": 4, "duration_seconds": 0.614}, "timestamp": "2026-01-17T18:10:26.903732"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 986.009, "latencies_ms": [986.009], "images_per_second": 1.014, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "tent: 1\nmotorcycle: 1\nbags: 2\ntires: 2\nsunset: 1\ntrees: 4\ngrass: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.5, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.4, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.97, "peak": 37.01, "min": 22.85}, "VIN": {"avg": 65.46, "peak": 89.93, "min": 60.1}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 28.97, "energy_joules_est": 28.58, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T18:10:27.898108"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 693.41, "latencies_ms": [693.41], "images_per_second": 1.442, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The green tent is positioned in the foreground, slightly to the left of the motorcycle. The motorcycle is situated in the background, closer to the center and slightly to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.4, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.53, "peak": 35.06, "min": 24.41}, "VIN": {"avg": 60.94, "peak": 77.1, "min": 47.32}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.53, "energy_joules_est": 20.49, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T18:10:28.597504"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 770.149, "latencies_ms": [770.149], "images_per_second": 1.298, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The scene depicts a camping area in a wooded area during sunset. A green tent is set up next to a parked motorcycle, providing a peaceful and picturesque setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.4, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.72, "peak": 36.63, "min": 25.2}, "VIN": {"avg": 67.35, "peak": 97.02, "min": 59.46}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.72, "energy_joules_est": 23.68, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T18:10:29.374199"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 910.181, "latencies_ms": [910.181], "images_per_second": 1.099, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The tent is green, and the surrounding area is covered in dry grass. The lighting suggests it's either early morning or late evening. The materials appear to be canvas and possibly a metal frame for the motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.22, "peak": 35.85, "min": 23.64}, "VIN": {"avg": 64.33, "peak": 87.53, "min": 55.67}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.22, "energy_joules_est": 26.6, "sample_count": 6, "duration_seconds": 0.91}, "timestamp": "2026-01-17T18:10:30.291393"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 686.024, "latencies_ms": [686.024], "images_per_second": 1.458, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A vintage steam locomotive numbered 66371 pulls passenger cars along a track, with people waiting on the platform and buildings visible in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.78, "peak": 34.68, "min": 24.82}, "VIN": {"avg": 64.17, "peak": 83.26, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 20.45, "sample_count": 5, "duration_seconds": 0.687}, "timestamp": "2026-01-17T18:10:30.988179"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1039.394, "latencies_ms": [1039.394], "images_per_second": 0.962, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "steam locomotive: 1\ntrain carriages: 3\npeople: 3\nplatform: 1\nsign: 1\nbuildings: 2\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.7, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.76, "peak": 36.62, "min": 21.66}, "VIN": {"avg": 65.03, "peak": 101.65, "min": 57.4}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.76, "energy_joules_est": 28.87, "sample_count": 8, "duration_seconds": 1.04}, "timestamp": "2026-01-17T18:10:32.035763"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 768.195, "latencies_ms": [768.195], "images_per_second": 1.302, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The steam locomotive is positioned in the foreground, slightly to the left of the image. The train station and surrounding buildings are in the background, extending across the entire width of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.46, "peak": 35.04, "min": 24.42}, "VIN": {"avg": 66.58, "peak": 84.49, "min": 60.01}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.46, "energy_joules_est": 22.64, "sample_count": 5, "duration_seconds": 0.769}, "timestamp": "2026-01-17T18:10:32.810302"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 985.251, "latencies_ms": [985.251], "images_per_second": 1.015, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The black and white image depicts a vintage steam locomotive pulling passenger cars at a station. A sign indicates the station as \"Hornsea Town.\" The scene suggests a nostalgic atmosphere, possibly from the early days of rail travel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.85, "peak": 35.85, "min": 22.06}, "VIN": {"avg": 64.38, "peak": 80.7, "min": 55.91}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 27.45, "sample_count": 7, "duration_seconds": 0.986}, "timestamp": "2026-01-17T18:10:33.802229"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1278.811, "latencies_ms": [1278.811], "images_per_second": 0.782, "prompt_tokens": 18, "response_tokens_est": 71, "n_tiles": 1, "output_text": "The train is primarily dark in color, suggesting it may be an older model. The lighting is bright, illuminating the scene and highlighting the details of the train and platform. The train appears to be made of metal, further contributing to its historical appearance. The weather appears to be sunny, as indicated by the clear sky and the overall brightness of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 25.64, "peak": 34.65, "min": 20.48}, "VIN": {"avg": 63.42, "peak": 84.06, "min": 59.1}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.64, "energy_joules_est": 32.8, "sample_count": 10, "duration_seconds": 1.279}, "timestamp": "2026-01-17T18:10:35.087653"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 692.632, "latencies_ms": [692.632], "images_per_second": 1.444, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Numerous Chinese street signs and advertisements are suspended from wires above a bustling city street, displaying various characters and phrases.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.59, "peak": 33.06, "min": 24.03}, "VIN": {"avg": 65.97, "peak": 84.95, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 19.82, "sample_count": 5, "duration_seconds": 0.693}, "timestamp": "2026-01-17T18:10:35.792907"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 966.87, "latencies_ms": [966.87], "images_per_second": 1.034, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "building: 10\nsign: 10\nstreet: 10\ncar: 1\nstreetlights: 2\npower lines: 2\nbuildings: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.3, "peak": 36.24, "min": 22.06}, "VIN": {"avg": 62.65, "peak": 81.78, "min": 56.89}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.3, "energy_joules_est": 27.38, "sample_count": 7, "duration_seconds": 0.967}, "timestamp": "2026-01-17T18:10:36.766125"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1264.868, "latencies_ms": [1264.868], "images_per_second": 0.791, "prompt_tokens": 25, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The foreground features numerous signs and wires hanging from the buildings, creating a dense and intricate urban scene. The background includes buildings of varying heights and architectural styles, further emphasizing the density and complexity of the cityscape. The signs and wires are positioned at varying heights and angles, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.04, "peak": 35.06, "min": 20.48}, "VIN": {"avg": 62.53, "peak": 78.16, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.04, "energy_joules_est": 32.95, "sample_count": 9, "duration_seconds": 1.265}, "timestamp": "2026-01-17T18:10:38.037039"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 832.861, "latencies_ms": [832.861], "images_per_second": 1.201, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The black and white image depicts a densely populated urban street scene, likely in a densely populated Asian city. Numerous signs in Chinese characters hang overhead, contributing to the bustling atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.97, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 62.6, "peak": 74.27, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 23.3, "sample_count": 6, "duration_seconds": 0.833}, "timestamp": "2026-01-17T18:10:38.876307"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 851.25, "latencies_ms": [851.25], "images_per_second": 1.175, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The black and white photograph captures a dense urban environment with numerous signs and buildings. The signs are predominantly white with black Chinese characters. The lighting is subdued, creating a somber atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.68, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 63.52, "peak": 79.37, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.68, "energy_joules_est": 24.42, "sample_count": 6, "duration_seconds": 0.852}, "timestamp": "2026-01-17T18:10:39.734277"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 732.807, "latencies_ms": [732.807], "images_per_second": 1.365, "prompt_tokens": 8, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A man in blue shorts and sunglasses sits on a concrete ledge by a body of water, with a \"CLOSED Lakefront Trail\" sign in the foreground.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.39, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 62.82, "peak": 72.4, "min": 57.17}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.39, "energy_joules_est": 21.55, "sample_count": 5, "duration_seconds": 0.733}, "timestamp": "2026-01-17T18:10:40.480145"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1027.273, "latencies_ms": [1027.273], "images_per_second": 0.973, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 1, "output_text": "man: 1\nsign: 1\ngrass: 1\nwater: 1\nsign: 1\nwood: 1\npolice: 1\ncross: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.97, "peak": 35.85, "min": 22.07}, "VIN": {"avg": 65.18, "peak": 89.23, "min": 55.68}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 28.75, "sample_count": 7, "duration_seconds": 1.028}, "timestamp": "2026-01-17T18:10:41.514121"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1006.52, "latencies_ms": [1006.52], "images_per_second": 0.994, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The man is positioned in the foreground, slightly to the right of the lakefront trail sign. The trail sign is situated in the background, near the water's edge. The man is sitting on the concrete edge of a lake, further back from the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.2, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.1, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.12, "peak": 34.27, "min": 21.66}, "VIN": {"avg": 64.28, "peak": 78.36, "min": 59.62}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.12, "energy_joules_est": 27.31, "sample_count": 7, "duration_seconds": 1.007}, "timestamp": "2026-01-17T18:10:42.526474"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 963.457, "latencies_ms": [963.457], "images_per_second": 1.038, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A man is sitting on the edge of a concrete structure near a body of water, possibly a lake or river. A red and white sign reads \"Laketfront Trail Closed,\" indicating a park or trail closure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.6, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.01, "peak": 34.27, "min": 21.66}, "VIN": {"avg": 63.97, "peak": 79.3, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.01, "energy_joules_est": 26.03, "sample_count": 7, "duration_seconds": 0.964}, "timestamp": "2026-01-17T18:10:43.495881"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1101.011, "latencies_ms": [1101.011], "images_per_second": 0.908, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene is bathed in natural daylight, creating a bright and airy atmosphere. The grass appears slightly dry, suggesting it might be a sunny day. The blue wooden sign and the red and white \"CLOSED\" sign stand out against the green grass.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25544.6, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.6, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.54, "peak": 34.65, "min": 20.88}, "VIN": {"avg": 64.75, "peak": 96.23, "min": 55.99}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.54, "energy_joules_est": 29.23, "sample_count": 8, "duration_seconds": 1.101}, "timestamp": "2026-01-17T18:10:44.602921"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 669.968, "latencies_ms": [669.968], "images_per_second": 1.493, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A golden retriever dog stands attentively in a lush green field, its tongue hanging out, alongside a majestic brown horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.6, "ram_available_mb": 100227.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 30.62, "peak": 34.66, "min": 26.38}, "VIN": {"avg": 65.89, "peak": 82.5, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 20.54, "sample_count": 4, "duration_seconds": 0.671}, "timestamp": "2026-01-17T18:10:45.284416"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 816.012, "latencies_ms": [816.012], "images_per_second": 1.225, "prompt_tokens": 21, "response_tokens_est": 24, "n_tiles": 1, "output_text": "horse: 1\ndog: 1\ngrass: 2\nsky: 1\ntrees: 1\nground: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.28, "peak": 36.24, "min": 23.24}, "VIN": {"avg": 64.92, "peak": 85.4, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.28, "energy_joules_est": 23.91, "sample_count": 6, "duration_seconds": 0.817}, "timestamp": "2026-01-17T18:10:46.107248"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 773.687, "latencies_ms": [773.687], "images_per_second": 1.293, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The dog and horse are positioned close to the foreground, creating a sense of proximity and interaction. The horse is slightly further in the background, emphasizing the vastness of the grassy field.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.85, "peak": 35.04, "min": 24.81}, "VIN": {"avg": 65.7, "peak": 89.63, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.85, "energy_joules_est": 23.11, "sample_count": 5, "duration_seconds": 0.774}, "timestamp": "2026-01-17T18:10:46.887083"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1070.201, "latencies_ms": [1070.201], "images_per_second": 0.934, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "In the image, a brown horse and a golden retriever are standing in a green field. The horse is positioned slightly behind the dog, appearing to be observing or interacting with it. The setting suggests a peaceful outdoor environment, possibly a farm or park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.14, "peak": 35.85, "min": 21.28}, "VIN": {"avg": 64.92, "peak": 82.62, "min": 60.35}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.14, "energy_joules_est": 29.06, "sample_count": 8, "duration_seconds": 1.071}, "timestamp": "2026-01-17T18:10:47.963875"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 765.07, "latencies_ms": [765.07], "images_per_second": 1.307, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The horse and dog are brown and white respectively. The lighting is bright and sunny, creating a pleasant atmosphere. The scene takes place in a green field under a clear sky.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.54, "peak": 34.26, "min": 24.82}, "VIN": {"avg": 65.73, "peak": 83.05, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 22.61, "sample_count": 5, "duration_seconds": 0.765}, "timestamp": "2026-01-17T18:10:48.734826"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 549.935, "latencies_ms": [549.935], "images_per_second": 1.818, "prompt_tokens": 8, "response_tokens_est": 17, "n_tiles": 1, "output_text": "A volleyball game is in progress, with players in green and blue uniforms actively participating.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 31.21, "peak": 35.44, "min": 26.79}, "VIN": {"avg": 67.1, "peak": 82.73, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.21, "energy_joules_est": 17.18, "sample_count": 4, "duration_seconds": 0.551}, "timestamp": "2026-01-17T18:10:49.297679"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1411.087, "latencies_ms": [1411.087], "images_per_second": 0.709, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 1, "output_text": "volleyball: 8\nnet: 1\nvolleyball: 1\nvolleyball: 1\nvolleyball: 1\nvolleyball: 1\nvolleyball: 1\nvolleyball: 1\nvolleyball: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.7, "peak": 37.8, "min": 20.48}, "VIN": {"avg": 63.51, "peak": 92.74, "min": 56.49}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.7, "energy_joules_est": 37.68, "sample_count": 10, "duration_seconds": 1.411}, "timestamp": "2026-01-17T18:10:50.714820"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1042.797, "latencies_ms": [1042.797], "images_per_second": 0.959, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The volleyball game is taking place in a large indoor space. The players are spread out across the court, with some closer to the foreground and others further in the background. The volleyball is positioned near the center of the court, drawing the viewer's attention.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.84, "peak": 33.47, "min": 21.66}, "VIN": {"avg": 63.54, "peak": 80.65, "min": 59.48}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.84, "energy_joules_est": 28.0, "sample_count": 7, "duration_seconds": 1.043}, "timestamp": "2026-01-17T18:10:51.763661"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 846.022, "latencies_ms": [846.022], "images_per_second": 1.182, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "A group of people are playing volleyball in a large, indoor gymnasium. The players are wearing green and blue uniforms, and the game is taking place on a blue court with orange and white lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.4, "ram_available_mb": 100227.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.89, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 63.84, "peak": 83.15, "min": 54.79}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.89, "energy_joules_est": 23.61, "sample_count": 6, "duration_seconds": 0.846}, "timestamp": "2026-01-17T18:10:52.615841"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 770.756, "latencies_ms": [770.756], "images_per_second": 1.297, "prompt_tokens": 18, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The gym has light blue flooring and neutral lighting. The volleyball net is white, and the players are wearing various colors, including green and blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.54, "peak": 35.44, "min": 24.42}, "VIN": {"avg": 65.36, "peak": 80.91, "min": 59.25}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 22.79, "sample_count": 5, "duration_seconds": 0.771}, "timestamp": "2026-01-17T18:10:53.395345"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 834.972, "latencies_ms": [834.972], "images_per_second": 1.198, "prompt_tokens": 8, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A group of zebras and wildebeests are seen grazing and walking across a grassy plain near a body of water, with a large flock of flamingos visible in the distance.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.75, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 62.33, "peak": 72.21, "min": 53.23}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 24.01, "sample_count": 6, "duration_seconds": 0.835}, "timestamp": "2026-01-17T18:10:54.242406"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1060.954, "latencies_ms": [1060.954], "images_per_second": 0.943, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "zebras: 5\nflamingos: 10\nwildebeests: 4\ngrass: 8\nwater: 2\nhills: 2\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.13, "peak": 35.04, "min": 21.28}, "VIN": {"avg": 63.33, "peak": 89.97, "min": 52.9}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.55}}, "power_watts_avg": 27.13, "energy_joules_est": 28.8, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T18:10:55.310010"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1043.002, "latencies_ms": [1043.002], "images_per_second": 0.959, "prompt_tokens": 25, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The zebras are positioned in the foreground, moving across the grassy plain. The wildebeest are grazing in the background, near the left side of the image. The large flock of flamingos is situated in the background, near the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25544.1, "ram_available_mb": 100228.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.88, "peak": 34.66, "min": 21.27}, "VIN": {"avg": 62.28, "peak": 80.64, "min": 55.4}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.88, "energy_joules_est": 28.04, "sample_count": 8, "duration_seconds": 1.043}, "timestamp": "2026-01-17T18:10:56.359175"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1142.592, "latencies_ms": [1142.592], "images_per_second": 0.875, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The scene depicts a vast grassy plain with zebras and wildebeests grazing and moving across the landscape. A large flock of flamingos is gathered on a body of water in the background, creating a striking contrast between the animals and the vibrant pink hues of the flamingos.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.9, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.22, "peak": 34.66, "min": 20.88}, "VIN": {"avg": 63.41, "peak": 83.5, "min": 57.35}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.22, "energy_joules_est": 29.97, "sample_count": 9, "duration_seconds": 1.143}, "timestamp": "2026-01-17T18:10:57.507565"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 950.762, "latencies_ms": [950.762], "images_per_second": 1.052, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The scene is dominated by vibrant pink flamingos, which stand out against the green grass and light-colored sky. The lighting suggests a sunny day, and the materials appear to be natural, undisturbed ground and vegetation.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 26.89, "peak": 33.48, "min": 21.66}, "VIN": {"avg": 62.91, "peak": 80.75, "min": 56.58}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.89, "energy_joules_est": 25.57, "sample_count": 7, "duration_seconds": 0.951}, "timestamp": "2026-01-17T18:10:58.464383"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 599.588, "latencies_ms": [599.588], "images_per_second": 1.668, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "An orange and white cat is sitting on a wooden deck, looking at its reflection in the glass surface in front of it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25543.3, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.54, "peak": 34.28, "min": 26.4}, "VIN": {"avg": 69.19, "peak": 94.79, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.95, "min": 14.56}}, "power_watts_avg": 30.54, "energy_joules_est": 18.34, "sample_count": 4, "duration_seconds": 0.6}, "timestamp": "2026-01-17T18:10:59.075867"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1084.896, "latencies_ms": [1084.896], "images_per_second": 0.922, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "cat: 2\nmirror: 1\nwooden planks: 8\nglass: 1\nreflection: 1\ncat's fur: 2\ncat's eyes: 2", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.77, "peak": 37.03, "min": 21.66}, "VIN": {"avg": 65.08, "peak": 96.49, "min": 56.35}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.77, "energy_joules_est": 30.14, "sample_count": 8, "duration_seconds": 1.085}, "timestamp": "2026-01-17T18:11:00.167249"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 804.488, "latencies_ms": [804.488], "images_per_second": 1.243, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The orange and white cat is positioned in the foreground, looking at its reflection in the glass surface. The wooden deck extends into the background, providing a contrast to the cat's vibrant colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.55, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 66.28, "peak": 82.98, "min": 61.26}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.55, "energy_joules_est": 22.98, "sample_count": 6, "duration_seconds": 0.805}, "timestamp": "2026-01-17T18:11:00.977852"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 746.515, "latencies_ms": [746.515], "images_per_second": 1.34, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "An orange and white cat is sitting on a wooden deck, looking at its reflection in a mirror. The setting appears to be outdoors, possibly near a window or glass surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.8, "ram_available_mb": 100228.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 30.4, "peak": 35.83, "min": 25.21}, "VIN": {"avg": 69.04, "peak": 97.16, "min": 59.43}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.4, "energy_joules_est": 22.72, "sample_count": 5, "duration_seconds": 0.747}, "timestamp": "2026-01-17T18:11:01.730991"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 796.912, "latencies_ms": [796.912], "images_per_second": 1.255, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The cat is primarily orange and white. The lighting appears to be natural, possibly from sunlight filtering through the glass. The cat is sitting on a weathered wooden deck, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.16, "peak": 36.23, "min": 24.81}, "VIN": {"avg": 68.9, "peak": 94.95, "min": 59.62}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.16, "energy_joules_est": 24.05, "sample_count": 5, "duration_seconds": 0.797}, "timestamp": "2026-01-17T18:11:02.538406"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 554.4, "latencies_ms": [554.4], "images_per_second": 1.804, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "Several boats are docked along the shore of a calm lake, with a large building visible in the background.", "error": null, "sys_before": {"cpu_percent": 23.1, "ram_used_mb": 25543.6, "ram_available_mb": 100228.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.0, "peak": 35.42, "min": 26.38}, "VIN": {"avg": 65.7, "peak": 78.83, "min": 59.76}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.0, "energy_joules_est": 17.2, "sample_count": 4, "duration_seconds": 0.555}, "timestamp": "2026-01-17T18:11:03.104296"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1055.095, "latencies_ms": [1055.095], "images_per_second": 0.948, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Boat: 4\nLamp post: 1\nBuildings: 2\nTrees: 6\nWater: 5\nSky: 1\nHills: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 28.7, "peak": 37.41, "min": 22.46}, "VIN": {"avg": 64.15, "peak": 78.19, "min": 59.54}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.7, "energy_joules_est": 30.29, "sample_count": 7, "duration_seconds": 1.055}, "timestamp": "2026-01-17T18:11:04.166370"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 706.972, "latencies_ms": [706.972], "images_per_second": 1.414, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The main objects are positioned in the foreground, with the water and a distant shoreline in the background. The boats are situated near the shore, further back from the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.3, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.46, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 63.35, "peak": 76.9, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.46, "energy_joules_est": 20.84, "sample_count": 5, "duration_seconds": 0.707}, "timestamp": "2026-01-17T18:11:04.879189"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 950.012, "latencies_ms": [950.012], "images_per_second": 1.053, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a serene lakeside town with several boats docked along the shore. The buildings in the background suggest a resort or hotel complex. The calm water reflects the hazy sky, creating a tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.8, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25542.7, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.69, "peak": 35.85, "min": 22.85}, "VIN": {"avg": 68.65, "peak": 97.24, "min": 60.6}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 27.27, "sample_count": 6, "duration_seconds": 0.951}, "timestamp": "2026-01-17T18:11:05.839456"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 920.363, "latencies_ms": [920.363], "images_per_second": 1.087, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The water is a calm, grayish-blue color. The lighting suggests an overcast day, with diffused light. The boats are primarily white and appear to be made of metal or fiberglass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25542.7, "ram_available_mb": 100229.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.23, "peak": 34.66, "min": 22.85}, "VIN": {"avg": 61.47, "peak": 72.56, "min": 54.32}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 25.99, "sample_count": 6, "duration_seconds": 0.921}, "timestamp": "2026-01-17T18:11:06.770094"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 522.342, "latencies_ms": [522.342], "images_per_second": 1.914, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A man walks his bicycle down a street lined with shops and signs, carrying a towel around his neck.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.69}, "VDD_GPU": {"avg": 25.07, "peak": 27.17, "min": 23.24}, "VIN": {"avg": 59.18, "peak": 60.67, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.07, "energy_joules_est": 13.11, "sample_count": 3, "duration_seconds": 0.523}, "timestamp": "2026-01-17T18:11:07.304974"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1041.87, "latencies_ms": [1041.87], "images_per_second": 0.96, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "building: 2\nsign: 2\nbicycle: 2\nman: 1\ntowel: 1\nperson: 1\numbrella: 1\nair conditioner: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.49}, "VDD_GPU": {"avg": 22.75, "peak": 27.56, "min": 20.09}, "VIN": {"avg": 60.87, "peak": 65.19, "min": 55.97}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.18}}, "power_watts_avg": 22.75, "energy_joules_est": 23.71, "sample_count": 8, "duration_seconds": 1.042}, "timestamp": "2026-01-17T18:11:08.356045"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 688.51, "latencies_ms": [688.51], "images_per_second": 1.452, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The man is positioned in the foreground, facing the camera. The bicycle is situated in the background, slightly to the right. The street scene extends into the background, suggesting an urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 23.16, "peak": 25.99, "min": 21.27}, "VIN": {"avg": 59.61, "peak": 62.82, "min": 53.65}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.16, "energy_joules_est": 15.95, "sample_count": 5, "duration_seconds": 0.689}, "timestamp": "2026-01-17T18:11:09.050976"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 964.928, "latencies_ms": [964.928], "images_per_second": 1.036, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a street scene in what appears to be a Chinese town. A man is walking with a bicycle, wearing a towel around his waist. Behind him, there are several shops and signs in Chinese characters, indicating a bustling and possibly touristy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 22.8, "peak": 26.8, "min": 20.49}, "VIN": {"avg": 60.48, "peak": 61.74, "min": 57.87}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 22.8, "energy_joules_est": 22.01, "sample_count": 7, "duration_seconds": 0.965}, "timestamp": "2026-01-17T18:11:10.022018"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 708.765, "latencies_ms": [708.765], "images_per_second": 1.411, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene is black and white, illuminated by natural light. The buildings appear to have a simple, possibly temporary construction style. The overall atmosphere is somewhat somber due to the man's expression.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.17, "peak": 26.01, "min": 21.27}, "VIN": {"avg": 59.76, "peak": 62.88, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 23.17, "energy_joules_est": 16.43, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T18:11:10.737410"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 532.757, "latencies_ms": [532.757], "images_per_second": 1.877, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A market stall displays numerous bunches of ripe, yellow bananas hanging from blue ropes, ready for purchase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.0, "ram_available_mb": 100229.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 24.82, "peak": 26.4, "min": 23.24}, "VIN": {"avg": 58.85, "peak": 63.04, "min": 54.99}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 24.82, "energy_joules_est": 13.23, "sample_count": 3, "duration_seconds": 0.533}, "timestamp": "2026-01-17T18:11:11.280584"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1018.631, "latencies_ms": [1018.631], "images_per_second": 0.982, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "bananas: 12\nwooden pole: 1\nblue metal gate: 1\ngrey cloth: 1\nwooden scale: 1\nblue metal container: 1\nwhite cloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.2, "ram_available_mb": 100228.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25543.5, "ram_available_mb": 100228.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 23.08, "peak": 27.19, "min": 20.48}, "VIN": {"avg": 62.29, "peak": 64.37, "min": 60.54}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 23.08, "energy_joules_est": 23.52, "sample_count": 7, "duration_seconds": 1.019}, "timestamp": "2026-01-17T18:11:12.310055"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 750.68, "latencies_ms": [750.68], "images_per_second": 1.332, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The bananas are primarily displayed in the foreground, hanging from the ceiling and suspended from the structure above. The background features the storefront and partially visible items, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25543.5, "ram_available_mb": 100228.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.08, "peak": 25.99, "min": 20.88}, "VIN": {"avg": 60.82, "peak": 65.32, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.08, "energy_joules_est": 17.34, "sample_count": 5, "duration_seconds": 0.751}, "timestamp": "2026-01-17T18:11:13.067336"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 968.931, "latencies_ms": [968.931], "images_per_second": 1.032, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a market stall displaying numerous bunches of ripe, yellow bananas hanging from the ceiling. The bananas are arranged in neat rows, creating a visually appealing display. The stall appears to be outdoors, with natural light illuminating the bananas and the surrounding area.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25540.3, "ram_available_mb": 100231.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.4, "ram_available_mb": 100231.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 22.4, "peak": 26.39, "min": 20.09}, "VIN": {"avg": 61.59, "peak": 64.66, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.4, "energy_joules_est": 21.71, "sample_count": 7, "duration_seconds": 0.969}, "timestamp": "2026-01-17T18:11:14.042353"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 950.184, "latencies_ms": [950.184], "images_per_second": 1.052, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The bananas are predominantly yellow, indicating they are ripe. The lighting appears to be natural, possibly sunlight, giving the bananas a vibrant appearance. The bananas are hanging from strings, suggesting they are likely for sale or display. The overall scene suggests a market setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.4, "ram_available_mb": 100231.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.4, "ram_available_mb": 100231.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 22.28, "peak": 26.0, "min": 20.09}, "VIN": {"avg": 59.22, "peak": 61.5, "min": 56.43}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.28, "energy_joules_est": 21.18, "sample_count": 7, "duration_seconds": 0.951}, "timestamp": "2026-01-17T18:11:14.998629"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 579.296, "latencies_ms": [579.296], "images_per_second": 1.726, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "An electric train travels through a picturesque countryside, passing through a lush green field with distant mountains in the background.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25540.4, "ram_available_mb": 100231.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.34, "peak": 32.7, "min": 25.59}, "VIN": {"avg": 69.55, "peak": 97.89, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.34, "energy_joules_est": 17.01, "sample_count": 4, "duration_seconds": 0.58}, "timestamp": "2026-01-17T18:11:15.589598"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1325.347, "latencies_ms": [1325.347], "images_per_second": 0.755, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Train car: 4\nTrain car: 2\nTrain car: 1\nTrain car: 1\nTrain car: 1\nTrain car: 1\nTrain car: 1\nTrain car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.1, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.56, "peak": 36.21, "min": 20.48}, "VIN": {"avg": 62.62, "peak": 79.0, "min": 54.16}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.56, "energy_joules_est": 35.21, "sample_count": 9, "duration_seconds": 1.326}, "timestamp": "2026-01-17T18:11:16.924929"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 975.586, "latencies_ms": [975.586], "images_per_second": 1.025, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The main object is a train traveling from left to right in the foreground, moving through a rural landscape. The background features rolling hills, fields, and distant buildings. The train is positioned near the center of the image, occupying a significant portion of the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.1, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.61, "peak": 33.08, "min": 21.66}, "VIN": {"avg": 64.02, "peak": 83.0, "min": 55.29}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.61, "energy_joules_est": 25.97, "sample_count": 7, "duration_seconds": 0.976}, "timestamp": "2026-01-17T18:11:17.907603"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1004.953, "latencies_ms": [1004.953], "images_per_second": 0.995, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The scene depicts a green and white electric train traveling through a rural landscape, pulling several red freight cars along tracks. The setting includes rolling green hills, fields, and distant mountains, creating a picturesque backdrop for the train's journey.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.4, "ram_available_mb": 100231.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.18, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 65.08, "peak": 93.77, "min": 56.31}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 27.32, "sample_count": 7, "duration_seconds": 1.005}, "timestamp": "2026-01-17T18:11:18.918872"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 927.59, "latencies_ms": [927.59], "images_per_second": 1.078, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The train is painted in green and white. The scene is illuminated by natural daylight, creating a bright and peaceful atmosphere. The train appears to be made of sturdy metal and wood, typical of older electric trains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.4, "ram_available_mb": 100231.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.69, "peak": 33.86, "min": 22.84}, "VIN": {"avg": 65.5, "peak": 81.23, "min": 60.76}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 27.69, "energy_joules_est": 25.69, "sample_count": 6, "duration_seconds": 0.928}, "timestamp": "2026-01-17T18:11:19.853376"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 573.983, "latencies_ms": [573.983], "images_per_second": 1.742, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A man in khaki shorts and a green hat is standing on a sandy beach, holding a kite and possibly preparing to fly it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25540.1, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 24.22, "peak": 26.77, "min": 22.06}, "VIN": {"avg": 61.23, "peak": 62.53, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 14.67, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.22, "energy_joules_est": 13.91, "sample_count": 4, "duration_seconds": 0.574}, "timestamp": "2026-01-17T18:11:20.435593"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 969.635, "latencies_ms": [969.635], "images_per_second": 1.031, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "kite: 1\nman: 1\nhat: 1\nsandals: 1\nbeach: 1\nocean: 2\nwaves: 2\nchair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.1, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25540.1, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 22.96, "peak": 27.18, "min": 20.09}, "VIN": {"avg": 61.41, "peak": 64.79, "min": 56.91}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.96, "energy_joules_est": 22.27, "sample_count": 7, "duration_seconds": 0.97}, "timestamp": "2026-01-17T18:11:21.411120"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 756.635, "latencies_ms": [756.635], "images_per_second": 1.322, "prompt_tokens": 25, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The man is standing on the left side of the image, facing the ocean. The kite is positioned in the background, near the right edge of the image. The man appears to be holding the kite string.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.1, "ram_available_mb": 100232.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25538.9, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.08, "peak": 25.99, "min": 20.88}, "VIN": {"avg": 61.64, "peak": 64.27, "min": 59.27}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.08, "energy_joules_est": 17.48, "sample_count": 5, "duration_seconds": 0.757}, "timestamp": "2026-01-17T18:11:22.175276"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 696.252, "latencies_ms": [696.252], "images_per_second": 1.436, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 1, "output_text": "A man is standing on a sandy beach, flying a kite while holding a kite string in his right hand. He appears to be enjoying the beach weather and the activity.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25538.9, "ram_available_mb": 100233.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.31, "peak": 26.38, "min": 21.27}, "VIN": {"avg": 61.04, "peak": 62.28, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 23.31, "energy_joules_est": 16.24, "sample_count": 5, "duration_seconds": 0.697}, "timestamp": "2026-01-17T18:11:22.878756"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 994.474, "latencies_ms": [994.474], "images_per_second": 1.006, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The beach is sandy and appears light gray. The sky is bright blue, suggesting sunny weather. The man is wearing khaki shorts and a dark shirt, and he is holding a kite string. The kite is partially visible and appears to be yellow and red.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 22.63, "peak": 26.79, "min": 20.1}, "VIN": {"avg": 60.16, "peak": 63.26, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.63, "energy_joules_est": 22.51, "sample_count": 7, "duration_seconds": 0.995}, "timestamp": "2026-01-17T18:11:23.879675"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 605.687, "latencies_ms": [605.687], "images_per_second": 1.651, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "Two orange plastic planters filled with lush green broccoli plants are placed side by side on a sandy ground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 29.15, "peak": 33.1, "min": 25.21}, "VIN": {"avg": 64.26, "peak": 79.91, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 17.66, "sample_count": 4, "duration_seconds": 0.606}, "timestamp": "2026-01-17T18:11:24.496671"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 969.95, "latencies_ms": [969.95], "images_per_second": 1.031, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "Broccoli: 2\nLeaves: 6\nBroccoli florets: 2\nPot: 2\nSoil: 1\nGround: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 28.18, "peak": 36.23, "min": 22.05}, "VIN": {"avg": 63.11, "peak": 85.43, "min": 53.76}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.18, "energy_joules_est": 27.34, "sample_count": 7, "duration_seconds": 0.97}, "timestamp": "2026-01-17T18:11:25.472889"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 656.477, "latencies_ms": [656.477], "images_per_second": 1.523, "prompt_tokens": 25, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, partially overlapping. The broccoli plants are situated in the background, slightly further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.42, "peak": 34.26, "min": 26.38}, "VIN": {"avg": 69.62, "peak": 95.0, "min": 60.41}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.42, "energy_joules_est": 19.98, "sample_count": 4, "duration_seconds": 0.657}, "timestamp": "2026-01-17T18:11:26.136993"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 880.219, "latencies_ms": [880.219], "images_per_second": 1.136, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The scene depicts a garden bed with two broccoli plants growing in raised orange plastic containers. The plants have large, green leaves and appear healthy, indicating a well-maintained growing environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 29.28, "peak": 36.63, "min": 23.24}, "VIN": {"avg": 63.18, "peak": 78.21, "min": 55.86}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.28, "energy_joules_est": 25.78, "sample_count": 6, "duration_seconds": 0.881}, "timestamp": "2026-01-17T18:11:27.023175"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1103.902, "latencies_ms": [1103.902], "images_per_second": 0.906, "prompt_tokens": 18, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The broccoli plants exhibit a vibrant green color, indicating healthy growth. The lighting appears to be natural, possibly sunlight, enhancing the freshness and color of the leaves. The plants are housed in terracotta-colored plastic containers, suggesting they are being grown in a controlled environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 26.98, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 61.43, "peak": 83.83, "min": 53.06}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.98, "energy_joules_est": 29.8, "sample_count": 8, "duration_seconds": 1.104}, "timestamp": "2026-01-17T18:11:28.133529"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 691.464, "latencies_ms": [691.464], "images_per_second": 1.446, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A young boy is sitting on the back of a pony, guided by an older man who is holding the pony's lead rope.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.81, "min": 14.3}, "VDD_GPU": {"avg": 29.23, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.0, "peak": 81.78, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 20.22, "sample_count": 5, "duration_seconds": 0.692}, "timestamp": "2026-01-17T18:11:28.837909"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1066.435, "latencies_ms": [1066.435], "images_per_second": 0.938, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "pony: 1\nman: 1\nchild: 1\nbuilding: 1\nleash: 1\nchair: 1\ntable: 1\nlantern: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.18, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 64.92, "peak": 87.57, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 28.99, "sample_count": 8, "duration_seconds": 1.067}, "timestamp": "2026-01-17T18:11:29.910503"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 994.183, "latencies_ms": [994.183], "images_per_second": 1.006, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The pony is positioned in the foreground, close to the man and child. The man and child are standing in the background, slightly further away. The pony is situated near a red building, which serves as the backdrop for the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.23, "peak": 33.86, "min": 22.06}, "VIN": {"avg": 62.78, "peak": 71.69, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 27.08, "sample_count": 7, "duration_seconds": 0.995}, "timestamp": "2026-01-17T18:11:30.910847"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 815.069, "latencies_ms": [815.069], "images_per_second": 1.227, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A young boy is riding a small pony outside a red building, possibly a house or stable. A man is leading the pony and guiding it through the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.29, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 65.09, "peak": 79.36, "min": 60.31}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 23.07, "sample_count": 6, "duration_seconds": 0.815}, "timestamp": "2026-01-17T18:11:31.731895"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 816.698, "latencies_ms": [816.698], "images_per_second": 1.224, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The pony is light brown with a white blaze on its face. The man is wearing a blue shirt and dark pants. The scene appears to be outdoors in bright sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.75, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 63.19, "peak": 82.62, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 23.49, "sample_count": 6, "duration_seconds": 0.817}, "timestamp": "2026-01-17T18:11:32.554511"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 621.35, "latencies_ms": [621.35], "images_per_second": 1.609, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A young boy in a blue striped shirt is walking along a dirt path in a field of blue flowers, holding a stuffed animal.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 30.81, "peak": 35.04, "min": 26.39}, "VIN": {"avg": 65.55, "peak": 74.08, "min": 61.58}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.81, "energy_joules_est": 19.16, "sample_count": 4, "duration_seconds": 0.622}, "timestamp": "2026-01-17T18:11:33.187458"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1303.229, "latencies_ms": [1303.229], "images_per_second": 0.767, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "bluebells: 100\npath: 50\ntoddler: 1\nstriped shirt: 1\nblue jeans: 1\nstuffed animal: 1\nshoes: 1\ngrass: 1\nflowers: 100", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.78, "peak": 37.01, "min": 20.87}, "VIN": {"avg": 62.48, "peak": 73.6, "min": 56.31}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.78, "energy_joules_est": 34.91, "sample_count": 9, "duration_seconds": 1.304}, "timestamp": "2026-01-17T18:11:34.500775"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1081.995, "latencies_ms": [1081.995], "images_per_second": 0.924, "prompt_tokens": 25, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The child is walking towards a field of bluebells. The bluebells are in the foreground, slightly blurred, creating a sense of depth. The child is walking on a path that extends into the background, further emphasizing the distance between the child and the flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 25.84, "peak": 33.06, "min": 20.88}, "VIN": {"avg": 62.61, "peak": 81.71, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.84, "energy_joules_est": 27.97, "sample_count": 8, "duration_seconds": 1.082}, "timestamp": "2026-01-17T18:11:35.588879"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 787.108, "latencies_ms": [787.108], "images_per_second": 1.27, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A young boy walks along a dirt path through a field of bluebells. He carries a stuffed animal, possibly a teddy bear, while exploring the vibrant flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 28.75, "peak": 33.85, "min": 24.03}, "VIN": {"avg": 65.12, "peak": 77.48, "min": 60.59}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 22.64, "sample_count": 5, "duration_seconds": 0.787}, "timestamp": "2026-01-17T18:11:36.383477"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 878.316, "latencies_ms": [878.316], "images_per_second": 1.139, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The scene is dominated by vibrant blue flowers, creating a striking contrast against the dirt path. The lighting suggests a sunny day, and the materials appear to be natural, organic elements like dirt and grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.7, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.3, "peak": 34.66, "min": 22.85}, "VIN": {"avg": 63.57, "peak": 84.6, "min": 55.13}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.3, "energy_joules_est": 24.86, "sample_count": 6, "duration_seconds": 0.879}, "timestamp": "2026-01-17T18:11:37.267644"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 530.574, "latencies_ms": [530.574], "images_per_second": 1.885, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A large orange rests on the edge of a parking lot, surrounded by parked cars and trees in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25534.7, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.51, "min": 14.2}, "VDD_GPU": {"avg": 32.43, "peak": 35.45, "min": 29.15}, "VIN": {"avg": 68.8, "peak": 81.84, "min": 62.13}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 32.43, "energy_joules_est": 17.22, "sample_count": 3, "duration_seconds": 0.531}, "timestamp": "2026-01-17T18:11:37.808256"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 950.646, "latencies_ms": [950.646], "images_per_second": 1.052, "prompt_tokens": 21, "response_tokens_est": 29, "n_tiles": 1, "output_text": "orange: 1\ncar: 2\ntree: 3\nroad: 4\nparking lot: 5\norange: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25535.1, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 29.15, "peak": 38.21, "min": 22.45}, "VIN": {"avg": 64.88, "peak": 86.12, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.15, "energy_joules_est": 27.72, "sample_count": 7, "duration_seconds": 0.951}, "timestamp": "2026-01-17T18:11:38.764921"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 741.421, "latencies_ms": [741.421], "images_per_second": 1.349, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The orange is positioned in the foreground, slightly to the left of the viewer. The parking lot with cars in the background extends into the distance, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.79, "peak": 34.66, "min": 24.83}, "VIN": {"avg": 63.95, "peak": 83.64, "min": 52.38}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.79, "energy_joules_est": 22.1, "sample_count": 5, "duration_seconds": 0.742}, "timestamp": "2026-01-17T18:11:39.514352"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 830.832, "latencies_ms": [830.832], "images_per_second": 1.204, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A large orange rests on the asphalt of a parking lot, contrasting with the surrounding cars and trees. The scene suggests a parking lot or roadside area with parked vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25535.1, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.89, "peak": 35.85, "min": 23.23}, "VIN": {"avg": 66.1, "peak": 97.27, "min": 56.13}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 28.89, "energy_joules_est": 24.02, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T18:11:40.351852"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 937.976, "latencies_ms": [937.976], "images_per_second": 1.066, "prompt_tokens": 18, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The orange is predominantly orange in color. The lighting appears to be natural daylight, creating a contrast between the bright orange and the darker asphalt. The image captures the orange's texture and color, highlighting its natural appearance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.85, "peak": 35.44, "min": 22.06}, "VIN": {"avg": 64.9, "peak": 93.79, "min": 56.49}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 26.13, "sample_count": 7, "duration_seconds": 0.938}, "timestamp": "2026-01-17T18:11:41.295817"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 679.068, "latencies_ms": [679.068], "images_per_second": 1.473, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A man in a gray suit and tie is sitting at a wooden table, smiling at the camera while enjoying a meal and drinks.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 30.48, "peak": 35.04, "min": 25.21}, "VIN": {"avg": 62.19, "peak": 79.19, "min": 51.68}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.48, "energy_joules_est": 20.71, "sample_count": 5, "duration_seconds": 0.68}, "timestamp": "2026-01-17T18:11:41.990644"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1023.442, "latencies_ms": [1023.442], "images_per_second": 0.977, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "bowl: 1\nkeys: 2\nman: 1\ntable: 1\nbeer bottles: 2\nman: 1\ntie: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 29.37, "peak": 37.01, "min": 22.85}, "VIN": {"avg": 62.56, "peak": 73.45, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.37, "energy_joules_est": 30.07, "sample_count": 7, "duration_seconds": 1.024}, "timestamp": "2026-01-17T18:11:43.020888"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1159.893, "latencies_ms": [1159.893], "images_per_second": 0.862, "prompt_tokens": 25, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The main object is a man sitting at a table, positioned in the foreground. The table occupies the middle ground, separating him from the background, which includes a wall and possibly a window or doorway. The table also holds a bowl, bottles, and keys, indicating a casual setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.52, "peak": 35.04, "min": 21.66}, "VIN": {"avg": 61.43, "peak": 78.54, "min": 54.61}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 31.93, "sample_count": 8, "duration_seconds": 1.16}, "timestamp": "2026-01-17T18:11:44.186873"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1115.614, "latencies_ms": [1115.614], "images_per_second": 0.896, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "A man is sitting at a wooden table, wearing a gray suit and tie. He is smiling and appears to be enjoying a meal or snack. Beside him on the table are three empty beer bottles, suggesting he may be having drinks with friends or family.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.03, "peak": 34.65, "min": 21.27}, "VIN": {"avg": 61.58, "peak": 69.18, "min": 53.2}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.03, "energy_joules_est": 30.17, "sample_count": 8, "duration_seconds": 1.116}, "timestamp": "2026-01-17T18:11:45.309408"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1131.356, "latencies_ms": [1131.356], "images_per_second": 0.884, "prompt_tokens": 18, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The man is wearing a gray suit and white shirt. His suit appears to be made of a smooth, possibly synthetic material. The lighting in the image is soft and warm, creating a pleasant atmosphere. The table is light-colored wood, and there are two bottles of Corona beer visible.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.23, "peak": 34.65, "min": 21.27}, "VIN": {"avg": 64.12, "peak": 77.93, "min": 60.59}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 30.82, "sample_count": 8, "duration_seconds": 1.132}, "timestamp": "2026-01-17T18:11:46.446826"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 652.775, "latencies_ms": [652.775], "images_per_second": 1.532, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The neatly made bed with white sheets and pillows is accompanied by a desk with a chair, a window, and a suitcase on the floor.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.31, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.52, "peak": 83.96, "min": 59.97}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.31, "energy_joules_est": 19.15, "sample_count": 5, "duration_seconds": 0.653}, "timestamp": "2026-01-17T18:11:47.110918"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1108.8, "latencies_ms": [1108.8], "images_per_second": 0.902, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "bed: 2\npillows: 2\ntowels: 2\ndesk: 1\nchair: 1\nsuitcase: 1\nwindow: 1\nlight: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 27.72, "peak": 36.63, "min": 21.66}, "VIN": {"avg": 62.82, "peak": 84.65, "min": 53.59}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.72, "energy_joules_est": 30.75, "sample_count": 8, "duration_seconds": 1.109}, "timestamp": "2026-01-17T18:11:48.226193"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 844.576, "latencies_ms": [844.576], "images_per_second": 1.184, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The bed occupies the foreground, positioned between the desk and chair. The desk and chair are located in the background, near the window. The bed is situated further back in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 27.89, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 64.53, "peak": 85.45, "min": 55.55}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.89, "energy_joules_est": 23.57, "sample_count": 6, "duration_seconds": 0.845}, "timestamp": "2026-01-17T18:11:49.076761"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 676.882, "latencies_ms": [676.882], "images_per_second": 1.477, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The scene depicts a hotel room with two twin beds, a desk, and a chair. The room is clean and appears to be well-maintained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 29.86, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 65.95, "peak": 96.1, "min": 54.71}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 20.22, "sample_count": 5, "duration_seconds": 0.677}, "timestamp": "2026-01-17T18:11:49.759364"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1117.997, "latencies_ms": [1117.997], "images_per_second": 0.894, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The room features light beige walls and carpeting. The bed is covered with white linens and pillows. The lighting is soft and diffused, creating a calm and inviting atmosphere. A window provides natural light, contributing to the overall ambiance of the hotel room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.48, "peak": 36.24, "min": 21.28}, "VIN": {"avg": 63.05, "peak": 83.88, "min": 53.06}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.48, "energy_joules_est": 30.73, "sample_count": 8, "duration_seconds": 1.118}, "timestamp": "2026-01-17T18:11:50.884270"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 642.128, "latencies_ms": [642.128], "images_per_second": 1.557, "prompt_tokens": 8, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Three stuffed animals, including a teddy bear, a snowman, and a polar bear, are arranged on a blue blanket with red and orange designs in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.93, "peak": 26.4, "min": 22.06}, "VIN": {"avg": 58.86, "peak": 61.21, "min": 54.98}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.93, "energy_joules_est": 15.38, "sample_count": 4, "duration_seconds": 0.643}, "timestamp": "2026-01-17T18:11:51.537163"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1116.097, "latencies_ms": [1116.097], "images_per_second": 0.896, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "bear: 1\npolar bear: 1\nsnowman: 1\nhat: 1\nred heart: 1\ncoca cola: 1\nbomb: 1\ngreen fabric: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.3, "peak": 26.77, "min": 19.7}, "VIN": {"avg": 60.94, "peak": 63.37, "min": 54.74}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.3, "energy_joules_est": 24.9, "sample_count": 8, "duration_seconds": 1.117}, "timestamp": "2026-01-17T18:11:52.659604"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 778.82, "latencies_ms": [778.82], "images_per_second": 1.284, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The main objects are positioned close together in the foreground, with the Coca-Cola bear and snowman slightly behind and to the right of the bear. The background features an orange surface with a painted design.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25534.8, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 23.0, "peak": 25.6, "min": 20.87}, "VIN": {"avg": 58.79, "peak": 63.69, "min": 53.99}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 23.0, "energy_joules_est": 17.92, "sample_count": 5, "duration_seconds": 0.779}, "timestamp": "2026-01-17T18:11:53.444757"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1139.678, "latencies_ms": [1139.678], "images_per_second": 0.877, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 1, "output_text": "Three stuffed animals, including a teddy bear, a polar bear, and a snowman, are arranged on a blue blanket against a vibrant orange background. The teddy bear wears a green hat and has a Coca-Cola scarf. The snowman also has a Coca-Cola hat and scarf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.05, "peak": 26.39, "min": 19.69}, "VIN": {"avg": 60.68, "peak": 65.57, "min": 55.69}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.05, "energy_joules_est": 25.14, "sample_count": 8, "duration_seconds": 1.14}, "timestamp": "2026-01-17T18:11:54.590908"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 785.243, "latencies_ms": [785.243], "images_per_second": 1.273, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The teddy bears are light brown and white. The lighting is soft and warm, creating a cozy atmosphere. The teddy bears appear to be made of plush fabric. The background features a vibrant orange and red painting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.93, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 59.08, "peak": 62.91, "min": 53.89}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.93, "energy_joules_est": 18.02, "sample_count": 5, "duration_seconds": 0.786}, "timestamp": "2026-01-17T18:11:55.383209"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 581.402, "latencies_ms": [581.402], "images_per_second": 1.72, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A clear glass bowl filled with bright, fresh oranges sits on a textured silver tablecloth.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 31.41, "peak": 33.88, "min": 27.56}, "VIN": {"avg": 61.3, "peak": 71.14, "min": 56.29}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.41, "energy_joules_est": 18.28, "sample_count": 4, "duration_seconds": 0.582}, "timestamp": "2026-01-17T18:11:55.978817"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1305.934, "latencies_ms": [1305.934], "images_per_second": 0.766, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Orange: 8\nOrange: 8\nOrange: 8\nOrange: 8\nOrange: 8\nOrange: 8\nOrange: 8\nOrange: 8\nOrange: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.78, "peak": 37.8, "min": 20.88}, "VIN": {"avg": 62.78, "peak": 80.84, "min": 56.05}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.78, "energy_joules_est": 36.29, "sample_count": 9, "duration_seconds": 1.306}, "timestamp": "2026-01-17T18:11:57.291061"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 828.905, "latencies_ms": [828.905], "images_per_second": 1.206, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The main objects are positioned relatively close to the viewer, creating a sense of proximity and closeness. The bowl is placed in the foreground, while the oranges are situated in the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.82, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 63.07, "peak": 80.33, "min": 54.43}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 23.9, "sample_count": 6, "duration_seconds": 0.829}, "timestamp": "2026-01-17T18:11:58.126965"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 853.339, "latencies_ms": [853.339], "images_per_second": 1.172, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A clear glass bowl filled with bright oranges sits on a textured silver surface, possibly a tablecloth. The oranges are arranged neatly, showcasing their vibrant color and healthy appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.74, "peak": 36.23, "min": 23.64}, "VIN": {"avg": 63.03, "peak": 76.43, "min": 56.09}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.74, "energy_joules_est": 25.39, "sample_count": 6, "duration_seconds": 0.854}, "timestamp": "2026-01-17T18:11:58.986127"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1208.794, "latencies_ms": [1208.794], "images_per_second": 0.827, "prompt_tokens": 18, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The oranges are bright orange and appear to be ripe. The lighting is soft and diffused, creating a gentle glow on the oranges. The bowl is made of clear glass and has a dark reddish-brown rim. The surface the bowl rests on appears to be textured, possibly a tablecloth or placemat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 27.14, "peak": 35.85, "min": 20.88}, "VIN": {"avg": 65.22, "peak": 89.36, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.14, "energy_joules_est": 32.81, "sample_count": 9, "duration_seconds": 1.209}, "timestamp": "2026-01-17T18:12:00.202142"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 514.163, "latencies_ms": [514.163], "images_per_second": 1.945, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A surfer skillfully rides a large, powerful wave in the ocean, showcasing their expertise and balance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 24.54, "peak": 26.38, "min": 22.84}, "VIN": {"avg": 60.91, "peak": 62.63, "min": 59.26}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.54, "energy_joules_est": 12.63, "sample_count": 3, "duration_seconds": 0.515}, "timestamp": "2026-01-17T18:12:00.725772"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 777.052, "latencies_ms": [777.052], "images_per_second": 1.287, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "wave: 1\nsurfer: 1\nsurfboard: 1\nwater: 1\nsky: 1\nocean: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25534.7, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.71, "peak": 27.17, "min": 21.27}, "VIN": {"avg": 60.67, "peak": 63.3, "min": 55.95}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.71, "energy_joules_est": 18.44, "sample_count": 5, "duration_seconds": 0.778}, "timestamp": "2026-01-17T18:12:01.513287"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 866.979, "latencies_ms": [866.979], "images_per_second": 1.153, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The main object is a large wave curling over, creating a dramatic foreground scene. The background features the ocean horizon, emphasizing the vastness of the scene. The wave is positioned close to the viewer, drawing attention to its size and power.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.7, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25534.7, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 22.98, "peak": 26.39, "min": 20.48}, "VIN": {"avg": 60.65, "peak": 62.75, "min": 59.52}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 22.98, "energy_joules_est": 19.93, "sample_count": 6, "duration_seconds": 0.867}, "timestamp": "2026-01-17T18:12:02.385960"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 560.28, "latencies_ms": [560.28], "images_per_second": 1.785, "prompt_tokens": 19, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A surfer is riding a large, powerful wave in the ocean. The sky is overcast, creating a dramatic backdrop for the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25535.1, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 26.0, "min": 21.66}, "VIN": {"avg": 58.69, "peak": 61.0, "min": 55.71}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 13.25, "sample_count": 4, "duration_seconds": 0.561}, "timestamp": "2026-01-17T18:12:02.952821"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1010.192, "latencies_ms": [1010.192], "images_per_second": 0.99, "prompt_tokens": 18, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The ocean is a deep blue color, creating a striking contrast with the white foam of the waves. The lighting suggests an overcast sky, casting a soft, diffused light over the scene. The waves appear to be powerful and dynamic, captured in a moment of action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.79, "peak": 27.17, "min": 20.1}, "VIN": {"avg": 59.45, "peak": 62.7, "min": 55.38}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.79, "energy_joules_est": 23.03, "sample_count": 7, "duration_seconds": 1.01}, "timestamp": "2026-01-17T18:12:03.968902"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 615.478, "latencies_ms": [615.478], "images_per_second": 1.625, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A tabby and white cat is sitting in front of a laptop, attentively looking at the Twitter page displayed on the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 29.75, "peak": 33.09, "min": 26.01}, "VIN": {"avg": 63.03, "peak": 76.06, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 29.75, "energy_joules_est": 18.32, "sample_count": 4, "duration_seconds": 0.616}, "timestamp": "2026-01-17T18:12:04.598759"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1193.398, "latencies_ms": [1193.398], "images_per_second": 0.838, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "laptop: 1\ncat: 1\nkeyboard: 1\nlaptop screen: 1\nTwitter logo: 1\ntext: 1\ncouch: 1\npillow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 26.74, "peak": 36.24, "min": 20.89}, "VIN": {"avg": 62.84, "peak": 75.07, "min": 57.13}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.74, "energy_joules_est": 31.93, "sample_count": 9, "duration_seconds": 1.194}, "timestamp": "2026-01-17T18:12:05.798448"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 597.446, "latencies_ms": [597.446], "images_per_second": 1.674, "prompt_tokens": 25, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The cat is positioned in the foreground, close to the laptop. The laptop is situated in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.61, "min": 14.2}, "VDD_GPU": {"avg": 30.53, "peak": 33.88, "min": 26.39}, "VIN": {"avg": 65.39, "peak": 82.63, "min": 56.7}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.53, "energy_joules_est": 18.25, "sample_count": 4, "duration_seconds": 0.598}, "timestamp": "2026-01-17T18:12:06.401643"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 927.192, "latencies_ms": [927.192], "images_per_second": 1.079, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "A tabby and white cat is sitting on a couch, attentively looking at a laptop screen displaying the Twitter website. The laptop is positioned on a surface, possibly a table or desk, in a casual indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.15, "peak": 37.42, "min": 22.85}, "VIN": {"avg": 62.39, "peak": 77.12, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 29.15, "energy_joules_est": 27.05, "sample_count": 7, "duration_seconds": 0.928}, "timestamp": "2026-01-17T18:12:07.335537"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 932.982, "latencies_ms": [932.982], "images_per_second": 1.072, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The cat is primarily white with brown and gray stripes. The lighting in the image is soft and diffused, suggesting an indoor setting with natural light. The cat appears to be comfortably perched on the laptop keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 64.11, "peak": 83.91, "min": 57.56}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 25.95, "sample_count": 7, "duration_seconds": 0.933}, "timestamp": "2026-01-17T18:12:08.274208"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 493.99, "latencies_ms": [493.99], "images_per_second": 2.024, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A group of six horses, including two brown ones and three black ones, are gathered around a hay feeder in a field.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 25.34, "peak": 27.18, "min": 23.64}, "VIN": {"avg": 58.94, "peak": 61.05, "min": 57.56}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.34, "energy_joules_est": 12.54, "sample_count": 3, "duration_seconds": 0.495}, "timestamp": "2026-01-17T18:12:08.781168"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 736.944, "latencies_ms": [736.944], "images_per_second": 1.357, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "horse: 5\nhay: 2\nfence: 1\nground: 2\ntrees: 2\nhouse: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 24.1, "peak": 27.56, "min": 21.66}, "VIN": {"avg": 61.03, "peak": 64.52, "min": 59.28}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.1, "energy_joules_est": 17.77, "sample_count": 5, "duration_seconds": 0.737}, "timestamp": "2026-01-17T18:12:09.524440"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 772.172, "latencies_ms": [772.172], "images_per_second": 1.295, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main objects are positioned in a relatively open space, with a young horse on the left and a group of horses in the background. The horses are grouped together near the hay pile, suggesting they are in a feeding area or stable.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.64, "peak": 26.79, "min": 21.28}, "VIN": {"avg": 58.27, "peak": 61.72, "min": 54.65}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.64, "energy_joules_est": 18.26, "sample_count": 5, "duration_seconds": 0.772}, "timestamp": "2026-01-17T18:12:10.302470"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 694.741, "latencies_ms": [694.741], "images_per_second": 1.439, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "A group of horses, including adults and younger ones, are gathered in a field, eating hay from a trough. The setting appears to be a rural area with a house and trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.4, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 61.06, "peak": 63.65, "min": 57.22}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.4, "energy_joules_est": 16.27, "sample_count": 5, "duration_seconds": 0.695}, "timestamp": "2026-01-17T18:12:11.002778"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 747.02, "latencies_ms": [747.02], "images_per_second": 1.339, "prompt_tokens": 18, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The horses are primarily dark brown or black. The lighting appears to be natural daylight, creating a bright and open atmosphere. The scene suggests a rural setting with a mix of grass and hay.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25534.4, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.39, "peak": 26.38, "min": 21.26}, "VIN": {"avg": 60.88, "peak": 64.4, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.39, "energy_joules_est": 17.48, "sample_count": 5, "duration_seconds": 0.747}, "timestamp": "2026-01-17T18:12:11.755711"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 735.145, "latencies_ms": [735.145], "images_per_second": 1.36, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A man wearing a black wetsuit skillfully maneuvers a yellow surfboard, crouching low and carving through the waves in the ocean.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 28.89, "peak": 33.46, "min": 24.41}, "VIN": {"avg": 67.94, "peak": 95.78, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.97, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 21.26, "sample_count": 5, "duration_seconds": 0.736}, "timestamp": "2026-01-17T18:12:12.502061"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 969.844, "latencies_ms": [969.844], "images_per_second": 1.031, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "surfboard: 1\nperson: 1\nwater: 1\nwaves: 2\ncliff: 1\nsky: 1\nshore: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.13, "peak": 36.23, "min": 22.06}, "VIN": {"avg": 65.27, "peak": 92.78, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.13, "energy_joules_est": 27.29, "sample_count": 7, "duration_seconds": 0.97}, "timestamp": "2026-01-17T18:12:13.477938"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 712.786, "latencies_ms": [712.786], "images_per_second": 1.403, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The surfer is positioned in the foreground of the image, riding a wave near the shore. The ocean extends in the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 29.7, "peak": 35.45, "min": 24.43}, "VIN": {"avg": 67.17, "peak": 96.58, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.7, "energy_joules_est": 21.18, "sample_count": 5, "duration_seconds": 0.713}, "timestamp": "2026-01-17T18:12:14.196648"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1043.767, "latencies_ms": [1043.767], "images_per_second": 0.958, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "A man is surfing on a wave in the ocean, wearing a black wetsuit. He is riding a yellow surfboard through the water, creating a splash as he moves forward. The background features a sandy beach and a cliff, contributing to the coastal setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.22, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 62.61, "peak": 90.96, "min": 54.25}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.22, "energy_joules_est": 28.42, "sample_count": 8, "duration_seconds": 1.044}, "timestamp": "2026-01-17T18:12:15.246765"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 707.649, "latencies_ms": [707.649], "images_per_second": 1.413, "prompt_tokens": 18, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The surfer is wearing a dark-colored wetsuit. The water appears a greenish-blue, and the sky is partly cloudy.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25534.9, "ram_available_mb": 100237.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.79}, "VDD_GPU": {"avg": 29.14, "peak": 33.86, "min": 24.41}, "VIN": {"avg": 66.93, "peak": 99.3, "min": 55.65}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.14, "energy_joules_est": 20.63, "sample_count": 5, "duration_seconds": 0.708}, "timestamp": "2026-01-17T18:12:15.960715"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 656.872, "latencies_ms": [656.872], "images_per_second": 1.522, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "Three uniquely carved pumpkins are displayed in a hallway, accompanied by a vase of pink flowers and a small figurine.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 31.3, "peak": 35.42, "min": 26.77}, "VIN": {"avg": 69.53, "peak": 97.36, "min": 57.02}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.3, "energy_joules_est": 20.57, "sample_count": 4, "duration_seconds": 0.657}, "timestamp": "2026-01-17T18:12:16.629644"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1225.885, "latencies_ms": [1225.885], "images_per_second": 0.816, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 1, "output_text": "pumpkin: 3\nflowers: 2\nperson: 1\ncarved pumpkin: 1\nglass vase: 1\npikachu: 1\nblackboard: 1\npictures: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.11, "min": 13.79}, "VDD_GPU": {"avg": 26.91, "peak": 36.6, "min": 20.88}, "VIN": {"avg": 64.94, "peak": 94.22, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.91, "energy_joules_est": 33.0, "sample_count": 9, "duration_seconds": 1.226}, "timestamp": "2026-01-17T18:12:17.861379"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 873.863, "latencies_ms": [873.863], "images_per_second": 1.144, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The pumpkins are positioned close together, creating a sense of proximity and visual balance. The flowers are placed in the foreground, slightly in front of the pumpkins, drawing the viewer's attention to the centerpiece.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.9, "peak": 33.88, "min": 22.85}, "VIN": {"avg": 64.2, "peak": 84.15, "min": 55.4}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.9, "energy_joules_est": 24.39, "sample_count": 6, "duration_seconds": 0.874}, "timestamp": "2026-01-17T18:12:18.741193"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1182.516, "latencies_ms": [1182.516], "images_per_second": 0.846, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The scene depicts a Halloween-themed display featuring three carved pumpkins and a vase of pink flowers. A small figurine of a man is perched on one of the pumpkins. The pumpkins are decorated for various Halloween themes, including Jack-o'-lanterns and smiling faces.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.12, "peak": 35.44, "min": 20.48}, "VIN": {"avg": 62.61, "peak": 75.26, "min": 55.85}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.12, "energy_joules_est": 30.9, "sample_count": 9, "duration_seconds": 1.183}, "timestamp": "2026-01-17T18:12:19.930705"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 852.082, "latencies_ms": [852.082], "images_per_second": 1.174, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The pumpkins are orange and feature painted faces. The lighting appears to be artificial, creating a warm glow. The materials appear to be carved pumpkins and glass, suggesting they are crafted from natural materials.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 27.82, "peak": 33.85, "min": 22.84}, "VIN": {"avg": 64.39, "peak": 77.78, "min": 56.57}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.82, "energy_joules_est": 23.72, "sample_count": 6, "duration_seconds": 0.853}, "timestamp": "2026-01-17T18:12:20.788761"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 479.637, "latencies_ms": [479.637], "images_per_second": 2.085, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A white sink with a faucet is situated next to a black trash bag on the floor in a bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 25.46, "peak": 27.56, "min": 23.62}, "VIN": {"avg": 60.32, "peak": 62.66, "min": 57.54}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 25.46, "energy_joules_est": 12.22, "sample_count": 3, "duration_seconds": 0.48}, "timestamp": "2026-01-17T18:12:21.282167"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1089.318, "latencies_ms": [1089.318], "images_per_second": 0.918, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "sink: 1\nmirror: 1\ntoiletries: 2\nshelf: 1\ndoor: 1\ntrash bag: 1\npipes: 1\nfloor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.39}, "VDD_GPU": {"avg": 22.9, "peak": 27.97, "min": 20.09}, "VIN": {"avg": 58.7, "peak": 62.19, "min": 54.17}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.9, "energy_joules_est": 24.95, "sample_count": 8, "duration_seconds": 1.09}, "timestamp": "2026-01-17T18:12:22.378444"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 636.996, "latencies_ms": [636.996], "images_per_second": 1.57, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The sink is positioned to the left of the mirror and further back in the room. The door is situated to the right of the sink and further back in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.63, "peak": 26.0, "min": 21.66}, "VIN": {"avg": 60.72, "peak": 65.07, "min": 57.3}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 15.07, "sample_count": 4, "duration_seconds": 0.638}, "timestamp": "2026-01-17T18:12:23.025743"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 695.788, "latencies_ms": [695.788], "images_per_second": 1.437, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The bathroom is dimly lit and appears somewhat neglected or unkempt. A white sink, a mirror, and a door are visible, along with a black trash bag on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 23.4, "peak": 26.39, "min": 21.27}, "VIN": {"avg": 59.75, "peak": 62.73, "min": 53.67}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.4, "energy_joules_est": 16.29, "sample_count": 5, "duration_seconds": 0.696}, "timestamp": "2026-01-17T18:12:23.727103"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 773.262, "latencies_ms": [773.262], "images_per_second": 1.293, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The walls are painted a pale yellow. The lighting in the room is soft and warm, creating a calm atmosphere. The bathroom appears to be unfinished, with visible dirt and stains on the walls.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.5, "ram_available_mb": 100236.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 23.63, "peak": 26.79, "min": 21.26}, "VIN": {"avg": 61.95, "peak": 62.52, "min": 60.7}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.63, "energy_joules_est": 18.28, "sample_count": 5, "duration_seconds": 0.774}, "timestamp": "2026-01-17T18:12:24.510677"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 513.18, "latencies_ms": [513.18], "images_per_second": 1.949, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A young child sits on a bed, focused intently on a laptop screen in front of them.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25535.5, "ram_available_mb": 100236.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25535.5, "ram_available_mb": 100236.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.4, "min": 14.1}, "VDD_GPU": {"avg": 31.77, "peak": 34.27, "min": 28.74}, "VIN": {"avg": 72.17, "peak": 96.04, "min": 54.77}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.77, "energy_joules_est": 16.32, "sample_count": 3, "duration_seconds": 0.514}, "timestamp": "2026-01-17T18:12:25.037538"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1126.99, "latencies_ms": [1126.99], "images_per_second": 0.887, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "laptop: 1\nbed: 1\ntoddler: 1\nchild: 1\nhair clip: 1\ntoys: 1\nwindow: 1\nwall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.5, "ram_available_mb": 100236.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 28.01, "peak": 37.8, "min": 21.66}, "VIN": {"avg": 62.91, "peak": 83.72, "min": 55.6}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 28.01, "energy_joules_est": 31.58, "sample_count": 8, "duration_seconds": 1.127}, "timestamp": "2026-01-17T18:12:26.171406"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 709.448, "latencies_ms": [709.448], "images_per_second": 1.41, "prompt_tokens": 25, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The baby is positioned near the laptop, seemingly observing its screen. The laptop is placed on a surface that extends into the background, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.47, "peak": 34.26, "min": 24.83}, "VIN": {"avg": 62.7, "peak": 81.56, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.47, "energy_joules_est": 20.92, "sample_count": 5, "duration_seconds": 0.71}, "timestamp": "2026-01-17T18:12:26.887029"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 575.83, "latencies_ms": [575.83], "images_per_second": 1.737, "prompt_tokens": 19, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A young child is sitting on a bed, focused on a laptop screen. The setting appears to be a bedroom or home environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.8, "peak": 36.23, "min": 27.18}, "VIN": {"avg": 68.94, "peak": 98.56, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.8, "energy_joules_est": 18.32, "sample_count": 4, "duration_seconds": 0.576}, "timestamp": "2026-01-17T18:12:27.468735"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 689.867, "latencies_ms": [689.867], "images_per_second": 1.45, "prompt_tokens": 18, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The baby is wearing a white shirt. The laptop is silver. The background is plain white. The lighting is soft and diffused.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 31.26, "peak": 37.78, "min": 25.21}, "VIN": {"avg": 64.8, "peak": 84.73, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.26, "energy_joules_est": 21.58, "sample_count": 5, "duration_seconds": 0.69}, "timestamp": "2026-01-17T18:12:28.164759"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 552.482, "latencies_ms": [552.482], "images_per_second": 1.81, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A skier in a brown jacket and black pants is carving down a snowy slope, leaving a trail of snow behind him.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 25.69, "peak": 29.14, "min": 22.84}, "VIN": {"avg": 61.11, "peak": 65.54, "min": 56.88}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.69, "energy_joules_est": 14.2, "sample_count": 4, "duration_seconds": 0.553}, "timestamp": "2026-01-17T18:12:28.732149"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1202.228, "latencies_ms": [1202.228], "images_per_second": 0.832, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "skier: 1\npulled_ski: 1\ngloves: 2\ngloves: 2\ngloves: 2\nskis: 2\nsnow: 2\ntree: 2\nsnow: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25535.5, "ram_available_mb": 100236.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.69}, "VDD_GPU": {"avg": 22.54, "peak": 27.96, "min": 19.7}, "VIN": {"avg": 59.36, "peak": 62.13, "min": 51.61}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.54, "energy_joules_est": 27.11, "sample_count": 9, "duration_seconds": 1.203}, "timestamp": "2026-01-17T18:12:29.941539"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 615.83, "latencies_ms": [615.83], "images_per_second": 1.624, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The skier is positioned in the foreground, moving towards the left side of the image. The snowy slope and trees in the background create a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25535.5, "ram_available_mb": 100236.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.44, "peak": 25.6, "min": 21.67}, "VIN": {"avg": 60.32, "peak": 62.39, "min": 56.36}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.44, "energy_joules_est": 14.44, "sample_count": 4, "duration_seconds": 0.616}, "timestamp": "2026-01-17T18:12:30.563643"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 594.919, "latencies_ms": [594.919], "images_per_second": 1.681, "prompt_tokens": 19, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A skier is carving down a snowy slope, leaning forward with poles in hand, amidst a backdrop of snow-covered trees.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 24.22, "peak": 26.79, "min": 22.06}, "VIN": {"avg": 56.51, "peak": 59.83, "min": 53.42}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.22, "energy_joules_est": 14.42, "sample_count": 4, "duration_seconds": 0.595}, "timestamp": "2026-01-17T18:12:31.164263"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 829.899, "latencies_ms": [829.899], "images_per_second": 1.205, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The skier is wearing a brown jacket and dark pants. The lighting is bright and sunny, creating a contrast against the white snow. The skier is skiing down a snow-covered slope with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.8, "ram_available_mb": 100236.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25534.5, "ram_available_mb": 100237.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 23.31, "peak": 27.18, "min": 20.88}, "VIN": {"avg": 59.55, "peak": 62.58, "min": 56.03}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.31, "energy_joules_est": 19.36, "sample_count": 6, "duration_seconds": 0.83}, "timestamp": "2026-01-17T18:12:32.000549"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 512.953, "latencies_ms": [512.953], "images_per_second": 1.949, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The hotel room features a king-sized bed with white linens, a wooden headboard, and a blue armchair.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25534.5, "ram_available_mb": 100237.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25534.5, "ram_available_mb": 100237.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 24.55, "peak": 26.39, "min": 22.85}, "VIN": {"avg": 61.96, "peak": 63.1, "min": 60.37}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 24.55, "energy_joules_est": 12.61, "sample_count": 3, "duration_seconds": 0.514}, "timestamp": "2026-01-17T18:12:32.523089"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1064.107, "latencies_ms": [1064.107], "images_per_second": 0.94, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "bed: 1\npillows: 4\nlamp: 2\narmchair: 1\nottoman: 1\nsuitcase: 1\nwindow: 1\ncurtains: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.5, "ram_available_mb": 100237.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25534.0, "ram_available_mb": 100238.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.59}, "VDD_GPU": {"avg": 22.7, "peak": 27.57, "min": 20.09}, "VIN": {"avg": 60.48, "peak": 63.3, "min": 56.91}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 22.7, "energy_joules_est": 24.16, "sample_count": 8, "duration_seconds": 1.064}, "timestamp": "2026-01-17T18:12:33.592771"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 872.461, "latencies_ms": [872.461], "images_per_second": 1.146, "prompt_tokens": 25, "response_tokens_est": 51, "n_tiles": 1, "output_text": "The main objects are positioned in a balanced and harmonious manner, with the bed occupying the foreground and the chair and suitcase placed near the left side of the image. The window and lamp are situated in the background, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.0, "ram_available_mb": 100238.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.58, "peak": 26.0, "min": 20.48}, "VIN": {"avg": 60.88, "peak": 62.01, "min": 59.47}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.58, "energy_joules_est": 19.71, "sample_count": 6, "duration_seconds": 0.873}, "timestamp": "2026-01-17T18:12:34.471277"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 705.219, "latencies_ms": [705.219], "images_per_second": 1.418, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The scene depicts a hotel room with a large bed, a blue armchair, and a window providing natural light. The room is clean and organized, with a lamp providing soft lighting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.08, "peak": 25.99, "min": 20.88}, "VIN": {"avg": 60.27, "peak": 60.84, "min": 59.45}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.08, "energy_joules_est": 16.29, "sample_count": 5, "duration_seconds": 0.706}, "timestamp": "2026-01-17T18:12:35.185121"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 840.289, "latencies_ms": [840.289], "images_per_second": 1.19, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The room features a light-colored bed with white bedding and pillows. The walls are painted in a light beige color, and the carpet is a neutral tone. A window with sheer curtains allows natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 22.98, "peak": 26.4, "min": 20.48}, "VIN": {"avg": 60.72, "peak": 62.5, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 22.98, "energy_joules_est": 19.32, "sample_count": 6, "duration_seconds": 0.841}, "timestamp": "2026-01-17T18:12:36.031561"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 769.798, "latencies_ms": [769.798], "images_per_second": 1.299, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A skier in vibrant orange pants and a white jacket skillfully maneuvers a red rail on a snowy slope, surrounded by other winter sports enthusiasts.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25534.7, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.52, "peak": 33.48, "min": 24.03}, "VIN": {"avg": 67.65, "peak": 89.39, "min": 59.69}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.52, "energy_joules_est": 21.97, "sample_count": 5, "duration_seconds": 0.77}, "timestamp": "2026-01-17T18:12:36.812689"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1228.928, "latencies_ms": [1228.928], "images_per_second": 0.814, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "red pipe: 1\nskier: 1\nsnowboarder: 1\nfence: 1\nski lift: 1\nsnow: 1\nskiers: 2\nclear sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.7, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.04, "peak": 35.04, "min": 20.48}, "VIN": {"avg": 63.94, "peak": 78.99, "min": 59.98}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.04, "energy_joules_est": 32.01, "sample_count": 9, "duration_seconds": 1.229}, "timestamp": "2026-01-17T18:12:38.047557"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 817.102, "latencies_ms": [817.102], "images_per_second": 1.224, "prompt_tokens": 25, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The red object dominates the foreground, positioned to the left of the main subject. The background features more snow skiers and ski lifts, indicating a ski resort environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25535.0, "ram_available_mb": 100237.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25534.7, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.83, "peak": 33.86, "min": 22.85}, "VIN": {"avg": 63.44, "peak": 74.59, "min": 59.73}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.83, "energy_joules_est": 22.75, "sample_count": 6, "duration_seconds": 0.817}, "timestamp": "2026-01-17T18:12:38.875576"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1031.363, "latencies_ms": [1031.363], "images_per_second": 0.97, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene depicts a snowy slope with several skiers and snowboarders enjoying winter sports. A prominent feature is a red, curved rail or ramp that serves as a challenging obstacle for participants to navigate and perform tricks.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25534.7, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25534.7, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.69, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 64.35, "peak": 83.25, "min": 55.2}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.69, "energy_joules_est": 28.57, "sample_count": 7, "duration_seconds": 1.032}, "timestamp": "2026-01-17T18:12:39.913579"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 809.506, "latencies_ms": [809.506], "images_per_second": 1.235, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The snow is bright white, indicative of clear, sunny weather. The red metal feature stands out against the snowy landscape, adding a pop of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.7, "ram_available_mb": 100237.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.49, "peak": 34.26, "min": 23.24}, "VIN": {"avg": 65.21, "peak": 86.54, "min": 57.28}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 23.08, "sample_count": 6, "duration_seconds": 0.81}, "timestamp": "2026-01-17T18:12:40.731635"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 589.653, "latencies_ms": [589.653], "images_per_second": 1.696, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A parking meter with graffiti on its side is mounted on a metal pole against a graffiti-covered wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 30.81, "peak": 35.04, "min": 26.39}, "VIN": {"avg": 68.0, "peak": 85.24, "min": 61.94}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.81, "energy_joules_est": 18.18, "sample_count": 4, "duration_seconds": 0.59}, "timestamp": "2026-01-17T18:12:41.334779"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1082.389, "latencies_ms": [1082.389], "images_per_second": 0.924, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "Parking meter: 2\nGraffiti: 5\nMetal fence: 1\nStreet sign: 1\nPipe: 1\nBuilding: 1\nGround: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.91, "peak": 37.01, "min": 21.66}, "VIN": {"avg": 62.8, "peak": 80.77, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 27.91, "energy_joules_est": 30.22, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T18:12:42.423500"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 672.566, "latencies_ms": [672.566], "images_per_second": 1.487, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The parking meter is positioned in the foreground, slightly to the right of the graffiti wall. The graffiti wall extends into the background, creating a layered effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.42, "peak": 34.26, "min": 26.39}, "VIN": {"avg": 68.81, "peak": 93.47, "min": 57.06}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.42, "energy_joules_est": 20.47, "sample_count": 4, "duration_seconds": 0.673}, "timestamp": "2026-01-17T18:12:43.102978"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 945.715, "latencies_ms": [945.715], "images_per_second": 1.057, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The scene depicts a vibrant urban street corner with a parking meter covered in graffiti. The graffiti is predominantly in black, white, blue, and yellow, covering the side of the building and adding to the overall artistic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.2, "ram_available_mb": 100237.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.02, "peak": 35.85, "min": 22.06}, "VIN": {"avg": 65.29, "peak": 84.81, "min": 60.54}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.02, "energy_joules_est": 26.52, "sample_count": 7, "duration_seconds": 0.946}, "timestamp": "2026-01-17T18:12:44.055126"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1051.436, "latencies_ms": [1051.436], "images_per_second": 0.951, "prompt_tokens": 18, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The building's exterior is covered in vibrant graffiti in shades of black, blue, yellow, and white. The lighting appears to be natural daylight, creating a contrast against the dark wall. The materials used are likely metal and paint, giving the structure an industrial feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.74, "peak": 34.66, "min": 21.27}, "VIN": {"avg": 63.08, "peak": 87.16, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.74, "energy_joules_est": 28.13, "sample_count": 8, "duration_seconds": 1.052}, "timestamp": "2026-01-17T18:12:45.113102"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 711.333, "latencies_ms": [711.333], "images_per_second": 1.406, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A surfer in a black wetsuit rides a wave on a white surfboard, skillfully carving through the deep blue ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25535.0, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.67, "peak": 33.88, "min": 24.03}, "VIN": {"avg": 62.55, "peak": 76.98, "min": 57.01}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 28.67, "energy_joules_est": 20.41, "sample_count": 5, "duration_seconds": 0.712}, "timestamp": "2026-01-17T18:12:45.835622"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 845.056, "latencies_ms": [845.056], "images_per_second": 1.183, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "person: 1\nsurfboard: 1\nwave: 1\nwater: 1\nsky: 1\nocean: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25535.0, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.0, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.09, "peak": 36.63, "min": 23.25}, "VIN": {"avg": 66.28, "peak": 95.29, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.09, "energy_joules_est": 24.6, "sample_count": 6, "duration_seconds": 0.846}, "timestamp": "2026-01-17T18:12:46.687230"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 848.291, "latencies_ms": [848.291], "images_per_second": 1.179, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The surfer is positioned near the center of the image, riding a wave that extends towards the right side of the frame. The ocean occupies the majority of the background, creating a sense of depth and vastness.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.0, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.82, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 66.56, "peak": 101.15, "min": 55.77}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 24.46, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T18:12:47.542740"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 854.291, "latencies_ms": [854.291], "images_per_second": 1.171, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A surfer is riding a wave in the deep blue ocean. The scene is captured from a distance, emphasizing the vastness of the ocean and the surfer's skillful maneuver.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.62, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 65.66, "peak": 96.01, "min": 55.51}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.62, "energy_joules_est": 24.46, "sample_count": 6, "duration_seconds": 0.855}, "timestamp": "2026-01-17T18:12:48.403055"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 866.87, "latencies_ms": [866.87], "images_per_second": 1.154, "prompt_tokens": 18, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The ocean is a deep blue color, creating a striking contrast with the white foam of the waves. The lighting appears to be soft and diffused, possibly suggesting an overcast sky or twilight conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.88, "peak": 35.45, "min": 23.23}, "VIN": {"avg": 63.39, "peak": 74.68, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 25.04, "sample_count": 6, "duration_seconds": 0.867}, "timestamp": "2026-01-17T18:12:49.275968"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 547.169, "latencies_ms": [547.169], "images_per_second": 1.828, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A yellow double-decker bus is stopped at a bus stop, picking up passengers.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25535.5, "ram_available_mb": 100236.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25535.5, "ram_available_mb": 100236.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 31.31, "peak": 35.44, "min": 26.79}, "VIN": {"avg": 64.03, "peak": 80.52, "min": 54.94}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.31, "energy_joules_est": 17.15, "sample_count": 4, "duration_seconds": 0.548}, "timestamp": "2026-01-17T18:12:49.835391"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 949.385, "latencies_ms": [949.385], "images_per_second": 1.053, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "bus: 2\nstreet: 1\nbuildings: 2\nbus stop: 1\nman: 2\nwoman: 1\nflowers: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.5, "ram_available_mb": 100236.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.0, "ram_available_mb": 100236.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 28.97, "peak": 37.82, "min": 22.45}, "VIN": {"avg": 64.36, "peak": 94.61, "min": 54.61}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.97, "energy_joules_est": 27.51, "sample_count": 7, "duration_seconds": 0.95}, "timestamp": "2026-01-17T18:12:50.790807"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1185.025, "latencies_ms": [1185.025], "images_per_second": 0.844, "prompt_tokens": 25, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The double-decker bus is positioned in the foreground, moving towards the viewer. The bus is situated near a bus stop and appears to be at a bus stop near a building. The foreground features the bus and the bus stop, while the background includes other elements like a road, buildings, and greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.0, "ram_available_mb": 100236.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.3, "ram_available_mb": 100235.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.08, "peak": 35.04, "min": 20.48}, "VIN": {"avg": 63.3, "peak": 83.89, "min": 53.45}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.08, "energy_joules_est": 30.93, "sample_count": 9, "duration_seconds": 1.186}, "timestamp": "2026-01-17T18:12:51.982527"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1121.955, "latencies_ms": [1121.955], "images_per_second": 0.891, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 1, "output_text": "A yellow double-decker bus is stopped at a bus stop, picking up passengers. The bus is labeled \"Lytham\" and \"St Annes,\" indicating its route.  Two people are waiting at the bus stop, one near the front and the other near the back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.3, "ram_available_mb": 100235.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.2, "ram_available_mb": 100235.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.53, "peak": 33.86, "min": 21.27}, "VIN": {"avg": 64.49, "peak": 87.13, "min": 56.37}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.53, "energy_joules_est": 29.77, "sample_count": 8, "duration_seconds": 1.122}, "timestamp": "2026-01-17T18:12:53.111851"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 950.371, "latencies_ms": [950.371], "images_per_second": 1.052, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The bus is primarily yellow and black. The lighting appears to be overcast, creating a somewhat muted atmosphere. The bus's materials appear to be standard, durable construction. The weather appears to be overcast, contributing to the subdued lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.2, "ram_available_mb": 100235.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.35, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 64.26, "peak": 83.86, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.35, "energy_joules_est": 26.0, "sample_count": 7, "duration_seconds": 0.951}, "timestamp": "2026-01-17T18:12:54.068100"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 785.943, "latencies_ms": [785.943], "images_per_second": 1.272, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A yellow and red single-engine propeller airplane with the registration SP-AWF is captured in mid-flight, moving from left to right with a slight downward tilt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 29.3, "peak": 34.66, "min": 24.41}, "VIN": {"avg": 67.57, "peak": 87.0, "min": 61.12}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 23.04, "sample_count": 5, "duration_seconds": 0.786}, "timestamp": "2026-01-17T18:12:54.867508"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1001.758, "latencies_ms": [1001.758], "images_per_second": 0.998, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "airplane: 1\npropeller: 1\nwings: 2\ntail: 1\nbody: 1\nwheel: 2\ntext: SP-AWF", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.19, "peak": 35.42, "min": 22.45}, "VIN": {"avg": 64.21, "peak": 90.94, "min": 53.93}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.19, "energy_joules_est": 28.25, "sample_count": 7, "duration_seconds": 1.002}, "timestamp": "2026-01-17T18:12:55.879089"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 807.657, "latencies_ms": [807.657], "images_per_second": 1.238, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The main object is a yellow and red propeller plane flying in the sky. The plane is positioned in the foreground, slightly to the right of the viewer. The background is primarily a light gray sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 28.43, "peak": 34.66, "min": 23.24}, "VIN": {"avg": 65.17, "peak": 84.61, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.43, "energy_joules_est": 22.98, "sample_count": 6, "duration_seconds": 0.808}, "timestamp": "2026-01-17T18:12:56.693586"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 805.623, "latencies_ms": [805.623], "images_per_second": 1.241, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A yellow and blue propeller plane with the registration SP-AWF is flying through a cloudy sky. The plane appears to be in motion, moving from left to right in the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.69, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 64.66, "peak": 77.59, "min": 59.63}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 23.12, "sample_count": 6, "duration_seconds": 0.806}, "timestamp": "2026-01-17T18:12:57.506433"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 641.876, "latencies_ms": [641.876], "images_per_second": 1.558, "prompt_tokens": 18, "response_tokens_est": 25, "n_tiles": 1, "output_text": "The plane is predominantly yellow with red and blue accents. The lighting conditions appear to be overcast, creating a muted atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 31.02, "peak": 35.45, "min": 26.39}, "VIN": {"avg": 70.75, "peak": 96.45, "min": 60.63}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.02, "energy_joules_est": 19.92, "sample_count": 4, "duration_seconds": 0.642}, "timestamp": "2026-01-17T18:12:58.154268"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 686.699, "latencies_ms": [686.699], "images_per_second": 1.456, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "An aerial view from an airplane window showcases a vast parking lot filled with numerous cars, with residential and commercial buildings scattered throughout the landscape.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.5, "ram_available_mb": 100235.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 31.12, "peak": 37.01, "min": 25.6}, "VIN": {"avg": 67.65, "peak": 98.75, "min": 57.25}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 31.12, "energy_joules_est": 21.38, "sample_count": 5, "duration_seconds": 0.687}, "timestamp": "2026-01-17T18:12:58.852490"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 999.928, "latencies_ms": [999.928], "images_per_second": 1.0, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "airplane wing: 1\nbuildings: 2\nparking lot: 10\ncars: 20\ntrees: 5\nfields: 2\nsky: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.5, "ram_available_mb": 100235.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.81, "peak": 37.01, "min": 22.45}, "VIN": {"avg": 62.16, "peak": 75.47, "min": 52.82}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.81, "energy_joules_est": 28.82, "sample_count": 7, "duration_seconds": 1.0}, "timestamp": "2026-01-17T18:12:59.858276"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1144.142, "latencies_ms": [1144.142], "images_per_second": 0.874, "prompt_tokens": 25, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The airplane wing is positioned to the left of the image, occupying a significant portion of the foreground. The parking lot and residential area are situated in the background, extending across the image's width. The vast expanse of the parking lot and the residential area suggest a considerable distance between the plane and the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.34, "peak": 35.06, "min": 20.87}, "VIN": {"avg": 63.52, "peak": 83.91, "min": 59.62}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.34, "energy_joules_est": 30.15, "sample_count": 9, "duration_seconds": 1.145}, "timestamp": "2026-01-17T18:13:01.012841"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1045.722, "latencies_ms": [1045.722], "images_per_second": 0.956, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The scene depicts an aerial view of an airport runway with a large parking lot filled with cars. A portion of an airplane wing is visible in the top left corner. The parking lot is surrounded by green spaces and houses, typical of an urban area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.64, "peak": 33.88, "min": 21.28}, "VIN": {"avg": 60.96, "peak": 76.16, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.64, "energy_joules_est": 27.87, "sample_count": 8, "duration_seconds": 1.046}, "timestamp": "2026-01-17T18:13:02.065395"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 980.622, "latencies_ms": [980.622], "images_per_second": 1.02, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The sky is blue with a few scattered clouds. The parking lot is filled with various colored cars, reflecting the vibrant atmosphere of the location. The image appears to be taken from an airplane, capturing the expansive view from the wing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.41, "peak": 34.27, "min": 22.05}, "VIN": {"avg": 62.57, "peak": 75.7, "min": 57.22}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.41, "energy_joules_est": 26.89, "sample_count": 7, "duration_seconds": 0.981}, "timestamp": "2026-01-17T18:13:03.051995"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 454.602, "latencies_ms": [454.602], "images_per_second": 2.2, "prompt_tokens": 8, "response_tokens_est": 19, "n_tiles": 1, "output_text": "A person is holding a pink flip phone with a picture of Disney Princess Aurora on the screen.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 24.94, "peak": 26.77, "min": 23.23}, "VIN": {"avg": 57.55, "peak": 59.2, "min": 55.41}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 24.94, "energy_joules_est": 11.35, "sample_count": 3, "duration_seconds": 0.455}, "timestamp": "2026-01-17T18:13:03.516743"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 962.825, "latencies_ms": [962.825], "images_per_second": 1.039, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "Cell phone: 1\nGift bag: 1\nCup: 1\nScarf: 1\nJeans: 2\nShoes: 1\nCard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 23.24, "peak": 27.56, "min": 20.48}, "VIN": {"avg": 60.16, "peak": 62.17, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 23.24, "energy_joules_est": 22.38, "sample_count": 7, "duration_seconds": 0.963}, "timestamp": "2026-01-17T18:13:04.485716"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 591.523, "latencies_ms": [591.523], "images_per_second": 1.691, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The pink flip phone is held in the foreground, positioned between the person's hands. The background includes other individuals and objects, suggesting a social setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 23.93, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 60.66, "peak": 64.07, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.93, "energy_joules_est": 14.16, "sample_count": 4, "duration_seconds": 0.592}, "timestamp": "2026-01-17T18:13:05.085014"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 731.353, "latencies_ms": [731.353], "images_per_second": 1.367, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "Two people are sitting on the floor, one holding a pink flip phone displaying a picture of Disney Princess Aurora. The other person appears to be holding a drink. The setting appears to be a casual gathering or party.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 23.79, "peak": 26.79, "min": 21.27}, "VIN": {"avg": 61.72, "peak": 65.93, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.79, "energy_joules_est": 17.41, "sample_count": 5, "duration_seconds": 0.732}, "timestamp": "2026-01-17T18:13:05.822204"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 928.987, "latencies_ms": [928.987], "images_per_second": 1.076, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The pink flip phone is prominently featured in the image. The lighting is dim, creating a cozy atmosphere. The phone appears to be made of plastic and has a small screen displaying a Disney princess image. The background is dimly lit, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.74, "peak": 26.8, "min": 20.09}, "VIN": {"avg": 60.6, "peak": 64.87, "min": 53.2}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.74, "energy_joules_est": 21.13, "sample_count": 7, "duration_seconds": 0.929}, "timestamp": "2026-01-17T18:13:06.757372"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 612.425, "latencies_ms": [612.425], "images_per_second": 1.633, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "Two zebras stand in a field of tall, dry grass, their black and white stripes contrasting with the golden hues.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 29.93, "peak": 33.08, "min": 25.99}, "VIN": {"avg": 63.31, "peak": 73.44, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 18.34, "sample_count": 4, "duration_seconds": 0.613}, "timestamp": "2026-01-17T18:13:07.381101"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 847.953, "latencies_ms": [847.953], "images_per_second": 1.179, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "zebra: 2\ntall grass: 2\ntree: 1\nleaves: 1\nsky: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.8, "peak": 36.62, "min": 23.64}, "VIN": {"avg": 66.14, "peak": 98.4, "min": 56.53}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 29.8, "energy_joules_est": 25.28, "sample_count": 6, "duration_seconds": 0.848}, "timestamp": "2026-01-17T18:13:08.235198"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 766.954, "latencies_ms": [766.954], "images_per_second": 1.304, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The zebra on the left is positioned in the foreground, while the zebra on the right is further in the background. The foreground and background are relatively close together, while the zebra on the right is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.24, "peak": 35.44, "min": 25.2}, "VIN": {"avg": 66.0, "peak": 83.62, "min": 57.24}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.24, "energy_joules_est": 23.2, "sample_count": 5, "duration_seconds": 0.767}, "timestamp": "2026-01-17T18:13:09.008642"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 708.931, "latencies_ms": [708.931], "images_per_second": 1.411, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "Two zebras stand in a field of tall, dry grass, facing the camera. The background features trees and shrubs, suggesting a natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 30.56, "peak": 35.83, "min": 25.21}, "VIN": {"avg": 64.13, "peak": 79.64, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.56, "energy_joules_est": 21.67, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T18:13:09.723860"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1024.992, "latencies_ms": [1024.992], "images_per_second": 0.976, "prompt_tokens": 18, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The zebras are black and white, contrasting with the dry, golden-brown grass. The lighting suggests a sunny day, with some areas appearing slightly brighter than others. The materials appear to be natural, dry grasses and possibly some sparse shrubs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.41, "peak": 36.24, "min": 22.45}, "VIN": {"avg": 66.33, "peak": 97.68, "min": 59.64}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.41, "energy_joules_est": 29.13, "sample_count": 7, "duration_seconds": 1.025}, "timestamp": "2026-01-17T18:13:10.759978"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 578.811, "latencies_ms": [578.811], "images_per_second": 1.728, "prompt_tokens": 8, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A man in black shorts carries a yellow surfboard towards the ocean, preparing to enter the waves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 30.13, "peak": 34.27, "min": 25.99}, "VIN": {"avg": 67.45, "peak": 83.41, "min": 59.9}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.13, "energy_joules_est": 17.45, "sample_count": 4, "duration_seconds": 0.579}, "timestamp": "2026-01-17T18:13:11.351156"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 942.932, "latencies_ms": [942.932], "images_per_second": 1.061, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "surfboard: 1\nperson: 1\nwater: 6\nwaves: 4\nsky: 1\nsand: 1\nocean: 6", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 28.69, "peak": 37.01, "min": 22.45}, "VIN": {"avg": 63.42, "peak": 85.22, "min": 55.74}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.69, "energy_joules_est": 27.06, "sample_count": 7, "duration_seconds": 0.943}, "timestamp": "2026-01-17T18:13:12.303956"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 707.287, "latencies_ms": [707.287], "images_per_second": 1.414, "prompt_tokens": 25, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The man is walking towards the ocean, carrying a yellow surfboard. The ocean extends into the background, creating a sense of distance and vastness.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25537.1, "ram_available_mb": 100235.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.93, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 62.75, "peak": 76.4, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.93, "energy_joules_est": 21.18, "sample_count": 5, "duration_seconds": 0.708}, "timestamp": "2026-01-17T18:13:13.017078"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 782.151, "latencies_ms": [782.151], "images_per_second": 1.279, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A man is walking into the ocean with a yellow surfboard, preparing to surf the waves. The ocean is blue and choppy, with waves breaking around the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.1, "ram_available_mb": 100235.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.63, "peak": 36.62, "min": 25.21}, "VIN": {"avg": 66.08, "peak": 79.3, "min": 61.08}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 30.63, "energy_joules_est": 23.97, "sample_count": 5, "duration_seconds": 0.783}, "timestamp": "2026-01-17T18:13:13.804855"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 712.113, "latencies_ms": [712.113], "images_per_second": 1.404, "prompt_tokens": 18, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The surfer is wearing a yellow shirt and dark shorts. The ocean is a dark blue, and the sky is light blue, suggesting fair weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.49, "peak": 36.24, "min": 25.21}, "VIN": {"avg": 64.67, "peak": 81.39, "min": 60.04}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.49, "energy_joules_est": 21.72, "sample_count": 5, "duration_seconds": 0.712}, "timestamp": "2026-01-17T18:13:14.524376"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 512.197, "latencies_ms": [512.197], "images_per_second": 1.952, "prompt_tokens": 8, "response_tokens_est": 18, "n_tiles": 1, "output_text": "A black and white cow stands calmly on a sandy beach, gazing directly at the camera.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.4, "min": 14.0}, "VDD_GPU": {"avg": 33.34, "peak": 36.23, "min": 29.93}, "VIN": {"avg": 66.04, "peak": 80.83, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 33.34, "energy_joules_est": 17.09, "sample_count": 3, "duration_seconds": 0.513}, "timestamp": "2026-01-17T18:13:15.045891"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 975.875, "latencies_ms": [975.875], "images_per_second": 1.025, "prompt_tokens": 21, "response_tokens_est": 30, "n_tiles": 1, "output_text": "Cow: 1\nSand: 1\nWater: 1\nGround: 1\nPlant: 1\nBag: 1\nEar tag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 29.94, "peak": 39.38, "min": 22.85}, "VIN": {"avg": 66.23, "peak": 97.64, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 29.23, "sample_count": 7, "duration_seconds": 0.976}, "timestamp": "2026-01-17T18:13:16.027627"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 708.918, "latencies_ms": [708.918], "images_per_second": 1.411, "prompt_tokens": 25, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The cow is positioned near the foreground of the image, facing the viewer. The body of water in the background extends into the distance, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.94, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 67.34, "peak": 90.62, "min": 60.58}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.34, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 21.23, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-17T18:13:16.742561"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 611.184, "latencies_ms": [611.184], "images_per_second": 1.636, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A black and white cow stands on a sandy beach, facing the camera. The calm water in the background creates a serene and picturesque setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 32.09, "peak": 36.23, "min": 27.56}, "VIN": {"avg": 69.55, "peak": 95.73, "min": 59.41}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 32.09, "energy_joules_est": 19.62, "sample_count": 4, "duration_seconds": 0.611}, "timestamp": "2026-01-17T18:13:17.359554"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 726.193, "latencies_ms": [726.193], "images_per_second": 1.377, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The cow is black and white. The lighting is soft and diffused, creating a calm atmosphere. The cow is standing on a sandy beach next to a body of water.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 31.11, "peak": 37.42, "min": 25.59}, "VIN": {"avg": 67.48, "peak": 90.18, "min": 59.93}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.11, "energy_joules_est": 22.6, "sample_count": 5, "duration_seconds": 0.727}, "timestamp": "2026-01-17T18:13:18.093023"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 616.292, "latencies_ms": [616.292], "images_per_second": 1.623, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A woman, dressed in winter attire, stands confidently in the snow, holding skis and poles, smiling warmly at the camera.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.71, "min": 13.79}, "VDD_GPU": {"avg": 25.3, "peak": 28.35, "min": 22.84}, "VIN": {"avg": 61.72, "peak": 62.81, "min": 60.51}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 25.3, "energy_joules_est": 15.6, "sample_count": 4, "duration_seconds": 0.617}, "timestamp": "2026-01-17T18:13:18.718730"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1018.538, "latencies_ms": [1018.538], "images_per_second": 0.982, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "woman: 2\nskis: 1\nsnowboard: 1\ngloves: 1\nski poles: 1\nbelt: 1\nscarf: 1\ntree: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 23.12, "peak": 27.56, "min": 20.48}, "VIN": {"avg": 59.55, "peak": 62.18, "min": 57.17}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.12, "energy_joules_est": 23.55, "sample_count": 7, "duration_seconds": 1.019}, "timestamp": "2026-01-17T18:13:19.743089"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 689.653, "latencies_ms": [689.653], "images_per_second": 1.45, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The woman is positioned in the foreground, holding ski poles and standing near a pair of skis. The background features trees and a cloudy sky, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 23.32, "peak": 26.0, "min": 21.27}, "VIN": {"avg": 61.02, "peak": 62.47, "min": 60.05}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 23.32, "energy_joules_est": 16.09, "sample_count": 5, "duration_seconds": 0.69}, "timestamp": "2026-01-17T18:13:20.438778"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 713.587, "latencies_ms": [713.587], "images_per_second": 1.401, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A woman is posing in a snowy setting, dressed in ski gear and holding skis and poles. She appears to be enjoying a winter activity, possibly skiing or snowboarding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 23.48, "peak": 26.39, "min": 21.28}, "VIN": {"avg": 60.59, "peak": 62.57, "min": 56.52}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.48, "energy_joules_est": 16.76, "sample_count": 5, "duration_seconds": 0.714}, "timestamp": "2026-01-17T18:13:21.158418"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 776.907, "latencies_ms": [776.907], "images_per_second": 1.287, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The woman is wearing black and white clothing. The lighting appears to be natural, possibly from the sky, creating a contrast against the darker background. The image also shows snow and pine branches, suggesting an outdoor winter setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 23.32, "peak": 26.0, "min": 21.26}, "VIN": {"avg": 62.52, "peak": 64.04, "min": 61.35}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.32, "energy_joules_est": 18.12, "sample_count": 5, "duration_seconds": 0.777}, "timestamp": "2026-01-17T18:13:21.941238"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 537.301, "latencies_ms": [537.301], "images_per_second": 1.861, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A black and white dog is standing on a sandy beach, holding a yellow frisbee in its mouth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 31.24, "peak": 33.86, "min": 28.36}, "VIN": {"avg": 69.66, "peak": 86.31, "min": 60.71}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 31.24, "energy_joules_est": 16.8, "sample_count": 3, "duration_seconds": 0.538}, "timestamp": "2026-01-17T18:13:22.490348"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 992.465, "latencies_ms": [992.465], "images_per_second": 1.008, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "yellow frisbee: 1\nsand: 6\nocean: 6\ndog: 1\npink nose: 1\nwaves: 2\nisland: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 7.6, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 28.81, "peak": 38.19, "min": 22.45}, "VIN": {"avg": 65.55, "peak": 85.77, "min": 59.26}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.81, "energy_joules_est": 28.6, "sample_count": 7, "duration_seconds": 0.993}, "timestamp": "2026-01-17T18:13:23.488615"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 916.018, "latencies_ms": [916.018], "images_per_second": 1.092, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The dog is positioned in the foreground, facing the camera. The yellow frisbee is held in the dog's mouth, close to the dog's nose. The beach extends in the background, meeting the turquoise ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 7.3, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.3}, "VDD_GPU": {"avg": 27.63, "peak": 34.66, "min": 22.06}, "VIN": {"avg": 66.84, "peak": 89.62, "min": 59.77}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.95}}, "power_watts_avg": 27.63, "energy_joules_est": 25.32, "sample_count": 7, "duration_seconds": 0.916}, "timestamp": "2026-01-17T18:13:24.410708"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 858.756, "latencies_ms": [858.756], "images_per_second": 1.164, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The scene is set on a sandy beach near the ocean, where a dog is holding a yellow frisbee in its mouth. The ocean is visible in the background, with waves gently lapping at the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 28.68, "peak": 35.44, "min": 23.23}, "VIN": {"avg": 64.88, "peak": 79.12, "min": 60.78}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 28.68, "energy_joules_est": 24.64, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T18:13:25.275394"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 946.39, "latencies_ms": [946.39], "images_per_second": 1.057, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The dog is primarily black and white with patches of gray. The beach is sandy and appears wet, suggesting recent rain or sun. The sky is overcast and gray, casting a soft light on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 27.9, "peak": 35.44, "min": 22.05}, "VIN": {"avg": 64.4, "peak": 83.25, "min": 56.35}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.9, "energy_joules_est": 26.42, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T18:13:26.227715"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 439.479, "latencies_ms": [439.479], "images_per_second": 2.275, "prompt_tokens": 8, "response_tokens_est": 14, "n_tiles": 1, "output_text": "A group of people are gathered in a kitchen, preparing food together.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.4, "min": 14.0}, "VDD_GPU": {"avg": 32.43, "peak": 35.85, "min": 29.15}, "VIN": {"avg": 67.67, "peak": 79.55, "min": 60.64}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 32.43, "energy_joules_est": 14.27, "sample_count": 3, "duration_seconds": 0.44}, "timestamp": "2026-01-17T18:13:26.678660"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1182.436, "latencies_ms": [1182.436], "images_per_second": 0.846, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "Refrigerator: 2\nCountertop: 1\nPot: 1\nBowl: 1\nGlass: 1\nBox: 1\nShelves: 2\nCooking utensils: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 28.4, "peak": 40.16, "min": 21.66}, "VIN": {"avg": 63.36, "peak": 79.56, "min": 59.18}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.4, "energy_joules_est": 33.59, "sample_count": 8, "duration_seconds": 1.183}, "timestamp": "2026-01-17T18:13:27.871263"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 809.733, "latencies_ms": [809.733], "images_per_second": 1.235, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The large stainless steel refrigerator occupies the central foreground, positioned between the people and the countertop. The kitchen area extends beyond the refrigerator, occupying the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 28.1, "peak": 33.48, "min": 23.24}, "VIN": {"avg": 65.7, "peak": 94.27, "min": 55.72}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.1, "energy_joules_est": 22.76, "sample_count": 6, "duration_seconds": 0.81}, "timestamp": "2026-01-17T18:13:28.687134"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 730.865, "latencies_ms": [730.865], "images_per_second": 1.368, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "A group of people are gathered in a kitchen, preparing food together. The kitchen features stainless steel appliances and counters, with various cooking utensils and ingredients visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.93, "peak": 35.44, "min": 24.82}, "VIN": {"avg": 63.09, "peak": 81.35, "min": 55.41}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 29.93, "energy_joules_est": 21.89, "sample_count": 5, "duration_seconds": 0.731}, "timestamp": "2026-01-17T18:13:29.424122"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 886.649, "latencies_ms": [886.649], "images_per_second": 1.128, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The kitchen is equipped with stainless steel appliances and features natural lighting, creating a bright and clean atmosphere. The walls are made of concrete blocks, giving the space a functional and somewhat rustic appearance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.36, "peak": 36.24, "min": 22.85}, "VIN": {"avg": 59.43, "peak": 64.91, "min": 49.9}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 25.15, "sample_count": 6, "duration_seconds": 0.887}, "timestamp": "2026-01-17T18:13:30.317572"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 649.143, "latencies_ms": [649.143], "images_per_second": 1.54, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "A bathroom features a white toilet, neatly arranged toiletries on a shelf, and a towel rack with folded white towels.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 31.1, "peak": 35.04, "min": 26.77}, "VIN": {"avg": 65.11, "peak": 78.59, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.1, "energy_joules_est": 20.2, "sample_count": 4, "duration_seconds": 0.65}, "timestamp": "2026-01-17T18:13:30.977666"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1551.211, "latencies_ms": [1551.211], "images_per_second": 0.645, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 1, "output_text": "toilet: 1\ntowel: 2\nsoap dispenser: 2\nshampoo bottle: 2\nbody wash bottle: 2\ntoilet paper: 1\nbathroom phone: 1\nmirror: 1\ntoilet paper holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 25.31, "peak": 37.01, "min": 19.7}, "VIN": {"avg": 61.7, "peak": 73.52, "min": 57.47}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 25.31, "energy_joules_est": 39.27, "sample_count": 12, "duration_seconds": 1.552}, "timestamp": "2026-01-17T18:13:32.535085"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 892.606, "latencies_ms": [892.606], "images_per_second": 1.12, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The toilet is positioned in the foreground, slightly to the right of the shelf. The shelf is situated in the background, slightly to the left of the toilet. The toilet is situated further back in the image, near the edge of the frame.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.63, "peak": 33.08, "min": 22.84}, "VIN": {"avg": 63.17, "peak": 80.77, "min": 54.66}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 24.68, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T18:13:33.433945"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 930.946, "latencies_ms": [930.946], "images_per_second": 1.074, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The bathroom features a modern design with beige tile walls, a white toilet, and a built-in shelf with toiletries and towels. A telephone is mounted on the wall, and a toilet brush and holder are positioned nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25537.0, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 27.68, "peak": 35.04, "min": 22.06}, "VIN": {"avg": 64.72, "peak": 83.72, "min": 60.01}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.68, "energy_joules_est": 25.78, "sample_count": 7, "duration_seconds": 0.931}, "timestamp": "2026-01-17T18:13:34.370596"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 971.346, "latencies_ms": [971.346], "images_per_second": 1.029, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The bathroom features a beige color scheme, creating a warm and inviting atmosphere. The lighting is soft and diffused, enhancing the overall ambiance. The materials include white tiles and polished metal, creating a clean and sleek aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25537.0, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 64.96, "peak": 92.61, "min": 53.78}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 27.01, "sample_count": 7, "duration_seconds": 0.972}, "timestamp": "2026-01-17T18:13:35.348329"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 618.117, "latencies_ms": [618.117], "images_per_second": 1.618, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A woman in a green top hat sits in the driver's seat of a decorated truck, accompanied by her beagle wearing a green shamrock hat.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25537.3, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.59}, "VDD_GPU": {"avg": 23.93, "peak": 26.39, "min": 22.06}, "VIN": {"avg": 58.09, "peak": 60.21, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 14.46, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 23.93, "energy_joules_est": 14.8, "sample_count": 4, "duration_seconds": 0.618}, "timestamp": "2026-01-17T18:13:35.975567"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1109.546, "latencies_ms": [1109.546], "images_per_second": 0.901, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "woman: 1\ndog: 1\nhat: 1\nIrish flag: 1\nmirror: 1\nshamrock: 1\ntruck: 1\nwindshield wiper: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25537.3, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25537.3, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 15.01, "min": 13.49}, "VDD_GPU": {"avg": 22.26, "peak": 26.79, "min": 19.7}, "VIN": {"avg": 59.62, "peak": 63.0, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.26, "energy_joules_est": 24.71, "sample_count": 8, "duration_seconds": 1.11}, "timestamp": "2026-01-17T18:13:37.091225"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 717.022, "latencies_ms": [717.022], "images_per_second": 1.395, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The woman and dog are positioned in the left foreground of the image. The Irish flag and shamrock decoration are placed near the back of the vehicle, creating a sense of depth and perspective.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25537.3, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25537.3, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 23.01, "peak": 25.6, "min": 20.89}, "VIN": {"avg": 61.8, "peak": 64.86, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 23.01, "energy_joules_est": 16.51, "sample_count": 5, "duration_seconds": 0.717}, "timestamp": "2026-01-17T18:13:37.814445"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 812.333, "latencies_ms": [812.333], "images_per_second": 1.231, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "A woman and a beagle are sitting in the back of a truck decorated for St. Patrick's Day. The truck is adorned with an Irish flag, a shamrock, and a green top hat, suggesting the celebration.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.3, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.97, "peak": 26.38, "min": 20.48}, "VIN": {"avg": 60.78, "peak": 62.15, "min": 58.96}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 22.97, "energy_joules_est": 18.67, "sample_count": 6, "duration_seconds": 0.813}, "timestamp": "2026-01-17T18:13:38.636962"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 810.553, "latencies_ms": [810.553], "images_per_second": 1.234, "prompt_tokens": 18, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The vehicle is decorated with green and orange colors, reflecting the Irish flag. The lighting suggests an outdoor setting, likely during daytime. The materials appear to be metal and plastic, typical for vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 22.98, "peak": 26.39, "min": 20.48}, "VIN": {"avg": 60.74, "peak": 63.4, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 22.98, "energy_joules_est": 18.63, "sample_count": 6, "duration_seconds": 0.811}, "timestamp": "2026-01-17T18:13:39.453248"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 607.554, "latencies_ms": [607.554], "images_per_second": 1.646, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "An adult elephant is walking through a shallow pool of water, surrounded by rocks and dirt, with several people observing in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 30.53, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 67.53, "peak": 83.53, "min": 60.43}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.53, "energy_joules_est": 18.57, "sample_count": 4, "duration_seconds": 0.608}, "timestamp": "2026-01-17T18:13:40.072596"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1197.056, "latencies_ms": [1197.056], "images_per_second": 0.835, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 1, "output_text": "Elephant: 1\nLog: 1\nWater: 1\nRocks: 3\nGround: 2\nFence: 1\nGround: 1\nPlants: 2\nPeople: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.96, "peak": 36.62, "min": 20.88}, "VIN": {"avg": 62.25, "peak": 80.56, "min": 53.96}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.96, "energy_joules_est": 32.28, "sample_count": 9, "duration_seconds": 1.197}, "timestamp": "2026-01-17T18:13:41.276309"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 845.133, "latencies_ms": [845.133], "images_per_second": 1.183, "prompt_tokens": 25, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The main object, the elephant, is positioned in the foreground, facing the viewer. The background features other elements like rocks, trees, and people, suggesting the setting is a zoo or wildlife park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.57, "peak": 34.66, "min": 23.25}, "VIN": {"avg": 66.94, "peak": 93.48, "min": 60.43}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.57, "energy_joules_est": 24.15, "sample_count": 6, "duration_seconds": 0.845}, "timestamp": "2026-01-17T18:13:42.127339"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 913.225, "latencies_ms": [913.225], "images_per_second": 1.095, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The scene depicts an elephant in a zoo enclosure, walking through a shallow pool of water near rocks. The enclosure features a dirt ground, trees, and fencing, with visitors observing the elephant from nearby.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.68, "peak": 35.04, "min": 22.05}, "VIN": {"avg": 64.93, "peak": 90.79, "min": 57.63}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.68, "energy_joules_est": 25.29, "sample_count": 7, "duration_seconds": 0.914}, "timestamp": "2026-01-17T18:13:43.046588"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 986.942, "latencies_ms": [986.942], "images_per_second": 1.013, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The elephant is gray and appears to be wet. The lighting is bright, likely from sunlight, creating a clear reflection on the wet ground. The enclosure features natural elements like rocks and grass, and includes artificial structures like a log and a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.6, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.85, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 61.62, "peak": 83.33, "min": 53.1}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 27.85, "energy_joules_est": 27.49, "sample_count": 7, "duration_seconds": 0.987}, "timestamp": "2026-01-17T18:13:44.039379"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 681.77, "latencies_ms": [681.77], "images_per_second": 1.467, "prompt_tokens": 8, "response_tokens_est": 24, "n_tiles": 1, "output_text": "Four people in winter gear, including skis, stand on a snowy mountain slope, smiling and enjoying their skiing adventure.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.6, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.3, "peak": 34.26, "min": 24.41}, "VIN": {"avg": 66.19, "peak": 93.17, "min": 53.56}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 19.99, "sample_count": 5, "duration_seconds": 0.682}, "timestamp": "2026-01-17T18:13:44.732653"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1121.836, "latencies_ms": [1121.836], "images_per_second": 0.891, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "person: 4\nski: 4\nsnow: 8\nmountain: 8\nsky: 1\ntrees: 2\nski poles: 4\ngloves: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.6, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 27.18, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 63.17, "peak": 73.86, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 27.18, "energy_joules_est": 30.5, "sample_count": 8, "duration_seconds": 1.122}, "timestamp": "2026-01-17T18:13:45.861351"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 761.367, "latencies_ms": [761.367], "images_per_second": 1.313, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The four people are positioned in the foreground of the image, standing relatively close together. The snow-covered slope and snow-capped mountains in the background create a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.06, "peak": 33.85, "min": 24.42}, "VIN": {"avg": 65.08, "peak": 83.26, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.06, "energy_joules_est": 22.14, "sample_count": 5, "duration_seconds": 0.762}, "timestamp": "2026-01-17T18:13:46.629094"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 900.832, "latencies_ms": [900.832], "images_per_second": 1.11, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 1, "output_text": "A group of four skiers is posing for a photo on a snowy mountain. They are wearing winter clothing and holding ski poles. The background features a snow-covered mountain landscape with ski lifts visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.31, "peak": 35.85, "min": 22.45}, "VIN": {"avg": 65.49, "peak": 94.18, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.31, "energy_joules_est": 25.51, "sample_count": 7, "duration_seconds": 0.901}, "timestamp": "2026-01-17T18:13:47.536806"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1095.495, "latencies_ms": [1095.495], "images_per_second": 0.913, "prompt_tokens": 18, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The group is wearing bright, colorful winter clothing. The lighting is bright and sunny, creating a pleasant atmosphere. The snow appears to be well-groomed and smooth, typical of ski slopes. The weather appears to be clear and sunny, ideal for skiing activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 26.94, "peak": 34.65, "min": 21.28}, "VIN": {"avg": 61.53, "peak": 82.89, "min": 52.06}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.94, "energy_joules_est": 29.52, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T18:13:48.638653"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 546.381, "latencies_ms": [546.381], "images_per_second": 1.83, "prompt_tokens": 8, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A person is holding a black iPhone in their hand, displaying a photo of a tree on the screen.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 30.43, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 62.39, "peak": 75.74, "min": 51.49}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.43, "energy_joules_est": 16.64, "sample_count": 4, "duration_seconds": 0.547}, "timestamp": "2026-01-17T18:13:49.196218"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1070.69, "latencies_ms": [1070.69], "images_per_second": 0.934, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 1, "output_text": "phone: 2\nkeyboard: 1\nmirror: 1\nhand: 1\ntree: 1\nsky: 1\nwindow: 1\nbutton: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.82, "peak": 37.41, "min": 21.27}, "VIN": {"avg": 62.67, "peak": 81.81, "min": 54.36}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 15.35, "min": 13.76}}, "power_watts_avg": 27.82, "energy_joules_est": 29.8, "sample_count": 8, "duration_seconds": 1.071}, "timestamp": "2026-01-17T18:13:50.274136"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 886.504, "latencies_ms": [886.504], "images_per_second": 1.128, "prompt_tokens": 25, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The main object is a smartphone held in the left foreground of the image. The reflection in the phone's screen shows a portion of a keyboard and a person's hand in the background. The phone is positioned near the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25537.3, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 28.23, "peak": 33.88, "min": 23.24}, "VIN": {"avg": 65.04, "peak": 82.05, "min": 59.64}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 25.03, "sample_count": 6, "duration_seconds": 0.887}, "timestamp": "2026-01-17T18:13:51.166738"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 824.873, "latencies_ms": [824.873], "images_per_second": 1.212, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "A person is holding a black iPhone in their hand, displaying a photo of a tree and a clock showing 19:45. The photo appears to be taken indoors, near a keyboard.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25537.3, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25537.8, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.22, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 63.55, "peak": 80.18, "min": 57.63}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.22, "energy_joules_est": 23.29, "sample_count": 6, "duration_seconds": 0.825}, "timestamp": "2026-01-17T18:13:51.997615"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 988.551, "latencies_ms": [988.551], "images_per_second": 1.012, "prompt_tokens": 18, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The phone's screen displays a black and white image, likely taken in low light conditions. The reflection shows a person's hand and a keyboard, suggesting the phone is held in a dimly lit environment. The phone appears to be made of metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.8, "ram_available_mb": 100234.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.79}, "VDD_GPU": {"avg": 27.63, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 63.14, "peak": 79.41, "min": 56.62}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 27.63, "energy_joules_est": 27.32, "sample_count": 7, "duration_seconds": 0.989}, "timestamp": "2026-01-17T18:13:52.992006"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 719.84, "latencies_ms": [719.84], "images_per_second": 1.389, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A Denver's Road Home parking meter is mounted on a metal pole beside a sidewalk, with a sign urging people to end homelessness.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25538.8, "ram_available_mb": 100233.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 29.15, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 62.04, "peak": 76.04, "min": 54.77}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.15, "energy_joules_est": 21.01, "sample_count": 5, "duration_seconds": 0.721}, "timestamp": "2026-01-17T18:13:53.724275"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1040.779, "latencies_ms": [1040.779], "images_per_second": 0.961, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "Parking meter: 1\nSign: 1\nPlants: 2\nFence: 1\nSidewalk: 1\nTrees: 2\nGrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.02, "peak": 35.85, "min": 22.06}, "VIN": {"avg": 61.84, "peak": 73.4, "min": 57.8}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 28.02, "energy_joules_est": 29.18, "sample_count": 7, "duration_seconds": 1.041}, "timestamp": "2026-01-17T18:13:54.775435"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 847.945, "latencies_ms": [847.945], "images_per_second": 1.179, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The parking meter is positioned in the foreground, slightly to the right of the image. The sign is situated in the background, near the parking meter. The parking meter and sign are located close to the sidewalk.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.02, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 63.3, "peak": 77.52, "min": 57.76}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.02, "energy_joules_est": 23.77, "sample_count": 6, "duration_seconds": 0.848}, "timestamp": "2026-01-17T18:13:55.630637"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 786.525, "latencies_ms": [786.525], "images_per_second": 1.271, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 1, "output_text": "A Denver's Road Home parking meter is situated on a sidewalk near a fence and trees. A sign on the meter promotes a campaign to end homelessness.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.78, "peak": 35.44, "min": 24.83}, "VIN": {"avg": 65.05, "peak": 80.87, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 23.44, "sample_count": 5, "duration_seconds": 0.787}, "timestamp": "2026-01-17T18:13:56.426976"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 729.985, "latencies_ms": [729.985], "images_per_second": 1.37, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The parking meter is primarily red and gray. The lighting appears to be natural daylight. The materials appear to be metal and plastic. The weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.1, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 29.85, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 67.05, "peak": 88.38, "min": 60.02}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.85, "energy_joules_est": 21.8, "sample_count": 5, "duration_seconds": 0.73}, "timestamp": "2026-01-17T18:13:57.162909"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 666.648, "latencies_ms": [666.648], "images_per_second": 1.5, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A group of zebras graze peacefully in a grassy field, their distinctive black and white stripes contrasting against the green and brown surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.7, "ram_available_mb": 100233.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 30.26, "peak": 35.86, "min": 24.82}, "VIN": {"avg": 64.16, "peak": 77.2, "min": 58.82}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.26, "energy_joules_est": 20.18, "sample_count": 5, "duration_seconds": 0.667}, "timestamp": "2026-01-17T18:13:57.841926"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 868.522, "latencies_ms": [868.522], "images_per_second": 1.151, "prompt_tokens": 21, "response_tokens_est": 24, "n_tiles": 1, "output_text": "zebra: 5\ngrass: 6\ntree: 1\nbush: 2\nground: 4\nsky: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.7, "ram_available_mb": 100233.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.71, "min": 13.69}, "VDD_GPU": {"avg": 29.48, "peak": 36.23, "min": 23.64}, "VIN": {"avg": 64.9, "peak": 83.89, "min": 57.98}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.48, "energy_joules_est": 25.61, "sample_count": 6, "duration_seconds": 0.869}, "timestamp": "2026-01-17T18:13:58.716576"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 936.636, "latencies_ms": [936.636], "images_per_second": 1.068, "prompt_tokens": 25, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The main zebra is positioned in the foreground, grazing on the dry grass. The background features other zebras, creating a sense of depth and space. The zebra is relatively close to the viewer, emphasizing its presence in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.68, "peak": 34.65, "min": 22.06}, "VIN": {"avg": 63.68, "peak": 76.21, "min": 60.4}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.68, "energy_joules_est": 25.93, "sample_count": 7, "duration_seconds": 0.937}, "timestamp": "2026-01-17T18:13:59.660018"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 841.609, "latencies_ms": [841.609], "images_per_second": 1.188, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "A group of zebras grazes in a grassy field with scattered trees and shrubs in the background. The zebras are spread out across the field, displaying their distinctive black and white stripes.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25538.8, "ram_available_mb": 100233.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.35, "peak": 34.65, "min": 22.85}, "VIN": {"avg": 66.32, "peak": 90.23, "min": 60.15}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.35, "energy_joules_est": 23.87, "sample_count": 6, "duration_seconds": 0.842}, "timestamp": "2026-01-17T18:14:00.507789"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1156.302, "latencies_ms": [1156.302], "images_per_second": 0.865, "prompt_tokens": 18, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The zebras exhibit striking black and white stripes, creating a mesmerizing pattern. The lighting in the image is soft and diffused, possibly indicating a cloudy day or a shaded area. The zebras are grazing in a grassy field with dry, light brown vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 26.3, "peak": 35.06, "min": 20.88}, "VIN": {"avg": 65.55, "peak": 96.88, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.3, "energy_joules_est": 30.42, "sample_count": 9, "duration_seconds": 1.157}, "timestamp": "2026-01-17T18:14:01.670057"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 696.357, "latencies_ms": [696.357], "images_per_second": 1.436, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A surfer in a black wetsuit rides a wave on a surfboard, skillfully navigating the powerful ocean currents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.0, "ram_available_mb": 100233.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25539.3, "ram_available_mb": 100232.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.31, "peak": 34.66, "min": 24.42}, "VIN": {"avg": 66.36, "peak": 94.55, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.31, "energy_joules_est": 20.43, "sample_count": 5, "duration_seconds": 0.697}, "timestamp": "2026-01-17T18:14:02.377137"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 884.593, "latencies_ms": [884.593], "images_per_second": 1.13, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "surfboard: 1\nwetsuit: 1\nwater: 1\nwave: 1\nperson: 1\nocean: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.3, "ram_available_mb": 100232.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.5, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.21, "peak": 36.24, "min": 23.24}, "VIN": {"avg": 64.44, "peak": 80.18, "min": 53.51}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.21, "energy_joules_est": 25.85, "sample_count": 6, "duration_seconds": 0.885}, "timestamp": "2026-01-17T18:14:03.267728"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 684.408, "latencies_ms": [684.408], "images_per_second": 1.461, "prompt_tokens": 25, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The surfer is positioned in the foreground of the image, riding a wave. The ocean extends in the background, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25539.5, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25539.5, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.86, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 64.6, "peak": 77.23, "min": 57.79}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 20.45, "sample_count": 5, "duration_seconds": 0.685}, "timestamp": "2026-01-17T18:14:03.958922"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 885.371, "latencies_ms": [885.371], "images_per_second": 1.129, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a surfer skillfully riding a wave in the ocean. The surfer is wearing a black wetsuit and is positioned on a surfboard, demonstrating their expertise in surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.5, "ram_available_mb": 100232.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25539.8, "ram_available_mb": 100232.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 29.34, "peak": 36.23, "min": 23.62}, "VIN": {"avg": 62.8, "peak": 79.26, "min": 54.63}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.34, "energy_joules_est": 26.0, "sample_count": 6, "duration_seconds": 0.886}, "timestamp": "2026-01-17T18:14:04.855008"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 671.957, "latencies_ms": [671.957], "images_per_second": 1.488, "prompt_tokens": 18, "response_tokens_est": 27, "n_tiles": 1, "output_text": "The surfer is wearing a black wetsuit. The ocean appears dark blue-green, and the lighting suggests a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.8, "ram_available_mb": 100232.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25539.8, "ram_available_mb": 100232.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 29.85, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 64.05, "peak": 73.56, "min": 59.81}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.85, "energy_joules_est": 20.08, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T18:14:05.533724"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 671.203, "latencies_ms": [671.203], "images_per_second": 1.49, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "Two individuals in white snowsuits are standing on a snowy mountain slope, one holding a pair of skis and the other a backpack.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.8, "ram_available_mb": 100232.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25539.8, "ram_available_mb": 100232.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 31.91, "peak": 36.24, "min": 27.18}, "VIN": {"avg": 67.54, "peak": 91.05, "min": 53.98}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.55}}, "power_watts_avg": 31.91, "energy_joules_est": 21.44, "sample_count": 4, "duration_seconds": 0.672}, "timestamp": "2026-01-17T18:14:06.214785"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1239.113, "latencies_ms": [1239.113], "images_per_second": 0.807, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 1, "output_text": "skis: 2\nbackpack: 1\ngloves: 1\ngloves: 1\ncamera: 1\ncamera: 1\npole: 1\nsun: 1\nsnow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.8, "ram_available_mb": 100232.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 27.13, "peak": 37.42, "min": 20.87}, "VIN": {"avg": 61.84, "peak": 79.73, "min": 54.73}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.13, "energy_joules_est": 33.63, "sample_count": 9, "duration_seconds": 1.24}, "timestamp": "2026-01-17T18:14:07.462160"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 782.367, "latencies_ms": [782.367], "images_per_second": 1.278, "prompt_tokens": 25, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The skis are positioned in the foreground, while the snowboarder is further back, near the sun. The snowboarder's position suggests they are moving away from the sun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 28.75, "peak": 33.47, "min": 24.03}, "VIN": {"avg": 62.53, "peak": 76.04, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 28.75, "energy_joules_est": 22.51, "sample_count": 5, "duration_seconds": 0.783}, "timestamp": "2026-01-17T18:14:08.250610"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 865.602, "latencies_ms": [865.602], "images_per_second": 1.155, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "Two individuals in winter gear are on a snowy mountain slope, preparing for skiing or snowboarding. They are standing near ski poles and skis, with the sun setting in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.62, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 66.52, "peak": 92.01, "min": 59.61}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.62, "energy_joules_est": 24.79, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T18:14:09.122682"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 923.264, "latencies_ms": [923.264], "images_per_second": 1.083, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The snow is white and appears smooth. The lighting suggests either sunrise or sunset, creating a dramatic contrast against the sky. The snow is likely made of snow or ice, contributing to the overall wintery atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.19, "peak": 35.45, "min": 22.45}, "VIN": {"avg": 64.01, "peak": 83.99, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.19, "energy_joules_est": 26.05, "sample_count": 7, "duration_seconds": 0.924}, "timestamp": "2026-01-17T18:14:10.052398"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 745.476, "latencies_ms": [745.476], "images_per_second": 1.341, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A youth baseball game is in progress, with a batter in a red uniform swinging a bat, while a catcher in black crouches behind him.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.62, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 65.8, "peak": 86.81, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 22.1, "sample_count": 5, "duration_seconds": 0.746}, "timestamp": "2026-01-17T18:14:10.811038"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1346.883, "latencies_ms": [1346.883], "images_per_second": 0.742, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "baseball bat: 1\nbaseball glove: 1\nbaseball: 1\ncatcher: 1\numpire: 1\nbaseball field: 1\nfence: 1\ncars: 2\nspectators: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.04, "peak": 36.23, "min": 20.1}, "VIN": {"avg": 61.79, "peak": 77.57, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.04, "energy_joules_est": 35.08, "sample_count": 10, "duration_seconds": 1.347}, "timestamp": "2026-01-17T18:14:12.164587"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 911.356, "latencies_ms": [911.356], "images_per_second": 1.097, "prompt_tokens": 25, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The batter is positioned in the foreground, facing the catcher and pitcher. The catcher and pitcher are in the background, positioned behind the batter. The background also includes parked cars and a chain-link fence.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25540.0, "ram_available_mb": 100232.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25540.5, "ram_available_mb": 100231.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.81, "min": 14.3}, "VDD_GPU": {"avg": 27.84, "peak": 33.88, "min": 22.85}, "VIN": {"avg": 66.16, "peak": 91.17, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.84, "energy_joules_est": 25.38, "sample_count": 6, "duration_seconds": 0.912}, "timestamp": "2026-01-17T18:14:13.082124"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1128.08, "latencies_ms": [1128.08], "images_per_second": 0.886, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 1, "output_text": "A youth baseball game is in progress. A batter in a red uniform is swinging at a pitch, while a catcher in a black uniform crouches behind home plate, ready to catch the ball. Spectators are watching from behind a chain-link fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.5, "ram_available_mb": 100231.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 26.78, "peak": 34.65, "min": 21.27}, "VIN": {"avg": 65.44, "peak": 95.28, "min": 58.14}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.78, "energy_joules_est": 30.22, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T18:14:14.217341"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 843.153, "latencies_ms": [843.153], "images_per_second": 1.186, "prompt_tokens": 18, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The batter is wearing a red uniform. The field appears to be well-maintained and appears to be made of a hard, potentially synthetic material. The lighting suggests an outdoor setting on sunny days.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.16, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 68.58, "peak": 96.27, "min": 60.22}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.16, "energy_joules_est": 23.76, "sample_count": 6, "duration_seconds": 0.844}, "timestamp": "2026-01-17T18:14:15.066660"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1048.616, "latencies_ms": [1048.616], "images_per_second": 0.954, "prompt_tokens": 8, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A tall glass filled with a creamy beverage, possibly a milkshake or a smoothie, topped with whipped cream, sits next to a slice of layered vanilla cake on a white plate, accompanied by a fork and a napkin.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.82, "peak": 35.45, "min": 21.66}, "VIN": {"avg": 61.88, "peak": 82.88, "min": 55.05}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.82, "energy_joules_est": 29.19, "sample_count": 8, "duration_seconds": 1.049}, "timestamp": "2026-01-17T18:14:16.126721"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1091.685, "latencies_ms": [1091.685], "images_per_second": 0.916, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "Cake: 1\nMilkshake: 1\nFork: 2\nNapkin: 1\nTable: 1\nGlass: 1\nWater: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.52, "peak": 35.04, "min": 21.66}, "VIN": {"avg": 62.12, "peak": 83.6, "min": 52.95}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 30.06, "sample_count": 8, "duration_seconds": 1.092}, "timestamp": "2026-01-17T18:14:17.224716"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 991.446, "latencies_ms": [991.446], "images_per_second": 1.009, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The cake and milkshake are placed in the foreground of the image, with the milkshake positioned slightly behind and to the left of the cake. The background of the image is blurred, indicating a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.3, "peak": 34.65, "min": 22.45}, "VIN": {"avg": 61.05, "peak": 71.65, "min": 54.79}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.3, "energy_joules_est": 28.07, "sample_count": 7, "duration_seconds": 0.992}, "timestamp": "2026-01-17T18:14:18.222005"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1167.24, "latencies_ms": [1167.24], "images_per_second": 0.857, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a cozy setting with a table featuring a slice of layered cake, a tall glass of chocolate milkshake with whipped cream, and two forks. The table is situated in a restaurant or cafe, with other tables and chairs visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 27.05, "peak": 35.44, "min": 20.88}, "VIN": {"avg": 63.58, "peak": 80.83, "min": 59.16}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.05, "energy_joules_est": 31.59, "sample_count": 9, "duration_seconds": 1.168}, "timestamp": "2026-01-17T18:14:19.396039"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 891.371, "latencies_ms": [891.371], "images_per_second": 1.122, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The milkshake is brown and appears creamy. The cake is light yellow and looks moist and fluffy. The table is dark brown, and the lighting is soft and warm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 29.21, "peak": 34.66, "min": 23.64}, "VIN": {"avg": 63.77, "peak": 81.83, "min": 56.88}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.21, "energy_joules_est": 26.05, "sample_count": 6, "duration_seconds": 0.892}, "timestamp": "2026-01-17T18:14:20.297508"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 672.515, "latencies_ms": [672.515], "images_per_second": 1.487, "prompt_tokens": 8, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A three-tiered wedding cake, featuring white frosting, blue and gold accents, and floral decorations, is prominently displayed on a table covered with a blue tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 24.11, "peak": 27.57, "min": 21.66}, "VIN": {"avg": 60.22, "peak": 63.25, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.11, "energy_joules_est": 16.23, "sample_count": 5, "duration_seconds": 0.673}, "timestamp": "2026-01-17T18:14:20.980822"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 983.063, "latencies_ms": [983.063], "images_per_second": 1.017, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "cake: 3\nfloral arrangement: 1\ndecorative top: 1\nchandelier: 1\ntablecloth: 1\nchairs: 2\nwater view: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.84, "peak": 26.76, "min": 20.48}, "VIN": {"avg": 58.58, "peak": 62.56, "min": 54.47}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.84, "energy_joules_est": 22.46, "sample_count": 7, "duration_seconds": 0.983}, "timestamp": "2026-01-17T18:14:21.969913"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 866.601, "latencies_ms": [866.601], "images_per_second": 1.154, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The large three-tiered wedding cake dominates the foreground, positioned slightly to the right of the viewer. The dining area with tables, chairs, and umbrellas extends in the background, extending beyond the cake's immediate vicinity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.85, "peak": 25.99, "min": 20.48}, "VIN": {"avg": 60.29, "peak": 61.78, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.85, "energy_joules_est": 19.81, "sample_count": 6, "duration_seconds": 0.867}, "timestamp": "2026-01-17T18:14:22.843016"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1066.839, "latencies_ms": [1066.839], "images_per_second": 0.937, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The scene depicts a wedding reception held in a spacious room with large windows overlooking the ocean. A three-tiered wedding cake, featuring blue and gold accents, is prominently displayed on a table covered with a blue tablecloth. The cake is adorned with white flowers and a bride and groom figurine on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 22.11, "peak": 26.0, "min": 19.7}, "VIN": {"avg": 61.23, "peak": 64.97, "min": 56.44}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 22.11, "energy_joules_est": 23.6, "sample_count": 8, "duration_seconds": 1.067}, "timestamp": "2026-01-17T18:14:23.917167"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 830.311, "latencies_ms": [830.311], "images_per_second": 1.204, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The wedding cake is predominantly white with gold accents. The lighting in the room is warm and inviting, creating a pleasant atmosphere. The cake appears to be made of edible materials like fondant and possibly some type of edible flowers.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 22.65, "peak": 25.59, "min": 20.48}, "VIN": {"avg": 59.62, "peak": 64.16, "min": 55.27}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 22.65, "energy_joules_est": 18.82, "sample_count": 6, "duration_seconds": 0.831}, "timestamp": "2026-01-17T18:14:24.753638"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 571.505, "latencies_ms": [571.505], "images_per_second": 1.75, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A woman in a red and blue sweater is cooking on a stove, skillfully stirring food in a pot.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 30.13, "peak": 33.48, "min": 26.39}, "VIN": {"avg": 65.85, "peak": 77.39, "min": 60.31}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.13, "energy_joules_est": 17.23, "sample_count": 4, "duration_seconds": 0.572}, "timestamp": "2026-01-17T18:14:25.336594"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1277.615, "latencies_ms": [1277.615], "images_per_second": 0.783, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 1, "output_text": "pan: 2\nplate: 1\npot: 1\nstove: 1\nsaucepan: 1\nspices: 2\nshelves: 2\nutensils: 2\nwoman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25539.9, "ram_available_mb": 100232.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.0, "peak": 37.03, "min": 20.88}, "VIN": {"avg": 64.99, "peak": 95.57, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.0, "energy_joules_est": 34.5, "sample_count": 9, "duration_seconds": 1.278}, "timestamp": "2026-01-17T18:14:26.624323"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 823.042, "latencies_ms": [823.042], "images_per_second": 1.215, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The woman is standing near a stove and preparing food. She is positioned in the foreground of the image, with the stove and pots behind her. The kitchen setting is visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25540.2, "ram_available_mb": 100232.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.03, "peak": 33.47, "min": 22.85}, "VIN": {"avg": 64.21, "peak": 88.59, "min": 57.71}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.03, "energy_joules_est": 23.08, "sample_count": 6, "duration_seconds": 0.823}, "timestamp": "2026-01-17T18:14:27.453746"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 954.295, "latencies_ms": [954.295], "images_per_second": 1.048, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A woman is cooking in a kitchen, using a spoon to stir food on a plate. She is wearing a red and blue sweater and gray pants. The kitchen has various items and appliances, including a stove, pots, and bottles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.91, "peak": 35.45, "min": 22.06}, "VIN": {"avg": 64.06, "peak": 89.16, "min": 54.64}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.91, "energy_joules_est": 26.65, "sample_count": 7, "duration_seconds": 0.955}, "timestamp": "2026-01-17T18:14:28.414189"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 694.043, "latencies_ms": [694.043], "images_per_second": 1.441, "prompt_tokens": 18, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The woman is wearing a red and blue sweater. The kitchen has warm lighting, creating a cozy atmosphere. The scene suggests a home cooking environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 29.77, "peak": 34.65, "min": 24.82}, "VIN": {"avg": 65.18, "peak": 83.15, "min": 57.61}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.77, "energy_joules_est": 20.67, "sample_count": 5, "duration_seconds": 0.694}, "timestamp": "2026-01-17T18:14:29.115114"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 636.619, "latencies_ms": [636.619], "images_per_second": 1.571, "prompt_tokens": 8, "response_tokens_est": 28, "n_tiles": 1, "output_text": "A woman in a pink shirt and black pants is holding a rope attached to a white horse, standing in a fenced-in area.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 32.1, "peak": 36.24, "min": 27.57}, "VIN": {"avg": 66.43, "peak": 81.37, "min": 60.69}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 32.1, "energy_joules_est": 20.45, "sample_count": 4, "duration_seconds": 0.637}, "timestamp": "2026-01-17T18:14:29.763503"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1164.836, "latencies_ms": [1164.836], "images_per_second": 0.858, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "horse: 1\nrope: 1\nwoman: 1\npants: 1\nboots: 1\nbelt: 1\nshirt: 1\nfence: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.22, "peak": 37.01, "min": 20.88}, "VIN": {"avg": 63.08, "peak": 85.09, "min": 57.43}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.22, "energy_joules_est": 31.73, "sample_count": 9, "duration_seconds": 1.166}, "timestamp": "2026-01-17T18:14:30.934901"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 998.024, "latencies_ms": [998.024], "images_per_second": 1.002, "prompt_tokens": 25, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The white horse is positioned to the left of the woman, who is positioned in the foreground. The horse and woman are relatively close together, with the horse being more prominent in the foreground. The background is slightly blurred, drawing focus to the woman and the horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.58, "peak": 34.66, "min": 22.07}, "VIN": {"avg": 64.96, "peak": 92.09, "min": 54.47}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.58, "energy_joules_est": 27.53, "sample_count": 7, "duration_seconds": 0.998}, "timestamp": "2026-01-17T18:14:31.943051"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 909.986, "latencies_ms": [909.986], "images_per_second": 1.099, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "A woman is leading a white horse across a dirt field. She is wearing a pink shirt, black pants, and brown boots. The setting appears to be a fenced-in area, possibly a horse stable or riding arena.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.1, "peak": 33.88, "min": 22.85}, "VIN": {"avg": 64.47, "peak": 96.23, "min": 55.07}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.55}}, "power_watts_avg": 28.1, "energy_joules_est": 25.59, "sample_count": 6, "duration_seconds": 0.911}, "timestamp": "2026-01-17T18:14:32.859769"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 966.921, "latencies_ms": [966.921], "images_per_second": 1.034, "prompt_tokens": 18, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The horse is light gray or white. The lighting is bright, likely from natural sunlight, creating a pleasant atmosphere. The horse appears to be wearing some type of bridle or harness. The setting suggests an outdoor, natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.4, "ram_available_mb": 100233.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 63.47, "peak": 87.85, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 26.89, "sample_count": 7, "duration_seconds": 0.967}, "timestamp": "2026-01-17T18:14:33.833636"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 637.413, "latencies_ms": [637.413], "images_per_second": 1.569, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A city street is lined with tall buildings, cars, and trees, with traffic signs and streetlights visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 30.43, "peak": 34.27, "min": 26.39}, "VIN": {"avg": 61.99, "peak": 71.28, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.43, "energy_joules_est": 19.42, "sample_count": 4, "duration_seconds": 0.638}, "timestamp": "2026-01-17T18:14:34.482643"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1401.9, "latencies_ms": [1401.9], "images_per_second": 0.713, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 1, "output_text": "7AM-7PM sign: 1\nBicycle: 1\nTraffic light: 2\nStreet sign: 2\nBus: 1\nCars: 4\nTruck: 1\nTrees: 4\nBuildings: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.1, "ram_available_mb": 100234.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.31, "peak": 36.63, "min": 20.49}, "VIN": {"avg": 61.48, "peak": 82.2, "min": 55.55}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 26.31, "energy_joules_est": 36.9, "sample_count": 10, "duration_seconds": 1.402}, "timestamp": "2026-01-17T18:14:35.890510"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1002.947, "latencies_ms": [1002.947], "images_per_second": 0.997, "prompt_tokens": 25, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The foreground features a street with cars, trees, and street signs. The background showcases a city street with buildings, cars, and traffic lights. The perspective suggests the viewer is standing on the sidewalk, observing the city's layout.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.18, "peak": 33.47, "min": 22.06}, "VIN": {"avg": 65.5, "peak": 94.4, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.18, "energy_joules_est": 27.28, "sample_count": 7, "duration_seconds": 1.004}, "timestamp": "2026-01-17T18:14:36.899909"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 907.063, "latencies_ms": [907.063], "images_per_second": 1.102, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The scene depicts a city street on a foggy day, with cars driving down the road and buildings lining the sides. Traffic lights and street signs are visible, indicating a controlled urban environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25538.6, "ram_available_mb": 100233.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.42, "peak": 34.65, "min": 23.24}, "VIN": {"avg": 63.61, "peak": 75.43, "min": 59.13}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 25.79, "sample_count": 6, "duration_seconds": 0.908}, "timestamp": "2026-01-17T18:14:37.812970"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1103.568, "latencies_ms": [1103.568], "images_per_second": 0.906, "prompt_tokens": 18, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The scene is dominated by muted colors due to the overcast sky and bare trees. The lighting is soft and diffused, contributing to the overall subdued atmosphere. Visible materials include brick, glass, and metal, typical of urban structures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.3, "ram_available_mb": 100235.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.93, "peak": 34.65, "min": 21.28}, "VIN": {"avg": 64.73, "peak": 94.91, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.93, "energy_joules_est": 29.73, "sample_count": 8, "duration_seconds": 1.104}, "timestamp": "2026-01-17T18:14:38.923894"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 531.313, "latencies_ms": [531.313], "images_per_second": 1.882, "prompt_tokens": 8, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A stainless steel toilet with a visible bowl and seat is situated in a tiled bathroom, accompanied by a blue toilet brush.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.3, "ram_available_mb": 100235.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 24.69, "peak": 26.39, "min": 23.24}, "VIN": {"avg": 59.15, "peak": 60.2, "min": 57.37}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 24.69, "energy_joules_est": 13.13, "sample_count": 3, "duration_seconds": 0.532}, "timestamp": "2026-01-17T18:14:39.465376"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1726.648, "latencies_ms": [1726.648], "images_per_second": 0.579, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 1, "output_text": "toilet: 1\ntoilet brush: 1\ntoilet cleaner: 1\ntoilet seat: 1\ntoilet bowl: 1\ntoilet lid: 1\ntoilet base: 1\ntoilet flushing mechanism: 1\ntoilet seat cover: 1\ntoilet seat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 13.79}, "VDD_GPU": {"avg": 21.4, "peak": 27.18, "min": 19.3}, "VIN": {"avg": 58.75, "peak": 62.89, "min": 52.54}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 21.4, "energy_joules_est": 36.96, "sample_count": 13, "duration_seconds": 1.727}, "timestamp": "2026-01-17T18:14:41.198537"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 709.305, "latencies_ms": [709.305], "images_per_second": 1.41, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The toilet is positioned in the foreground, slightly to the right of the image. The toilet brush is situated near the toilet, closer to the viewer. The tiled floor extends from the toilet towards the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 22.77, "peak": 25.21, "min": 20.89}, "VIN": {"avg": 59.0, "peak": 63.71, "min": 51.74}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 22.77, "energy_joules_est": 16.16, "sample_count": 5, "duration_seconds": 0.71}, "timestamp": "2026-01-17T18:14:41.914220"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 788.69, "latencies_ms": [788.69], "images_per_second": 1.268, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The scene depicts a bathroom interior with a stainless steel toilet, tiled walls, and a metal frame. A toilet brush and a person's feet are visible, suggesting the bathroom is clean and ready for use.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.6, "ram_available_mb": 100235.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25535.9, "ram_available_mb": 100236.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 23.04, "peak": 26.39, "min": 20.48}, "VIN": {"avg": 58.24, "peak": 61.17, "min": 55.06}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 23.04, "energy_joules_est": 18.18, "sample_count": 6, "duration_seconds": 0.789}, "timestamp": "2026-01-17T18:14:42.708620"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 655.337, "latencies_ms": [655.337], "images_per_second": 1.526, "prompt_tokens": 18, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The bathroom features a metallic toilet bowl and silver fixtures. The floor is tiled with beige tiles. The lighting is bright, likely from overhead fixtures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.6, "ram_available_mb": 100236.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 25535.4, "ram_available_mb": 100236.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 23.53, "peak": 25.99, "min": 21.66}, "VIN": {"avg": 57.21, "peak": 61.46, "min": 53.65}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 23.53, "energy_joules_est": 15.43, "sample_count": 4, "duration_seconds": 0.656}, "timestamp": "2026-01-17T18:14:43.370103"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 680.483, "latencies_ms": [680.483], "images_per_second": 1.47, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A pink bicycle with a flower garland on the handlebars is parked in a bike shop, surrounded by other bicycles in various colors.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25535.4, "ram_available_mb": 100236.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25535.4, "ram_available_mb": 100236.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 29.39, "peak": 34.27, "min": 24.42}, "VIN": {"avg": 65.68, "peak": 85.43, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.39, "energy_joules_est": 20.01, "sample_count": 5, "duration_seconds": 0.681}, "timestamp": "2026-01-17T18:14:44.064052"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1499.857, "latencies_ms": [1499.857], "images_per_second": 0.667, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 1, "output_text": "bicycle: 5\nbicycle wheel: 4\nbicycle seat: 2\nbicycle handlebars: 2\nbicycle frame: 1\nbicycle seat: 1\nbicycle tire: 2\nbicycle wheel rim: 2\nbicycle pedals: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.4, "ram_available_mb": 100236.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25535.4, "ram_available_mb": 100236.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 13.9}, "VDD_GPU": {"avg": 25.39, "peak": 36.24, "min": 19.7}, "VIN": {"avg": 62.77, "peak": 92.14, "min": 56.03}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 25.39, "energy_joules_est": 38.09, "sample_count": 11, "duration_seconds": 1.5}, "timestamp": "2026-01-17T18:14:45.570311"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1015.009, "latencies_ms": [1015.009], "images_per_second": 0.985, "prompt_tokens": 25, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The main object, a pink bicycle, is positioned in the foreground, angled towards the viewer. The background features other bicycles and accessories, creating a sense of depth and perspective. The bicycle is situated close to the viewer, drawing attention to its design and color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.4, "ram_available_mb": 100236.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25535.6, "ram_available_mb": 100236.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 26.55, "peak": 32.68, "min": 21.66}, "VIN": {"avg": 62.19, "peak": 74.61, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.55, "energy_joules_est": 26.97, "sample_count": 7, "duration_seconds": 1.016}, "timestamp": "2026-01-17T18:14:46.592133"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1128.815, "latencies_ms": [1128.815], "images_per_second": 0.886, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a bike shop with a row of bicycles neatly lined up against a wall. The bikes are various colors, including pink and blue, and feature decorative elements like flowers. The shop has a light-colored wooden floor and displays additional bikes and accessories.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25535.6, "ram_available_mb": 100236.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.6, "ram_available_mb": 100236.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 26.24, "peak": 34.26, "min": 20.88}, "VIN": {"avg": 65.0, "peak": 91.92, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.24, "energy_joules_est": 29.63, "sample_count": 8, "duration_seconds": 1.129}, "timestamp": "2026-01-17T18:14:47.727082"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 818.694, "latencies_ms": [818.694], "images_per_second": 1.221, "prompt_tokens": 18, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The pink bicycle stands out against the light wood floor. The bicycle is adorned with a floral garland, adding a touch of color and festive flair to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.6, "ram_available_mb": 100236.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 27.64, "peak": 33.85, "min": 22.85}, "VIN": {"avg": 63.5, "peak": 77.11, "min": 59.83}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.64, "energy_joules_est": 22.64, "sample_count": 6, "duration_seconds": 0.819}, "timestamp": "2026-01-17T18:14:48.551975"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 738.212, "latencies_ms": [738.212], "images_per_second": 1.355, "prompt_tokens": 8, "response_tokens_est": 31, "n_tiles": 1, "output_text": "A majestic giraffe with a long neck and distinctive brown and tan spots stands tall in a dry savanna landscape, surrounded by sparse trees and shrubs.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 30.17, "peak": 35.45, "min": 24.82}, "VIN": {"avg": 66.89, "peak": 90.36, "min": 59.85}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.17, "energy_joules_est": 22.28, "sample_count": 5, "duration_seconds": 0.738}, "timestamp": "2026-01-17T18:14:49.302516"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 884.218, "latencies_ms": [884.218], "images_per_second": 1.131, "prompt_tokens": 21, "response_tokens_est": 27, "n_tiles": 1, "output_text": "giraffe: 1\ntrees: 2\nshrubs: 2\nsky: 1\nground: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 29.08, "peak": 35.83, "min": 23.64}, "VIN": {"avg": 63.53, "peak": 78.8, "min": 58.16}, "VDD_CPU_SOC_MSS": {"avg": 14.63, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 29.08, "energy_joules_est": 25.74, "sample_count": 6, "duration_seconds": 0.885}, "timestamp": "2026-01-17T18:14:50.193436"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 859.712, "latencies_ms": [859.712], "images_per_second": 1.163, "prompt_tokens": 25, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The giraffe is positioned to the right of the image, partially in the foreground. The background consists of trees, bushes, and a cloudy sky, suggesting the giraffe is in a natural, outdoor environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.1, "ram_available_mb": 100238.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25534.3, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.62, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 64.23, "peak": 77.47, "min": 59.03}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 28.62, "energy_joules_est": 24.61, "sample_count": 6, "duration_seconds": 0.86}, "timestamp": "2026-01-17T18:14:51.059055"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 796.531, "latencies_ms": [796.531], "images_per_second": 1.255, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A giraffe stands in a dry, grassy savanna, gazing towards the right side of the frame. The landscape is dotted with sparse trees and shrubs, under a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.3, "ram_available_mb": 100237.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.61, "peak": 35.04, "min": 23.24}, "VIN": {"avg": 63.46, "peak": 81.25, "min": 55.77}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.61, "energy_joules_est": 22.81, "sample_count": 6, "duration_seconds": 0.797}, "timestamp": "2026-01-17T18:14:51.862236"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 848.539, "latencies_ms": [848.539], "images_per_second": 1.178, "prompt_tokens": 18, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The giraffe's coat is a mosaic of brown and tan spots. Its neck is long and slender. The lighting suggests an overcast sky, giving the scene a subdued atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.6, "ram_available_mb": 100237.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 28.88, "peak": 35.44, "min": 23.24}, "VIN": {"avg": 65.14, "peak": 95.02, "min": 54.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 24.52, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T18:14:52.717070"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 649.543, "latencies_ms": [649.543], "images_per_second": 1.54, "prompt_tokens": 8, "response_tokens_est": 23, "n_tiles": 1, "output_text": "Two young children are sitting atop luggage on a shopping cart, waiting in line at a busy budget car rental station.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.4, "min": 13.79}, "VDD_GPU": {"avg": 30.73, "peak": 34.66, "min": 26.4}, "VIN": {"avg": 64.04, "peak": 77.64, "min": 55.86}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.95, "min": 14.16}}, "power_watts_avg": 30.73, "energy_joules_est": 19.98, "sample_count": 4, "duration_seconds": 0.65}, "timestamp": "2026-01-17T18:14:53.382878"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 890.704, "latencies_ms": [890.704], "images_per_second": 1.123, "prompt_tokens": 21, "response_tokens_est": 26, "n_tiles": 1, "output_text": "car: 4\nsuitcase: 2\nluggage cart: 1\nchild: 2\nwoman: 1\nsign: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25534.8, "ram_available_mb": 100237.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 29.34, "peak": 36.23, "min": 23.64}, "VIN": {"avg": 64.51, "peak": 87.21, "min": 55.89}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.34, "energy_joules_est": 26.15, "sample_count": 6, "duration_seconds": 0.891}, "timestamp": "2026-01-17T18:14:54.284338"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1220.779, "latencies_ms": [1220.779], "images_per_second": 0.819, "prompt_tokens": 25, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The foreground features two children sitting on luggage carts, positioned between the cars on the left and the cars on the right. The cars are parked in a parking lot, indicating a relatively close proximity. The children's presence and the presence of luggage carts suggest they are in a parking area or near a parking garage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 26.3, "peak": 35.06, "min": 20.88}, "VIN": {"avg": 64.96, "peak": 93.35, "min": 59.32}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.3, "energy_joules_est": 32.12, "sample_count": 9, "duration_seconds": 1.221}, "timestamp": "2026-01-17T18:14:55.511719"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 904.408, "latencies_ms": [904.408], "images_per_second": 1.106, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Two young children are traveling in a luggage cart amidst a busy parking lot filled with various cars. The scene suggests a travel-related event or activity, possibly a family vacation or a public transportation hub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.52, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 64.14, "peak": 84.66, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.52, "energy_joules_est": 24.9, "sample_count": 7, "duration_seconds": 0.905}, "timestamp": "2026-01-17T18:14:56.422343"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 927.418, "latencies_ms": [927.418], "images_per_second": 1.078, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The children appear to be wearing light-colored clothing. The scene is well-lit, suggesting sunny weather. The luggage on the cart is dark gray or black. The cars are parked in a somewhat congested area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.8, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 65.88, "peak": 96.87, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.8, "energy_joules_est": 25.8, "sample_count": 7, "duration_seconds": 0.928}, "timestamp": "2026-01-17T18:14:57.357505"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 611.295, "latencies_ms": [611.295], "images_per_second": 1.636, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A blue and yellow bus is traveling down a road in a small town, passing a flower bed and passing a building with red trim.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 24.62, "peak": 27.18, "min": 22.45}, "VIN": {"avg": 60.64, "peak": 63.38, "min": 59.44}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 24.62, "energy_joules_est": 15.07, "sample_count": 4, "duration_seconds": 0.612}, "timestamp": "2026-01-17T18:14:57.981325"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 946.935, "latencies_ms": [946.935], "images_per_second": 1.056, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "bus: 1\nvan: 1\nstreet light: 2\nbuildings: 3\ntrees: 4\nhill: 5\ngrass: 6\nflowers: 7", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25535.1, "ram_available_mb": 100237.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 13.79}, "VDD_GPU": {"avg": 22.9, "peak": 27.18, "min": 20.09}, "VIN": {"avg": 60.67, "peak": 65.05, "min": 57.83}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 22.9, "energy_joules_est": 21.7, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T18:14:58.934783"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 848.331, "latencies_ms": [848.331], "images_per_second": 1.179, "prompt_tokens": 25, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The bus is positioned in the foreground, moving towards the left side of the image. The street and buildings are in the background, creating a sense of depth and perspective. The bus is relatively close to the viewer, while the buildings and the street are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25535.3, "ram_available_mb": 100236.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 22.72, "peak": 26.0, "min": 20.49}, "VIN": {"avg": 60.12, "peak": 63.98, "min": 54.92}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 22.72, "energy_joules_est": 19.29, "sample_count": 6, "duration_seconds": 0.849}, "timestamp": "2026-01-17T18:14:59.789393"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1254.166, "latencies_ms": [1254.166], "images_per_second": 0.797, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 1, "output_text": "The scene depicts a bus stop in a small town with a yellow and blue bus parked. The bus is labeled \"Ink\" and appears to be picking up passengers. Passengers are waiting near the bus stop, and several cars are parked nearby. The town is nestled against a hillside with trees and buildings, creating a picturesque and tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 21.71, "peak": 26.01, "min": 19.3}, "VIN": {"avg": 60.85, "peak": 63.92, "min": 57.35}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.16}}, "power_watts_avg": 21.71, "energy_joules_est": 27.24, "sample_count": 9, "duration_seconds": 1.255}, "timestamp": "2026-01-17T18:15:01.050322"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 682.297, "latencies_ms": [682.297], "images_per_second": 1.466, "prompt_tokens": 18, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The bus is predominantly yellow and blue. The lighting is bright, likely from street lamps. The bus appears to be made of metal and plastic. The weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.8, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4216.4, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.9}, "VDD_GPU": {"avg": 22.85, "peak": 25.6, "min": 20.88}, "VIN": {"avg": 60.12, "peak": 61.48, "min": 59.25}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 22.85, "energy_joules_est": 15.6, "sample_count": 5, "duration_seconds": 0.683}, "timestamp": "2026-01-17T18:15:01.738722"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 630.299, "latencies_ms": [630.299], "images_per_second": 1.587, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A large brown bird, possibly a pelican, perches on a rocky outcropping overlooking the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.9}, "VDD_GPU": {"avg": 30.02, "peak": 33.88, "min": 25.99}, "VIN": {"avg": 68.37, "peak": 93.82, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 14.56, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 30.02, "energy_joules_est": 18.93, "sample_count": 4, "duration_seconds": 0.631}, "timestamp": "2026-01-17T18:15:02.379372"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1206.855, "latencies_ms": [1206.855], "images_per_second": 0.829, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "pelican: 1\nbeach: 1\nrocks: 1\nocean: 1\nhills: 1\ntrees: 1\nthatched umbrellas: 1\nwooden pier: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.92, "peak": 37.03, "min": 20.88}, "VIN": {"avg": 64.04, "peak": 82.57, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 26.92, "energy_joules_est": 32.5, "sample_count": 9, "duration_seconds": 1.207}, "timestamp": "2026-01-17T18:15:03.593004"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 755.41, "latencies_ms": [755.41], "images_per_second": 1.324, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The bird is positioned in the foreground, close to the rocky outcrop and beach. The beach extends in the background, separating the foreground from the distant hills and buildings.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.2}, "VDD_GPU": {"avg": 29.23, "peak": 34.26, "min": 24.43}, "VIN": {"avg": 63.4, "peak": 83.95, "min": 53.95}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.23, "energy_joules_est": 22.09, "sample_count": 5, "duration_seconds": 0.756}, "timestamp": "2026-01-17T18:15:04.354496"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1071.854, "latencies_ms": [1071.854], "images_per_second": 0.933, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The scene depicts a coastal area with a rocky shore, calm blue water, and a cloudy sky. A pelican is perched on a rock, gazing out at the ocean, while a beach with straw umbrellas and lounge chairs is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 26.83, "peak": 35.45, "min": 21.27}, "VIN": {"avg": 63.49, "peak": 83.05, "min": 57.86}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.35, "min": 14.16}}, "power_watts_avg": 26.83, "energy_joules_est": 28.77, "sample_count": 8, "duration_seconds": 1.072}, "timestamp": "2026-01-17T18:15:05.432196"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 928.956, "latencies_ms": [928.956], "images_per_second": 1.076, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The sky is a mix of blue and white clouds. The water is a calm, light blue-green. The scene is illuminated by soft, diffused light, suggesting either early morning or late afternoon light.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25537.2, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.35, "peak": 34.27, "min": 22.06}, "VIN": {"avg": 64.68, "peak": 91.49, "min": 58.93}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.35, "energy_joules_est": 25.42, "sample_count": 7, "duration_seconds": 0.929}, "timestamp": "2026-01-17T18:15:06.367131"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 682.822, "latencies_ms": [682.822], "images_per_second": 1.465, "prompt_tokens": 8, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A young man wearing glasses sits in a blue armchair, holding a toothbrush in his right hand and a brown paper bag in his left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.2, "ram_available_mb": 100234.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 29.63, "peak": 34.68, "min": 24.82}, "VIN": {"avg": 64.2, "peak": 83.95, "min": 54.32}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 29.63, "energy_joules_est": 20.24, "sample_count": 5, "duration_seconds": 0.683}, "timestamp": "2026-01-17T18:15:07.062276"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1102.232, "latencies_ms": [1102.232], "images_per_second": 0.907, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 1, "output_text": "bag: 1\ntoothbrush: 1\nglasses: 1\nchair: 1\nman: 1\npaper: 1\nremote: 2\nwatch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 27.67, "peak": 36.24, "min": 21.66}, "VIN": {"avg": 64.01, "peak": 90.77, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.17}}, "power_watts_avg": 27.67, "energy_joules_est": 30.51, "sample_count": 8, "duration_seconds": 1.103}, "timestamp": "2026-01-17T18:15:08.176160"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 933.666, "latencies_ms": [933.666], "images_per_second": 1.071, "prompt_tokens": 25, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The man is sitting in a chair, holding the toothbrush in his right hand and reading a newspaper in his left hand. The newspaper is positioned in the foreground, while the toothbrush and newspaper are situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 27.23, "peak": 33.47, "min": 22.07}, "VIN": {"avg": 63.93, "peak": 73.7, "min": 60.75}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.23, "energy_joules_est": 25.43, "sample_count": 7, "duration_seconds": 0.934}, "timestamp": "2026-01-17T18:15:09.115903"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 907.091, "latencies_ms": [907.091], "images_per_second": 1.102, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "A man is sitting in a blue armchair, holding a toothbrush in his right hand and a brown paper bag in his left. He is wearing glasses and a plaid shirt, and appears to be in a living room or similar indoor setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25537.0, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.76, "peak": 35.07, "min": 23.24}, "VIN": {"avg": 64.29, "peak": 87.83, "min": 53.88}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 26.1, "sample_count": 6, "duration_seconds": 0.908}, "timestamp": "2026-01-17T18:15:10.033402"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 983.531, "latencies_ms": [983.531], "images_per_second": 1.017, "prompt_tokens": 18, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The room is dimly lit, creating a warm ambiance. The walls are painted a light color, and the chair appears to be made of a soft, textured fabric. The man is holding a toothbrush and a bag of chips.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.69}, "VDD_GPU": {"avg": 27.12, "peak": 34.26, "min": 21.66}, "VIN": {"avg": 62.76, "peak": 85.84, "min": 50.51}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 27.12, "energy_joules_est": 26.69, "sample_count": 7, "duration_seconds": 0.984}, "timestamp": "2026-01-17T18:15:11.022982"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 710.504, "latencies_ms": [710.504], "images_per_second": 1.407, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "A tennis player in an orange shirt and black shorts is poised to strike a tennis ball with a blue racket on a green court.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.14, "peak": 33.86, "min": 24.41}, "VIN": {"avg": 67.22, "peak": 97.78, "min": 54.74}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.14, "energy_joules_est": 20.72, "sample_count": 5, "duration_seconds": 0.711}, "timestamp": "2026-01-17T18:15:11.744532"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1064.851, "latencies_ms": [1064.851], "images_per_second": 0.939, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 1, "output_text": "Tennis racket: 1\nTennis ball: 1\nTennis net: 1\nTennis shoes: 2\nTennis court: 4\nTennis player: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.37, "peak": 35.83, "min": 21.27}, "VIN": {"avg": 60.58, "peak": 75.07, "min": 54.59}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 27.37, "energy_joules_est": 29.16, "sample_count": 8, "duration_seconds": 1.065}, "timestamp": "2026-01-17T18:15:12.816382"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 887.19, "latencies_ms": [887.19], "images_per_second": 1.127, "prompt_tokens": 25, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The tennis player is positioned near the net, preparing to hit the ball. The tennis ball is visible in the air, near the player's racket. The player's stance suggests they are actively engaged in the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.22, "peak": 34.26, "min": 22.85}, "VIN": {"avg": 63.2, "peak": 80.12, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 28.22, "energy_joules_est": 25.05, "sample_count": 6, "duration_seconds": 0.888}, "timestamp": "2026-01-17T18:15:13.709670"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 793.848, "latencies_ms": [793.848], "images_per_second": 1.26, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "A tennis player is preparing to hit a tennis ball during a match on a green court. He holds a blue tennis racket and is focused on the incoming ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 29.78, "peak": 34.66, "min": 24.82}, "VIN": {"avg": 66.24, "peak": 84.33, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 23.66, "sample_count": 5, "duration_seconds": 0.795}, "timestamp": "2026-01-17T18:15:14.511043"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 774.015, "latencies_ms": [774.015], "images_per_second": 1.292, "prompt_tokens": 18, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The tennis player is wearing an orange shirt and orange and white shoes. The green tennis court is well-lit, and the player appears to be focused on the incoming ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 14.0}, "VDD_GPU": {"avg": 30.01, "peak": 35.04, "min": 24.82}, "VIN": {"avg": 63.1, "peak": 79.59, "min": 55.88}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.01, "energy_joules_est": 23.24, "sample_count": 5, "duration_seconds": 0.774}, "timestamp": "2026-01-17T18:15:15.291323"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 673.564, "latencies_ms": [673.564], "images_per_second": 1.485, "prompt_tokens": 8, "response_tokens_est": 26, "n_tiles": 1, "output_text": "The kitchen is well-lit and features yellow walls, wooden cabinets, a sink, a stove, and a fire extinguisher.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 31.18, "peak": 36.23, "min": 25.59}, "VIN": {"avg": 67.22, "peak": 87.31, "min": 60.55}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 31.18, "energy_joules_est": 21.03, "sample_count": 5, "duration_seconds": 0.674}, "timestamp": "2026-01-17T18:15:15.978429"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1127.781, "latencies_ms": [1127.781], "images_per_second": 0.887, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "fire extinguisher: 1\nsink: 1\noven: 1\ncountertop: 1\ncupboard: 2\nrug: 1\ntable: 1\nwindow: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 28.9, "peak": 37.41, "min": 22.07}, "VIN": {"avg": 62.93, "peak": 88.88, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.9, "energy_joules_est": 32.61, "sample_count": 8, "duration_seconds": 1.128}, "timestamp": "2026-01-17T18:15:17.112678"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 937.458, "latencies_ms": [937.458], "images_per_second": 1.067, "prompt_tokens": 25, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The main objects are positioned in a diagonal arrangement, with the kitchen area on the left and the dining area on the right. The foreground is dominated by the dining table and chairs, while the kitchen counters and appliances are located further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.53, "peak": 34.66, "min": 22.45}, "VIN": {"avg": 63.04, "peak": 80.66, "min": 55.08}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.53, "energy_joules_est": 26.75, "sample_count": 7, "duration_seconds": 0.938}, "timestamp": "2026-01-17T18:15:18.056944"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 895.439, "latencies_ms": [895.439], "images_per_second": 1.117, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a compact, well-lit kitchen or galley area within a ship, likely used for food preparation and storage. The yellow walls and wooden cabinets create a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 29.81, "peak": 35.45, "min": 24.03}, "VIN": {"avg": 63.17, "peak": 80.54, "min": 55.37}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.81, "energy_joules_est": 26.71, "sample_count": 6, "duration_seconds": 0.896}, "timestamp": "2026-01-17T18:15:18.959512"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 993.752, "latencies_ms": [993.752], "images_per_second": 1.006, "prompt_tokens": 18, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The kitchen is painted in a pale yellow color. The lighting is bright and fluorescent, illuminating the space effectively. The materials appear to be sturdy wood and metal, contributing to the overall functional and aesthetic appeal of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.86, "peak": 35.45, "min": 22.85}, "VIN": {"avg": 64.39, "peak": 91.26, "min": 55.57}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.86, "energy_joules_est": 28.69, "sample_count": 7, "duration_seconds": 0.994}, "timestamp": "2026-01-17T18:15:19.959607"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 797.481, "latencies_ms": [797.481], "images_per_second": 1.254, "prompt_tokens": 8, "response_tokens_est": 40, "n_tiles": 1, "output_text": "A large sandwich, filled with layers of meat, cheese, lettuce, tomato, and pickles, sits on a white paper plate on a wooden table, accompanied by a computer keyboard and a phone.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.35, "peak": 34.26, "min": 23.23}, "VIN": {"avg": 62.82, "peak": 76.73, "min": 55.97}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.35, "energy_joules_est": 22.62, "sample_count": 6, "duration_seconds": 0.798}, "timestamp": "2026-01-17T18:15:20.767942"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1102.609, "latencies_ms": [1102.609], "images_per_second": 0.907, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 1, "output_text": "sandwich: 2\npickles: 2\ntomato: 1\nlettuce: 2\ncheese: 1\nham: 1\nbread: 2\npepper: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.0}, "VDD_GPU": {"avg": 27.02, "peak": 35.04, "min": 21.27}, "VIN": {"avg": 63.01, "peak": 83.56, "min": 54.68}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.02, "energy_joules_est": 29.8, "sample_count": 8, "duration_seconds": 1.103}, "timestamp": "2026-01-17T18:15:21.876742"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 924.949, "latencies_ms": [924.949], "images_per_second": 1.081, "prompt_tokens": 25, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The main object is a large sandwich placed on a paper plate. The paper plate is positioned in the foreground, slightly to the right of the sandwich. The sandwich and plate are situated on a desk, with a computer keyboard and monitor in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 27.57, "peak": 34.26, "min": 22.07}, "VIN": {"avg": 63.71, "peak": 90.81, "min": 52.79}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.57, "energy_joules_est": 25.51, "sample_count": 7, "duration_seconds": 0.925}, "timestamp": "2026-01-17T18:15:22.808569"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 806.082, "latencies_ms": [806.082], "images_per_second": 1.241, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "A large sandwich, filled with layers of meat, cheese, lettuce, tomato, and pickles, is presented on a white plate on a desk next to a computer keyboard and a telephone.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 28.23, "peak": 34.27, "min": 23.24}, "VIN": {"avg": 63.24, "peak": 75.47, "min": 57.69}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 14.96, "min": 14.16}}, "power_watts_avg": 28.23, "energy_joules_est": 22.78, "sample_count": 6, "duration_seconds": 0.807}, "timestamp": "2026-01-17T18:15:23.621346"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1600.43, "latencies_ms": [1600.43], "images_per_second": 0.625, "prompt_tokens": 18, "response_tokens_est": 88, "n_tiles": 1, "output_text": "The sandwich is predominantly white and features a variety of colorful ingredients, including lettuce, tomato, cheese, and ham. The lighting in the image is bright, illuminating the sandwich and the plate. The sandwich appears to be made with fresh ingredients, suggesting it might be a homemade or fresh sandwich rather than a pre-packaged one. The plate appears to be disposable, further supporting the idea that it's likely a takeout or prepared meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 24.98, "peak": 35.45, "min": 19.7}, "VIN": {"avg": 63.76, "peak": 90.93, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 24.98, "energy_joules_est": 39.99, "sample_count": 12, "duration_seconds": 1.601}, "timestamp": "2026-01-17T18:15:25.227886"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 673.366, "latencies_ms": [673.366], "images_per_second": 1.485, "prompt_tokens": 8, "response_tokens_est": 30, "n_tiles": 1, "output_text": "The desk is equipped with two computer monitors, a keyboard, a mouse, a tablet, and various electronic devices, creating a well-equipped workspace.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 28.76, "peak": 33.48, "min": 24.43}, "VIN": {"avg": 66.07, "peak": 83.41, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.76, "energy_joules_est": 19.38, "sample_count": 5, "duration_seconds": 0.674}, "timestamp": "2026-01-17T18:15:25.911072"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1148.372, "latencies_ms": [1148.372], "images_per_second": 0.871, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 1, "output_text": "keyboard: 2\nmouse: 1\nmousepad: 1\ntablet: 1\nmonitor: 2\ncables: 4\ncables: 2\ncamera: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 27.37, "peak": 36.23, "min": 21.27}, "VIN": {"avg": 66.7, "peak": 99.84, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.37, "energy_joules_est": 31.44, "sample_count": 8, "duration_seconds": 1.149}, "timestamp": "2026-01-17T18:15:27.066000"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 835.938, "latencies_ms": [835.938], "images_per_second": 1.196, "prompt_tokens": 25, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The main objects are positioned in a way that creates a sense of depth and perspective. The left monitor is closer, while the right monitor is further away, emphasizing the spatial relationship between the devices.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.7, "ram_available_mb": 100235.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.29, "peak": 34.27, "min": 23.24}, "VIN": {"avg": 65.23, "peak": 93.39, "min": 52.86}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 23.66, "sample_count": 6, "duration_seconds": 0.836}, "timestamp": "2026-01-17T18:15:27.908707"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 853.87, "latencies_ms": [853.87], "images_per_second": 1.171, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The scene depicts a workspace with two computer monitors, a keyboard, a mouse, and various electronic devices arranged on a desk. The monitors display different content, suggesting the computer is actively in use.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.0}, "VDD_GPU": {"avg": 28.82, "peak": 35.45, "min": 23.23}, "VIN": {"avg": 64.58, "peak": 85.18, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 24.63, "sample_count": 6, "duration_seconds": 0.855}, "timestamp": "2026-01-17T18:15:28.769102"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 792.32, "latencies_ms": [792.32], "images_per_second": 1.262, "prompt_tokens": 18, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The desk is primarily gray, reflecting the ambient lighting. The monitors display a mix of colors, including shades of blue and white. The overall setup appears organized and functional.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 28.95, "peak": 35.45, "min": 23.24}, "VIN": {"avg": 67.17, "peak": 93.23, "min": 57.84}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 22.95, "sample_count": 6, "duration_seconds": 0.793}, "timestamp": "2026-01-17T18:15:29.567717"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 695.979, "latencies_ms": [695.979], "images_per_second": 1.437, "prompt_tokens": 8, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A large group of people is depicted on a tiled floor in a public restroom, appearing to be in a joyful and excited mood.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 31.12, "peak": 35.85, "min": 25.6}, "VIN": {"avg": 62.25, "peak": 80.28, "min": 55.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 21.68, "sample_count": 5, "duration_seconds": 0.697}, "timestamp": "2026-01-17T18:15:30.275197"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 978.729, "latencies_ms": [978.729], "images_per_second": 1.022, "prompt_tokens": 21, "response_tokens_est": 28, "n_tiles": 1, "output_text": "toilet: 1\ndoor: 1\nfloor: 8\npeople: 8\nchild: 1\nman: 2\nwoman: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.4, "ram_available_mb": 100235.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 13.9}, "VDD_GPU": {"avg": 29.43, "peak": 37.41, "min": 22.85}, "VIN": {"avg": 63.49, "peak": 84.87, "min": 54.32}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.43, "energy_joules_est": 28.82, "sample_count": 7, "duration_seconds": 0.979}, "timestamp": "2026-01-17T18:15:31.260150"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 746.722, "latencies_ms": [746.722], "images_per_second": 1.339, "prompt_tokens": 25, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The large family image is positioned in the foreground, slightly to the right of the toilet. The toilet is situated in the background, further away from the main family image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.6, "ram_available_mb": 100234.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 30.64, "peak": 35.44, "min": 25.21}, "VIN": {"avg": 65.76, "peak": 86.9, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.64, "energy_joules_est": 22.9, "sample_count": 5, "duration_seconds": 0.747}, "timestamp": "2026-01-17T18:15:32.014503"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 801.35, "latencies_ms": [801.35], "images_per_second": 1.248, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 1, "output_text": "The scene depicts a bathroom interior with a large, colorful mural depicting a family. The mural is likely a decorative element designed to bring a positive atmosphere to the space.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 30.12, "peak": 36.62, "min": 24.02}, "VIN": {"avg": 62.8, "peak": 79.08, "min": 52.63}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.12, "energy_joules_est": 24.16, "sample_count": 6, "duration_seconds": 0.802}, "timestamp": "2026-01-17T18:15:32.822864"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 892.691, "latencies_ms": [892.691], "images_per_second": 1.12, "prompt_tokens": 18, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The floor is tiled with light-colored tiles. The lighting in the image appears to be natural, possibly from overhead fixtures. The materials appear to be standard bathroom tiles. The weather is not explicitly visible in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4233.6, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.1}, "VDD_GPU": {"avg": 30.26, "peak": 36.23, "min": 24.03}, "VIN": {"avg": 61.73, "peak": 72.74, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.26, "energy_joules_est": 27.02, "sample_count": 6, "duration_seconds": 0.893}, "timestamp": "2026-01-17T18:15:33.725614"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 574.306, "latencies_ms": [574.306], "images_per_second": 1.741, "prompt_tokens": 8, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A gray bird with a pointed beak perches on a thin branch, gazing upwards and to the left.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25537.4, "ram_available_mb": 100234.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.51, "min": 14.0}, "VDD_GPU": {"avg": 31.4, "peak": 35.04, "min": 27.18}, "VIN": {"avg": 66.72, "peak": 83.6, "min": 56.48}, "VDD_CPU_SOC_MSS": {"avg": 14.77, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 31.4, "energy_joules_est": 18.05, "sample_count": 4, "duration_seconds": 0.575}, "timestamp": "2026-01-17T18:15:34.311622"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 941.746, "latencies_ms": [941.746], "images_per_second": 1.062, "prompt_tokens": 21, "response_tokens_est": 32, "n_tiles": 1, "output_text": "bird: 1\nbranch: 2\ntree: 2\nleaves: 2\nfeathers: 2\neye: 1\nbeak: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.37, "peak": 37.82, "min": 22.85}, "VIN": {"avg": 63.86, "peak": 86.04, "min": 55.07}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.37, "energy_joules_est": 27.68, "sample_count": 7, "duration_seconds": 0.943}, "timestamp": "2026-01-17T18:15:35.260247"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 806.946, "latencies_ms": [806.946], "images_per_second": 1.239, "prompt_tokens": 25, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The main object, a bird, occupies the foreground, perched on a branch. The background is blurred, drawing focus to the bird. The bird is positioned near the center of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25537.1, "ram_available_mb": 100235.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.2}, "VDD_GPU": {"avg": 28.67, "peak": 34.64, "min": 23.23}, "VIN": {"avg": 66.1, "peak": 95.61, "min": 55.25}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.67, "energy_joules_est": 23.14, "sample_count": 6, "duration_seconds": 0.807}, "timestamp": "2026-01-17T18:15:36.073107"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 667.303, "latencies_ms": [667.303], "images_per_second": 1.499, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 1, "output_text": "A gray bird perches on a branch in a natural setting with blurred green foliage in the background. The bird appears to be observing its surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.81, "min": 14.1}, "VDD_GPU": {"avg": 30.33, "peak": 35.44, "min": 25.21}, "VIN": {"avg": 63.55, "peak": 76.88, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 20.26, "sample_count": 5, "duration_seconds": 0.668}, "timestamp": "2026-01-17T18:15:36.747970"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 886.358, "latencies_ms": [886.358], "images_per_second": 1.128, "prompt_tokens": 18, "response_tokens_est": 42, "n_tiles": 1, "output_text": "The bird is gray and appears to be perched on a branch. The lighting suggests an outdoor setting with natural light filtering through the leaves. The materials appear to be natural wood and possibly some bark or twigs.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25536.9, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25536.8, "ram_available_mb": 100235.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4073.3, "gpu_mem_reserved_mb": 4538.0, "gpu_max_mem_alloc_mb": 4217.5, "gpu_max_mem_reserved_mb": 4538.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 29.74, "peak": 37.03, "min": 23.64}, "VIN": {"avg": 63.3, "peak": 83.69, "min": 56.79}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 29.74, "energy_joules_est": 26.37, "sample_count": 6, "duration_seconds": 0.887}, "timestamp": "2026-01-17T18:15:37.640640"}
