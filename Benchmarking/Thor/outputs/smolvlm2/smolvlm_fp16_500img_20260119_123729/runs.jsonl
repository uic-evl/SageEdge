{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2003.835, "latencies_ms": [2003.835], "images_per_second": 0.499, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A woman is standing in a kitchen with a dining table and chairs, a television on the wall, and a fireplace in the background.", "error": null, "sys_before": {"cpu_percent": 4.9, "ram_used_mb": 26760.3, "ram_available_mb": 99011.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26761.8, "ram_available_mb": 99010.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 5632.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 5632.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.52, "peak": 108.86, "min": 27.81}, "VIN_SYS_5V0": {"avg": 13.73, "peak": 15.04, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 14.24, "peak": 15.76, "min": 12.61}, "VDD_GPU": {"avg": 27.27, "peak": 36.26, "min": 15.78}}, "power_watts_avg": 27.27, "energy_joules_est": 54.66, "sample_count": 20, "duration_seconds": 2.004}, "timestamp": "2026-01-19T12:38:01.307340"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5066.511, "latencies_ms": [5066.511], "images_per_second": 0.197, "prompt_tokens": 1113, "response_tokens_est": 128, "n_tiles": 1, "output_text": " television: 1, television stand: 1, television screen: 1, television cord: 1, television stand base: 1, television stand top: 1, television stand back: 1, television stand side: 1, television stand front: 1, television stand bottom: 1, television stand top right: 1, television stand top left: 1, television stand bottom: 1, television stand bottom right: 1, television stand bottom left: 1, television stand bottom center: 1, television stand bottom center right: 1, television stand bottom center left: ", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26761.8, "ram_available_mb": 99010.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26764.9, "ram_available_mb": 99007.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 5632.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 5632.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.82, "peak": 103.78, "min": 27.57}, "VIN_SYS_5V0": {"avg": 13.69, "peak": 14.63, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.1, "peak": 15.36, "min": 11.83}, "VDD_GPU": {"avg": 21.47, "peak": 36.65, "min": 14.99}}, "power_watts_avg": 21.47, "energy_joules_est": 108.79, "sample_count": 50, "duration_seconds": 5.067}, "timestamp": "2026-01-19T12:38:06.470189"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2830.105, "latencies_ms": [2830.105], "images_per_second": 0.353, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The living room is adjacent to the kitchen, with the dining area in between. The living room is located to the left of the kitchen, and the dining area is situated in the middle. The living room is closer to the viewer than the kitchen, which is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26764.9, "ram_available_mb": 99007.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26766.1, "ram_available_mb": 99006.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 5632.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 5632.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 59.38, "peak": 108.28, "min": 27.97}, "VIN_SYS_5V0": {"avg": 13.61, "peak": 14.73, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.2, "peak": 15.36, "min": 11.83}, "VDD_GPU": {"avg": 24.51, "peak": 36.26, "min": 13.41}}, "power_watts_avg": 24.51, "energy_joules_est": 69.38, "sample_count": 28, "duration_seconds": 2.831}, "timestamp": "2026-01-19T12:38:09.382316"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1603.322, "latencies_ms": [1603.322], "images_per_second": 0.624, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A woman is standing in a kitchen with a dining table and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26766.1, "ram_available_mb": 99006.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26074.8, "ram_available_mb": 99697.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 5632.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 5632.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.0, "peak": 110.03, "min": 28.33}, "VIN_SYS_5V0": {"avg": 13.44, "peak": 14.73, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.16, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 28.57, "peak": 35.86, "min": 13.8}}, "power_watts_avg": 28.57, "energy_joules_est": 45.82, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T12:38:11.052208"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2392.231, "latencies_ms": [2392.231], "images_per_second": 0.418, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The room is bathed in warm yellow light, with hardwood floors and a large window letting in natural light. The walls are painted a vibrant green, and the furniture is made of wood and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26074.8, "ram_available_mb": 99697.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26072.6, "ram_available_mb": 99699.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 5632.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 5632.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.61, "peak": 104.15, "min": 26.96}, "VIN_SYS_5V0": {"avg": 13.53, "peak": 14.63, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.1, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 26.38, "peak": 37.04, "min": 16.16}}, "power_watts_avg": 26.38, "energy_joules_est": 63.12, "sample_count": 24, "duration_seconds": 2.393}, "timestamp": "2026-01-19T12:38:13.548251"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2427.133, "latencies_ms": [2427.133], "images_per_second": 0.412, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image features a large brown bear with a thick coat of fur, sitting on a grassy field and looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 10.5, "ram_used_mb": 26072.6, "ram_available_mb": 99699.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26056.8, "ram_available_mb": 99715.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.03, "peak": 110.45, "min": 28.47}, "VIN_SYS_5V0": {"avg": 13.62, "peak": 15.24, "min": 10.63}, "VDD_CPU_SOC_MSS": {"avg": 14.3, "peak": 16.14, "min": 11.43}, "VDD_GPU": {"avg": 27.36, "peak": 38.23, "min": 13.8}}, "power_watts_avg": 27.36, "energy_joules_est": 66.42, "sample_count": 24, "duration_seconds": 2.427}, "timestamp": "2026-01-19T12:38:16.062056"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3372.932, "latencies_ms": [3372.932], "images_per_second": 0.296, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bear: 1\n2. grass: 1\n3. nose: 1\n4. eyes: 1\n5. mouth: 1\n6. ears: 1\n7. fur: 1\n8. fur color: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26056.8, "ram_available_mb": 99715.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26065.0, "ram_available_mb": 99707.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.63, "peak": 110.26, "min": 28.44}, "VIN_SYS_5V0": {"avg": 13.7, "peak": 15.34, "min": 10.94}, "VDD_CPU_SOC_MSS": {"avg": 14.28, "peak": 16.14, "min": 11.82}, "VDD_GPU": {"avg": 25.66, "peak": 38.23, "min": 15.38}}, "power_watts_avg": 25.66, "energy_joules_est": 86.56, "sample_count": 33, "duration_seconds": 3.373}, "timestamp": "2026-01-19T12:38:19.497042"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2666.332, "latencies_ms": [2666.332], "images_per_second": 0.375, "prompt_tokens": 1450, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The bear is in the foreground of the image, with the green grass in the background. The bear is facing the camera, which is positioned to the left of the bear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26065.0, "ram_available_mb": 99707.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26092.6, "ram_available_mb": 99679.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.5, "peak": 105.39, "min": 29.36}, "VIN_SYS_5V0": {"avg": 13.72, "peak": 15.34, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.32, "peak": 16.14, "min": 11.82}, "VDD_GPU": {"avg": 27.57, "peak": 38.23, "min": 14.59}}, "power_watts_avg": 27.57, "energy_joules_est": 73.52, "sample_count": 26, "duration_seconds": 2.667}, "timestamp": "2026-01-19T12:38:22.215079"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2023.17, "latencies_ms": [2023.17], "images_per_second": 0.494, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A large brown bear is sitting on the grass and looking at the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26092.6, "ram_available_mb": 99679.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26095.0, "ram_available_mb": 99677.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.8, "peak": 110.2, "min": 27.81}, "VIN_SYS_5V0": {"avg": 13.74, "peak": 15.34, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.51, "peak": 16.14, "min": 12.22}, "VDD_GPU": {"avg": 30.05, "peak": 38.23, "min": 15.38}}, "power_watts_avg": 30.05, "energy_joules_est": 60.82, "sample_count": 20, "duration_seconds": 2.024}, "timestamp": "2026-01-19T12:38:24.302723"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2154.958, "latencies_ms": [2154.958], "images_per_second": 0.464, "prompt_tokens": 1442, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The bear has a brown fur coat with a black nose and is sitting on a green lawn.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26095.0, "ram_available_mb": 99677.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26095.3, "ram_available_mb": 99676.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.0, "peak": 106.3, "min": 27.68}, "VIN_SYS_5V0": {"avg": 13.7, "peak": 15.34, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.48, "peak": 16.14, "min": 12.22}, "VDD_GPU": {"avg": 30.31, "peak": 38.62, "min": 18.14}}, "power_watts_avg": 30.31, "energy_joules_est": 65.34, "sample_count": 21, "duration_seconds": 2.156}, "timestamp": "2026-01-19T12:38:26.491374"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3127.982, "latencies_ms": [3127.982], "images_per_second": 0.32, "prompt_tokens": 1432, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image depicts a cozy bedroom with a large bed covered in blue comforters, a wooden dresser with a mirror and a lamp, a bookshelf filled with books, and a window that offers a view of a lush green tree outside.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26095.3, "ram_available_mb": 99676.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26095.3, "ram_available_mb": 99676.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.89, "peak": 111.06, "min": 27.32}, "VIN_SYS_5V0": {"avg": 13.84, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.05, "peak": 38.23, "min": 16.17}}, "power_watts_avg": 26.05, "energy_joules_est": 81.51, "sample_count": 31, "duration_seconds": 3.129}, "timestamp": "2026-01-19T12:38:29.728631"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3454.06, "latencies_ms": [3454.06], "images_per_second": 0.29, "prompt_tokens": 1446, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. bed: 1\n2. mirror: 1\n3. dresser: 1\n4. bookshelf: 1\n5. window: 1\n6. books: 100\n7. plants: 4\n8. blanket: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26095.3, "ram_available_mb": 99676.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26095.3, "ram_available_mb": 99676.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.54, "peak": 115.36, "min": 27.84}, "VIN_SYS_5V0": {"avg": 13.7, "peak": 15.34, "min": 10.84}, "VDD_CPU_SOC_MSS": {"avg": 14.27, "peak": 16.14, "min": 11.83}, "VDD_GPU": {"avg": 25.35, "peak": 37.83, "min": 13.8}}, "power_watts_avg": 25.35, "energy_joules_est": 87.57, "sample_count": 34, "duration_seconds": 3.454}, "timestamp": "2026-01-19T12:38:33.280876"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3067.152, "latencies_ms": [3067.152], "images_per_second": 0.326, "prompt_tokens": 1450, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The bed is located on the left side of the room, with the window on the right side. The bookshelf is positioned in the background, near the window, while the dresser is situated in the foreground, closer to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26095.3, "ram_available_mb": 99676.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.44, "peak": 109.4, "min": 27.53}, "VIN_SYS_5V0": {"avg": 13.71, "peak": 15.34, "min": 10.94}, "VDD_CPU_SOC_MSS": {"avg": 14.31, "peak": 16.14, "min": 11.82}, "VDD_GPU": {"avg": 26.3, "peak": 37.83, "min": 13.8}}, "power_watts_avg": 26.3, "energy_joules_est": 80.68, "sample_count": 30, "duration_seconds": 3.068}, "timestamp": "2026-01-19T12:38:36.411151"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3473.779, "latencies_ms": [3473.779], "images_per_second": 0.288, "prompt_tokens": 1444, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image depicts a cozy bedroom with a large bed covered in blue comforter, a dresser with a mirror, and a bookshelf filled with books. The room is well-lit by natural light coming through a window, and there are plants placed around the room, adding a touch of greenery.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.42, "peak": 104.4, "min": 29.08}, "VIN_SYS_5V0": {"avg": 13.78, "peak": 15.34, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.34, "peak": 16.14, "min": 12.22}, "VDD_GPU": {"avg": 25.45, "peak": 38.23, "min": 14.59}}, "power_watts_avg": 25.45, "energy_joules_est": 88.42, "sample_count": 34, "duration_seconds": 3.474}, "timestamp": "2026-01-19T12:38:39.952797"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2565.785, "latencies_ms": [2565.785], "images_per_second": 0.39, "prompt_tokens": 1442, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The room is bathed in natural light from a window, the walls are adorned with floral wallpaper, and the bed is covered with a blue comforter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.53, "peak": 111.8, "min": 28.5}, "VIN_SYS_5V0": {"avg": 13.73, "peak": 15.34, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.34, "peak": 16.14, "min": 11.82}, "VDD_GPU": {"avg": 27.95, "peak": 38.23, "min": 14.59}}, "power_watts_avg": 27.95, "energy_joules_est": 71.73, "sample_count": 25, "duration_seconds": 2.566}, "timestamp": "2026-01-19T12:38:42.573527"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1552.385, "latencies_ms": [1552.385], "images_per_second": 0.644, "prompt_tokens": 1100, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A red stop sign is standing on the side of the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26095.9, "ram_available_mb": 99676.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.88, "peak": 114.2, "min": 28.13}, "VIN_SYS_5V0": {"avg": 13.55, "peak": 14.73, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.18, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 29.53, "peak": 36.65, "min": 15.38}}, "power_watts_avg": 29.53, "energy_joules_est": 45.87, "sample_count": 15, "duration_seconds": 1.553}, "timestamp": "2026-01-19T12:38:44.148595"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1293.544, "latencies_ms": [1293.544], "images_per_second": 0.773, "prompt_tokens": 1114, "response_tokens_est": 5, "n_tiles": 1, "output_text": " stop sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26095.9, "ram_available_mb": 99676.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 26095.9, "ram_available_mb": 99676.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.54, "peak": 119.1, "min": 28.09}, "VIN_SYS_5V0": {"avg": 13.67, "peak": 14.94, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 14.39, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 31.71, "peak": 37.04, "min": 19.71}}, "power_watts_avg": 31.71, "energy_joules_est": 41.03, "sample_count": 13, "duration_seconds": 1.294}, "timestamp": "2026-01-19T12:38:45.513986"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2449.107, "latencies_ms": [2449.107], "images_per_second": 0.408, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The stop sign is located in the foreground of the image, with the street and trees in the background. The stop sign is positioned to the left of the street, and the trees are located behind the sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26095.9, "ram_available_mb": 99676.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.09, "peak": 111.57, "min": 27.86}, "VIN_SYS_5V0": {"avg": 13.54, "peak": 14.63, "min": 11.06}, "VDD_CPU_SOC_MSS": {"avg": 14.15, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 27.06, "peak": 37.83, "min": 16.95}}, "power_watts_avg": 27.06, "energy_joules_est": 66.28, "sample_count": 24, "duration_seconds": 2.449}, "timestamp": "2026-01-19T12:38:48.020817"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1819.218, "latencies_ms": [1819.218], "images_per_second": 0.55, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A stop sign is on the side of the road, and there are trees and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.13, "peak": 103.87, "min": 27.73}, "VIN_SYS_5V0": {"avg": 13.47, "peak": 14.73, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.05, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 27.55, "peak": 36.26, "min": 14.59}}, "power_watts_avg": 27.55, "energy_joules_est": 50.13, "sample_count": 18, "duration_seconds": 1.82}, "timestamp": "2026-01-19T12:38:49.901380"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1649.238, "latencies_ms": [1649.238], "images_per_second": 0.606, "prompt_tokens": 1110, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The stop sign is red and white, and it is in a sunny area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.29, "peak": 107.7, "min": 28.13}, "VIN_SYS_5V0": {"avg": 13.51, "peak": 14.73, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.18, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 29.41, "peak": 36.65, "min": 16.17}}, "power_watts_avg": 29.41, "energy_joules_est": 48.51, "sample_count": 16, "duration_seconds": 1.65}, "timestamp": "2026-01-19T12:38:51.576738"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1609.06, "latencies_ms": [1609.06], "images_per_second": 0.621, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Three teddy bears of different colors are huddled together on a blue blanket.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.4, "peak": 113.45, "min": 28.81}, "VIN_SYS_5V0": {"avg": 13.6, "peak": 14.83, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.3, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 29.83, "peak": 37.04, "min": 18.53}}, "power_watts_avg": 29.83, "energy_joules_est": 48.02, "sample_count": 16, "duration_seconds": 1.61}, "timestamp": "2026-01-19T12:38:53.256868"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1317.097, "latencies_ms": [1317.097], "images_per_second": 0.759, "prompt_tokens": 1114, "response_tokens_est": 5, "n_tiles": 1, "output_text": " teddy bear: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26096.1, "ram_available_mb": 99676.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26097.3, "ram_available_mb": 99674.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.95, "peak": 108.14, "min": 28.14}, "VIN_SYS_5V0": {"avg": 13.4, "peak": 14.63, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.15, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 30.92, "peak": 37.04, "min": 17.75}}, "power_watts_avg": 30.92, "energy_joules_est": 40.74, "sample_count": 13, "duration_seconds": 1.318}, "timestamp": "2026-01-19T12:38:54.625606"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2740.769, "latencies_ms": [2740.769], "images_per_second": 0.365, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The teddy bears are positioned in a close cluster, with the brown teddy bear in the center and the orange teddy bear on the right. The blue blanket is partially visible in the foreground, while the brown teddy bear is positioned slightly behind the orange teddy bear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26097.3, "ram_available_mb": 99674.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.92, "peak": 111.96, "min": 28.46}, "VIN_SYS_5V0": {"avg": 13.59, "peak": 14.73, "min": 11.36}, "VDD_CPU_SOC_MSS": {"avg": 14.16, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 26.07, "peak": 37.82, "min": 16.16}}, "power_watts_avg": 26.07, "energy_joules_est": 71.46, "sample_count": 27, "duration_seconds": 2.741}, "timestamp": "2026-01-19T12:38:57.440619"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1918.23, "latencies_ms": [1918.23], "images_per_second": 0.521, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " Three teddy bears are sitting on a couch, one of them is brown, one is orange, and one is tan.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.51, "peak": 103.73, "min": 29.19}, "VIN_SYS_5V0": {"avg": 13.42, "peak": 14.73, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 13.99, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 26.83, "peak": 36.26, "min": 14.2}}, "power_watts_avg": 26.83, "energy_joules_est": 51.47, "sample_count": 19, "duration_seconds": 1.919}, "timestamp": "2026-01-19T12:38:59.428820"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2652.565, "latencies_ms": [2652.565], "images_per_second": 0.377, "prompt_tokens": 1110, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The teddy bears are in a variety of colors, including brown, orange, and beige. The lighting is soft and natural, coming from a window out of frame. The material of the teddy bears is plush and soft.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.52, "peak": 116.83, "min": 27.88}, "VIN_SYS_5V0": {"avg": 13.48, "peak": 14.63, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 13.92, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 25.12, "peak": 36.65, "min": 15.78}}, "power_watts_avg": 25.12, "energy_joules_est": 66.64, "sample_count": 26, "duration_seconds": 2.653}, "timestamp": "2026-01-19T12:39:02.139570"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1905.648, "latencies_ms": [1905.648], "images_per_second": 0.525, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A woman wearing a red jacket, black pants, and a black and white striped hat is skiing down a snowy hill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.31, "peak": 113.0, "min": 26.84}, "VIN_SYS_5V0": {"avg": 13.5, "peak": 14.73, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.03, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 27.57, "peak": 36.65, "min": 14.2}}, "power_watts_avg": 27.57, "energy_joules_est": 52.56, "sample_count": 19, "duration_seconds": 1.907}, "timestamp": "2026-01-19T12:39:04.132833"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3116.869, "latencies_ms": [3116.869], "images_per_second": 0.321, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. skis: 2\n2. poles: 2\n3. ski poles: 2\n4. skier: 1\n5. ski suit: 1\n6. ski boots: 1\n7. ski bindings: 1\n8. helmet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26097.1, "ram_available_mb": 99675.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26103.2, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.42, "peak": 101.04, "min": 27.66}, "VIN_SYS_5V0": {"avg": 13.61, "peak": 14.63, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.13, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 24.31, "peak": 36.26, "min": 15.78}}, "power_watts_avg": 24.31, "energy_joules_est": 75.78, "sample_count": 30, "duration_seconds": 3.117}, "timestamp": "2026-01-19T12:39:07.262787"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2642.521, "latencies_ms": [2642.521], "images_per_second": 0.378, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The skier is positioned in the foreground of the image, with the ski slope stretching out into the background. The skier is skiing towards the right side of the image, with the ski poles held in a forward-leaning position.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26101.3, "ram_available_mb": 99670.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.85, "peak": 100.77, "min": 28.31}, "VIN_SYS_5V0": {"avg": 13.77, "peak": 14.94, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 14.29, "peak": 15.75, "min": 13.0}, "VDD_GPU": {"avg": 25.26, "peak": 36.65, "min": 14.99}}, "power_watts_avg": 25.26, "energy_joules_est": 66.77, "sample_count": 26, "duration_seconds": 2.643}, "timestamp": "2026-01-19T12:39:09.977296"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1709.814, "latencies_ms": [1709.814], "images_per_second": 0.585, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman is skiing down a snowy slope with ski poles and wearing a red jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26101.3, "ram_available_mb": 99670.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26101.3, "ram_available_mb": 99670.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.49, "peak": 113.17, "min": 27.76}, "VIN_SYS_5V0": {"avg": 13.43, "peak": 14.73, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 28.26, "peak": 36.26, "min": 14.2}}, "power_watts_avg": 28.26, "energy_joules_est": 48.33, "sample_count": 17, "duration_seconds": 1.71}, "timestamp": "2026-01-19T12:39:11.754181"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2503.447, "latencies_ms": [2503.447], "images_per_second": 0.399, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The skier is wearing a vibrant red jacket and black pants, with a white and black striped beanie. The snow is pristine white, and the sky is overcast, casting a soft light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26101.3, "ram_available_mb": 99670.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26101.5, "ram_available_mb": 99670.7, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.85, "peak": 113.03, "min": 26.54}, "VIN_SYS_5V0": {"avg": 13.53, "peak": 14.73, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.04, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 25.81, "peak": 37.03, "min": 15.77}}, "power_watts_avg": 25.81, "energy_joules_est": 64.64, "sample_count": 25, "duration_seconds": 2.504}, "timestamp": "2026-01-19T12:39:14.363230"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2016.585, "latencies_ms": [2016.585], "images_per_second": 0.496, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image depicts a kitchen with wooden cabinets, a white refrigerator, and a white stove, all set against a beige tiled floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26101.5, "ram_available_mb": 99670.7, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26103.5, "ram_available_mb": 99668.7, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.85, "peak": 113.17, "min": 29.39}, "VIN_SYS_5V0": {"avg": 13.45, "peak": 14.73, "min": 10.84}, "VDD_CPU_SOC_MSS": {"avg": 14.02, "peak": 15.36, "min": 11.43}, "VDD_GPU": {"avg": 27.0, "peak": 36.65, "min": 13.8}}, "power_watts_avg": 27.0, "energy_joules_est": 54.47, "sample_count": 20, "duration_seconds": 2.017}, "timestamp": "2026-01-19T12:39:16.454326"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3191.13, "latencies_ms": [3191.13], "images_per_second": 0.313, "prompt_tokens": 1114, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. white refrigerator: 1\n2. white stove: 1\n3. white oven: 1\n4. white cabinet: 1\n5. white dishwasher: 1\n6. white sink: 1\n7. white countertop: 1\n8. white tile floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.5, "ram_available_mb": 99668.7, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26103.5, "ram_available_mb": 99668.7, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.6, "peak": 103.86, "min": 28.22}, "VIN_SYS_5V0": {"avg": 13.67, "peak": 14.73, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.18, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 24.04, "peak": 36.65, "min": 15.38}}, "power_watts_avg": 24.04, "energy_joules_est": 76.73, "sample_count": 31, "duration_seconds": 3.192}, "timestamp": "2026-01-19T12:39:19.688204"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2877.757, "latencies_ms": [2877.757], "images_per_second": 0.347, "prompt_tokens": 1118, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The stove is located to the left of the refrigerator, which is situated in the background. The sink is positioned near the stove, while the dishwasher is placed further to the right. The refrigerator is positioned closer to the foreground, with the stove and sink situated in the middle ground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26103.5, "ram_available_mb": 99668.7, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26103.2, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.36, "peak": 105.23, "min": 28.38}, "VIN_SYS_5V0": {"avg": 13.73, "peak": 14.83, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.21, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 24.62, "peak": 36.65, "min": 14.59}}, "power_watts_avg": 24.62, "energy_joules_est": 70.86, "sample_count": 28, "duration_seconds": 2.878}, "timestamp": "2026-01-19T12:39:22.610293"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2526.663, "latencies_ms": [2526.663], "images_per_second": 0.396, "prompt_tokens": 1112, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image captures a quaint, small kitchen with wooden cabinets, a white refrigerator, and a stove. The kitchen is bathed in natural light, highlighting the warm tones of the wood and the pristine white of the appliances.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26102.7, "ram_available_mb": 99669.4, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.59, "peak": 109.19, "min": 27.57}, "VIN_SYS_5V0": {"avg": 13.62, "peak": 14.83, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.12, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 25.04, "peak": 36.65, "min": 14.59}}, "power_watts_avg": 25.04, "energy_joules_est": 63.29, "sample_count": 25, "duration_seconds": 2.527}, "timestamp": "2026-01-19T12:39:25.212327"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1720.743, "latencies_ms": [1720.743], "images_per_second": 0.581, "prompt_tokens": 1110, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The kitchen is well-lit with natural light, and the cabinets are made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26102.7, "ram_available_mb": 99669.4, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.23, "peak": 113.68, "min": 28.43}, "VIN_SYS_5V0": {"avg": 13.46, "peak": 14.73, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.09, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 28.22, "peak": 36.26, "min": 14.2}}, "power_watts_avg": 28.22, "energy_joules_est": 48.57, "sample_count": 17, "duration_seconds": 1.721}, "timestamp": "2026-01-19T12:39:26.993925"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2571.428, "latencies_ms": [2571.428], "images_per_second": 0.389, "prompt_tokens": 1432, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A baseball player in a green shirt and cap is running towards the camera, while another player in a white shirt and cap is running away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.87, "peak": 114.84, "min": 27.94}, "VIN_SYS_5V0": {"avg": 13.83, "peak": 15.34, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.4, "peak": 16.14, "min": 11.82}, "VDD_GPU": {"avg": 27.92, "peak": 38.62, "min": 16.96}}, "power_watts_avg": 27.92, "energy_joules_est": 71.82, "sample_count": 25, "duration_seconds": 2.572}, "timestamp": "2026-01-19T12:39:29.610022"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3686.64, "latencies_ms": [3686.64], "images_per_second": 0.271, "prompt_tokens": 1446, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. baseball player: 2\n2. baseball player: 1\n3. baseball player: 1\n4. baseball player: 1\n5. baseball player: 1\n6. baseball player: 1\n7. baseball player: 1\n8. baseball player: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26102.7, "ram_available_mb": 99669.5, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.41, "peak": 112.3, "min": 27.97}, "VIN_SYS_5V0": {"avg": 13.68, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.32, "peak": 16.14, "min": 12.61}, "VDD_GPU": {"avg": 24.93, "peak": 38.21, "min": 15.78}}, "power_watts_avg": 24.93, "energy_joules_est": 91.92, "sample_count": 36, "duration_seconds": 3.687}, "timestamp": "2026-01-19T12:39:33.362097"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3014.51, "latencies_ms": [3014.51], "images_per_second": 0.332, "prompt_tokens": 1450, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The baseball player in the foreground is running towards the camera, while the other player is in the background, running towards the outfield. The baseball player in the foreground is closer to the camera than the other player in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26102.7, "ram_available_mb": 99669.5, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.37, "peak": 108.38, "min": 30.75}, "VIN_SYS_5V0": {"avg": 13.8, "peak": 15.34, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.42, "peak": 16.14, "min": 12.22}, "VDD_GPU": {"avg": 26.77, "peak": 38.21, "min": 14.59}}, "power_watts_avg": 26.77, "energy_joules_est": 80.72, "sample_count": 29, "duration_seconds": 3.015}, "timestamp": "2026-01-19T12:39:36.385291"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2326.064, "latencies_ms": [2326.064], "images_per_second": 0.43, "prompt_tokens": 1444, "response_tokens_est": 25, "n_tiles": 1, "output_text": " Two baseball players are running on the field. One player is wearing a green shirt and the other is wearing a white shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.24, "peak": 110.93, "min": 28.19}, "VIN_SYS_5V0": {"avg": 13.97, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 29.01, "peak": 38.6, "min": 15.77}}, "power_watts_avg": 29.01, "energy_joules_est": 67.49, "sample_count": 23, "duration_seconds": 2.326}, "timestamp": "2026-01-19T12:39:38.790573"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3165.223, "latencies_ms": [3165.223], "images_per_second": 0.316, "prompt_tokens": 1442, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image is a photograph capturing a moment of action in a baseball game. The colors are vibrant, with the green of the grass contrasting against the brown of the dirt field. The lighting is natural, suggesting it was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.93, "peak": 101.47, "min": 28.08}, "VIN_SYS_5V0": {"avg": 13.64, "peak": 15.34, "min": 10.84}, "VDD_CPU_SOC_MSS": {"avg": 14.12, "peak": 16.14, "min": 11.82}, "VDD_GPU": {"avg": 26.15, "peak": 38.23, "min": 15.78}}, "power_watts_avg": 26.15, "energy_joules_est": 82.79, "sample_count": 31, "duration_seconds": 3.166}, "timestamp": "2026-01-19T12:39:42.021467"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1856.858, "latencies_ms": [1856.858], "images_per_second": 0.539, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A tennis player is preparing to hit a tennis ball with a red and black racket on a blue court.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.27, "peak": 111.51, "min": 28.25}, "VIN_SYS_5V0": {"avg": 13.49, "peak": 14.73, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.07, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 27.94, "peak": 36.26, "min": 14.2}}, "power_watts_avg": 27.94, "energy_joules_est": 51.91, "sample_count": 18, "duration_seconds": 1.858}, "timestamp": "2026-01-19T12:39:43.913289"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3319.47, "latencies_ms": [3319.47], "images_per_second": 0.301, "prompt_tokens": 1113, "response_tokens_est": 70, "n_tiles": 1, "output_text": " 1. tennis player: 1\n2. racket: 1\n3. ball: 1\n4. tennis court: 1\n5. J.P. Morgan: 1\n6. POLO: 1\n7. man in black shirt: 1\n8. man in white shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.3, "ram_available_mb": 99668.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.26, "peak": 110.71, "min": 28.09}, "VIN_SYS_5V0": {"avg": 13.78, "peak": 14.83, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.27, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 24.3, "peak": 37.44, "min": 17.35}}, "power_watts_avg": 24.3, "energy_joules_est": 80.67, "sample_count": 32, "duration_seconds": 3.32}, "timestamp": "2026-01-19T12:39:47.247354"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2502.101, "latencies_ms": [2502.101], "images_per_second": 0.4, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The tennis player is in the foreground, with the J.P. Morgan advertisement in the background. The player is positioned to the left of the advertisement, and the advertisement is located near the baseline of the tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.24, "peak": 116.25, "min": 31.17}, "VIN_SYS_5V0": {"avg": 13.86, "peak": 14.94, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 14.41, "peak": 15.75, "min": 13.0}, "VDD_GPU": {"avg": 26.08, "peak": 36.65, "min": 14.99}}, "power_watts_avg": 26.08, "energy_joules_est": 65.27, "sample_count": 24, "duration_seconds": 2.503}, "timestamp": "2026-01-19T12:39:49.753458"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1769.73, "latencies_ms": [1769.73], "images_per_second": 0.565, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A tennis player is playing on a court with a J.P. Morgan advertisement in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.0, "peak": 107.61, "min": 27.94}, "VIN_SYS_5V0": {"avg": 13.85, "peak": 15.04, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 14.53, "peak": 15.75, "min": 13.0}, "VDD_GPU": {"avg": 28.93, "peak": 36.65, "min": 15.78}}, "power_watts_avg": 28.93, "energy_joules_est": 51.21, "sample_count": 17, "duration_seconds": 1.77}, "timestamp": "2026-01-19T12:39:51.545456"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1716.709, "latencies_ms": [1716.709], "images_per_second": 0.583, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The tennis player is wearing a white shirt and black shorts, and the court is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.03, "peak": 104.05, "min": 28.82}, "VIN_SYS_5V0": {"avg": 13.71, "peak": 14.94, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 14.3, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 29.61, "peak": 37.44, "min": 17.75}}, "power_watts_avg": 29.61, "energy_joules_est": 50.84, "sample_count": 17, "duration_seconds": 1.717}, "timestamp": "2026-01-19T12:39:53.323144"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1922.284, "latencies_ms": [1922.284], "images_per_second": 0.52, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A group of children and adults are posing for a picture on a tennis court, with one of the adults holding a trophy.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.2, "peak": 104.76, "min": 29.32}, "VIN_SYS_5V0": {"avg": 13.57, "peak": 14.73, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.16, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 28.34, "peak": 37.04, "min": 16.96}}, "power_watts_avg": 28.34, "energy_joules_est": 54.5, "sample_count": 19, "duration_seconds": 1.923}, "timestamp": "2026-01-19T12:39:55.319885"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2879.386, "latencies_ms": [2879.386], "images_per_second": 0.347, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. boy: 4\n2. boy: 2\n3. boy: 2\n4. boy: 2\n5. boy: 2\n6. boy: 2\n7. boy: 2\n8. boy: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.22, "peak": 108.83, "min": 29.33}, "VIN_SYS_5V0": {"avg": 13.7, "peak": 14.73, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.17, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 24.93, "peak": 36.65, "min": 15.78}}, "power_watts_avg": 24.93, "energy_joules_est": 71.8, "sample_count": 28, "duration_seconds": 2.88}, "timestamp": "2026-01-19T12:39:58.237160"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2923.047, "latencies_ms": [2923.047], "images_per_second": 0.342, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The group of children is standing in front of the man, with the man being in the center of the group. The tennis rackets are held by the children, with some children holding multiple rackets. The tennis court is in the background, with the fence surrounding the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.22, "peak": 113.39, "min": 33.7}, "VIN_SYS_5V0": {"avg": 13.78, "peak": 14.94, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.21, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 24.78, "peak": 36.65, "min": 14.98}}, "power_watts_avg": 24.78, "energy_joules_est": 72.44, "sample_count": 28, "duration_seconds": 2.924}, "timestamp": "2026-01-19T12:40:01.166320"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1637.871, "latencies_ms": [1637.871], "images_per_second": 0.611, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of children and adults are posing for a picture on a tennis court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.87, "peak": 110.25, "min": 27.92}, "VIN_SYS_5V0": {"avg": 13.85, "peak": 15.04, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 14.4, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 29.43, "peak": 37.04, "min": 14.98}}, "power_watts_avg": 29.43, "energy_joules_est": 48.23, "sample_count": 16, "duration_seconds": 1.639}, "timestamp": "2026-01-19T12:40:02.840422"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2050.338, "latencies_ms": [2050.338], "images_per_second": 0.488, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken on a sunny day with clear blue skies. The tennis court is blue and the players are wearing various colors of tennis attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.84, "peak": 101.46, "min": 27.98}, "VIN_SYS_5V0": {"avg": 13.72, "peak": 14.94, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.32, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 28.18, "peak": 37.04, "min": 18.14}}, "power_watts_avg": 28.18, "energy_joules_est": 57.79, "sample_count": 20, "duration_seconds": 2.051}, "timestamp": "2026-01-19T12:40:04.930708"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2179.125, "latencies_ms": [2179.125], "images_per_second": 0.459, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, a group of people are enjoying a day by the river, with a bridge arching above them, and a swan gracefully swimming in the water.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.05, "peak": 103.1, "min": 32.84}, "VIN_SYS_5V0": {"avg": 13.76, "peak": 14.83, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.33, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 27.25, "peak": 36.26, "min": 15.78}}, "power_watts_avg": 27.25, "energy_joules_est": 59.4, "sample_count": 21, "duration_seconds": 2.18}, "timestamp": "2026-01-19T12:40:07.124266"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2922.542, "latencies_ms": [2922.542], "images_per_second": 0.342, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. bridge: 1\n2. people: 4\n3. birds: 2\n4. rocks: 2\n5. river: 1\n6. stones: 1\n7. woman: 1\n8. woman's bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.04, "peak": 111.72, "min": 26.5}, "VIN_SYS_5V0": {"avg": 13.84, "peak": 15.04, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 14.3, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 24.78, "peak": 37.03, "min": 15.77}}, "power_watts_avg": 24.78, "energy_joules_est": 72.43, "sample_count": 29, "duration_seconds": 2.923}, "timestamp": "2026-01-19T12:40:10.153503"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2519.67, "latencies_ms": [2519.67], "images_per_second": 0.397, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The woman is standing on the right side of the image, while the bridge spans across the river in the background. The ducks are located near the woman, and the people are sitting on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.49, "peak": 105.15, "min": 26.5}, "VIN_SYS_5V0": {"avg": 13.57, "peak": 14.73, "min": 10.94}, "VDD_CPU_SOC_MSS": {"avg": 14.1, "peak": 15.36, "min": 11.43}, "VDD_GPU": {"avg": 25.41, "peak": 36.65, "min": 13.41}}, "power_watts_avg": 25.41, "energy_joules_est": 64.03, "sample_count": 25, "duration_seconds": 2.52}, "timestamp": "2026-01-19T12:40:12.756815"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2636.972, "latencies_ms": [2636.972], "images_per_second": 0.379, "prompt_tokens": 1111, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a serene scene of a group of people enjoying a day by the river, with a bridge arching over the water in the background. The sun is setting, casting a warm glow on the scene and highlighting the tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.37, "peak": 116.46, "min": 27.86}, "VIN_SYS_5V0": {"avg": 13.62, "peak": 14.73, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.1, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 25.13, "peak": 36.63, "min": 14.2}}, "power_watts_avg": 25.13, "energy_joules_est": 66.28, "sample_count": 26, "duration_seconds": 2.637}, "timestamp": "2026-01-19T12:40:15.459902"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2939.865, "latencies_ms": [2939.865], "images_per_second": 0.34, "prompt_tokens": 1109, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image features a woman in a black top and a long skirt, standing on a stone pavement, with a bridge in the background. The bridge is made of metal and has a curved design. The lighting is natural, with the sun shining brightly, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.9, "ram_available_mb": 99668.2, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.41, "peak": 111.8, "min": 28.03}, "VIN_SYS_5V0": {"avg": 13.7, "peak": 14.83, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.14, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 24.41, "peak": 36.24, "min": 14.59}}, "power_watts_avg": 24.41, "energy_joules_est": 71.77, "sample_count": 29, "duration_seconds": 2.94}, "timestamp": "2026-01-19T12:40:18.474193"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1638.544, "latencies_ms": [1638.544], "images_per_second": 0.61, "prompt_tokens": 1100, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman is taking a picture of herself with a Hello Kitty phone case.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.0, "peak": 106.73, "min": 28.93}, "VIN_SYS_5V0": {"avg": 13.53, "peak": 14.73, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.18, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 28.75, "peak": 36.65, "min": 14.2}}, "power_watts_avg": 28.75, "energy_joules_est": 47.13, "sample_count": 16, "duration_seconds": 1.639}, "timestamp": "2026-01-19T12:40:20.154419"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2919.19, "latencies_ms": [2919.19], "images_per_second": 0.343, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. woman: 1\n2. phone: 1\n3. ear: 1\n4. hair: 1\n5. wrist: 1\n6. hand: 1\n7. wristwatch: 1\n8. bracelet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.1, "peak": 109.77, "min": 27.82}, "VIN_SYS_5V0": {"avg": 13.74, "peak": 14.94, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.26, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 24.99, "peak": 37.03, "min": 15.77}}, "power_watts_avg": 24.99, "energy_joules_est": 72.97, "sample_count": 29, "duration_seconds": 2.92}, "timestamp": "2026-01-19T12:40:23.169067"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2992.751, "latencies_ms": [2992.751], "images_per_second": 0.334, "prompt_tokens": 1118, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The woman is holding the phone in her right hand, which is positioned in the foreground of the image. The phone is in front of her, and she is looking at it with her left hand. The background of the image is blurred, indicating that the focus is on the woman and her phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.49, "peak": 107.33, "min": 28.24}, "VIN_SYS_5V0": {"avg": 13.67, "peak": 14.73, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.17, "peak": 15.36, "min": 11.82}, "VDD_GPU": {"avg": 24.23, "peak": 36.24, "min": 13.8}}, "power_watts_avg": 24.23, "energy_joules_est": 72.53, "sample_count": 29, "duration_seconds": 2.993}, "timestamp": "2026-01-19T12:40:26.196385"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1647.193, "latencies_ms": [1647.193], "images_per_second": 0.607, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman is taking a picture of herself with a Hello Kitty phone case.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26102.4, "ram_available_mb": 99669.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26100.9, "ram_available_mb": 99671.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.49, "peak": 117.89, "min": 29.03}, "VIN_SYS_5V0": {"avg": 13.66, "peak": 14.83, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.38, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 28.7, "peak": 37.04, "min": 14.59}}, "power_watts_avg": 28.7, "energy_joules_est": 47.28, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T12:40:27.866213"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2362.962, "latencies_ms": [2362.962], "images_per_second": 0.423, "prompt_tokens": 1110, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The woman is wearing a white shirt with a black and white print, and she is holding a Hello Kitty phone case. The lighting is bright and natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26100.9, "ram_available_mb": 99671.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26100.9, "ram_available_mb": 99671.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.0, "peak": 113.7, "min": 29.26}, "VIN_SYS_5V0": {"avg": 13.8, "peak": 14.94, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 14.35, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 27.0, "peak": 37.44, "min": 17.74}}, "power_watts_avg": 27.0, "energy_joules_est": 63.82, "sample_count": 23, "duration_seconds": 2.364}, "timestamp": "2026-01-19T12:40:30.268098"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1639.058, "latencies_ms": [1639.058], "images_per_second": 0.61, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A group of children are riding in a red and yellow train car on a track.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26100.9, "ram_available_mb": 99671.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26101.1, "ram_available_mb": 99671.1, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.66, "peak": 117.13, "min": 29.07}, "VIN_SYS_5V0": {"avg": 13.81, "peak": 14.94, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.42, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 29.51, "peak": 37.04, "min": 15.38}}, "power_watts_avg": 29.51, "energy_joules_est": 48.38, "sample_count": 16, "duration_seconds": 1.639}, "timestamp": "2026-01-19T12:40:31.928795"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3085.411, "latencies_ms": [3085.411], "images_per_second": 0.324, "prompt_tokens": 1114, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. children: 6\n2. train: 1\n3. children's seats: 2\n4. train track: 1\n5. children's hands: 2\n6. children's feet: 2\n7. children's legs: 2\n8. children's heads: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26101.1, "ram_available_mb": 99671.1, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26101.1, "ram_available_mb": 99671.1, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.75, "peak": 112.7, "min": 28.49}, "VIN_SYS_5V0": {"avg": 13.9, "peak": 15.04, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 14.53, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 25.08, "peak": 37.83, "min": 16.95}}, "power_watts_avg": 25.08, "energy_joules_est": 77.39, "sample_count": 30, "duration_seconds": 3.086}, "timestamp": "2026-01-19T12:40:35.053233"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2716.537, "latencies_ms": [2716.537], "images_per_second": 0.368, "prompt_tokens": 1118, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The children are sitting in a row on a red and yellow train car, which is positioned in the foreground of the image. The train car is moving along a track that is located in the middle of the image, with the children seated on the left side of the track.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26101.1, "ram_available_mb": 99671.1, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26103.8, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.48, "peak": 108.32, "min": 26.71}, "VIN_SYS_5V0": {"avg": 13.85, "peak": 15.14, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.54, "peak": 16.14, "min": 12.61}, "VDD_GPU": {"avg": 25.2, "peak": 37.83, "min": 14.59}}, "power_watts_avg": 25.2, "energy_joules_est": 68.46, "sample_count": 27, "duration_seconds": 2.717}, "timestamp": "2026-01-19T12:40:37.873467"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1523.443, "latencies_ms": [1523.443], "images_per_second": 0.656, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A group of children are riding in a red and yellow train car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.8, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26103.8, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.44, "peak": 109.86, "min": 28.21}, "VIN_SYS_5V0": {"avg": 13.39, "peak": 14.73, "min": 10.94}, "VDD_CPU_SOC_MSS": {"avg": 14.23, "peak": 15.75, "min": 11.82}, "VDD_GPU": {"avg": 28.93, "peak": 37.04, "min": 13.8}}, "power_watts_avg": 28.93, "energy_joules_est": 44.09, "sample_count": 15, "duration_seconds": 1.524}, "timestamp": "2026-01-19T12:40:39.439538"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1931.738, "latencies_ms": [1931.738], "images_per_second": 0.518, "prompt_tokens": 1110, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a wooden floor. The lighting is dim and the colors are muted.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.8, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.23, "peak": 109.08, "min": 27.97}, "VIN_SYS_5V0": {"avg": 13.74, "peak": 14.94, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 15.75, "min": 13.0}, "VDD_GPU": {"avg": 28.94, "peak": 37.44, "min": 18.92}}, "power_watts_avg": 28.94, "energy_joules_est": 55.92, "sample_count": 19, "duration_seconds": 1.932}, "timestamp": "2026-01-19T12:40:41.426404"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2128.037, "latencies_ms": [2128.037], "images_per_second": 0.47, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a sandwich on a plate with a side of fries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.31, "peak": 114.15, "min": 27.84}, "VIN_SYS_5V0": {"avg": 13.86, "peak": 15.44, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.53, "peak": 16.14, "min": 12.22}, "VDD_GPU": {"avg": 29.29, "peak": 38.6, "min": 16.17}}, "power_watts_avg": 29.29, "energy_joules_est": 62.35, "sample_count": 21, "duration_seconds": 2.129}, "timestamp": "2026-01-19T12:40:43.620769"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3255.86, "latencies_ms": [3255.86], "images_per_second": 0.307, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. plate: 1\n2. sandwich: 1\n3. cup: 1\n4. food: 1\n5. table: 1\n6. background: 1\n7. person: 0\n8. tablecloth: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.9, "peak": 103.44, "min": 28.19}, "VIN_SYS_5V0": {"avg": 13.87, "peak": 15.54, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.55, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 26.58, "peak": 39.0, "min": 16.56}}, "power_watts_avg": 26.58, "energy_joules_est": 86.56, "sample_count": 32, "duration_seconds": 3.257}, "timestamp": "2026-01-19T12:40:46.960784"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3017.057, "latencies_ms": [3017.057], "images_per_second": 0.331, "prompt_tokens": 1450, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The main object, a sandwich, is located in the foreground of the image, with the plate and cup positioned in the background. The sandwich is situated to the left of the plate, and the cup is placed to the right of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.6, "ram_available_mb": 99668.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.95, "peak": 127.99, "min": 29.05}, "VIN_SYS_5V0": {"avg": 13.84, "peak": 15.54, "min": 10.84}, "VDD_CPU_SOC_MSS": {"avg": 14.59, "peak": 16.54, "min": 11.82}, "VDD_GPU": {"avg": 27.0, "peak": 40.18, "min": 14.2}}, "power_watts_avg": 27.0, "energy_joules_est": 81.47, "sample_count": 29, "duration_seconds": 3.018}, "timestamp": "2026-01-19T12:40:50.000530"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1978.042, "latencies_ms": [1978.042], "images_per_second": 0.506, "prompt_tokens": 1444, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a sandwich on a plate with a cup of sauce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.77, "peak": 130.71, "min": 34.36}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.75, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 31.25, "peak": 40.57, "min": 15.77}}, "power_watts_avg": 31.25, "energy_joules_est": 61.83, "sample_count": 19, "duration_seconds": 1.979}, "timestamp": "2026-01-19T12:40:51.988277"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2319.102, "latencies_ms": [2319.102], "images_per_second": 0.431, "prompt_tokens": 1442, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image is in black and white, with a white plate and a white bowl on a dark table. The lighting is soft and natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.25, "peak": 113.92, "min": 27.83}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.85, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 14.93, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.21, "peak": 40.97, "min": 18.53}}, "power_watts_avg": 30.21, "energy_joules_est": 70.07, "sample_count": 23, "duration_seconds": 2.32}, "timestamp": "2026-01-19T12:40:54.388102"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1272.949, "latencies_ms": [1272.949], "images_per_second": 0.786, "prompt_tokens": 766, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A person is standing on a paddleboard in the water, with a city in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5033.1, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.5, "peak": 104.44, "min": 28.89}, "VIN_SYS_5V0": {"avg": 13.33, "peak": 14.12, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.06, "peak": 14.96, "min": 11.82}, "VDD_GPU": {"avg": 26.35, "peak": 35.07, "min": 15.78}}, "power_watts_avg": 26.35, "energy_joules_est": 33.56, "sample_count": 13, "duration_seconds": 1.274}, "timestamp": "2026-01-19T12:40:55.757070"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2180.233, "latencies_ms": [2180.233], "images_per_second": 0.459, "prompt_tokens": 780, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. paddle: 1\n4. water: 1\n5. land: 1\n6. sky: 1\n7. buildings: 1\n8. trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.26, "peak": 116.06, "min": 29.41}, "VIN_SYS_5V0": {"avg": 13.78, "peak": 14.4, "min": 11.04}, "VDD_CPU_SOC_MSS": {"avg": 14.74, "peak": 15.76, "min": 12.22}, "VDD_GPU": {"avg": 23.99, "peak": 35.07, "min": 16.17}}, "power_watts_avg": 23.99, "energy_joules_est": 52.31, "sample_count": 21, "duration_seconds": 2.181}, "timestamp": "2026-01-19T12:40:57.955474"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1489.809, "latencies_ms": [1489.809], "images_per_second": 0.671, "prompt_tokens": 784, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The person is in the foreground of the image, paddling a surfboard on the water. The background shows a shoreline with buildings and trees, indicating a coastal location.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.57, "peak": 98.17, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 14.63, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 26.35, "peak": 36.65, "min": 17.75}}, "power_watts_avg": 26.35, "energy_joules_est": 39.27, "sample_count": 15, "duration_seconds": 1.49}, "timestamp": "2026-01-19T12:40:59.525174"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1270.163, "latencies_ms": [1270.163], "images_per_second": 0.787, "prompt_tokens": 778, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A person is standing on a paddle board in the water, with a city in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.13, "peak": 101.8, "min": 27.36}, "VIN_SYS_5V0": {"avg": 13.48, "peak": 14.22, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.33, "peak": 14.97, "min": 13.0}, "VDD_GPU": {"avg": 26.5, "peak": 35.07, "min": 15.77}}, "power_watts_avg": 26.5, "energy_joules_est": 33.67, "sample_count": 13, "duration_seconds": 1.271}, "timestamp": "2026-01-19T12:41:00.888938"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1589.848, "latencies_ms": [1589.848], "images_per_second": 0.629, "prompt_tokens": 776, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is in black and white, with the water being the most prominent color. The sky is clear, and the person is wearing a wetsuit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.2, "ram_available_mb": 99669.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.08, "peak": 116.78, "min": 26.56}, "VIN_SYS_5V0": {"avg": 13.56, "peak": 14.43, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 15.75, "min": 12.22}, "VDD_GPU": {"avg": 25.52, "peak": 38.23, "min": 16.17}}, "power_watts_avg": 25.52, "energy_joules_est": 40.58, "sample_count": 16, "duration_seconds": 1.59}, "timestamp": "2026-01-19T12:41:02.562131"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1737.881, "latencies_ms": [1737.881], "images_per_second": 0.575, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A white computer desk with a laptop, keyboard, mouse, speakers, and a computer monitor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.36, "peak": 112.25, "min": 27.91}, "VIN_SYS_5V0": {"avg": 13.59, "peak": 14.83, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.23, "peak": 15.36, "min": 12.22}, "VDD_GPU": {"avg": 28.66, "peak": 36.26, "min": 14.59}}, "power_watts_avg": 28.66, "energy_joules_est": 49.82, "sample_count": 17, "duration_seconds": 1.738}, "timestamp": "2026-01-19T12:41:04.344621"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2243.317, "latencies_ms": [2243.317], "images_per_second": 0.446, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " computer: 1, laptop: 1, keyboard: 1, mouse: 1, speakers: 2, monitor: 1, mousepad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.63, "peak": 106.43, "min": 27.9}, "VIN_SYS_5V0": {"avg": 13.69, "peak": 14.83, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.25, "peak": 15.75, "min": 12.22}, "VDD_GPU": {"avg": 27.17, "peak": 37.03, "min": 17.74}}, "power_watts_avg": 27.17, "energy_joules_est": 60.96, "sample_count": 22, "duration_seconds": 2.244}, "timestamp": "2026-01-19T12:41:06.633314"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2334.26, "latencies_ms": [2334.26], "images_per_second": 0.428, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The computer monitor is positioned to the right of the keyboard, and the laptop is to the left of the monitor. The speakers are placed in front of the keyboard, and the mouse is to the right of the keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.39, "peak": 121.86, "min": 28.91}, "VIN_SYS_5V0": {"avg": 13.91, "peak": 15.14, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 14.62, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.88, "peak": 39.0, "min": 15.38}}, "power_watts_avg": 26.88, "energy_joules_est": 62.76, "sample_count": 23, "duration_seconds": 2.335}, "timestamp": "2026-01-19T12:41:09.027911"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1401.985, "latencies_ms": [1401.985], "images_per_second": 0.713, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A desk with a computer, laptop, and speakers on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.56, "peak": 113.93, "min": 28.76}, "VIN_SYS_5V0": {"avg": 13.8, "peak": 15.14, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.99, "peak": 38.6, "min": 14.98}}, "power_watts_avg": 30.99, "energy_joules_est": 43.46, "sample_count": 14, "duration_seconds": 1.402}, "timestamp": "2026-01-19T12:41:10.499383"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1470.422, "latencies_ms": [1470.422], "images_per_second": 0.68, "prompt_tokens": 1109, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The desk is white and the computer is silver.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.23, "peak": 103.92, "min": 27.69}, "VIN_SYS_5V0": {"avg": 13.59, "peak": 14.94, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.28, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 31.13, "peak": 37.44, "min": 20.11}}, "power_watts_avg": 31.13, "energy_joules_est": 45.79, "sample_count": 15, "duration_seconds": 1.471}, "timestamp": "2026-01-19T12:41:12.071802"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3554.249, "latencies_ms": [3554.249], "images_per_second": 0.281, "prompt_tokens": 1099, "response_tokens_est": 92, "n_tiles": 1, "output_text": " The image captures a bustling highway scene under a concrete overpass, where multiple vehicles are in motion, including a taxi with the number 120 on its side, and a black SUV with the license plate \"4T0055\". The overpass is adorned with green highway signs, prominently displaying \"North 101 Ventura Blvd\" and \"Hollywood Blvd Sunset Blvd\", guiding drivers along their routes.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.03, "peak": 125.11, "min": 27.95}, "VIN_SYS_5V0": {"avg": 13.87, "peak": 14.94, "min": 10.73}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.75, "min": 11.43}, "VDD_GPU": {"avg": 24.58, "peak": 38.6, "min": 16.95}}, "power_watts_avg": 24.58, "energy_joules_est": 87.38, "sample_count": 35, "duration_seconds": 3.555}, "timestamp": "2026-01-19T12:41:15.716170"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2237.696, "latencies_ms": [2237.696], "images_per_second": 0.447, "prompt_tokens": 1113, "response_tokens_est": 44, "n_tiles": 1, "output_text": " TAXI: 1, VAN: 1, SUV: 1, CAR: 1, BUS: 1, CAR: 1, CAR: 1, CAR: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.4, "ram_available_mb": 99668.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26104.3, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.83, "peak": 122.47, "min": 28.38}, "VIN_SYS_5V0": {"avg": 13.89, "peak": 15.14, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.39, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 27.39, "energy_joules_est": 61.3, "sample_count": 22, "duration_seconds": 2.238}, "timestamp": "2026-01-19T12:41:18.006976"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2699.824, "latencies_ms": [2699.824], "images_per_second": 0.37, "prompt_tokens": 1117, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The vehicles are positioned on the right side of the road, with the taxi closest to the camera. The highway sign is located above the road, with the North Ventura and Hollywood Blvd signs positioned to the left of the taxi. The Sunset Blvd sign is positioned to the right of the taxi.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.3, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.07, "peak": 123.72, "min": 32.73}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.24, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.5, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.5, "energy_joules_est": 71.56, "sample_count": 26, "duration_seconds": 2.7}, "timestamp": "2026-01-19T12:41:20.710514"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2719.353, "latencies_ms": [2719.353], "images_per_second": 0.368, "prompt_tokens": 1111, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image captures a bustling highway scene under a concrete overpass. Vehicles, including a black SUV and a white taxi, are in motion, adhering to the traffic rules. The overpass, marked with directional signs for North Ventura, Hollywood Blvd, and Sunset Blvd, guides the flow of traffic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.48, "peak": 121.03, "min": 26.91}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 12.76}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 26.18, "peak": 39.0, "min": 16.95}}, "power_watts_avg": 26.18, "energy_joules_est": 71.21, "sample_count": 27, "duration_seconds": 2.72}, "timestamp": "2026-01-19T12:41:23.523330"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2305.812, "latencies_ms": [2305.812], "images_per_second": 0.434, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image depicts a busy highway with multiple lanes of traffic, vehicles of various colors and sizes, and a clear blue sky. The vehicles are moving in different directions, and the road is lined with trees and buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.32, "peak": 103.43, "min": 28.91}, "VIN_SYS_5V0": {"avg": 13.74, "peak": 14.83, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.59, "peak": 15.75, "min": 12.61}, "VDD_GPU": {"avg": 26.63, "peak": 36.65, "min": 14.59}}, "power_watts_avg": 26.63, "energy_joules_est": 61.42, "sample_count": 23, "duration_seconds": 2.306}, "timestamp": "2026-01-19T12:41:25.920428"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2050.799, "latencies_ms": [2050.799], "images_per_second": 0.488, "prompt_tokens": 1432, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A red double decker bus is driving down a street with a sign that says Aldwych on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26104.1, "ram_available_mb": 99668.1, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.89, "peak": 121.71, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.85, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 17.32, "min": 12.22}, "VDD_GPU": {"avg": 30.38, "peak": 41.36, "min": 14.98}}, "power_watts_avg": 30.38, "energy_joules_est": 62.33, "sample_count": 20, "duration_seconds": 2.052}, "timestamp": "2026-01-19T12:41:28.007223"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2419.573, "latencies_ms": [2419.573], "images_per_second": 0.413, "prompt_tokens": 1446, "response_tokens_est": 38, "n_tiles": 1, "output_text": " 1. red double decker bus\n2. license plate\n3. street sign\n4. building\n5. trees\n6. people\n7. advertisement\n8. sky", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.1, "ram_available_mb": 99668.1, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26103.9, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.92, "peak": 126.35, "min": 28.02}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.85, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.14, "peak": 41.36, "min": 18.14}}, "power_watts_avg": 30.14, "energy_joules_est": 72.95, "sample_count": 24, "duration_seconds": 2.42}, "timestamp": "2026-01-19T12:41:30.504637"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2350.904, "latencies_ms": [2350.904], "images_per_second": 0.425, "prompt_tokens": 1450, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The red double-decker bus is in the foreground, parked on the street, while the background features a city street with other vehicles and buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.9, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26103.9, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.72, "peak": 117.53, "min": 29.57}, "VIN_SYS_5V0": {"avg": 13.97, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.14, "min": 12.61}, "VDD_GPU": {"avg": 29.52, "peak": 38.62, "min": 15.78}}, "power_watts_avg": 29.52, "energy_joules_est": 69.41, "sample_count": 23, "duration_seconds": 2.351}, "timestamp": "2026-01-19T12:41:32.901396"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1762.479, "latencies_ms": [1762.479], "images_per_second": 0.567, "prompt_tokens": 1444, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A red double decker bus is driving down a street in London.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.9, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26103.9, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.39, "peak": 123.14, "min": 32.31}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.85, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 33.3, "peak": 41.36, "min": 17.35}}, "power_watts_avg": 33.3, "energy_joules_est": 58.7, "sample_count": 17, "duration_seconds": 1.763}, "timestamp": "2026-01-19T12:41:34.667761"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2054.668, "latencies_ms": [2054.668], "images_per_second": 0.487, "prompt_tokens": 1442, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The bus is red with a yellow stripe and has a black front grill. The sky is cloudy and the sun is shining.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26103.9, "ram_available_mb": 99668.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.68, "peak": 121.07, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.95, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 32.94, "peak": 42.15, "min": 21.68}}, "power_watts_avg": 32.94, "energy_joules_est": 67.69, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T12:41:36.748830"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1401.245, "latencies_ms": [1401.245], "images_per_second": 0.714, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black and white cat is laying on top of a laptop computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.06, "peak": 132.32, "min": 28.28}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.44, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 32.34, "peak": 39.78, "min": 18.92}}, "power_watts_avg": 32.34, "energy_joules_est": 45.33, "sample_count": 14, "duration_seconds": 1.402}, "timestamp": "2026-01-19T12:41:38.216743"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2328.043, "latencies_ms": [2328.043], "images_per_second": 0.43, "prompt_tokens": 1113, "response_tokens_est": 48, "n_tiles": 1, "output_text": " 1. black and white cat\n2. laptop\n3. keyboard\n4. white wall\n5. white baseboard\n6. black and white whiskers\n7. black and white fur\n8. black and white eyes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26104.6, "ram_available_mb": 99667.6, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.22, "peak": 116.27, "min": 31.45}, "VIN_SYS_5V0": {"avg": 13.97, "peak": 15.24, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.97, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.2, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 28.2, "energy_joules_est": 65.66, "sample_count": 23, "duration_seconds": 2.329}, "timestamp": "2026-01-19T12:41:40.606313"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2333.298, "latencies_ms": [2333.298], "images_per_second": 0.429, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The black and white cat is positioned to the left of the laptop keyboard, which is situated in the middle of the image. The laptop keyboard is located in the foreground of the image, while the cat is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.6, "ram_available_mb": 99667.6, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.92, "peak": 106.68, "min": 29.35}, "VIN_SYS_5V0": {"avg": 13.96, "peak": 14.94, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.75, "min": 13.0}, "VDD_GPU": {"avg": 27.14, "peak": 37.04, "min": 15.77}}, "power_watts_avg": 27.14, "energy_joules_est": 63.34, "sample_count": 23, "duration_seconds": 2.334}, "timestamp": "2026-01-19T12:41:42.983202"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1420.056, "latencies_ms": [1420.056], "images_per_second": 0.704, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black and white cat is laying on top of a laptop computer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.08, "peak": 109.38, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.34, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 31.44, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.44, "energy_joules_est": 44.67, "sample_count": 14, "duration_seconds": 1.421}, "timestamp": "2026-01-19T12:41:44.448065"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1491.48, "latencies_ms": [1491.48], "images_per_second": 0.67, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The cat is black and white, and the laptop is silver.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.34, "peak": 103.99, "min": 28.99}, "VIN_SYS_5V0": {"avg": 13.82, "peak": 15.04, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 14.6, "peak": 15.75, "min": 13.0}, "VDD_GPU": {"avg": 31.73, "peak": 37.82, "min": 20.5}}, "power_watts_avg": 31.73, "energy_joules_est": 47.33, "sample_count": 15, "duration_seconds": 1.492}, "timestamp": "2026-01-19T12:41:46.007770"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1758.777, "latencies_ms": [1758.777], "images_per_second": 0.569, "prompt_tokens": 1100, "response_tokens_est": 27, "n_tiles": 1, "output_text": " In the image, there are two airplanes flying in the sky above a large bridge, with the Sydney Opera House visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.4, "peak": 125.85, "min": 29.38}, "VIN_SYS_5V0": {"avg": 13.89, "peak": 15.14, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 30.71, "peak": 40.18, "min": 19.71}}, "power_watts_avg": 30.71, "energy_joules_est": 54.03, "sample_count": 17, "duration_seconds": 1.759}, "timestamp": "2026-01-19T12:41:47.788005"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2564.74, "latencies_ms": [2564.74], "images_per_second": 0.39, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. Airplane: 2\n2. Bridge: 1\n3. Building: 1\n4. Cityscape: 1\n5. Clouds: 1\n6. Flags: 1\n7. Water: 1\n8. People: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.69, "peak": 125.36, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.34, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 27.19, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 27.19, "energy_joules_est": 69.74, "sample_count": 25, "duration_seconds": 2.565}, "timestamp": "2026-01-19T12:41:50.371854"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2018.842, "latencies_ms": [2018.842], "images_per_second": 0.495, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The two airplanes are flying above the Sydney Harbour Bridge, which is positioned in the foreground of the image. The Sydney Opera House is situated in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.47, "peak": 105.55, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.34, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 13.79}, "VDD_GPU": {"avg": 28.43, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.43, "energy_joules_est": 57.4, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T12:41:52.456837"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2408.116, "latencies_ms": [2408.116], "images_per_second": 0.415, "prompt_tokens": 1112, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a moment of aerial spectacle over the iconic Sydney Harbour Bridge and the Sydney Opera House. Two military jets, painted in shades of red and white, are soaring in formation, their wings spread wide as they traverse the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.68, "peak": 116.14, "min": 28.37}, "VIN_SYS_5V0": {"avg": 13.84, "peak": 14.94, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 15.75, "min": 13.0}, "VDD_GPU": {"avg": 26.25, "peak": 37.03, "min": 16.16}}, "power_watts_avg": 26.25, "energy_joules_est": 63.22, "sample_count": 24, "duration_seconds": 2.408}, "timestamp": "2026-01-19T12:41:54.940771"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1590.312, "latencies_ms": [1590.312], "images_per_second": 0.629, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The sky is overcast with a grayish hue, and the bridge is a dark brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.52, "peak": 118.03, "min": 28.3}, "VIN_SYS_5V0": {"avg": 13.86, "peak": 15.34, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.9, "peak": 39.39, "min": 14.99}}, "power_watts_avg": 29.9, "energy_joules_est": 47.56, "sample_count": 16, "duration_seconds": 1.591}, "timestamp": "2026-01-19T12:41:56.597826"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1388.05, "latencies_ms": [1388.05], "images_per_second": 0.72, "prompt_tokens": 1099, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A black and white photo of a zebra nursing its young.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.15, "peak": 107.39, "min": 27.97}, "VIN_SYS_5V0": {"avg": 13.72, "peak": 15.24, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.83, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.83, "energy_joules_est": 44.21, "sample_count": 14, "duration_seconds": 1.389}, "timestamp": "2026-01-19T12:41:58.067912"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1479.192, "latencies_ms": [1479.192], "images_per_second": 0.676, "prompt_tokens": 1113, "response_tokens_est": 15, "n_tiles": 1, "output_text": " zebra: 1\ngrass: 1\nbaby zebra: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.59, "peak": 130.97, "min": 27.29}, "VIN_SYS_5V0": {"avg": 13.65, "peak": 15.24, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.92, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 31.92, "energy_joules_est": 47.22, "sample_count": 15, "duration_seconds": 1.479}, "timestamp": "2026-01-19T12:41:59.636876"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2182.95, "latencies_ms": [2182.95], "images_per_second": 0.458, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The zebra on the left is positioned in the foreground, while the other zebra is in the background. The zebra on the left is also in the foreground, while the other zebra is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.41, "peak": 113.16, "min": 33.77}, "VIN_SYS_5V0": {"avg": 13.79, "peak": 15.14, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 16.14, "min": 12.21}, "VDD_GPU": {"avg": 28.67, "peak": 40.57, "min": 18.93}}, "power_watts_avg": 28.67, "energy_joules_est": 62.59, "sample_count": 21, "duration_seconds": 2.183}, "timestamp": "2026-01-19T12:42:01.826362"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1443.313, "latencies_ms": [1443.313], "images_per_second": 0.693, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A black and white photo of a zebra nursing its baby in the wild.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.75, "peak": 119.53, "min": 31.96}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.44, "min": 12.46}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.54, "min": 13.79}, "VDD_GPU": {"avg": 31.86, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 31.86, "energy_joules_est": 46.0, "sample_count": 14, "duration_seconds": 1.444}, "timestamp": "2026-01-19T12:42:03.277616"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2163.179, "latencies_ms": [2163.179], "images_per_second": 0.462, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image is in black and white, with the zebra's stripes standing out against the grayscale background. The lighting is natural, coming from the side, casting shadows and highlights on the zebra's fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.95, "peak": 129.77, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.44, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 13.79}, "VDD_GPU": {"avg": 29.37, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 29.37, "energy_joules_est": 63.54, "sample_count": 21, "duration_seconds": 2.163}, "timestamp": "2026-01-19T12:42:05.454711"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2447.435, "latencies_ms": [2447.435], "images_per_second": 0.409, "prompt_tokens": 1099, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image depicts a cozy, compact bedroom with a bed adorned with a colorful quilt, a round wooden table, and a chair, all set against a backdrop of a stone wall and a window that offers a view of a building across the street.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 59.69, "peak": 113.6, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.05, "peak": 15.04, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.75, "min": 13.4}, "VDD_GPU": {"avg": 26.75, "peak": 36.63, "min": 16.56}}, "power_watts_avg": 26.75, "energy_joules_est": 65.49, "sample_count": 24, "duration_seconds": 2.448}, "timestamp": "2026-01-19T12:42:07.959563"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2541.146, "latencies_ms": [2541.146], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 1\n2. chair: 1\n3. table: 1\n4. lamp: 1\n5. window: 2\n6. door: 1\n7. wall: 1\n8. floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.3, "peak": 121.02, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.34, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.65, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.65, "energy_joules_est": 67.73, "sample_count": 25, "duration_seconds": 2.542}, "timestamp": "2026-01-19T12:42:10.548054"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2128.694, "latencies_ms": [2128.694], "images_per_second": 0.47, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The bed is positioned in the center of the room, with the round table and chair to its left. The window is located to the right of the bed, and the purple carpet covers the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.37, "peak": 126.0, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.02, "peak": 15.34, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.99, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.99, "energy_joules_est": 59.59, "sample_count": 21, "duration_seconds": 2.129}, "timestamp": "2026-01-19T12:42:12.740254"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1483.757, "latencies_ms": [1483.757], "images_per_second": 0.674, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A small room with a bed, a table, and a chair is shown.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26104.4, "ram_available_mb": 99667.8, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.0, "peak": 118.34, "min": 29.12}, "VIN_SYS_5V0": {"avg": 13.92, "peak": 15.34, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.97, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.97, "energy_joules_est": 45.96, "sample_count": 15, "duration_seconds": 1.484}, "timestamp": "2026-01-19T12:42:14.296733"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1913.971, "latencies_ms": [1913.971], "images_per_second": 0.522, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The room is painted in a deep purple color, and the walls are made of stone. The room is well-lit by natural light coming through the windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.7, "ram_available_mb": 99667.5, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.01, "peak": 119.71, "min": 30.58}, "VIN_SYS_5V0": {"avg": 13.91, "peak": 15.34, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.66, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 29.66, "energy_joules_est": 56.78, "sample_count": 19, "duration_seconds": 1.914}, "timestamp": "2026-01-19T12:42:16.281612"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1490.721, "latencies_ms": [1490.721], "images_per_second": 0.671, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A purple bus with the number 96 on it is driving down the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.82, "peak": 123.37, "min": 28.12}, "VIN_SYS_5V0": {"avg": 13.93, "peak": 15.24, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.15, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.15, "energy_joules_est": 46.46, "sample_count": 15, "duration_seconds": 1.492}, "timestamp": "2026-01-19T12:42:17.840703"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2525.608, "latencies_ms": [2525.608], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bus: 1\n2. person: 1\n3. pole: 1\n4. sign: 1\n5. tree: 1\n6. building: 1\n7. street: 1\n8. sidewalk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.9, "ram_available_mb": 99667.3, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.52, "peak": 119.41, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.24, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.4, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.4, "energy_joules_est": 69.21, "sample_count": 25, "duration_seconds": 2.526}, "timestamp": "2026-01-19T12:42:20.425729"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2054.478, "latencies_ms": [2054.478], "images_per_second": 0.487, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The purple bus is parked on the left side of the street, with a person walking on the right side. The bus is in the foreground, while the background shows a building and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.12, "peak": 121.68, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.24, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.55, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.55, "energy_joules_est": 58.66, "sample_count": 20, "duration_seconds": 2.055}, "timestamp": "2026-01-19T12:42:22.499627"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1464.097, "latencies_ms": [1464.097], "images_per_second": 0.683, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A purple bus is driving down a street with a man walking on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.78, "peak": 120.58, "min": 27.76}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.44, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 31.31, "peak": 39.39, "min": 17.35}}, "power_watts_avg": 31.31, "energy_joules_est": 45.85, "sample_count": 15, "duration_seconds": 1.464}, "timestamp": "2026-01-19T12:42:24.062562"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2018.178, "latencies_ms": [2018.178], "images_per_second": 0.495, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The bus is purple with a white roof and has a license plate that reads \"NA52 BUU\". The sky is clear and blue, and the sun is shining brightly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.34, "peak": 127.09, "min": 28.54}, "VIN_SYS_5V0": {"avg": 13.85, "peak": 15.24, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 29.02, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 29.02, "energy_joules_est": 58.58, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T12:42:26.148163"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1392.747, "latencies_ms": [1392.747], "images_per_second": 0.718, "prompt_tokens": 1099, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A white bowl filled with green apples is placed on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.78, "peak": 120.82, "min": 29.15}, "VIN_SYS_5V0": {"avg": 13.94, "peak": 15.34, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.46, "peak": 39.39, "min": 16.17}}, "power_watts_avg": 31.46, "energy_joules_est": 43.83, "sample_count": 14, "duration_seconds": 1.393}, "timestamp": "2026-01-19T12:42:27.603547"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1206.495, "latencies_ms": [1206.495], "images_per_second": 0.829, "prompt_tokens": 1113, "response_tokens_est": 5, "n_tiles": 1, "output_text": " apple: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.24, "peak": 126.32, "min": 28.0}, "VIN_SYS_5V0": {"avg": 13.82, "peak": 15.34, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 33.59, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 33.59, "energy_joules_est": 40.54, "sample_count": 12, "duration_seconds": 1.207}, "timestamp": "2026-01-19T12:42:28.864707"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2214.773, "latencies_ms": [2214.773], "images_per_second": 0.452, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The apples are in the foreground, with the bowl placed in the background. The apples are positioned on the left side of the bowl, with the bowl taking up the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.73, "peak": 120.63, "min": 26.91}, "VIN_SYS_5V0": {"avg": 13.85, "peak": 15.24, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.07, "peak": 40.97, "min": 16.95}}, "power_watts_avg": 29.07, "energy_joules_est": 64.39, "sample_count": 22, "duration_seconds": 2.215}, "timestamp": "2026-01-19T12:42:31.140934"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1365.326, "latencies_ms": [1365.326], "images_per_second": 0.732, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A bunch of green apples are in a white bowl.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.78, "peak": 124.97, "min": 26.95}, "VIN_SYS_5V0": {"avg": 13.69, "peak": 15.24, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.74, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 30.68, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 30.68, "energy_joules_est": 41.9, "sample_count": 14, "duration_seconds": 1.366}, "timestamp": "2026-01-19T12:42:32.603384"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1411.368, "latencies_ms": [1411.368], "images_per_second": 0.709, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The apples are green and shiny, and the bowl is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.7, "peak": 119.68, "min": 28.75}, "VIN_SYS_5V0": {"avg": 13.62, "peak": 15.04, "min": 11.14}, "VDD_CPU_SOC_MSS": {"avg": 14.68, "peak": 16.14, "min": 12.22}, "VDD_GPU": {"avg": 32.37, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 32.37, "energy_joules_est": 45.71, "sample_count": 14, "duration_seconds": 1.412}, "timestamp": "2026-01-19T12:42:34.053925"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1985.584, "latencies_ms": [1985.584], "images_per_second": 0.504, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a baseball game is in progress with a batter, catcher, and umpire positioned at home plate, while a pitcher is preparing to throw the ball.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.68, "peak": 134.98, "min": 27.12}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.24, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.65, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 29.65, "energy_joules_est": 58.9, "sample_count": 20, "duration_seconds": 1.986}, "timestamp": "2026-01-19T12:42:36.139038"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2638.065, "latencies_ms": [2638.065], "images_per_second": 0.379, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. batter: 1\n2. catcher: 1\n3. umpire: 1\n4. pitcher: 1\n5. home plate: 1\n6. baseball: 1\n7. grass: 1\n8. dirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.4, "peak": 106.45, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.24, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.52, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.52, "energy_joules_est": 69.98, "sample_count": 26, "duration_seconds": 2.639}, "timestamp": "2026-01-19T12:42:38.827659"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2163.356, "latencies_ms": [2163.356], "images_per_second": 0.462, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire behind him. The pitcher is in the background, standing on the mound. The batter is closer to the camera than the pitcher.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.99, "peak": 126.18, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.34, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.2, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.2, "energy_joules_est": 61.02, "sample_count": 21, "duration_seconds": 2.164}, "timestamp": "2026-01-19T12:42:41.013164"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1593.784, "latencies_ms": [1593.784], "images_per_second": 0.627, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A baseball game is being played on a field with a batter, catcher, and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.29, "peak": 110.03, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.44, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.85, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.85, "energy_joules_est": 49.18, "sample_count": 16, "duration_seconds": 1.594}, "timestamp": "2026-01-19T12:42:42.672895"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2643.022, "latencies_ms": [2643.022], "images_per_second": 0.378, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image captures a moment of intense action in a baseball game, with the vibrant colors of the players' uniforms contrasting against the lush green of the field. The lighting is natural, casting a warm glow over the scene, and the weather appears to be clear, with no signs of rain or wind.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.81, "peak": 128.34, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.34, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.91, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 26.91, "energy_joules_est": 71.13, "sample_count": 26, "duration_seconds": 2.643}, "timestamp": "2026-01-19T12:42:45.360452"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2444.191, "latencies_ms": [2444.191], "images_per_second": 0.409, "prompt_tokens": 1099, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a vibrant outdoor gathering, where a white cake adorned with an array of red and blue berries sits on a red tablecloth, accompanied by a variety of plates filled with an assortment of cheeses, grapes, and crackers, all neatly arranged on the table.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.02, "peak": 126.07, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.44, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 27.02, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.02, "energy_joules_est": 66.06, "sample_count": 24, "duration_seconds": 2.445}, "timestamp": "2026-01-19T12:42:47.861077"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2047.299, "latencies_ms": [2047.299], "images_per_second": 0.488, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " cake: 1, glasses: 10, plates: 10, cheese: 1, bread: 1, knife: 1, grapes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99667.9, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.78, "peak": 122.7, "min": 27.96}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.44, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 28.47, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.47, "energy_joules_est": 58.29, "sample_count": 20, "duration_seconds": 2.048}, "timestamp": "2026-01-19T12:42:49.933863"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2063.172, "latencies_ms": [2063.172], "images_per_second": 0.485, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The cake is positioned to the left of the plates, with the glasses arranged in a row behind it. The plates are placed in the foreground, with the cake and glasses in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.13, "peak": 124.12, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.44, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 28.9, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.9, "energy_joules_est": 59.63, "sample_count": 20, "duration_seconds": 2.063}, "timestamp": "2026-01-19T12:42:52.013573"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1526.671, "latencies_ms": [1526.671], "images_per_second": 0.655, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A table is set up outside with a cake, wine glasses, and plates of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.44, "peak": 106.01, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.44, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.54, "min": 13.79}, "VDD_GPU": {"avg": 31.28, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 31.28, "energy_joules_est": 47.77, "sample_count": 15, "duration_seconds": 1.527}, "timestamp": "2026-01-19T12:42:53.581941"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2348.884, "latencies_ms": [2348.884], "images_per_second": 0.426, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image depicts a vibrant outdoor gathering with a red tablecloth, a white cake adorned with red and blue berries, and a variety of cheeses and grapes. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.6, "peak": 129.02, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.22, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 28.22, "energy_joules_est": 66.3, "sample_count": 23, "duration_seconds": 2.349}, "timestamp": "2026-01-19T12:42:55.962813"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1442.248, "latencies_ms": [1442.248], "images_per_second": 0.693, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person is riding a wave on a blue surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.91, "peak": 119.9, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.44, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 31.49, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.49, "energy_joules_est": 45.44, "sample_count": 14, "duration_seconds": 1.443}, "timestamp": "2026-01-19T12:42:57.428156"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2621.472, "latencies_ms": [2621.472], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. wave: 1\n4. water: 1\n5. sky: 0\n6. sand: 0\n7. rocks: 0\n8. sky: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.52, "peak": 124.71, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 27.33, "peak": 40.57, "min": 17.35}}, "power_watts_avg": 27.33, "energy_joules_est": 71.65, "sample_count": 26, "duration_seconds": 2.622}, "timestamp": "2026-01-19T12:43:00.115792"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2116.216, "latencies_ms": [2116.216], "images_per_second": 0.473, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, with the wave and the ocean extending into the background. The surfer is relatively close to the camera, while the wave is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.25, "peak": 126.15, "min": 27.79}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.34, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.85, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.85, "energy_joules_est": 58.94, "sample_count": 21, "duration_seconds": 2.116}, "timestamp": "2026-01-19T12:43:02.311440"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1498.106, "latencies_ms": [1498.106], "images_per_second": 0.668, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A young man is riding a wave on a blue surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.53, "peak": 128.95, "min": 27.49}, "VIN_SYS_5V0": {"avg": 13.89, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.78, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.71, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.71, "energy_joules_est": 46.02, "sample_count": 15, "duration_seconds": 1.499}, "timestamp": "2026-01-19T12:43:03.876898"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2655.563, "latencies_ms": [2655.563], "images_per_second": 0.377, "prompt_tokens": 1109, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image captures a dynamic scene of a surfer riding a wave, with the surfer's blue surfboard contrasting against the green of the ocean. The lighting is natural and bright, suggesting a sunny day, and the water appears to be a deep green color, indicating a clear and possibly deep ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.83, "peak": 121.57, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.34, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.1, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 27.1, "energy_joules_est": 71.98, "sample_count": 26, "duration_seconds": 2.656}, "timestamp": "2026-01-19T12:43:06.582022"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1607.153, "latencies_ms": [1607.153], "images_per_second": 0.622, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A black and white photo of a group of children posing for a picture in front of a brick building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.66, "peak": 102.83, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.44, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.36, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.36, "energy_joules_est": 48.81, "sample_count": 16, "duration_seconds": 1.608}, "timestamp": "2026-01-19T12:43:08.243547"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2238.736, "latencies_ms": [2238.736], "images_per_second": 0.447, "prompt_tokens": 1113, "response_tokens_est": 45, "n_tiles": 1, "output_text": " 1. group of children\n2. brick wall\n3. steps\n4. children's clothing\n5. children's shoes\n6. children's hair\n7. children's faces\n8. children's eyes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.48, "peak": 119.16, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.34, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.37, "peak": 39.78, "min": 18.52}}, "power_watts_avg": 28.37, "energy_joules_est": 63.52, "sample_count": 22, "duration_seconds": 2.239}, "timestamp": "2026-01-19T12:43:10.536655"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2197.523, "latencies_ms": [2197.523], "images_per_second": 0.455, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The group of children is positioned in front of a brick building, with the children arranged in rows and columns. The children are standing and sitting in various poses, with some children positioned closer to the camera than others.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26104.2, "ram_available_mb": 99668.0, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25981.6, "ram_available_mb": 99790.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.27, "peak": 128.38, "min": 26.72}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.34, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 27.83, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.83, "energy_joules_est": 61.17, "sample_count": 22, "duration_seconds": 2.198}, "timestamp": "2026-01-19T12:43:12.826845"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1561.9, "latencies_ms": [1561.9], "images_per_second": 0.64, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of children are posing for a black and white photo in front of a brick building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25981.6, "ram_available_mb": 99790.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25981.6, "ram_available_mb": 99790.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.26, "peak": 105.78, "min": 27.25}, "VIN_SYS_5V0": {"avg": 13.9, "peak": 15.34, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.17, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.17, "energy_joules_est": 47.13, "sample_count": 16, "duration_seconds": 1.562}, "timestamp": "2026-01-19T12:43:14.485730"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2310.853, "latencies_ms": [2310.853], "images_per_second": 0.433, "prompt_tokens": 1109, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image is a black and white photograph with a grainy texture, capturing a group of children in a school setting. The children are dressed in various styles of clothing, suggesting a time period from the mid-20th century.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25981.6, "ram_available_mb": 99790.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25981.5, "ram_available_mb": 99790.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.41, "peak": 124.47, "min": 28.28}, "VIN_SYS_5V0": {"avg": 13.98, "peak": 15.34, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.93, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 27.6, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.6, "energy_joules_est": 63.79, "sample_count": 23, "duration_seconds": 2.311}, "timestamp": "2026-01-19T12:43:16.883278"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1403.699, "latencies_ms": [1403.699], "images_per_second": 0.712, "prompt_tokens": 1100, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A plate of bread and a wine glass are on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25981.5, "ram_available_mb": 99790.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.62, "peak": 112.54, "min": 28.52}, "VIN_SYS_5V0": {"avg": 13.9, "peak": 15.34, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.88, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.88, "energy_joules_est": 43.37, "sample_count": 14, "duration_seconds": 1.405}, "timestamp": "2026-01-19T12:43:18.349636"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2036.242, "latencies_ms": [2036.242], "images_per_second": 0.491, "prompt_tokens": 1114, "response_tokens_est": 37, "n_tiles": 1, "output_text": " knife: 1, plate: 1, bread: 2, wine glass: 1, bread roll: 1, butter: 1, napkin: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.09, "peak": 128.29, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.44, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.34, "peak": 40.97, "min": 19.32}}, "power_watts_avg": 29.34, "energy_joules_est": 59.76, "sample_count": 20, "duration_seconds": 2.037}, "timestamp": "2026-01-19T12:43:20.436751"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1788.841, "latencies_ms": [1788.841], "images_per_second": 0.559, "prompt_tokens": 1118, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The bread is on the left side of the plate, the knife is on the right side, and the wine glass is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.63, "peak": 125.27, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.44, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.57, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.57, "energy_joules_est": 52.9, "sample_count": 18, "duration_seconds": 1.789}, "timestamp": "2026-01-19T12:43:22.300967"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1573.653, "latencies_ms": [1573.653], "images_per_second": 0.635, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " In a restaurant, a plate of bread and a glass of wine are served on a table.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.23, "peak": 120.74, "min": 28.14}, "VIN_SYS_5V0": {"avg": 13.82, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.87, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.87, "energy_joules_est": 47.01, "sample_count": 16, "duration_seconds": 1.574}, "timestamp": "2026-01-19T12:43:23.972101"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1642.812, "latencies_ms": [1642.812], "images_per_second": 0.609, "prompt_tokens": 1110, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image is taken in a restaurant with a wooden table, the lighting is natural and the colors are warm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.78, "peak": 115.23, "min": 28.46}, "VIN_SYS_5V0": {"avg": 13.86, "peak": 15.34, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 30.69, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 30.69, "energy_joules_est": 50.43, "sample_count": 16, "duration_seconds": 1.643}, "timestamp": "2026-01-19T12:43:25.644190"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1793.208, "latencies_ms": [1793.208], "images_per_second": 0.558, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A skier in a colorful suit is jumping in the air with skis attached to their feet, while another skier stands nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.28, "peak": 118.44, "min": 27.47}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.44, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 30.03, "peak": 39.78, "min": 18.92}}, "power_watts_avg": 30.03, "energy_joules_est": 53.87, "sample_count": 18, "duration_seconds": 1.794}, "timestamp": "2026-01-19T12:43:27.529721"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3019.352, "latencies_ms": [3019.352], "images_per_second": 0.331, "prompt_tokens": 1113, "response_tokens_est": 74, "n_tiles": 1, "output_text": " 1. skier: 1\n2. ski poles: 2\n3. skis: 2\n4. snowboard: 1\n5. snowboarder: 1\n6. snowboarder's pants: 1\n7. snowboarder's helmet: 1\n8. snowboarder's goggles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.27, "peak": 122.35, "min": 28.07}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.24, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 25.5, "peak": 39.78, "min": 16.17}}, "power_watts_avg": 25.5, "energy_joules_est": 77.0, "sample_count": 30, "duration_seconds": 3.02}, "timestamp": "2026-01-19T12:43:30.648053"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1975.3, "latencies_ms": [1975.3], "images_per_second": 0.506, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The skier is in the foreground, jumping over a snow ramp, while the trees are in the background. The skier is closer to the camera than the trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.3, "ram_available_mb": 99789.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25982.5, "ram_available_mb": 99789.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.67, "peak": 118.94, "min": 27.63}, "VIN_SYS_5V0": {"avg": 13.92, "peak": 15.34, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.13, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 28.13, "energy_joules_est": 55.57, "sample_count": 20, "duration_seconds": 1.976}, "timestamp": "2026-01-19T12:43:32.721724"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1900.907, "latencies_ms": [1900.907], "images_per_second": 0.526, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A skier is jumping in the air with skis attached to their feet. There are trees in the background and another skier standing on the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.5, "ram_available_mb": 99789.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25982.5, "ram_available_mb": 99789.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.89, "peak": 126.17, "min": 28.8}, "VIN_SYS_5V0": {"avg": 13.92, "peak": 15.34, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 28.79, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.79, "energy_joules_est": 54.74, "sample_count": 19, "duration_seconds": 1.901}, "timestamp": "2026-01-19T12:43:34.689984"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2184.51, "latencies_ms": [2184.51], "images_per_second": 0.458, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a skier in mid-air against a backdrop of a clear blue sky, with snow-covered trees in the background. The skier is wearing a colorful outfit and is holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.5, "ram_available_mb": 99789.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25982.8, "ram_available_mb": 99789.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.5, "peak": 106.12, "min": 34.21}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 28.29, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.29, "energy_joules_est": 61.81, "sample_count": 21, "duration_seconds": 2.185}, "timestamp": "2026-01-19T12:43:36.880767"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1835.963, "latencies_ms": [1835.963], "images_per_second": 0.545, "prompt_tokens": 1100, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A person is standing in the snow with ski poles, wearing a green shirt and black pants, and looking at the snow-covered mountains in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25982.8, "ram_available_mb": 99789.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25983.0, "ram_available_mb": 99789.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.69, "peak": 129.25, "min": 27.93}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.93, "min": 14.19}, "VDD_GPU": {"avg": 29.68, "peak": 39.39, "min": 17.74}}, "power_watts_avg": 29.68, "energy_joules_est": 54.51, "sample_count": 18, "duration_seconds": 1.837}, "timestamp": "2026-01-19T12:43:38.765697"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2554.136, "latencies_ms": [2554.136], "images_per_second": 0.392, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. ski poles: 2\n3. snow: 1\n4. rocks: 2\n5. tree: 1\n6. clouds: 2\n7. sky: 1\n8. mountain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.0, "ram_available_mb": 99789.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.11, "peak": 114.26, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.34, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 26.95, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 26.95, "energy_joules_est": 68.84, "sample_count": 25, "duration_seconds": 2.554}, "timestamp": "2026-01-19T12:43:41.359891"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2087.32, "latencies_ms": [2087.32], "images_per_second": 0.479, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The person is standing in the foreground, with the snowy landscape stretching out into the background. The person is positioned to the left of the frame, with the snowy mountain range visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.21, "peak": 121.68, "min": 32.62}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.44, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 28.39, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.39, "energy_joules_est": 59.27, "sample_count": 20, "duration_seconds": 2.088}, "timestamp": "2026-01-19T12:43:43.451610"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1560.362, "latencies_ms": [1560.362], "images_per_second": 0.641, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A person is walking on a snowy mountain with ski poles, wearing a green shirt and black pants.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.53, "peak": 127.0, "min": 34.55}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 30.92, "peak": 39.39, "min": 17.74}}, "power_watts_avg": 30.92, "energy_joules_est": 48.26, "sample_count": 15, "duration_seconds": 1.561}, "timestamp": "2026-01-19T12:43:45.019223"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2283.261, "latencies_ms": [2283.261], "images_per_second": 0.438, "prompt_tokens": 1110, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features a person wearing a green jacket and black pants, standing in a snowy landscape with a clear blue sky and white clouds in the background. The snow is pristine white, and the person is using ski poles to navigate the terrain.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.09, "peak": 126.9, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 28.18, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 28.18, "energy_joules_est": 64.36, "sample_count": 23, "duration_seconds": 2.284}, "timestamp": "2026-01-19T12:43:47.396680"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1395.525, "latencies_ms": [1395.525], "images_per_second": 0.717, "prompt_tokens": 1099, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A banana and a chocolate donut are placed on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.27, "peak": 120.59, "min": 28.67}, "VIN_SYS_5V0": {"avg": 13.88, "peak": 15.34, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.98, "peak": 39.0, "min": 14.98}}, "power_watts_avg": 30.98, "energy_joules_est": 43.26, "sample_count": 14, "duration_seconds": 1.396}, "timestamp": "2026-01-19T12:43:48.860898"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1476.719, "latencies_ms": [1476.719], "images_per_second": 0.677, "prompt_tokens": 1113, "response_tokens_est": 16, "n_tiles": 1, "output_text": " banana: 1, donut: 1, plastic bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.55, "peak": 115.86, "min": 28.7}, "VIN_SYS_5V0": {"avg": 13.94, "peak": 15.24, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 32.12, "peak": 40.18, "min": 20.11}}, "power_watts_avg": 32.12, "energy_joules_est": 47.44, "sample_count": 15, "duration_seconds": 1.477}, "timestamp": "2026-01-19T12:43:50.431542"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2008.242, "latencies_ms": [2008.242], "images_per_second": 0.498, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The banana is located in the foreground, to the left of the donut. The donut is positioned in the middle of the image, with the banana placed directly beneath it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25985.8, "ram_available_mb": 99786.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 123.56, "min": 28.45}, "VIN_SYS_5V0": {"avg": 13.93, "peak": 15.14, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 29.18, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.18, "energy_joules_est": 58.61, "sample_count": 20, "duration_seconds": 2.009}, "timestamp": "2026-01-19T12:43:52.516641"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1408.537, "latencies_ms": [1408.537], "images_per_second": 0.71, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A banana and a chocolate donut are placed on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25985.8, "ram_available_mb": 99786.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25983.9, "ram_available_mb": 99788.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.99, "peak": 124.4, "min": 28.71}, "VIN_SYS_5V0": {"avg": 13.92, "peak": 15.34, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.41, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.41, "energy_joules_est": 44.26, "sample_count": 14, "duration_seconds": 1.409}, "timestamp": "2026-01-19T12:43:53.979159"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2056.723, "latencies_ms": [2056.723], "images_per_second": 0.486, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a banana and a chocolate donut placed on a table. The banana is yellow and the donut is brown. The lighting is bright and the table is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.16, "peak": 120.45, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.34, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.61, "peak": 40.18, "min": 19.71}}, "power_watts_avg": 29.61, "energy_joules_est": 60.92, "sample_count": 20, "duration_seconds": 2.057}, "timestamp": "2026-01-19T12:43:56.068151"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2047.902, "latencies_ms": [2047.902], "images_per_second": 0.488, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " A white mug with a pirate skull and crossbones design and the words \"PIRATE WELLS EXTREME\" is placed on a table next to a large knife.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.44, "peak": 119.71, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.44, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 28.76, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 28.76, "energy_joules_est": 58.92, "sample_count": 20, "duration_seconds": 2.049}, "timestamp": "2026-01-19T12:43:58.153892"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1452.375, "latencies_ms": [1452.375], "images_per_second": 0.689, "prompt_tokens": 1113, "response_tokens_est": 15, "n_tiles": 1, "output_text": " mug: 1, knife: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.65, "peak": 116.64, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.44, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 31.75, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.75, "energy_joules_est": 46.12, "sample_count": 14, "duration_seconds": 1.453}, "timestamp": "2026-01-19T12:43:59.621854"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2429.033, "latencies_ms": [2429.033], "images_per_second": 0.412, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The mug is located to the left of the knife, which is positioned in the foreground of the image. The mug is placed on a surface that appears to be a table or countertop, while the knife is resting on the same surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 120.3, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.34, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 27.96, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.96, "energy_joules_est": 67.92, "sample_count": 24, "duration_seconds": 2.429}, "timestamp": "2026-01-19T12:44:02.115975"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1678.894, "latencies_ms": [1678.894], "images_per_second": 0.596, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A white mug with a pirate skull and crossbones design is sitting on a table next to a large knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.98, "peak": 122.32, "min": 26.89}, "VIN_SYS_5V0": {"avg": 13.98, "peak": 15.44, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.69, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.69, "energy_joules_est": 49.86, "sample_count": 17, "duration_seconds": 1.679}, "timestamp": "2026-01-19T12:44:03.892981"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1774.936, "latencies_ms": [1774.936], "images_per_second": 0.563, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The mug is white and the knife is black. The mug is on a table and the knife is on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.81, "peak": 114.24, "min": 27.03}, "VIN_SYS_5V0": {"avg": 13.83, "peak": 15.34, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 29.44, "peak": 39.78, "min": 16.96}}, "power_watts_avg": 29.44, "energy_joules_est": 52.27, "sample_count": 18, "duration_seconds": 1.775}, "timestamp": "2026-01-19T12:44:05.768305"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2118.881, "latencies_ms": [2118.881], "images_per_second": 0.472, "prompt_tokens": 1432, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A group of people are gathered around a bar counter in a wine tasting room, with bottles of wine and glasses on display.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.42, "peak": 120.51, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 16.05, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 17.32, "min": 12.22}, "VDD_GPU": {"avg": 30.3, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 30.3, "energy_joules_est": 64.23, "sample_count": 21, "duration_seconds": 2.12}, "timestamp": "2026-01-19T12:44:07.960853"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2939.201, "latencies_ms": [2939.201], "images_per_second": 0.34, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 2\n2. woman: 1\n3. man: 1\n4. man: 1\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.64, "peak": 134.64, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 16.05, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 28.27, "peak": 41.76, "min": 17.35}}, "power_watts_avg": 28.27, "energy_joules_est": 83.1, "sample_count": 29, "duration_seconds": 2.94}, "timestamp": "2026-01-19T12:44:10.973270"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4892.428, "latencies_ms": [4892.428], "images_per_second": 0.204, "prompt_tokens": 1450, "response_tokens_est": 128, "n_tiles": 1, "output_text": " The man in the white shirt is standing near the bar, while the woman in the black jacket is standing near the door. The man in the blue shirt is standing near the woman in the black jacket. The man in the white shirt is standing near the man in the blue shirt. The man in the white shirt is standing near the man in the blue shirt. The man in the white shirt is standing near the man in the blue shirt. The man in the white shirt is standing near the man in the blue shirt. The man in the white shirt is standing near the man in the blue shirt. The man in the white shirt is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.61, "peak": 108.19, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 24.66, "peak": 41.76, "min": 15.77}}, "power_watts_avg": 24.66, "energy_joules_est": 120.66, "sample_count": 48, "duration_seconds": 4.893}, "timestamp": "2026-01-19T12:44:15.958996"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1854.195, "latencies_ms": [1854.195], "images_per_second": 0.539, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are gathered in a room with a bar, discussing wine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.32, "peak": 125.98, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 16.05, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.98, "peak": 41.36, "min": 14.99}}, "power_watts_avg": 31.98, "energy_joules_est": 59.31, "sample_count": 18, "duration_seconds": 1.855}, "timestamp": "2026-01-19T12:44:17.837557"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2174.873, "latencies_ms": [2174.873], "images_per_second": 0.46, "prompt_tokens": 1442, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken in a room with a wooden floor and a green wall. The lighting is natural, coming from the windows in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.33, "peak": 126.68, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 16.05, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.16, "peak": 42.15, "min": 20.5}}, "power_watts_avg": 32.16, "energy_joules_est": 69.96, "sample_count": 21, "duration_seconds": 2.175}, "timestamp": "2026-01-19T12:44:20.023137"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1980.91, "latencies_ms": [1980.91], "images_per_second": 0.505, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, there is a large field with two white birds standing in the grass, and in the background, there is a large industrial facility with multiple structures and a crane.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.51, "peak": 126.41, "min": 27.27}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.66}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.16, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.16, "energy_joules_est": 57.78, "sample_count": 20, "duration_seconds": 1.981}, "timestamp": "2026-01-19T12:44:22.113696"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2603.9, "latencies_ms": [2603.9], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. birds: 2\n2. boats: 1\n3. cranes: 1\n4. oil rigs: 1\n5. sky: 1\n6. clouds: 1\n7. grass: 1\n8. water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.25, "peak": 127.21, "min": 28.1}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.34, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.33, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.33, "energy_joules_est": 68.58, "sample_count": 26, "duration_seconds": 2.604}, "timestamp": "2026-01-19T12:44:24.814259"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2045.624, "latencies_ms": [2045.624], "images_per_second": 0.489, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The white birds are in the foreground, while the oil rigs are in the background. The sky is above the birds and the oil rigs, and the water is below them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.95, "peak": 104.54, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.34, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.25, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 28.25, "energy_joules_est": 57.8, "sample_count": 20, "duration_seconds": 2.046}, "timestamp": "2026-01-19T12:44:26.904380"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1872.507, "latencies_ms": [1872.507], "images_per_second": 0.534, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the foreground, a field of tall grass stretches out, while in the background, a large industrial complex with multiple buildings and towers dominates the skyline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.68, "peak": 122.75, "min": 26.99}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.44, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 29.2, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.2, "energy_joules_est": 54.69, "sample_count": 19, "duration_seconds": 1.873}, "timestamp": "2026-01-19T12:44:28.881274"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2487.72, "latencies_ms": [2487.72], "images_per_second": 0.402, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a field of tall grass with two white birds standing in the foreground, while in the background, there is a large industrial facility with multiple white structures and a blue crane. The sky is filled with clouds and there is a small bird flying in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.79, "peak": 124.59, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.24, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 27.09, "peak": 39.39, "min": 15.78}}, "power_watts_avg": 27.09, "energy_joules_est": 67.41, "sample_count": 24, "duration_seconds": 2.488}, "timestamp": "2026-01-19T12:44:31.381317"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1526.137, "latencies_ms": [1526.137], "images_per_second": 0.655, "prompt_tokens": 1100, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man is sitting on a toilet in a bathroom with a checkerboard pattern on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.1, "peak": 126.94, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 31.21, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.21, "energy_joules_est": 47.64, "sample_count": 15, "duration_seconds": 1.527}, "timestamp": "2026-01-19T12:44:32.947840"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2835.65, "latencies_ms": [2835.65], "images_per_second": 0.353, "prompt_tokens": 1114, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. man: 1\n2. toilet: 1\n3. man's pants: 1\n4. man's shirt: 1\n5. man's shoes: 1\n6. man's belt: 1\n7. man's ear: 1\n8. man's hair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.92, "peak": 131.41, "min": 28.18}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.44, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 26.51, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 26.51, "energy_joules_est": 75.18, "sample_count": 28, "duration_seconds": 2.836}, "timestamp": "2026-01-19T12:44:35.861915"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2538.939, "latencies_ms": [2538.939], "images_per_second": 0.394, "prompt_tokens": 1118, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The man is sitting on the toilet, which is in the foreground of the image. The toilet is located in the middle of the image, with the man's body positioned in the foreground. The man is looking down at the toilet, which is in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.61, "peak": 121.74, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.34, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.53, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.53, "energy_joules_est": 67.37, "sample_count": 25, "duration_seconds": 2.539}, "timestamp": "2026-01-19T12:44:38.459353"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1341.44, "latencies_ms": [1341.44], "images_per_second": 0.745, "prompt_tokens": 1112, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A man is sitting on a toilet in a bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.9, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.95, "peak": 104.01, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.34, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.16, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 31.16, "energy_joules_est": 41.81, "sample_count": 13, "duration_seconds": 1.342}, "timestamp": "2026-01-19T12:44:39.815325"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1506.766, "latencies_ms": [1506.766], "images_per_second": 0.664, "prompt_tokens": 1110, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The image is in black and white, with a man squatting next to a toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.96, "peak": 114.86, "min": 27.7}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.44, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 32.73, "peak": 40.97, "min": 22.07}}, "power_watts_avg": 32.73, "energy_joules_est": 49.33, "sample_count": 15, "duration_seconds": 1.507}, "timestamp": "2026-01-19T12:44:41.378186"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1780.228, "latencies_ms": [1780.228], "images_per_second": 0.562, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " In the image, a group of people are standing on a snow-covered mountain, with their footprints creating a trail in the snow.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.79, "peak": 122.29, "min": 27.7}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.34, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.16, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.16, "energy_joules_est": 53.7, "sample_count": 18, "duration_seconds": 1.781}, "timestamp": "2026-01-19T12:44:43.260486"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2787.471, "latencies_ms": [2787.471], "images_per_second": 0.359, "prompt_tokens": 1113, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. group of people: 4\n2. mountain: 1\n3. snow: 1\n4. rocks: 1\n5. sky: 1\n6. snow-covered mountain: 1\n7. ski tracks: 1\n8. snow-covered slope: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.14, "peak": 117.14, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.44, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 26.31, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.31, "energy_joules_est": 73.35, "sample_count": 27, "duration_seconds": 2.788}, "timestamp": "2026-01-19T12:44:46.068309"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2177.622, "latencies_ms": [2177.622], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The group of people is positioned in the foreground of the image, with the mountain range in the background. The mountain range is located to the right of the group, and the sky is visible above the mountain range.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.4, "peak": 128.59, "min": 34.64}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.56}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 28.33, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.33, "energy_joules_est": 61.7, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T12:44:48.253312"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1618.369, "latencies_ms": [1618.369], "images_per_second": 0.618, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A group of people are standing on a snow-covered mountain, with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.15, "peak": 120.89, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.9, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.9, "energy_joules_est": 50.01, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T12:44:49.922643"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2139.091, "latencies_ms": [2139.091], "images_per_second": 0.467, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features a group of people standing on a snow-covered mountain, with the sky above them being a clear blue. The snow is pristine white, and the mountain's surface is rugged and rocky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.7, "peak": 127.61, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.44, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.8, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 28.8, "energy_joules_est": 61.63, "sample_count": 21, "duration_seconds": 2.14}, "timestamp": "2026-01-19T12:44:52.103463"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1857.815, "latencies_ms": [1857.815], "images_per_second": 0.538, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A bowl of rice, broccoli, and a red bean dish is on a table.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.78, "peak": 125.28, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.15, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.09, "peak": 41.36, "min": 16.56}}, "power_watts_avg": 32.09, "energy_joules_est": 59.63, "sample_count": 18, "duration_seconds": 1.858}, "timestamp": "2026-01-19T12:44:53.981516"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2907.659, "latencies_ms": [2907.659], "images_per_second": 0.344, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. broccoli: 2\n2. rice: 1\n3. bean: 1\n4. bowl: 1\n5. table: 1\n6. plate: 1\n7. food: 1\n8. tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.94, "peak": 117.84, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.15, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 28.93, "peak": 42.15, "min": 17.35}}, "power_watts_avg": 28.93, "energy_joules_est": 84.13, "sample_count": 29, "duration_seconds": 2.908}, "timestamp": "2026-01-19T12:44:56.995505"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2560.711, "latencies_ms": [2560.711], "images_per_second": 0.391, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The broccoli is located to the left of the rice, which is in the center of the bowl. The red bean dish is on top of the rice, and the white rice is on the bottom of the bowl.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.82, "peak": 127.7, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 16.05, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.39, "peak": 41.36, "min": 14.98}}, "power_watts_avg": 29.39, "energy_joules_est": 75.27, "sample_count": 25, "duration_seconds": 2.561}, "timestamp": "2026-01-19T12:44:59.599400"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1647.574, "latencies_ms": [1647.574], "images_per_second": 0.607, "prompt_tokens": 1444, "response_tokens_est": 9, "n_tiles": 1, "output_text": " A bowl of food is on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.01, "peak": 120.01, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 16.05, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 33.49, "peak": 41.36, "min": 16.56}}, "power_watts_avg": 33.49, "energy_joules_est": 55.19, "sample_count": 16, "duration_seconds": 1.648}, "timestamp": "2026-01-19T12:45:01.263956"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1702.091, "latencies_ms": [1702.091], "images_per_second": 0.588, "prompt_tokens": 1442, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The bowl is white and the food is red and green.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.8, "peak": 108.95, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.15, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 34.92, "peak": 42.54, "min": 23.25}}, "power_watts_avg": 34.92, "energy_joules_est": 59.45, "sample_count": 17, "duration_seconds": 1.702}, "timestamp": "2026-01-19T12:45:03.041183"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1671.546, "latencies_ms": [1671.546], "images_per_second": 0.598, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A person is standing on a wooden platform with their feet on a skateboard, wearing black and white sneakers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.91, "peak": 128.54, "min": 27.76}, "VIN_SYS_5V0": {"avg": 13.98, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.12, "peak": 40.97, "min": 20.49}}, "power_watts_avg": 31.12, "energy_joules_est": 52.05, "sample_count": 17, "duration_seconds": 1.672}, "timestamp": "2026-01-19T12:45:04.811211"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2819.23, "latencies_ms": [2819.23], "images_per_second": 0.355, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. person: 1\n2. skateboard: 1\n3. wooden plank: 1\n4. grass: 1\n5. person's legs: 1\n6. person's feet: 1\n7. person's shoes: 1\n8. person's pants: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.97, "peak": 126.08, "min": 28.12}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.08, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.08, "energy_joules_est": 73.54, "sample_count": 28, "duration_seconds": 2.82}, "timestamp": "2026-01-19T12:45:07.727737"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2212.417, "latencies_ms": [2212.417], "images_per_second": 0.452, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The skateboard is in the foreground, with the person's feet on it. The person is standing on the skateboard, with their feet on the deck. The background is a grassy area with some wooden structures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.35, "peak": 109.63, "min": 31.28}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.49, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 27.49, "energy_joules_est": 60.83, "sample_count": 22, "duration_seconds": 2.213}, "timestamp": "2026-01-19T12:45:10.016303"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1504.642, "latencies_ms": [1504.642], "images_per_second": 0.665, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is riding a skateboard on a wooden ramp in a grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.34, "peak": 130.74, "min": 28.21}, "VIN_SYS_5V0": {"avg": 13.99, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.68, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.68, "energy_joules_est": 46.17, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T12:45:11.587323"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2654.943, "latencies_ms": [2654.943], "images_per_second": 0.377, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image features a person wearing black and white sneakers with a checkered pattern on the bottom, standing on a wooden ramp with a skateboard. The lighting is natural and bright, suggesting it is daytime. The weather appears to be clear, as there are no visible clouds in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.28, "peak": 116.57, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.08, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 27.08, "energy_joules_est": 71.91, "sample_count": 26, "duration_seconds": 2.656}, "timestamp": "2026-01-19T12:45:14.291373"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1562.602, "latencies_ms": [1562.602], "images_per_second": 0.64, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A bunch of bananas are on a desk with a keyboard and a computer monitor in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25983.8, "ram_available_mb": 99788.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.69, "peak": 127.15, "min": 30.72}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.44, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.86, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.86, "energy_joules_est": 48.23, "sample_count": 15, "duration_seconds": 1.563}, "timestamp": "2026-01-19T12:45:15.863144"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1147.126, "latencies_ms": [1147.126], "images_per_second": 0.872, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " banana: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25983.8, "ram_available_mb": 99788.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.84, "peak": 128.11, "min": 30.82}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.54, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 34.45, "peak": 40.18, "min": 20.5}}, "power_watts_avg": 34.45, "energy_joules_est": 39.54, "sample_count": 11, "duration_seconds": 1.148}, "timestamp": "2026-01-19T12:45:17.026108"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2220.94, "latencies_ms": [2220.94], "images_per_second": 0.45, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The bananas are located in the foreground of the image, with the keyboard and computer monitor in the background. The bananas are positioned to the left of the keyboard, and the computer monitor is to the right of the keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.74, "peak": 122.7, "min": 27.95}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 30.12, "peak": 41.76, "min": 18.53}}, "power_watts_avg": 30.12, "energy_joules_est": 66.92, "sample_count": 22, "duration_seconds": 2.222}, "timestamp": "2026-01-19T12:45:19.310113"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1539.076, "latencies_ms": [1539.076], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A bunch of bananas are on a desk with a keyboard and computer monitor in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.36, "peak": 119.61, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.44, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.05, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.05, "energy_joules_est": 47.8, "sample_count": 15, "duration_seconds": 1.539}, "timestamp": "2026-01-19T12:45:20.872629"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1442.867, "latencies_ms": [1442.867], "images_per_second": 0.693, "prompt_tokens": 1109, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The bananas are yellow and ripe, and the table is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.54, "peak": 112.06, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.44, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 32.95, "peak": 40.18, "min": 20.11}}, "power_watts_avg": 32.95, "energy_joules_est": 47.55, "sample_count": 14, "duration_seconds": 1.443}, "timestamp": "2026-01-19T12:45:22.329148"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1752.259, "latencies_ms": [1752.259], "images_per_second": 0.571, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image depicts a plate of food, including white rice, vegetables, and chicken, placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.1, "ram_available_mb": 99788.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.72, "peak": 114.07, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 31.1, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 31.1, "energy_joules_est": 54.51, "sample_count": 17, "duration_seconds": 1.753}, "timestamp": "2026-01-19T12:45:24.107976"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2099.295, "latencies_ms": [2099.295], "images_per_second": 0.476, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " plate: 1, fork: 1, knife: 1, glass: 1, rice: 1, carrots: 2, broccoli: 2, chicken: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.41, "peak": 123.55, "min": 26.98}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.44, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.44, "peak": 39.39, "min": 17.35}}, "power_watts_avg": 28.44, "energy_joules_est": 59.72, "sample_count": 21, "duration_seconds": 2.1}, "timestamp": "2026-01-19T12:45:26.293669"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2171.233, "latencies_ms": [2171.233], "images_per_second": 0.461, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The plate with the food is in the foreground, and the glass of water is in the background. The fork and knife are placed on the plate, and the rice is on the right side of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.17, "peak": 110.62, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.34, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.28, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.28, "energy_joules_est": 61.42, "sample_count": 21, "duration_seconds": 2.172}, "timestamp": "2026-01-19T12:45:28.475781"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1481.696, "latencies_ms": [1481.696], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A plate of food is on a table with a glass of water and a fork.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.22, "peak": 113.57, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 31.41, "peak": 39.39, "min": 17.74}}, "power_watts_avg": 31.41, "energy_joules_est": 46.55, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T12:45:30.045209"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2498.398, "latencies_ms": [2498.398], "images_per_second": 0.4, "prompt_tokens": 1109, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image features a plate of food with a variety of colors, including red, green, and white. The lighting in the image is bright and natural, suggesting that the photo was taken during the day. The plate is made of ceramic and is placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.21, "peak": 120.61, "min": 34.94}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.34, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.73, "peak": 39.78, "min": 18.53}}, "power_watts_avg": 27.73, "energy_joules_est": 69.29, "sample_count": 24, "duration_seconds": 2.499}, "timestamp": "2026-01-19T12:45:32.547201"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1907.749, "latencies_ms": [1907.749], "images_per_second": 0.524, "prompt_tokens": 1100, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A little girl wearing a white shirt and a colorful skirt is playing with a Wii remote in a living room with a couch, a coffee table, and a rug.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.74, "peak": 125.84, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 29.26, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.26, "energy_joules_est": 55.83, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T12:45:34.536792"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2795.996, "latencies_ms": [2795.996], "images_per_second": 0.358, "prompt_tokens": 1114, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. couch: 1\n2. woman: 1\n3. girl: 1\n4. woman in white: 1\n5. woman in green: 1\n6. woman in white dress: 1\n7. woman in black: 1\n8. woman in blue: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.85, "peak": 111.81, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.34, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.33, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.33, "energy_joules_est": 73.63, "sample_count": 27, "duration_seconds": 2.796}, "timestamp": "2026-01-19T12:45:37.345170"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2210.16, "latencies_ms": [2210.16], "images_per_second": 0.452, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The girl is in the foreground, playing with a toy on the floor. The couch is in the background, and the people are standing behind it. The rug is in the foreground, and the stairs are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.97, "peak": 114.02, "min": 27.65}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.76}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 27.9, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 27.9, "energy_joules_est": 61.67, "sample_count": 22, "duration_seconds": 2.21}, "timestamp": "2026-01-19T12:45:39.643672"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1660.577, "latencies_ms": [1660.577], "images_per_second": 0.602, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A little girl is playing with a Wii remote in a living room with a couch and a coffee table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.8, "peak": 127.05, "min": 32.69}, "VIN_SYS_5V0": {"avg": 13.97, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.7, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.7, "energy_joules_est": 49.33, "sample_count": 16, "duration_seconds": 1.661}, "timestamp": "2026-01-19T12:45:41.310997"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1949.23, "latencies_ms": [1949.23], "images_per_second": 0.513, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming in from the windows. The colors in the room are vibrant and the materials are mostly wood and fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.87, "peak": 125.83, "min": 30.3}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 29.88, "peak": 40.18, "min": 20.1}}, "power_watts_avg": 29.88, "energy_joules_est": 58.26, "sample_count": 19, "duration_seconds": 1.95}, "timestamp": "2026-01-19T12:45:43.288351"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2203.265, "latencies_ms": [2203.265], "images_per_second": 0.454, "prompt_tokens": 1099, "response_tokens_est": 44, "n_tiles": 1, "output_text": " In the image, a man in a dark suit and tie is shaking hands with another man in a patterned shirt and tie, both standing in a large room filled with tables and chairs, suggesting a formal event or gathering.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.88, "peak": 114.13, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 28.03, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.03, "energy_joules_est": 61.77, "sample_count": 22, "duration_seconds": 2.204}, "timestamp": "2026-01-19T12:45:45.591452"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2566.178, "latencies_ms": [2566.178], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 2\n2. tie: 1\n3. glasses: 1\n4. shirt: 1\n5. tie: 1\n6. suit: 1\n7. shirt: 1\n8. tie: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.99, "peak": 118.8, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.64, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.64, "energy_joules_est": 68.38, "sample_count": 25, "duration_seconds": 2.567}, "timestamp": "2026-01-19T12:45:48.199906"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2590.974, "latencies_ms": [2590.974], "images_per_second": 0.386, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The man in the dark suit is standing to the right of the man in the patterned shirt, and the man in the patterned shirt is standing to the left of the man in the dark suit. The man in the patterned shirt is closer to the camera than the man in the dark suit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.32, "peak": 116.5, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.36}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 26.86, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.86, "energy_joules_est": 69.6, "sample_count": 25, "duration_seconds": 2.591}, "timestamp": "2026-01-19T12:45:50.801561"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1910.331, "latencies_ms": [1910.331], "images_per_second": 0.523, "prompt_tokens": 1111, "response_tokens_est": 33, "n_tiles": 1, "output_text": " In a formal setting, two men are shaking hands, one in a traditional outfit and the other in a business suit, indicating a possible cultural exchange or business meeting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.39, "peak": 122.96, "min": 27.08}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 29.2, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 29.2, "energy_joules_est": 55.79, "sample_count": 19, "duration_seconds": 1.911}, "timestamp": "2026-01-19T12:45:52.782776"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1599.729, "latencies_ms": [1599.729], "images_per_second": 0.625, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The room is well-lit with warm lighting, and the attendees are dressed in formal attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.66, "peak": 130.17, "min": 27.39}, "VIN_SYS_5V0": {"avg": 14.0, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.29, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.29, "energy_joules_est": 48.46, "sample_count": 16, "duration_seconds": 1.6}, "timestamp": "2026-01-19T12:45:54.455954"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1520.798, "latencies_ms": [1520.798], "images_per_second": 0.658, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man wearing a white shirt and a striped tie is standing in a dark room.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.34, "peak": 121.89, "min": 28.7}, "VIN_SYS_5V0": {"avg": 13.92, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.18, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.18, "energy_joules_est": 47.44, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T12:45:56.027987"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2551.581, "latencies_ms": [2551.581], "images_per_second": 0.392, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. tie: 1\n2. shirt: 1\n3. hand: 1\n4. person: 1\n5. tie: 1\n6. shirt: 1\n7. hand: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.7, "peak": 121.06, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.31, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 27.31, "energy_joules_est": 69.7, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T12:45:58.630642"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2095.627, "latencies_ms": [2095.627], "images_per_second": 0.477, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The man is standing in the foreground, wearing a white shirt and a striped tie. The background is dark and out of focus, suggesting that the man is in a dimly lit room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.68, "peak": 119.89, "min": 27.55}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.98, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.98, "energy_joules_est": 58.66, "sample_count": 21, "duration_seconds": 2.096}, "timestamp": "2026-01-19T12:46:00.818040"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1478.107, "latencies_ms": [1478.107], "images_per_second": 0.677, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man wearing a tie and a shirt is standing in a dark room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.61, "peak": 129.72, "min": 28.4}, "VIN_SYS_5V0": {"avg": 13.81, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.84, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.84, "energy_joules_est": 44.13, "sample_count": 15, "duration_seconds": 1.479}, "timestamp": "2026-01-19T12:46:02.383815"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1650.982, "latencies_ms": [1650.982], "images_per_second": 0.606, "prompt_tokens": 1110, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The man is wearing a white shirt and a striped tie. The lighting is dim and the background is dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.38, "peak": 113.9, "min": 28.7}, "VIN_SYS_5V0": {"avg": 13.95, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.93, "min": 12.21}, "VDD_GPU": {"avg": 30.95, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 30.95, "energy_joules_est": 51.11, "sample_count": 16, "duration_seconds": 1.651}, "timestamp": "2026-01-19T12:46:04.053885"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2169.077, "latencies_ms": [2169.077], "images_per_second": 0.461, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " In the room, there is a red armchair, a blue and red plaid couch, a television on a wooden stand, a whiteboard with writing on it, and a picture of a man and a woman.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.03, "peak": 122.38, "min": 30.49}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.54, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.04, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 29.04, "energy_joules_est": 63.01, "sample_count": 21, "duration_seconds": 2.17}, "timestamp": "2026-01-19T12:46:06.245838"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1829.577, "latencies_ms": [1829.577], "images_per_second": 0.547, "prompt_tokens": 1113, "response_tokens_est": 30, "n_tiles": 1, "output_text": " chair: 1, sofa: 1, television: 1, whiteboard: 1, wall: 1, poster: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.39, "peak": 108.51, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.94, "peak": 39.39, "min": 17.35}}, "power_watts_avg": 29.94, "energy_joules_est": 54.79, "sample_count": 18, "duration_seconds": 1.83}, "timestamp": "2026-01-19T12:46:08.115342"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2662.692, "latencies_ms": [2662.692], "images_per_second": 0.376, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The red armchair is positioned to the left of the blue and red plaid couch, which is in the foreground of the room. The television is placed on a wooden stand in the middle of the room, with the whiteboard on the left wall and the framed picture on the right wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.63, "peak": 117.53, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 26.87, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 26.87, "energy_joules_est": 71.57, "sample_count": 26, "duration_seconds": 2.663}, "timestamp": "2026-01-19T12:46:10.825734"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1573.697, "latencies_ms": [1573.697], "images_per_second": 0.635, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A room with a plaid couch, a red chair, and a TV on a stand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.42, "peak": 128.3, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 30.39, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.39, "energy_joules_est": 47.83, "sample_count": 16, "duration_seconds": 1.574}, "timestamp": "2026-01-19T12:46:12.492850"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1488.402, "latencies_ms": [1488.402], "images_per_second": 0.672, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The room is painted yellow and has a plaid couch and a red chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.28, "peak": 114.72, "min": 28.42}, "VIN_SYS_5V0": {"avg": 13.91, "peak": 15.34, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 31.39, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 31.39, "energy_joules_est": 46.74, "sample_count": 15, "duration_seconds": 1.489}, "timestamp": "2026-01-19T12:46:14.059955"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1921.103, "latencies_ms": [1921.103], "images_per_second": 0.521, "prompt_tokens": 1432, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man wearing a yellow shirt and black pants is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.02, "peak": 127.33, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 16.15, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.0, "peak": 42.54, "min": 18.92}}, "power_watts_avg": 32.0, "energy_joules_est": 61.5, "sample_count": 19, "duration_seconds": 1.922}, "timestamp": "2026-01-19T12:46:16.035643"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3279.548, "latencies_ms": [3279.548], "images_per_second": 0.305, "prompt_tokens": 1446, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Ocean: 1\n4. Wave: 1\n5. Water: 1\n6. Surfboard's edge: 1\n7. Surfer's arm: 1\n8. Surfer's leg: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.48, "peak": 125.73, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 27.8, "peak": 41.76, "min": 18.52}}, "power_watts_avg": 27.8, "energy_joules_est": 91.19, "sample_count": 32, "duration_seconds": 3.28}, "timestamp": "2026-01-19T12:46:19.373274"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2567.709, "latencies_ms": [2567.709], "images_per_second": 0.389, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, riding a wave that is in the middle ground. The surfer is facing towards the right side of the image, with the wave moving towards the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.9, "peak": 109.47, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 16.05, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.5, "peak": 41.36, "min": 15.77}}, "power_watts_avg": 29.5, "energy_joules_est": 75.76, "sample_count": 25, "duration_seconds": 2.568}, "timestamp": "2026-01-19T12:46:21.975301"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1787.874, "latencies_ms": [1787.874], "images_per_second": 0.559, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man wearing a yellow shirt is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.99, "peak": 123.85, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.15, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.76, "peak": 41.36, "min": 16.95}}, "power_watts_avg": 32.76, "energy_joules_est": 58.6, "sample_count": 18, "duration_seconds": 1.789}, "timestamp": "2026-01-19T12:46:23.848219"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2352.647, "latencies_ms": [2352.647], "images_per_second": 0.425, "prompt_tokens": 1442, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The surfer is wearing a yellow shirt and black pants while riding a wave on a white surfboard. The water is a deep blue color and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.76, "peak": 121.37, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 16.26, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.92, "peak": 42.54, "min": 18.92}}, "power_watts_avg": 30.92, "energy_joules_est": 72.75, "sample_count": 23, "duration_seconds": 2.353}, "timestamp": "2026-01-19T12:46:26.243347"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1495.859, "latencies_ms": [1495.859], "images_per_second": 0.669, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black cat is sitting in front of a computer monitor, looking at the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.48, "peak": 115.47, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 31.44, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.44, "energy_joules_est": 47.06, "sample_count": 15, "duration_seconds": 1.497}, "timestamp": "2026-01-19T12:46:27.812283"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2059.691, "latencies_ms": [2059.691], "images_per_second": 0.486, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " 1. black cat\n2. laptop\n3. keyboard\n4. computer monitor\n5. phone\n6. mouse\n7. white keyboard\n8. white mouse", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.23, "peak": 127.57, "min": 28.18}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.44, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.29, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 29.29, "energy_joules_est": 60.34, "sample_count": 20, "duration_seconds": 2.06}, "timestamp": "2026-01-19T12:46:29.900520"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2244.521, "latencies_ms": [2244.521], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The cat is in the foreground, looking at the computer screen. The laptop is on the left side of the desk, and the phone is on the right side. The cat is closer to the computer screen than the laptop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.3, "ram_available_mb": 99787.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.41, "peak": 113.04, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 28.08, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 28.08, "energy_joules_est": 63.04, "sample_count": 22, "duration_seconds": 2.245}, "timestamp": "2026-01-19T12:46:32.188874"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1504.216, "latencies_ms": [1504.216], "images_per_second": 0.665, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black cat is sitting in front of a computer monitor, looking at the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.94, "peak": 106.49, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 31.1, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.1, "energy_joules_est": 46.8, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T12:46:33.754773"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2040.246, "latencies_ms": [2040.246], "images_per_second": 0.49, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image is taken in a room with a black cat sitting on a desk, looking at a computer screen. The room is well-lit with natural light coming in from a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.7, "peak": 114.21, "min": 27.46}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.41, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 29.41, "energy_joules_est": 60.03, "sample_count": 20, "duration_seconds": 2.041}, "timestamp": "2026-01-19T12:46:35.835320"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1863.474, "latencies_ms": [1863.474], "images_per_second": 0.537, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A group of people, including a child wearing a helmet, are gathered outside a building, with one person cutting a red ribbon with a pair of scissors.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.67, "peak": 117.23, "min": 27.55}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 29.7, "peak": 39.39, "min": 16.55}}, "power_watts_avg": 29.7, "energy_joules_est": 55.37, "sample_count": 18, "duration_seconds": 1.864}, "timestamp": "2026-01-19T12:46:37.721038"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2727.534, "latencies_ms": [2727.534], "images_per_second": 0.367, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. People: 10\n2. Balloons: 2\n3. Helmet: 1\n4. Helmet: 1\n5. Helmet: 1\n6. Helmet: 1\n7. Helmet: 1\n8. Helmet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.7, "peak": 127.98, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 26.69, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 26.69, "energy_joules_est": 72.81, "sample_count": 27, "duration_seconds": 2.728}, "timestamp": "2026-01-19T12:46:40.523272"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2145.922, "latencies_ms": [2145.922], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The red ribbon is in the foreground, held by the man in the black suit. The child is in the middle ground, holding the ribbon. The crowd is in the background, standing behind the child.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.17, "peak": 126.02, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.44, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 28.11, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.11, "energy_joules_est": 60.34, "sample_count": 21, "duration_seconds": 2.147}, "timestamp": "2026-01-19T12:46:42.709657"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2182.456, "latencies_ms": [2182.456], "images_per_second": 0.458, "prompt_tokens": 1111, "response_tokens_est": 43, "n_tiles": 1, "output_text": " A group of people are gathered outside a building, cutting a red ribbon with a pair of scissors. The ribbon is being held by a man in a suit, while a little girl in a pink jacket stands nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.02, "peak": 127.94, "min": 35.25}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 28.39, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 28.39, "energy_joules_est": 61.97, "sample_count": 21, "duration_seconds": 2.183}, "timestamp": "2026-01-19T12:46:44.900010"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2224.143, "latencies_ms": [2224.143], "images_per_second": 0.45, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image is taken during the day with natural lighting. The colors in the image are vibrant, with the red ribbon being the most prominent. The weather appears to be clear, as there are no signs of rain or snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.19, "peak": 113.99, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 28.24, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 28.24, "energy_joules_est": 62.82, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T12:46:47.192458"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1751.418, "latencies_ms": [1751.418], "images_per_second": 0.571, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A white and pink bus with the number 65745 on the front is parked on the side of the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.43, "peak": 125.99, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.92, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.92, "energy_joules_est": 52.43, "sample_count": 17, "duration_seconds": 1.752}, "timestamp": "2026-01-19T12:46:48.971741"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.771, "latencies_ms": [2552.771], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bus: 1\n2. people: 1\n3. building: 1\n4. flowers: 1\n5. sky: 1\n6. road: 1\n7. sidewalk: 1\n8. license plate: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.29, "peak": 123.43, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 27.31, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.31, "energy_joules_est": 69.73, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T12:46:51.575885"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2048.359, "latencies_ms": [2048.359], "images_per_second": 0.488, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The bus is positioned on the left side of the image, with the sidewalk and street on the right. The bus is in the foreground, with the buildings and people in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.91, "peak": 123.38, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 28.63, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.63, "energy_joules_est": 58.66, "sample_count": 20, "duration_seconds": 2.049}, "timestamp": "2026-01-19T12:46:53.654109"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1426.809, "latencies_ms": [1426.809], "images_per_second": 0.701, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A white and pink bus is parked on the side of the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.28, "peak": 123.53, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 31.77, "peak": 39.39, "min": 16.96}}, "power_watts_avg": 31.77, "energy_joules_est": 45.34, "sample_count": 14, "duration_seconds": 1.427}, "timestamp": "2026-01-19T12:46:55.120745"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1939.499, "latencies_ms": [1939.499], "images_per_second": 0.516, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The bus is white, pink, and blue, and it is parked on a street with a yellow line. The sky is cloudy and the bus is in motion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.53, "peak": 128.22, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.25, "peak": 40.97, "min": 20.1}}, "power_watts_avg": 30.25, "energy_joules_est": 58.68, "sample_count": 19, "duration_seconds": 1.94}, "timestamp": "2026-01-19T12:46:57.100084"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1570.195, "latencies_ms": [1570.195], "images_per_second": 0.637, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man is sitting cross-legged in front of a mirror, holding a cup in his hand.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.49, "peak": 126.42, "min": 27.69}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.53, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 30.53, "energy_joules_est": 47.96, "sample_count": 16, "duration_seconds": 1.571}, "timestamp": "2026-01-19T12:46:58.779343"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2589.836, "latencies_ms": [2589.836], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. mirror - 1\n2. man - 1\n3. chair - 1\n4. wall - 2\n5. floor - 1\n6. door - 1\n7. table - 1\n8. picture - 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.98, "peak": 114.71, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.34, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 26.93, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 26.93, "energy_joules_est": 69.76, "sample_count": 25, "duration_seconds": 2.59}, "timestamp": "2026-01-19T12:47:01.380926"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2402.452, "latencies_ms": [2402.452], "images_per_second": 0.416, "prompt_tokens": 1118, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The man is sitting in front of the mirror, which is positioned on the left side of the image. The mirror is reflecting the room, which is located in the background. The man is sitting on the floor, which is in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.47, "peak": 117.61, "min": 27.01}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.75, "min": 12.76}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.12, "peak": 39.39, "min": 16.55}}, "power_watts_avg": 27.12, "energy_joules_est": 65.17, "sample_count": 24, "duration_seconds": 2.403}, "timestamp": "2026-01-19T12:47:03.888766"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1707.492, "latencies_ms": [1707.492], "images_per_second": 0.586, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man is sitting in front of a mirror, holding a cup. He is wearing a green shirt and black shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.66, "peak": 118.95, "min": 28.94}, "VIN_SYS_5V0": {"avg": 13.95, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.09, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.09, "energy_joules_est": 49.68, "sample_count": 17, "duration_seconds": 1.708}, "timestamp": "2026-01-19T12:47:05.659648"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1572.301, "latencies_ms": [1572.301], "images_per_second": 0.636, "prompt_tokens": 1110, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the wooden floor is polished and clean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.05, "peak": 114.4, "min": 27.03}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.61, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.61, "energy_joules_est": 48.14, "sample_count": 16, "duration_seconds": 1.573}, "timestamp": "2026-01-19T12:47:07.327914"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3681.699, "latencies_ms": [3681.699], "images_per_second": 0.272, "prompt_tokens": 1099, "response_tokens_est": 99, "n_tiles": 1, "output_text": " In the image, a group of people are gathered in a room, each holding a surfboard. The surfboards are adorned with various designs and colors, including yellow, red, and blue. One of the surfboards has a prominent logo on it, while another has a black and white design. The people are standing close to each other, creating a sense of camaraderie and shared interest in surfing. In the background, there is a man taking a photo of the group, capturing the moment.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.77, "peak": 115.52, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.44, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 24.58, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 24.58, "energy_joules_est": 90.52, "sample_count": 36, "duration_seconds": 3.683}, "timestamp": "2026-01-19T12:47:11.080095"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2627.807, "latencies_ms": [2627.807], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 4\n2. surfboard: 4\n3. camera: 1\n4. person: 1\n5. surfboard: 1\n6. person: 1\n7. surfboard: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.91, "peak": 128.95, "min": 28.26}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 26.32, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.32, "energy_joules_est": 69.18, "sample_count": 26, "duration_seconds": 2.628}, "timestamp": "2026-01-19T12:47:13.793782"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2266.068, "latencies_ms": [2266.068], "images_per_second": 0.441, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The surfboards are positioned in the foreground, with the group of people standing behind them. The person taking the photo is positioned to the left of the surfboards, while the person holding the camera is positioned to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.85, "peak": 113.3, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 27.67, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.67, "energy_joules_est": 62.71, "sample_count": 22, "duration_seconds": 2.266}, "timestamp": "2026-01-19T12:47:16.085003"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1480.456, "latencies_ms": [1480.456], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are posing for a photo in a room with surfboards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.51, "peak": 118.9, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 31.02, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.02, "energy_joules_est": 45.93, "sample_count": 15, "duration_seconds": 1.481}, "timestamp": "2026-01-19T12:47:17.666242"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1812.527, "latencies_ms": [1812.527], "images_per_second": 0.552, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a warm ambiance, and the surfboards are made of wood with vibrant colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.5, "peak": 128.15, "min": 28.59}, "VIN_SYS_5V0": {"avg": 13.99, "peak": 15.44, "min": 11.24}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 30.08, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 30.08, "energy_joules_est": 54.53, "sample_count": 18, "duration_seconds": 1.813}, "timestamp": "2026-01-19T12:47:19.548199"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2356.337, "latencies_ms": [2356.337], "images_per_second": 0.424, "prompt_tokens": 1099, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image features a large, golden-colored airplane with the text \"POLSKIE LINIE LICZNICZE\" and \"LOT\" written on its side, parked on a runway with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.35, "peak": 119.71, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 27.68, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 27.68, "energy_joules_est": 65.25, "sample_count": 23, "duration_seconds": 2.357}, "timestamp": "2026-01-19T12:47:21.949727"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2542.665, "latencies_ms": [2542.665], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. airplane: 1\n2. tail: 1\n3. wing: 2\n4. engine: 2\n5. runway: 1\n6. clouds: 1\n7. sky: 1\n8. logo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.97, "peak": 129.27, "min": 29.95}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 26.86, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.86, "energy_joules_est": 68.31, "sample_count": 25, "duration_seconds": 2.543}, "timestamp": "2026-01-19T12:47:24.558071"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1971.261, "latencies_ms": [1971.261], "images_per_second": 0.507, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The airplane is in the foreground, with the runway and other planes in the background. The airplane is to the left of the runway, and the sky is above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.12, "peak": 115.35, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.93, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.93, "energy_joules_est": 57.04, "sample_count": 19, "duration_seconds": 1.972}, "timestamp": "2026-01-19T12:47:26.540314"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2303.8, "latencies_ms": [2303.8], "images_per_second": 0.434, "prompt_tokens": 1111, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a moment on a runway where a golden-colored airplane, adorned with blue and white accents, is parked. The sky above is a canvas of blue, dotted with fluffy white clouds, creating a serene backdrop for the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.82, "peak": 128.38, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 27.92, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 27.92, "energy_joules_est": 64.33, "sample_count": 23, "duration_seconds": 2.304}, "timestamp": "2026-01-19T12:47:28.935024"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1497.858, "latencies_ms": [1497.858], "images_per_second": 0.668, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The airplane is gold and blue, and the sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.17, "peak": 133.72, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.01, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.71, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.71, "energy_joules_est": 46.01, "sample_count": 15, "duration_seconds": 1.498}, "timestamp": "2026-01-19T12:47:30.502198"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1518.708, "latencies_ms": [1518.708], "images_per_second": 0.658, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a toilet with a lid up and water in it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.43, "peak": 116.91, "min": 28.09}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.44, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.75, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 31.75, "energy_joules_est": 48.24, "sample_count": 15, "duration_seconds": 1.519}, "timestamp": "2026-01-19T12:47:32.069858"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1423.604, "latencies_ms": [1423.604], "images_per_second": 0.702, "prompt_tokens": 1114, "response_tokens_est": 14, "n_tiles": 1, "output_text": " toilet: 1\nlid: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.25, "peak": 127.11, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 32.5, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 32.5, "energy_joules_est": 46.28, "sample_count": 14, "duration_seconds": 1.424}, "timestamp": "2026-01-19T12:47:33.528838"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2114.926, "latencies_ms": [2114.926], "images_per_second": 0.473, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The toilet is located in the foreground of the image, with the person's legs visible in the bottom left corner. The sink is situated in the background, with the faucet and mirror above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.07, "peak": 117.94, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 29.17, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 29.17, "energy_joules_est": 61.71, "sample_count": 21, "duration_seconds": 2.115}, "timestamp": "2026-01-19T12:47:35.702828"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1512.334, "latencies_ms": [1512.334], "images_per_second": 0.661, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a toilet with a lid up and water in it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.31, "peak": 113.24, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.36, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.36, "energy_joules_est": 45.93, "sample_count": 15, "duration_seconds": 1.513}, "timestamp": "2026-01-19T12:47:37.267501"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1311.227, "latencies_ms": [1311.227], "images_per_second": 0.763, "prompt_tokens": 1110, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The toilet is white and the floor is grey.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.69, "peak": 131.01, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 32.91, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 32.91, "energy_joules_est": 43.16, "sample_count": 13, "duration_seconds": 1.311}, "timestamp": "2026-01-19T12:47:38.628683"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1708.01, "latencies_ms": [1708.01], "images_per_second": 0.585, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A person is skiing down a snowy trail in the woods, wearing a blue jacket, black pants, and a yellow helmet.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.1, "peak": 123.08, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.52, "peak": 41.36, "min": 21.28}}, "power_watts_avg": 31.52, "energy_joules_est": 53.86, "sample_count": 17, "duration_seconds": 1.709}, "timestamp": "2026-01-19T12:47:40.397206"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.179, "latencies_ms": [2610.179], "images_per_second": 0.383, "prompt_tokens": 1114, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. skis: 2\n3. ski poles: 2\n4. snow: 1\n5. trees: 1\n6. snowboard: 0\n7. helmet: 1\n8. goggles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.41, "peak": 113.61, "min": 28.18}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.82, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.82, "energy_joules_est": 70.02, "sample_count": 26, "duration_seconds": 2.611}, "timestamp": "2026-01-19T12:47:43.111250"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2268.978, "latencies_ms": [2268.978], "images_per_second": 0.441, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The skier is positioned in the foreground of the image, with the snowy forest and trees in the background. The skier is moving towards the right side of the image, with the snowy trail leading off into the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.2, "peak": 125.55, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.4, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 27.4, "energy_joules_est": 62.18, "sample_count": 22, "duration_seconds": 2.269}, "timestamp": "2026-01-19T12:47:45.411632"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1430.998, "latencies_ms": [1430.998], "images_per_second": 0.699, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person is skiing down a snowy hill with trees in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.33, "peak": 118.22, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 30.96, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 30.96, "energy_joules_est": 44.32, "sample_count": 14, "duration_seconds": 1.431}, "timestamp": "2026-01-19T12:47:46.872565"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1994.679, "latencies_ms": [1994.679], "images_per_second": 0.501, "prompt_tokens": 1110, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features a person skiing in a snowy forest with a white background and a gray sky. The skier is wearing a blue jacket, black pants, and a yellow helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.87, "peak": 122.39, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.55, "peak": 40.57, "min": 18.52}}, "power_watts_avg": 29.55, "energy_joules_est": 58.96, "sample_count": 20, "duration_seconds": 1.995}, "timestamp": "2026-01-19T12:47:48.962808"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2431.563, "latencies_ms": [2431.563], "images_per_second": 0.411, "prompt_tokens": 1099, "response_tokens_est": 52, "n_tiles": 1, "output_text": " In the image, a tennis player is seen in the midst of a game, wearing a yellow shirt and black shorts, while a man in an orange shirt watches from the stands. The court is marked with white lines, and the background is filled with spectators.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.0, "peak": 123.2, "min": 28.32}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.02, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.02, "energy_joules_est": 65.72, "sample_count": 24, "duration_seconds": 2.432}, "timestamp": "2026-01-19T12:47:51.471377"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2589.041, "latencies_ms": [2589.041], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. tennis racket: 1\n3. ball: 1\n4. person: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.91, "peak": 111.86, "min": 32.71}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.44, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.81, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.81, "energy_joules_est": 69.42, "sample_count": 25, "duration_seconds": 2.589}, "timestamp": "2026-01-19T12:47:54.066766"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2064.679, "latencies_ms": [2064.679], "images_per_second": 0.484, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the audience in the background. The tennis court is in the foreground, with the player and the audience in the middle ground.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.59, "peak": 117.58, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 28.98, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.98, "energy_joules_est": 59.85, "sample_count": 20, "duration_seconds": 2.065}, "timestamp": "2026-01-19T12:47:56.153178"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1459.755, "latencies_ms": [1459.755], "images_per_second": 0.685, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A tennis player is playing on a blue court with a crowd of spectators watching.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.86, "peak": 121.05, "min": 33.76}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 32.14, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 32.14, "energy_joules_est": 46.92, "sample_count": 14, "duration_seconds": 1.46}, "timestamp": "2026-01-19T12:47:57.617810"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1484.288, "latencies_ms": [1484.288], "images_per_second": 0.674, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The tennis court is blue with white lines, and the crowd is wearing various colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.57, "peak": 108.95, "min": 26.89}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 32.78, "peak": 40.57, "min": 22.07}}, "power_watts_avg": 32.78, "energy_joules_est": 48.66, "sample_count": 15, "duration_seconds": 1.485}, "timestamp": "2026-01-19T12:47:59.186345"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1934.267, "latencies_ms": [1934.267], "images_per_second": 0.517, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image shows a dining table with two bowls of food, one containing a curry dish and the other a bowl of fruit, placed on a white napkin.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.99, "peak": 128.81, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.61, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.61, "energy_joules_est": 57.3, "sample_count": 19, "duration_seconds": 1.935}, "timestamp": "2026-01-19T12:48:01.174873"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2619.74, "latencies_ms": [2619.74], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. bowl: 2\n2. plate: 1\n3. napkin: 1\n4. glass: 1\n5. tablecloth: 1\n6. food: 2\n7. table: 1\n8. tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.17, "peak": 124.27, "min": 27.21}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.69, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.69, "energy_joules_est": 69.94, "sample_count": 26, "duration_seconds": 2.621}, "timestamp": "2026-01-19T12:48:03.882858"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2343.049, "latencies_ms": [2343.049], "images_per_second": 0.427, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The bowl of pickled vegetables is located to the left of the bowl of fruit, with the latter being closer to the camera. The tablecloth is positioned in the background, while the glass of water is placed near the edge of the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.19, "peak": 117.26, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.24, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.24, "energy_joules_est": 63.85, "sample_count": 23, "duration_seconds": 2.344}, "timestamp": "2026-01-19T12:48:06.285265"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2103.588, "latencies_ms": [2103.588], "images_per_second": 0.475, "prompt_tokens": 1111, "response_tokens_est": 39, "n_tiles": 1, "output_text": " In a dimly lit restaurant, a plate of food is presented on a table. The plate contains two bowls of food, one filled with a red curry and the other with a sweet dessert.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.6, "peak": 122.96, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 28.18, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 28.18, "energy_joules_est": 59.29, "sample_count": 21, "duration_seconds": 2.104}, "timestamp": "2026-01-19T12:48:08.480352"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2546.649, "latencies_ms": [2546.649], "images_per_second": 0.393, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a table with two bowls of food, one containing a curry dish and the other containing a bowl of fruit. The curry dish is brown and the fruit is orange and white. The lighting is dim, and the tablecloth is white.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.4, "ram_available_mb": 99787.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.32, "peak": 127.78, "min": 29.93}, "VIN_SYS_5V0": {"avg": 13.91, "peak": 15.34, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.16, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.16, "energy_joules_est": 66.63, "sample_count": 25, "duration_seconds": 2.547}, "timestamp": "2026-01-19T12:48:11.098429"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2004.526, "latencies_ms": [2004.526], "images_per_second": 0.499, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, there are three sheep standing in a grassy area, with one sheep looking directly at the camera, and the other two sheep facing away from the camera.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.95, "peak": 125.26, "min": 27.95}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 28.25, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.25, "energy_joules_est": 56.65, "sample_count": 20, "duration_seconds": 2.005}, "timestamp": "2026-01-19T12:48:13.197666"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2682.551, "latencies_ms": [2682.551], "images_per_second": 0.373, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. sheep: 4\n2. grass: 1\n3. fence: 1\n4. brick wall: 1\n5. wooden structure: 1\n6. metal structure: 1\n7. wooden planks: 1\n8. metal fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.88, "peak": 118.27, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 12.22}, "VDD_GPU": {"avg": 26.5, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.5, "energy_joules_est": 71.1, "sample_count": 26, "duration_seconds": 2.683}, "timestamp": "2026-01-19T12:48:15.912093"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2174.16, "latencies_ms": [2174.16], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The sheep are positioned in the foreground of the image, with the brick wall serving as the background. The sheep are standing close to each other, with one sheep in the foreground and two others in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.9, "peak": 127.35, "min": 30.99}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 28.39, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.39, "energy_joules_est": 61.74, "sample_count": 21, "duration_seconds": 2.175}, "timestamp": "2026-01-19T12:48:18.109577"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2117.129, "latencies_ms": [2117.129], "images_per_second": 0.472, "prompt_tokens": 1111, "response_tokens_est": 41, "n_tiles": 1, "output_text": " In a sunny day, a group of sheep are standing in a grassy field, looking at the camera. The sheep are of different sizes and colors, with some having curly wool and others having straight wool.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.61, "peak": 118.86, "min": 28.3}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 28.54, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.54, "energy_joules_est": 60.43, "sample_count": 21, "duration_seconds": 2.118}, "timestamp": "2026-01-19T12:48:20.292453"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1645.435, "latencies_ms": [1645.435], "images_per_second": 0.608, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The sheep are in a grassy area with a brick wall in the background, and the lighting is natural daylight.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.87, "peak": 122.43, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.58, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.58, "energy_joules_est": 50.33, "sample_count": 16, "duration_seconds": 1.646}, "timestamp": "2026-01-19T12:48:21.959324"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1920.233, "latencies_ms": [1920.233], "images_per_second": 0.521, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The image features a bunch of bananas and an apple placed on a blue and white patterned background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.32, "peak": 114.06, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.16, "peak": 41.76, "min": 18.92}}, "power_watts_avg": 32.16, "energy_joules_est": 61.77, "sample_count": 19, "duration_seconds": 1.921}, "timestamp": "2026-01-19T12:48:23.948345"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1650.003, "latencies_ms": [1650.003], "images_per_second": 0.606, "prompt_tokens": 1446, "response_tokens_est": 9, "n_tiles": 1, "output_text": " apple: 1, banana: 7", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.58, "peak": 132.3, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.36, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 34.0, "peak": 42.15, "min": 18.53}}, "power_watts_avg": 34.0, "energy_joules_est": 56.11, "sample_count": 16, "duration_seconds": 1.65}, "timestamp": "2026-01-19T12:48:25.617226"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2858.224, "latencies_ms": [2858.224], "images_per_second": 0.35, "prompt_tokens": 1450, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The apple is positioned in the center of the image, with the bananas surrounding it. The bananas are arranged in a circular pattern around the apple, creating a symmetrical composition. The background is a patterned fabric, which provides a contrasting backdrop to the vibrant colors of the fruit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.99, "peak": 123.22, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 29.84, "peak": 42.94, "min": 18.52}}, "power_watts_avg": 29.84, "energy_joules_est": 85.3, "sample_count": 28, "duration_seconds": 2.859}, "timestamp": "2026-01-19T12:48:28.531181"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2346.978, "latencies_ms": [2346.978], "images_per_second": 0.426, "prompt_tokens": 1444, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a bunch of bananas and an apple placed on a blue patterned background. The bananas are arranged in a circular formation around the apple, creating a visually appealing display.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.23, "peak": 120.48, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.15, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.27, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 30.27, "energy_joules_est": 71.06, "sample_count": 23, "duration_seconds": 2.347}, "timestamp": "2026-01-19T12:48:30.931474"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3139.06, "latencies_ms": [3139.06], "images_per_second": 0.319, "prompt_tokens": 1442, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image features a vibrant display of bananas and an apple, with the bananas arranged in a circular pattern around the apple. The bananas are a bright yellow color, while the apple is a striking combination of red and yellow hues. The background is a textured blue and white pattern, providing a stark contrast to the colorful fruits.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.66, "peak": 131.79, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 27.77, "peak": 41.74, "min": 16.95}}, "power_watts_avg": 27.77, "energy_joules_est": 87.18, "sample_count": 31, "duration_seconds": 3.139}, "timestamp": "2026-01-19T12:48:34.164191"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1718.461, "latencies_ms": [1718.461], "images_per_second": 0.582, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A blue and white tram with red seats is parked on a track, with trees and a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.9, "peak": 120.27, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.81, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.81, "energy_joules_est": 51.24, "sample_count": 17, "duration_seconds": 1.719}, "timestamp": "2026-01-19T12:48:35.942379"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2608.942, "latencies_ms": [2608.942], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. tram: 1\n2. windows: 12\n3. doors: 4\n4. people: 2\n5. trees: 1\n6. cables: 10\n7. tracks: 1\n8. trolley: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.81, "peak": 117.15, "min": 27.69}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.02, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.02, "energy_joules_est": 70.51, "sample_count": 26, "duration_seconds": 2.609}, "timestamp": "2026-01-19T12:48:38.639653"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2142.034, "latencies_ms": [2142.034], "images_per_second": 0.467, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The tram is positioned on the left side of the image, with the tracks running parallel to it. The background features a clear blue sky and trees, while the foreground shows a concrete platform and a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.47, "peak": 129.89, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.13, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.13, "energy_joules_est": 60.26, "sample_count": 21, "duration_seconds": 2.142}, "timestamp": "2026-01-19T12:48:40.826343"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1587.383, "latencies_ms": [1587.383], "images_per_second": 0.63, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A blue and white tram is parked at a station, with trees and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.13, "peak": 114.03, "min": 29.95}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.61, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.61, "energy_joules_est": 48.6, "sample_count": 16, "duration_seconds": 1.588}, "timestamp": "2026-01-19T12:48:42.499463"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1455.688, "latencies_ms": [1455.688], "images_per_second": 0.687, "prompt_tokens": 1109, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The train is blue and white, and the sky is clear blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.02, "peak": 115.7, "min": 31.19}, "VIN_SYS_5V0": {"avg": 13.98, "peak": 15.44, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.88, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.88, "energy_joules_est": 46.42, "sample_count": 14, "duration_seconds": 1.456}, "timestamp": "2026-01-19T12:48:43.964015"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1995.042, "latencies_ms": [1995.042], "images_per_second": 0.501, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white bathtub, a shower with a red curtain, a double sink vanity with two mirrors above it, and a red bath mat on the floor.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.78, "peak": 127.2, "min": 27.09}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.88, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 29.88, "energy_joules_est": 59.63, "sample_count": 20, "duration_seconds": 1.996}, "timestamp": "2026-01-19T12:48:46.062139"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2667.36, "latencies_ms": [2667.36], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. Bathtub: 1\n2. Shower: 1\n3. Mirror: 2\n4. Sink: 2\n5. Cabinet: 2\n6. Towel: 1\n7. Floor: 1\n8. Wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.35, "peak": 119.56, "min": 30.04}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.43, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.43, "energy_joules_est": 70.51, "sample_count": 26, "duration_seconds": 2.668}, "timestamp": "2026-01-19T12:48:48.776067"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2137.793, "latencies_ms": [2137.793], "images_per_second": 0.468, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The shower is located to the left of the sink, and the bathtub is situated behind the shower curtain. The sink is positioned in the foreground, with the mirror above it reflecting the bathroom's interior.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.59, "peak": 112.81, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.18, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.18, "energy_joules_est": 60.26, "sample_count": 21, "duration_seconds": 2.138}, "timestamp": "2026-01-19T12:48:50.964241"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1500.717, "latencies_ms": [1500.717], "images_per_second": 0.666, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A bathroom with a shower, bathtub, and sink is shown in the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.14, "peak": 119.56, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.99, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 30.99, "energy_joules_est": 46.52, "sample_count": 15, "duration_seconds": 1.501}, "timestamp": "2026-01-19T12:48:52.533336"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1902.983, "latencies_ms": [1902.983], "images_per_second": 0.525, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The bathroom has a warm color scheme with beige walls and a red rug on the floor. The lighting is bright and natural, coming from the ceiling lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.3, "peak": 125.83, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.88, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 29.88, "energy_joules_est": 56.87, "sample_count": 19, "duration_seconds": 1.903}, "timestamp": "2026-01-19T12:48:54.512421"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1829.947, "latencies_ms": [1829.947], "images_per_second": 0.546, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A black and white photo captures a surfer skillfully riding a wave, with the water spray and foam around them creating a dynamic and powerful scene.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.25, "peak": 127.66, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.57, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 29.57, "energy_joules_est": 54.13, "sample_count": 18, "duration_seconds": 1.83}, "timestamp": "2026-01-19T12:48:56.395712"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2966.498, "latencies_ms": [2966.498], "images_per_second": 0.337, "prompt_tokens": 1113, "response_tokens_est": 72, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Wave: 1\n4. Ocean: 1\n5. Surfboard leash: 1\n6. Water droplets: 1\n7. Surfer's hair: 1\n8. Surfer's wetsuit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.87, "peak": 129.56, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.44, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 26.01, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 26.01, "energy_joules_est": 77.17, "sample_count": 29, "duration_seconds": 2.967}, "timestamp": "2026-01-19T12:48:59.415230"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2179.725, "latencies_ms": [2179.725], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that dominates the majority of the image. The surfer is in the center of the image, with the wave extending towards the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.14, "peak": 116.5, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.99, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.99, "energy_joules_est": 61.02, "sample_count": 21, "duration_seconds": 2.18}, "timestamp": "2026-01-19T12:49:01.613646"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1554.147, "latencies_ms": [1554.147], "images_per_second": 0.643, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A surfer is riding a wave in the ocean, performing a trick on his surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.31, "peak": 103.77, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.39, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.39, "energy_joules_est": 48.8, "sample_count": 15, "duration_seconds": 1.555}, "timestamp": "2026-01-19T12:49:03.186454"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2470.022, "latencies_ms": [2470.022], "images_per_second": 0.405, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image is in black and white, with the surfer and the wave being the main subjects. The surfer is wearing a wetsuit, and the wave is crashing around him. The lighting is natural, and the water appears to be a deep blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.37, "peak": 107.15, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.01, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 28.01, "energy_joules_est": 69.2, "sample_count": 24, "duration_seconds": 2.471}, "timestamp": "2026-01-19T12:49:05.689266"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1754.107, "latencies_ms": [1754.107], "images_per_second": 0.57, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A baseball player in a white uniform with the number 10 on it is standing at home plate with a bat in his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.28, "peak": 114.97, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.2, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.2, "energy_joules_est": 52.99, "sample_count": 17, "duration_seconds": 1.755}, "timestamp": "2026-01-19T12:49:07.462878"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2674.639, "latencies_ms": [2674.639], "images_per_second": 0.374, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. batter: 1\n2. catcher: 1\n3. umpire: 1\n4. pitcher: 1\n5. baseball: 1\n6. baseball bat: 1\n7. baseball glove: 1\n8. baseball field: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.51, "peak": 124.12, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.16, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 27.16, "energy_joules_est": 72.66, "sample_count": 26, "duration_seconds": 2.675}, "timestamp": "2026-01-19T12:49:10.167886"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1995.389, "latencies_ms": [1995.389], "images_per_second": 0.501, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire in the background. The batter is closer to the camera than the catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.29, "peak": 118.33, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 28.76, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.76, "energy_joules_est": 57.41, "sample_count": 20, "duration_seconds": 1.996}, "timestamp": "2026-01-19T12:49:12.250390"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1616.381, "latencies_ms": [1616.381], "images_per_second": 0.619, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A baseball game is being played in a stadium with a batter, catcher, and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.34, "peak": 129.43, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.48, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.48, "energy_joules_est": 49.29, "sample_count": 16, "duration_seconds": 1.617}, "timestamp": "2026-01-19T12:49:13.922633"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4406.95, "latencies_ms": [4406.95], "images_per_second": 0.227, "prompt_tokens": 1109, "response_tokens_est": 128, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a baseball game, with the batter, catcher, and umpire all dressed in their respective uniforms. The batter is poised in the batter's box, ready to swing at the incoming pitch, while the catcher is crouched behind him, ready to catch the ball if the batter misses. The umpire stands behind the catcher, overseeing the play. The colors in the image are predominantly green from the grass of the field, with the players' uniforms adding splashes of red, white, and black. The lighting is natural, suggesting it's daytime, and the weather appears to", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.61, "peak": 118.5, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 23.86, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 23.86, "energy_joules_est": 105.17, "sample_count": 43, "duration_seconds": 4.408}, "timestamp": "2026-01-19T12:49:18.396267"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1886.403, "latencies_ms": [1886.403], "images_per_second": 0.53, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black and white photo of a bunch of fruits and nuts on a white surface.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25984.9, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.24, "peak": 113.45, "min": 27.14}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 16.05, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 31.02, "peak": 41.36, "min": 15.38}}, "power_watts_avg": 31.02, "energy_joules_est": 58.53, "sample_count": 19, "duration_seconds": 1.887}, "timestamp": "2026-01-19T12:49:20.389212"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2331.049, "latencies_ms": [2331.049], "images_per_second": 0.429, "prompt_tokens": 1446, "response_tokens_est": 34, "n_tiles": 1, "output_text": " apple: 1, grapes: 1, pear: 1, orange: 1, grapes: 2, peanuts: 1, grapes: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.64, "peak": 126.6, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 16.15, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 17.32, "min": 12.22}, "VDD_GPU": {"avg": 30.61, "peak": 42.54, "min": 17.74}}, "power_watts_avg": 30.61, "energy_joules_est": 71.37, "sample_count": 23, "duration_seconds": 2.332}, "timestamp": "2026-01-19T12:49:22.785742"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3554.827, "latencies_ms": [3554.827], "images_per_second": 0.281, "prompt_tokens": 1450, "response_tokens_est": 75, "n_tiles": 1, "output_text": " The main objects are arranged in a diagonal line from the top left to the bottom right, with the largest object, a large fruit, positioned at the top center and the smallest, a small fruit, at the bottom right. The objects are placed on a flat surface, with the grapes and the small fruit positioned in the foreground, while the larger fruits are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.58, "peak": 116.74, "min": 27.58}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.36, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 26.66, "peak": 42.15, "min": 16.56}}, "power_watts_avg": 26.66, "energy_joules_est": 94.78, "sample_count": 35, "duration_seconds": 3.555}, "timestamp": "2026-01-19T12:49:26.432243"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2884.274, "latencies_ms": [2884.274], "images_per_second": 0.347, "prompt_tokens": 1444, "response_tokens_est": 54, "n_tiles": 1, "output_text": " In this black and white photo, we see a collection of fruits and nuts arranged on a white surface. The fruits include a large orange, a bunch of grapes, and a few apples. The nuts are scattered around the fruits, adding a contrast to the white background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.14, "peak": 128.89, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 16.15, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 17.32, "min": 12.22}, "VDD_GPU": {"avg": 28.24, "peak": 41.76, "min": 14.59}}, "power_watts_avg": 28.24, "energy_joules_est": 81.46, "sample_count": 28, "duration_seconds": 2.885}, "timestamp": "2026-01-19T12:49:29.349447"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2019.28, "latencies_ms": [2019.28], "images_per_second": 0.495, "prompt_tokens": 1442, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image is a black and white photograph with a high contrast, and the fruits are placed on a white surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.19, "peak": 119.93, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 31.72, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 31.72, "energy_joules_est": 64.06, "sample_count": 20, "duration_seconds": 2.02}, "timestamp": "2026-01-19T12:49:31.435448"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3948.345, "latencies_ms": [3948.345], "images_per_second": 0.253, "prompt_tokens": 1099, "response_tokens_est": 110, "n_tiles": 1, "output_text": " The image captures a bustling urban street scene, where a variety of vehicles, including cars and a bus, are in motion, while pedestrians navigate the sidewalks. The buildings lining the street are a mix of modern and older architectural styles, with some featuring balconies and others boasting large windows. The street itself is marked by clear lane markings and a bike lane, indicating a well-organized traffic system. The sky overhead is a clear blue, suggesting a pleasant day, and the presence of greenery along the sidewalk adds a touch of nature to the urban landscape.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.16, "peak": 130.86, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.44, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 24.31, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 24.31, "energy_joules_est": 96.0, "sample_count": 39, "duration_seconds": 3.949}, "timestamp": "2026-01-19T12:49:35.491071"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2569.487, "latencies_ms": [2569.487], "images_per_second": 0.389, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. street: 2\n2. car: 2\n3. building: 3\n4. streetlight: 2\n5. sidewalk: 1\n6. tree: 1\n7. road: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.46, "peak": 113.12, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.56, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.56, "energy_joules_est": 68.26, "sample_count": 25, "duration_seconds": 2.57}, "timestamp": "2026-01-19T12:49:38.100109"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2818.123, "latencies_ms": [2818.123], "images_per_second": 0.355, "prompt_tokens": 1117, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The main objects in the image are positioned in a way that the street is in the foreground, with the buildings and trees in the background. The street is on the left side of the image, while the buildings and trees are on the right side. The vehicles are parked on the street, and the trees are located near the sidewalk.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.42, "peak": 119.76, "min": 26.9}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.05, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.05, "energy_joules_est": 73.42, "sample_count": 28, "duration_seconds": 2.818}, "timestamp": "2026-01-19T12:49:41.014486"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2812.593, "latencies_ms": [2812.593], "images_per_second": 0.356, "prompt_tokens": 1111, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The image captures a bustling urban street scene, where modern buildings of varying heights line the street, their facades painted in a palette of white, red, and gray. Cars are seen parked along the side of the road, while pedestrians can be seen walking on the sidewalk, adding a dynamic element to the otherwise static cityscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.15, "peak": 130.65, "min": 30.89}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.78, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 25.78, "energy_joules_est": 72.53, "sample_count": 28, "duration_seconds": 2.813}, "timestamp": "2026-01-19T12:49:43.927605"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1704.09, "latencies_ms": [1704.09], "images_per_second": 0.587, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image features a street with a clear blue sky and white clouds, and the buildings are made of brick and concrete.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.06, "peak": 121.71, "min": 27.62}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.59, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 29.59, "energy_joules_est": 50.43, "sample_count": 17, "duration_seconds": 1.704}, "timestamp": "2026-01-19T12:49:45.701994"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2089.967, "latencies_ms": [2089.967], "images_per_second": 0.478, "prompt_tokens": 1100, "response_tokens_est": 40, "n_tiles": 1, "output_text": " In the image, there are two people standing close to each other, with one person wearing a blue shirt and the other wearing a white top. They are both smiling and appear to be enjoying themselves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.4, "peak": 121.31, "min": 27.02}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.39, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.39, "energy_joules_est": 59.35, "sample_count": 21, "duration_seconds": 2.091}, "timestamp": "2026-01-19T12:49:47.894191"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2553.082, "latencies_ms": [2553.082], "images_per_second": 0.392, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 2\n2. wall: 1\n3. television: 1\n4. chair: 1\n5. table: 1\n6. wall decoration: 1\n7. picture: 1\n8. light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.94, "peak": 127.8, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.71, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.71, "energy_joules_est": 68.2, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T12:49:50.494040"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2258.355, "latencies_ms": [2258.355], "images_per_second": 0.443, "prompt_tokens": 1118, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The man is standing to the left of the woman, with his arm around her shoulder. The woman is standing in front of the man, with her arm around his waist. The man is closer to the camera than the woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.23, "peak": 121.78, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 27.85, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.85, "energy_joules_est": 62.91, "sample_count": 22, "duration_seconds": 2.259}, "timestamp": "2026-01-19T12:49:52.785453"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1511.261, "latencies_ms": [1511.261], "images_per_second": 0.662, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man and a woman are standing close together in a bar, smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.97, "peak": 121.73, "min": 27.71}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.55, "peak": 39.0, "min": 16.56}}, "power_watts_avg": 30.55, "energy_joules_est": 46.18, "sample_count": 15, "duration_seconds": 1.512}, "timestamp": "2026-01-19T12:49:54.352090"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2244.908, "latencies_ms": [2244.908], "images_per_second": 0.445, "prompt_tokens": 1110, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image is taken in a dimly lit bar with a warm ambiance. The walls are adorned with various pictures and posters, adding to the cozy atmosphere. The lighting is soft and ambient, creating a relaxed and inviting environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.62, "peak": 123.03, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.4, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 28.4, "energy_joules_est": 63.77, "sample_count": 22, "duration_seconds": 2.245}, "timestamp": "2026-01-19T12:49:56.642456"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1998.363, "latencies_ms": [1998.363], "images_per_second": 0.5, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a woman is seen wearing a costume that includes a crown and a black and gold outfit, while a man with glasses and a beard is standing next to her.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.25, "peak": 124.0, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 28.56, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.56, "energy_joules_est": 57.09, "sample_count": 20, "duration_seconds": 1.999}, "timestamp": "2026-01-19T12:49:58.742391"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2658.319, "latencies_ms": [2658.319], "images_per_second": 0.376, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. crown: 1\n3. phone: 1\n4. hat: 1\n5. shirt: 1\n6. necklace: 1\n7. earring: 1\n8. wristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.05, "peak": 120.39, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.41, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.41, "energy_joules_est": 70.22, "sample_count": 26, "duration_seconds": 2.659}, "timestamp": "2026-01-19T12:50:01.448899"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2269.942, "latencies_ms": [2269.942], "images_per_second": 0.441, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The woman is in the foreground, wearing a crown and a black and gold costume. The man is in the background, wearing a black and white hat. The crowd is in the background, and the buildings are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.89, "peak": 126.41, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.92, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.92, "energy_joules_est": 63.38, "sample_count": 22, "duration_seconds": 2.27}, "timestamp": "2026-01-19T12:50:03.737698"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1547.039, "latencies_ms": [1547.039], "images_per_second": 0.646, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of people are gathered in a public place, with one person dressed in a costume.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.59, "peak": 127.44, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 31.39, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.39, "energy_joules_est": 48.57, "sample_count": 15, "duration_seconds": 1.547}, "timestamp": "2026-01-19T12:50:05.305729"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1806.892, "latencies_ms": [1806.892], "images_per_second": 0.553, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken during the day with natural lighting, and the woman is wearing a costume with a golden headpiece and black and gold armor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.53, "peak": 114.14, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.67, "peak": 40.18, "min": 20.1}}, "power_watts_avg": 30.67, "energy_joules_est": 55.43, "sample_count": 18, "duration_seconds": 1.807}, "timestamp": "2026-01-19T12:50:07.182319"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2020.245, "latencies_ms": [2020.245], "images_per_second": 0.495, "prompt_tokens": 1100, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a small, white bathroom with a white toilet, a white sink, and a white bathtub, all set against a backdrop of white tiled walls and a white floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.23, "peak": 124.4, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.67, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.67, "energy_joules_est": 57.95, "sample_count": 20, "duration_seconds": 2.021}, "timestamp": "2026-01-19T12:50:09.282063"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2081.254, "latencies_ms": [2081.254], "images_per_second": 0.48, "prompt_tokens": 1114, "response_tokens_est": 39, "n_tiles": 1, "output_text": " sink: 1, toilet: 1, window: 1, pipe: 1, bucket: 2, floor: 1, wall: 1, drain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.26, "peak": 134.22, "min": 27.07}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 28.14, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.14, "energy_joules_est": 58.58, "sample_count": 21, "duration_seconds": 2.082}, "timestamp": "2026-01-19T12:50:11.469618"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2524.119, "latencies_ms": [2524.119], "images_per_second": 0.396, "prompt_tokens": 1118, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The green bucket is located in the foreground, near the red bucket, which is positioned closer to the camera than the green bucket. The white pipe is situated in the background, behind the toilet, while the white wall is located in the foreground, in front of the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.21, "peak": 122.46, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.6, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.6, "energy_joules_est": 67.15, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T12:50:14.072964"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1880.181, "latencies_ms": [1880.181], "images_per_second": 0.532, "prompt_tokens": 1112, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image depicts a small, cramped bathroom with white tiled walls and floors. The bathroom features a small window, a toilet, and a sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.82, "peak": 122.99, "min": 31.47}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 28.67, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.67, "energy_joules_est": 53.91, "sample_count": 18, "duration_seconds": 1.88}, "timestamp": "2026-01-19T12:50:15.958844"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1713.013, "latencies_ms": [1713.013], "images_per_second": 0.584, "prompt_tokens": 1110, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The bathroom is painted white with a blue and white tile floor. The lighting is dim and the walls are covered in white tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.0, "peak": 125.85, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 30.57, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 30.57, "energy_joules_est": 52.39, "sample_count": 17, "duration_seconds": 1.714}, "timestamp": "2026-01-19T12:50:17.728545"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1723.028, "latencies_ms": [1723.028], "images_per_second": 0.58, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In the image, a man is standing next to an elephant, both of them are smiling and enjoying each other's company.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.96, "peak": 123.6, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.44, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.57, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.57, "energy_joules_est": 52.69, "sample_count": 17, "duration_seconds": 1.724}, "timestamp": "2026-01-19T12:50:19.505350"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2100.798, "latencies_ms": [2100.798], "images_per_second": 0.476, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " elephant: 1, man: 1, glasses: 1, shirt: 1, elephant's trunk: 1, elephant's ear: 1, elephant's eye: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.14, "peak": 118.48, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.71, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.71, "energy_joules_est": 60.32, "sample_count": 21, "duration_seconds": 2.101}, "timestamp": "2026-01-19T12:50:21.696918"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2371.7, "latencies_ms": [2371.7], "images_per_second": 0.422, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The man is standing to the left of the elephant, with the elephant's trunk reaching towards his head. The man is in the foreground of the image, while the elephant is in the background. The man is closer to the camera than the elephant.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.22, "peak": 120.95, "min": 30.27}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.44, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.44, "energy_joules_est": 65.09, "sample_count": 23, "duration_seconds": 2.372}, "timestamp": "2026-01-19T12:50:24.101201"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1622.345, "latencies_ms": [1622.345], "images_per_second": 0.616, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man wearing glasses and a beige shirt is standing next to a large elephant in a lush green forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.89, "peak": 127.4, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.73, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.73, "energy_joules_est": 49.87, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T12:50:25.771473"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1952.4, "latencies_ms": [1952.4], "images_per_second": 0.512, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image features a man wearing glasses and a beige shirt standing next to a large elephant in a natural setting with a clear blue sky and green trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.72, "peak": 112.47, "min": 28.14}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 29.74, "peak": 39.78, "min": 18.53}}, "power_watts_avg": 29.74, "energy_joules_est": 58.07, "sample_count": 19, "duration_seconds": 1.953}, "timestamp": "2026-01-19T12:50:27.755323"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1590.504, "latencies_ms": [1590.504], "images_per_second": 0.629, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " In the image, there are several children sitting on the grass, each holding a frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.26, "peak": 124.34, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.93, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.93, "energy_joules_est": 49.22, "sample_count": 16, "duration_seconds": 1.591}, "timestamp": "2026-01-19T12:50:29.420316"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1702.769, "latencies_ms": [1702.769], "images_per_second": 0.587, "prompt_tokens": 1113, "response_tokens_est": 23, "n_tiles": 1, "output_text": " 1. grass: 4\n2. children: 5\n3. frisbee: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.6, "peak": 116.06, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.0, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.46, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.46, "energy_joules_est": 51.88, "sample_count": 17, "duration_seconds": 1.703}, "timestamp": "2026-01-19T12:50:31.190125"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3204.685, "latencies_ms": [3204.685], "images_per_second": 0.312, "prompt_tokens": 1117, "response_tokens_est": 81, "n_tiles": 1, "output_text": " The frisbee is held by the child on the right, while the child on the left is holding a frisbee with a black and white design. The frisbee with the black and white design is in the foreground, while the frisbee with the red and white design is in the background. The child on the left is sitting closer to the camera than the child on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.29, "peak": 124.46, "min": 30.36}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 25.66, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 25.66, "energy_joules_est": 82.24, "sample_count": 31, "duration_seconds": 3.205}, "timestamp": "2026-01-19T12:50:34.421178"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1437.025, "latencies_ms": [1437.025], "images_per_second": 0.696, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A group of children are sitting on the grass playing with frisbees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.54, "peak": 116.54, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.74, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.74, "energy_joules_est": 45.62, "sample_count": 14, "duration_seconds": 1.437}, "timestamp": "2026-01-19T12:50:35.881820"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1864.507, "latencies_ms": [1864.507], "images_per_second": 0.536, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image is taken in a bright, sunny day with green grass and trees in the background. The children are wearing colorful clothes and holding frisbees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.6, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.28, "peak": 116.88, "min": 28.32}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.44, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 30.44, "energy_joules_est": 56.77, "sample_count": 19, "duration_seconds": 1.865}, "timestamp": "2026-01-19T12:50:37.850295"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1831.565, "latencies_ms": [1831.565], "images_per_second": 0.546, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A young girl wearing a red coat and holding a black umbrella with pink polka dots stands on a wet sidewalk in front of a bush.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.99, "peak": 105.26, "min": 28.0}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.93, "min": 12.21}, "VDD_GPU": {"avg": 29.2, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.2, "energy_joules_est": 53.51, "sample_count": 18, "duration_seconds": 1.832}, "timestamp": "2026-01-19T12:50:39.733359"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1952.738, "latencies_ms": [1952.738], "images_per_second": 0.512, "prompt_tokens": 1114, "response_tokens_est": 34, "n_tiles": 1, "output_text": " umbrella: 1, girl: 1, coat: 1, shoes: 1, bush: 1, house: 1, car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.98, "peak": 124.64, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.34, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 29.34, "energy_joules_est": 57.3, "sample_count": 19, "duration_seconds": 1.953}, "timestamp": "2026-01-19T12:50:41.701687"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2052.958, "latencies_ms": [2052.958], "images_per_second": 0.487, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The umbrella is held by the child, who is standing in front of the bush. The child is positioned in the foreground of the image, with the bush and the house in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.03, "peak": 118.34, "min": 30.18}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.98, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 28.98, "energy_joules_est": 59.51, "sample_count": 20, "duration_seconds": 2.053}, "timestamp": "2026-01-19T12:50:43.790424"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1609.834, "latencies_ms": [1609.834], "images_per_second": 0.621, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A little girl wearing a red coat and white shoes is standing on a wet sidewalk holding a black umbrella.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.55, "peak": 121.86, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.14, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 30.14, "energy_joules_est": 48.54, "sample_count": 16, "duration_seconds": 1.61}, "timestamp": "2026-01-19T12:50:45.463215"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2024.656, "latencies_ms": [2024.656], "images_per_second": 0.494, "prompt_tokens": 1110, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image features a young girl wearing a red coat and holding a black umbrella with pink polka dots. The scene is set on a rainy day with wet pavement and a gray sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.24, "peak": 118.53, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 28.98, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.98, "energy_joules_est": 58.7, "sample_count": 20, "duration_seconds": 2.025}, "timestamp": "2026-01-19T12:50:47.547776"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1894.577, "latencies_ms": [1894.577], "images_per_second": 0.528, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, a young elephant is walking towards the camera, while a group of elephants is walking away from the camera, with one elephant in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.84, "peak": 112.54, "min": 27.93}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.26, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 29.26, "energy_joules_est": 55.46, "sample_count": 19, "duration_seconds": 1.895}, "timestamp": "2026-01-19T12:50:49.537154"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2117.261, "latencies_ms": [2117.261], "images_per_second": 0.472, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " elephant: 1, trunk: 1, ear: 1, eye: 1, leg: 1, tail: 1, trunk tip: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.77, "peak": 114.13, "min": 28.15}, "VIN_SYS_5V0": {"avg": 13.94, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.92, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 27.92, "energy_joules_est": 59.12, "sample_count": 21, "duration_seconds": 2.118}, "timestamp": "2026-01-19T12:50:51.724224"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1988.59, "latencies_ms": [1988.59], "images_per_second": 0.503, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The main elephant is in the foreground, with the other elephants in the background. The elephants are walking towards the camera, with the main elephant in the center of the frame.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.47, "peak": 123.18, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.49, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.49, "energy_joules_est": 56.66, "sample_count": 20, "duration_seconds": 1.989}, "timestamp": "2026-01-19T12:50:53.816973"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2349.735, "latencies_ms": [2349.735], "images_per_second": 0.426, "prompt_tokens": 1111, "response_tokens_est": 49, "n_tiles": 1, "output_text": " In the image, a group of elephants is walking through a dry, dusty landscape. The elephants are of various sizes, with the smallest one being a baby elephant. The elephants are walking in a line, with the baby elephant in the front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.04, "peak": 124.78, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.48, "peak": 39.78, "min": 15.78}}, "power_watts_avg": 27.48, "energy_joules_est": 64.58, "sample_count": 23, "duration_seconds": 2.35}, "timestamp": "2026-01-19T12:50:56.214895"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2318.789, "latencies_ms": [2318.789], "images_per_second": 0.431, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features a group of elephants walking through a muddy area, with the elephants' skin appearing wet and their trunks hanging down. The lighting is natural, and the sky is overcast, giving the scene a soft and diffused look.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.66, "peak": 129.61, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 27.56, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.56, "energy_joules_est": 63.93, "sample_count": 23, "duration_seconds": 2.32}, "timestamp": "2026-01-19T12:50:58.605946"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2449.904, "latencies_ms": [2449.904], "images_per_second": 0.408, "prompt_tokens": 1432, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image captures a surfer skillfully riding a large wave, with the surfer wearing a red and green wetsuit, and the wave displaying a gradient of green and white.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.45, "peak": 133.53, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.26, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.15, "peak": 41.76, "min": 15.77}}, "power_watts_avg": 29.15, "energy_joules_est": 71.44, "sample_count": 24, "duration_seconds": 2.451}, "timestamp": "2026-01-19T12:51:01.098048"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2977.428, "latencies_ms": [2977.428], "images_per_second": 0.336, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. surfer: 1\n2. wave: 1\n3. surfboard: 1\n4. water: 1\n5. sky: 0\n6. logo: 1\n7. text: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.8, "ram_available_mb": 99787.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.98, "peak": 125.63, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.46, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 28.42, "peak": 42.15, "min": 16.95}}, "power_watts_avg": 28.42, "energy_joules_est": 84.64, "sample_count": 29, "duration_seconds": 2.978}, "timestamp": "2026-01-19T12:51:04.092680"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2703.627, "latencies_ms": [2703.627], "images_per_second": 0.37, "prompt_tokens": 1450, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The surfer is positioned on the left side of the image, with the wave dominating the right side. The surfer is in the foreground, with the wave in the background. The surfer is closer to the camera than the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.33, "peak": 127.7, "min": 27.24}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 29.11, "peak": 41.76, "min": 16.95}}, "power_watts_avg": 29.11, "energy_joules_est": 78.71, "sample_count": 27, "duration_seconds": 2.704}, "timestamp": "2026-01-19T12:51:06.879657"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2091.762, "latencies_ms": [2091.762], "images_per_second": 0.478, "prompt_tokens": 1444, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A surfer is riding a wave on a surfboard. The surfer is wearing a red shirt and is upside down.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.43, "peak": 128.21, "min": 27.62}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 16.26, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.01, "peak": 41.76, "min": 15.77}}, "power_watts_avg": 31.01, "energy_joules_est": 64.88, "sample_count": 21, "duration_seconds": 2.092}, "timestamp": "2026-01-19T12:51:09.053937"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2011.102, "latencies_ms": [2011.102], "images_per_second": 0.497, "prompt_tokens": 1442, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The surfer is wearing a red and green wetsuit, and the wave is a deep green color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.25, "peak": 113.53, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 16.26, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.83, "peak": 41.76, "min": 17.35}}, "power_watts_avg": 31.83, "energy_joules_est": 64.03, "sample_count": 20, "duration_seconds": 2.012}, "timestamp": "2026-01-19T12:51:11.136908"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1600.982, "latencies_ms": [1600.982], "images_per_second": 0.625, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " Two men on horseback are riding on a beach, with the ocean and a few people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.94, "peak": 126.29, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.22, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.22, "energy_joules_est": 50.0, "sample_count": 16, "duration_seconds": 1.602}, "timestamp": "2026-01-19T12:51:12.808298"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2860.985, "latencies_ms": [2860.985], "images_per_second": 0.35, "prompt_tokens": 1113, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. horse: 2\n2. rider: 2\n3. rider's hat: 1\n4. rider's shirt: 1\n5. rider's pants: 1\n6. rider's boots: 1\n7. rider's belt: 1\n8. rider's boots: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.07, "peak": 118.84, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.47, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 26.47, "energy_joules_est": 75.74, "sample_count": 28, "duration_seconds": 2.861}, "timestamp": "2026-01-19T12:51:15.713770"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2449.0, "latencies_ms": [2449.0], "images_per_second": 0.408, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The two men are positioned on the left side of the image, with the horse on the right side. The horse is in the foreground, while the men are in the background. The beach is in the middle ground, with the ocean and sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.71, "peak": 112.69, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 27.22, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.22, "energy_joules_est": 66.67, "sample_count": 24, "duration_seconds": 2.449}, "timestamp": "2026-01-19T12:51:18.207152"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1495.558, "latencies_ms": [1495.558], "images_per_second": 0.669, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two men on horseback are riding on a beach, with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.66, "peak": 125.13, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 31.07, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.07, "energy_joules_est": 46.47, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T12:51:19.780469"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1950.449, "latencies_ms": [1950.449], "images_per_second": 0.513, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image features a sandy beach with a clear blue sky and ocean in the background. The two men are dressed in white and are riding horses, which are also white.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.74, "peak": 105.91, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.88, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.88, "energy_joules_est": 58.29, "sample_count": 19, "duration_seconds": 1.951}, "timestamp": "2026-01-19T12:51:21.756984"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1597.8, "latencies_ms": [1597.8], "images_per_second": 0.626, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A motorcycle with a sidecar is parked in front of a garage, with a dog standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.77, "peak": 106.31, "min": 27.1}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.66, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.66, "energy_joules_est": 49.0, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T12:51:23.445061"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2595.47, "latencies_ms": [2595.47], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. motorcycle: 1\n2. dog: 1\n3. tire: 2\n4. wheel: 1\n5. handlebar: 1\n6. seat: 1\n7. garage door: 1\n8. light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.32, "peak": 123.85, "min": 31.51}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.44, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.25, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.25, "energy_joules_est": 70.74, "sample_count": 25, "duration_seconds": 2.596}, "timestamp": "2026-01-19T12:51:26.044755"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2070.579, "latencies_ms": [2070.579], "images_per_second": 0.483, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The motorcycle is positioned to the left of the garage, with the dog standing in front of it. The motorcycle is in the foreground, while the garage and the blue truck are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.15, "peak": 124.48, "min": 30.48}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.0, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.0, "energy_joules_est": 60.06, "sample_count": 20, "duration_seconds": 2.071}, "timestamp": "2026-01-19T12:51:28.127549"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1445.805, "latencies_ms": [1445.805], "images_per_second": 0.692, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A motorcycle is parked outside a garage with a dog standing next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.16, "peak": 120.64, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.11, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 32.11, "energy_joules_est": 46.43, "sample_count": 14, "duration_seconds": 1.446}, "timestamp": "2026-01-19T12:51:29.587404"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1393.963, "latencies_ms": [1393.963], "images_per_second": 0.717, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The motorcycle is red and silver, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.02, "peak": 124.85, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 33.32, "peak": 40.97, "min": 21.28}}, "power_watts_avg": 33.32, "energy_joules_est": 46.46, "sample_count": 14, "duration_seconds": 1.394}, "timestamp": "2026-01-19T12:51:31.055556"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1558.822, "latencies_ms": [1558.822], "images_per_second": 0.642, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man is flying a kite on a beach with a blue and black kite in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.27, "peak": 125.6, "min": 33.26}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 32.28, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 32.28, "energy_joules_est": 50.35, "sample_count": 15, "duration_seconds": 1.56}, "timestamp": "2026-01-19T12:51:32.626899"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2503.702, "latencies_ms": [2503.702], "images_per_second": 0.399, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. kite: 2\n3. beach: 1\n4. sand: 1\n5. water: 1\n6. trees: 1\n7. buildings: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.5, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.73, "peak": 130.93, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.85, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 27.85, "energy_joules_est": 69.74, "sample_count": 25, "duration_seconds": 2.504}, "timestamp": "2026-01-19T12:51:35.215860"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2111.78, "latencies_ms": [2111.78], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, with the beach and the lake in the background. The kite is flying in the sky, which is positioned above the man and the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.22, "peak": 120.27, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.2, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.2, "energy_joules_est": 59.56, "sample_count": 21, "duration_seconds": 2.112}, "timestamp": "2026-01-19T12:51:37.397076"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1486.313, "latencies_ms": [1486.313], "images_per_second": 0.673, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is flying a kite on a beach with a lake in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.62, "peak": 132.75, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.1, "energy_joules_est": 46.24, "sample_count": 15, "duration_seconds": 1.487}, "timestamp": "2026-01-19T12:51:38.959821"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1316.615, "latencies_ms": [1316.615], "images_per_second": 0.76, "prompt_tokens": 1109, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The sky is blue and the sun is shining.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.38, "peak": 131.07, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.44, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 33.03, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 33.03, "energy_joules_est": 43.5, "sample_count": 13, "duration_seconds": 1.317}, "timestamp": "2026-01-19T12:51:40.320761"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1601.167, "latencies_ms": [1601.167], "images_per_second": 0.625, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The image depicts a kitchen with wooden cabinets, a stainless steel refrigerator, and a black countertop.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.0, "peak": 117.44, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.23, "peak": 40.97, "min": 21.67}}, "power_watts_avg": 32.23, "energy_joules_est": 51.63, "sample_count": 16, "duration_seconds": 1.602}, "timestamp": "2026-01-19T12:51:41.989615"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2789.699, "latencies_ms": [2789.699], "images_per_second": 0.358, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. Kitchen cabinets: 10\n2. Sink: 1\n3. Countertop: 1\n4. Refrigerator: 1\n5. Stove: 1\n6. Microwave: 1\n7. Toaster: 1\n8. Toaster oven: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.45, "peak": 120.22, "min": 37.43}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.88, "peak": 39.78, "min": 18.53}}, "power_watts_avg": 26.88, "energy_joules_est": 75.0, "sample_count": 27, "duration_seconds": 2.79}, "timestamp": "2026-01-19T12:51:44.784651"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2167.067, "latencies_ms": [2167.067], "images_per_second": 0.461, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The green bottle is near the sink, the blue bottle is near the stove, and the red bow is near the stove. The microwave is above the stove, and the refrigerator is to the left of the stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.7, "ram_available_mb": 99787.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.45, "peak": 119.05, "min": 32.67}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 28.71, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.71, "energy_joules_est": 62.23, "sample_count": 21, "duration_seconds": 2.167}, "timestamp": "2026-01-19T12:51:46.960154"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1402.122, "latencies_ms": [1402.122], "images_per_second": 0.713, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A messy kitchen with a lot of stuff on the counter and cabinets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.9, "peak": 126.01, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 32.28, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 32.28, "energy_joules_est": 45.27, "sample_count": 14, "duration_seconds": 1.402}, "timestamp": "2026-01-19T12:51:48.413544"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1533.891, "latencies_ms": [1533.891], "images_per_second": 0.652, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The kitchen is well-lit with natural light, and the cabinets are made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.35, "peak": 119.69, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.49, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 32.49, "energy_joules_est": 49.84, "sample_count": 15, "duration_seconds": 1.534}, "timestamp": "2026-01-19T12:51:49.978258"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1408.483, "latencies_ms": [1408.483], "images_per_second": 0.71, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A kite with a red and white pattern is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.01, "peak": 123.0, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 32.92, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 32.92, "energy_joules_est": 46.38, "sample_count": 14, "duration_seconds": 1.409}, "timestamp": "2026-01-19T12:51:51.441667"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2067.627, "latencies_ms": [2067.627], "images_per_second": 0.484, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " kite: 1, string: 2, kite string: 1, kite tail: 1, kite frame: 1, kite fabric: 1, kite handle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.7, "peak": 126.5, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.84, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 29.84, "energy_joules_est": 61.71, "sample_count": 20, "duration_seconds": 2.068}, "timestamp": "2026-01-19T12:51:53.532064"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2161.969, "latencies_ms": [2161.969], "images_per_second": 0.463, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the kite strings are in the background. The kite is positioned to the left of the frame, and the kite strings are extending towards the right.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.39, "peak": 120.83, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.78, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 28.78, "energy_joules_est": 62.23, "sample_count": 21, "duration_seconds": 2.162}, "timestamp": "2026-01-19T12:51:55.718511"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1402.432, "latencies_ms": [1402.432], "images_per_second": 0.713, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A kite with a red and white pattern is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.64, "peak": 128.9, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.05, "peak": 39.39, "min": 17.34}}, "power_watts_avg": 32.05, "energy_joules_est": 44.97, "sample_count": 14, "duration_seconds": 1.403}, "timestamp": "2026-01-19T12:51:57.175699"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1520.342, "latencies_ms": [1520.342], "images_per_second": 0.658, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The kite is white with red and black patterns and is flying in a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.37, "peak": 117.97, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.59, "peak": 40.97, "min": 20.5}}, "power_watts_avg": 32.59, "energy_joules_est": 49.56, "sample_count": 15, "duration_seconds": 1.521}, "timestamp": "2026-01-19T12:51:58.742055"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2327.948, "latencies_ms": [2327.948], "images_per_second": 0.43, "prompt_tokens": 1099, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image depicts a spacious bedroom with two queen-sized beds, each adorned with white linens and a dark-colored bedspread, positioned side by side, with a window in the background allowing natural light to filter into the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.01, "peak": 123.79, "min": 27.8}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.15, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 28.15, "energy_joules_est": 65.55, "sample_count": 23, "duration_seconds": 2.329}, "timestamp": "2026-01-19T12:52:01.156543"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2510.074, "latencies_ms": [2510.074], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 55, "n_tiles": 1, "output_text": " 1. beds: 2\n2. pillows: 12\n3. lamps: 2\n4. paintings: 2\n5. window: 1\n6. door: 1\n7. floor: wooden\n8. ceiling: white", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25984.4, "ram_available_mb": 99787.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25988.4, "ram_available_mb": 99783.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.63, "peak": 134.59, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.78, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.78, "energy_joules_est": 67.23, "sample_count": 25, "duration_seconds": 2.51}, "timestamp": "2026-01-19T12:52:03.758119"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2109.515, "latencies_ms": [2109.515], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The beds are positioned on the left side of the room, with the window and door located on the right side. The beds are in the foreground, with the window and door in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25988.4, "ram_available_mb": 99783.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.9, "peak": 105.43, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.01, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.01, "energy_joules_est": 59.1, "sample_count": 21, "duration_seconds": 2.11}, "timestamp": "2026-01-19T12:52:05.944514"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1476.031, "latencies_ms": [1476.031], "images_per_second": 0.677, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The room is a bedroom with two beds, a window, and a door.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.47, "peak": 120.21, "min": 27.45}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.94, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.94, "energy_joules_est": 45.68, "sample_count": 15, "duration_seconds": 1.476}, "timestamp": "2026-01-19T12:52:07.516605"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1914.784, "latencies_ms": [1914.784], "images_per_second": 0.522, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The room is bathed in soft light from the large windows, and the walls are painted a soothing light green. The wooden floors add warmth to the space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.49, "peak": 128.0, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.71, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 29.71, "energy_joules_est": 56.9, "sample_count": 19, "duration_seconds": 1.915}, "timestamp": "2026-01-19T12:52:09.498121"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1890.16, "latencies_ms": [1890.16], "images_per_second": 0.529, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man wearing a white helmet and a white and green racing suit is riding a white motorcycle down a road with a crowd of people watching from behind a fence.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.33, "peak": 111.71, "min": 27.67}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.44, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.34, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 29.34, "energy_joules_est": 55.47, "sample_count": 19, "duration_seconds": 1.891}, "timestamp": "2026-01-19T12:52:11.486916"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1998.83, "latencies_ms": [1998.83], "images_per_second": 0.5, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. motorcycle: 1\n2. rider: 1\n3. helmet: 1\n4. fence: 1\n5. people: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.99, "peak": 122.5, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.6, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.6, "energy_joules_est": 57.18, "sample_count": 20, "duration_seconds": 1.999}, "timestamp": "2026-01-19T12:52:13.573820"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2188.41, "latencies_ms": [2188.41], "images_per_second": 0.457, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The motorcycle is positioned on the left side of the image, with the rider leaning into a turn. The spectators are located in the background, behind a fence, and appear to be watching the motorcycle from a distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.68, "peak": 129.91, "min": 30.05}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.44, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.92, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.92, "energy_joules_est": 61.11, "sample_count": 22, "duration_seconds": 2.189}, "timestamp": "2026-01-19T12:52:15.865659"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1479.969, "latencies_ms": [1479.969], "images_per_second": 0.676, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is riding a motorcycle down a road with a crowd of people watching.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.94, "peak": 131.09, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.84, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.84, "energy_joules_est": 45.65, "sample_count": 15, "duration_seconds": 1.48}, "timestamp": "2026-01-19T12:52:17.432908"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1735.903, "latencies_ms": [1735.903], "images_per_second": 0.576, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The motorcycle is white and green, and the rider is wearing a white helmet. The sky is cloudy and the road is wet.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.96, "peak": 115.66, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.34, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.92, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 30.92, "energy_joules_est": 53.68, "sample_count": 17, "duration_seconds": 1.736}, "timestamp": "2026-01-19T12:52:19.204855"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1617.498, "latencies_ms": [1617.498], "images_per_second": 0.618, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A table set with white tablecloths and glasses is adorned with a centerpiece of white flowers and candles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.74, "peak": 104.7, "min": 26.94}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 31.22, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.22, "energy_joules_est": 50.51, "sample_count": 16, "duration_seconds": 1.618}, "timestamp": "2026-01-19T12:52:20.881492"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1623.011, "latencies_ms": [1623.011], "images_per_second": 0.616, "prompt_tokens": 1113, "response_tokens_est": 21, "n_tiles": 1, "output_text": " tablecloth: 1\nglass: 4\nflower vase: 1\nflower: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.62, "peak": 120.46, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.25, "peak": 39.78, "min": 18.53}}, "power_watts_avg": 31.25, "energy_joules_est": 50.73, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T12:52:22.550999"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2518.719, "latencies_ms": [2518.719], "images_per_second": 0.397, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The vase is placed in the center of the table, with the glasses arranged around it. The glasses are positioned on either side of the vase, creating a symmetrical arrangement. The table is set against a dark background, which helps to draw attention to the centerpiece.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.72, "peak": 106.67, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.23, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.23, "energy_joules_est": 68.6, "sample_count": 25, "duration_seconds": 2.519}, "timestamp": "2026-01-19T12:52:25.144179"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2784.521, "latencies_ms": [2784.521], "images_per_second": 0.359, "prompt_tokens": 1111, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image captures a serene dining setting, where a table draped in pristine white tablecloth is elegantly set for a meal. The table is adorned with a centerpiece of white flowers, which are nestled in a clear glass vase, and surrounded by four empty wine glasses, their stems gleaming under the soft light.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25988.7, "ram_available_mb": 99783.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.34, "peak": 100.88, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.12, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.12, "energy_joules_est": 72.74, "sample_count": 27, "duration_seconds": 2.785}, "timestamp": "2026-01-19T12:52:27.966397"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2055.16, "latencies_ms": [2055.16], "images_per_second": 0.487, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The table is set with white tablecloths and napkins, and the flowers are white and green. The lighting is soft and ambient, and the table is set in a dark room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.53, "peak": 119.21, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.66, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.66, "energy_joules_est": 58.92, "sample_count": 20, "duration_seconds": 2.056}, "timestamp": "2026-01-19T12:52:30.052197"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1932.142, "latencies_ms": [1932.142], "images_per_second": 0.518, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A black and white photo of a clock on a pole with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.13, "peak": 134.81, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 31.62, "peak": 41.76, "min": 16.95}}, "power_watts_avg": 31.62, "energy_joules_est": 61.12, "sample_count": 19, "duration_seconds": 1.933}, "timestamp": "2026-01-19T12:52:32.034437"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2925.868, "latencies_ms": [2925.868], "images_per_second": 0.342, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. clock: 2\n2. pole: 1\n3. sky: 1\n4. clouds: 1\n5. grass: 1\n6. person: 1\n7. bird: 1\n8. smoke: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.27, "peak": 119.62, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.26, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.64, "peak": 42.15, "min": 17.74}}, "power_watts_avg": 28.64, "energy_joules_est": 83.82, "sample_count": 29, "duration_seconds": 2.927}, "timestamp": "2026-01-19T12:52:35.061635"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2338.14, "latencies_ms": [2338.14], "images_per_second": 0.428, "prompt_tokens": 1450, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The clock is positioned in the foreground, with the sky in the background. The clock is located to the left of the image, while the sky fills the entire background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.12, "peak": 131.04, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.36, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.16, "peak": 42.15, "min": 15.38}}, "power_watts_avg": 30.16, "energy_joules_est": 70.53, "sample_count": 23, "duration_seconds": 2.339}, "timestamp": "2026-01-19T12:52:37.446964"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1931.244, "latencies_ms": [1931.244], "images_per_second": 0.518, "prompt_tokens": 1444, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A black and white photo of a clock on a pole with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.31, "peak": 122.05, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 32.2, "peak": 41.76, "min": 17.35}}, "power_watts_avg": 32.2, "energy_joules_est": 62.2, "sample_count": 19, "duration_seconds": 1.932}, "timestamp": "2026-01-19T12:52:39.425681"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1764.733, "latencies_ms": [1764.733], "images_per_second": 0.567, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The clock is black and white, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.77, "peak": 125.95, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 33.31, "peak": 42.54, "min": 19.32}}, "power_watts_avg": 33.31, "energy_joules_est": 58.79, "sample_count": 18, "duration_seconds": 1.765}, "timestamp": "2026-01-19T12:52:41.295548"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1691.718, "latencies_ms": [1691.718], "images_per_second": 0.591, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A young man wearing a black cap and a black shirt is riding a skateboard on a paved surface in a park.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.01, "peak": 135.35, "min": 28.34}, "VIN_SYS_5V0": {"avg": 13.98, "peak": 15.44, "min": 11.26}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 30.5, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 30.5, "energy_joules_est": 51.62, "sample_count": 17, "duration_seconds": 1.693}, "timestamp": "2026-01-19T12:52:43.090594"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2815.686, "latencies_ms": [2815.686], "images_per_second": 0.355, "prompt_tokens": 1114, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. person: 1\n2. skateboard: 1\n3. black t-shirt: 1\n4. black pants: 1\n5. blue cap: 1\n6. gray shoes: 1\n7. red and white logo: 1\n8. tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.73, "peak": 129.73, "min": 27.93}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.08, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.08, "energy_joules_est": 73.45, "sample_count": 28, "duration_seconds": 2.816}, "timestamp": "2026-01-19T12:52:45.990293"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2052.651, "latencies_ms": [2052.651], "images_per_second": 0.487, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick on the pavement. The skateboarder is in the middle of the image, with the background featuring trees and a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.44, "peak": 125.75, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.21, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.21, "energy_joules_est": 57.92, "sample_count": 20, "duration_seconds": 2.053}, "timestamp": "2026-01-19T12:52:48.080388"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1489.127, "latencies_ms": [1489.127], "images_per_second": 0.672, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A young man is skateboarding on a paved area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.45, "peak": 117.09, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.85, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.63, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 30.63, "energy_joules_est": 45.63, "sample_count": 15, "duration_seconds": 1.49}, "timestamp": "2026-01-19T12:52:49.640972"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2354.293, "latencies_ms": [2354.293], "images_per_second": 0.425, "prompt_tokens": 1110, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image features a young man wearing a black t-shirt and a blue cap, performing a trick on a skateboard in an outdoor setting. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.2, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.56, "peak": 118.48, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.85, "peak": 40.57, "min": 18.52}}, "power_watts_avg": 27.85, "energy_joules_est": 65.57, "sample_count": 23, "duration_seconds": 2.355}, "timestamp": "2026-01-19T12:52:52.038387"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1843.44, "latencies_ms": [1843.44], "images_per_second": 0.542, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A white plate filled with orange carrots and green beans is placed on a kitchen counter, with a blue peeler and a bunch of red onions nearby.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.83, "peak": 110.32, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.59, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.59, "energy_joules_est": 54.57, "sample_count": 18, "duration_seconds": 1.844}, "timestamp": "2026-01-19T12:52:53.915899"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2034.043, "latencies_ms": [2034.043], "images_per_second": 0.492, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " carrot: 10, plate: 1, scissors: 1, knife: 1, bottle: 1, bean: 1, carrot peeler: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.62, "peak": 127.36, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.25, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 29.25, "energy_joules_est": 59.51, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T12:52:55.998298"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2170.287, "latencies_ms": [2170.287], "images_per_second": 0.461, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The plate of carrots is located in the foreground, with the peeler and beetroots placed nearby. The peeler is positioned to the right of the plate, while the beetroots are situated to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.98, "peak": 114.74, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.59, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.59, "energy_joules_est": 62.07, "sample_count": 21, "duration_seconds": 2.171}, "timestamp": "2026-01-19T12:52:58.183123"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1655.784, "latencies_ms": [1655.784], "images_per_second": 0.604, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " In a kitchen, a white plate is filled with fresh carrots and beets, while a blue peeler lies nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.83, "peak": 130.23, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.07, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.07, "energy_joules_est": 51.47, "sample_count": 16, "duration_seconds": 1.656}, "timestamp": "2026-01-19T12:52:59.856629"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1622.894, "latencies_ms": [1622.894], "images_per_second": 0.616, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The image is taken in a kitchen with natural lighting, and the carrots are fresh and vibrant in color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.27, "peak": 109.21, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.79}, "VDD_GPU": {"avg": 31.42, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 31.42, "energy_joules_est": 51.01, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T12:53:01.523248"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1542.376, "latencies_ms": [1542.376], "images_per_second": 0.648, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man in a suit is giving a presentation on a large screen in front of an audience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.76, "peak": 113.54, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.86, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 31.86, "energy_joules_est": 49.16, "sample_count": 15, "duration_seconds": 1.543}, "timestamp": "2026-01-19T12:53:03.096637"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2518.639, "latencies_ms": [2518.639], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 2\n3. head: 2\n4. hair: 2\n5. glasses: 1\n6. tie: 1\n7. suit: 1\n8. shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.71, "peak": 107.76, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.78, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 27.78, "energy_joules_est": 69.98, "sample_count": 25, "duration_seconds": 2.519}, "timestamp": "2026-01-19T12:53:05.688455"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2178.649, "latencies_ms": [2178.649], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man is in the foreground, and the audience is in the background. The man is standing in front of a large screen, which is in the foreground. The audience is watching the man on the screen.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.74, "peak": 126.51, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.16, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.16, "energy_joules_est": 61.36, "sample_count": 21, "duration_seconds": 2.179}, "timestamp": "2026-01-19T12:53:07.882813"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1467.389, "latencies_ms": [1467.389], "images_per_second": 0.681, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is giving a presentation on a large screen in front of an audience.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.33, "peak": 126.39, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.52, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.52, "energy_joules_est": 46.27, "sample_count": 15, "duration_seconds": 1.468}, "timestamp": "2026-01-19T12:53:09.455369"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2297.775, "latencies_ms": [2297.775], "images_per_second": 0.435, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image is of a man giving a presentation on a large screen. The colors are vibrant, with the man wearing a suit and tie. The lighting is bright and focused on the man, while the audience is in shadow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.16, "peak": 118.55, "min": 27.23}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.44, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 27.87, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.87, "energy_joules_est": 64.05, "sample_count": 23, "duration_seconds": 2.298}, "timestamp": "2026-01-19T12:53:11.847943"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3462.747, "latencies_ms": [3462.747], "images_per_second": 0.289, "prompt_tokens": 1099, "response_tokens_est": 87, "n_tiles": 1, "output_text": " In the image, two workers clad in blue uniforms and hard hats are diligently engaged in the task of laying down a street curb. The worker on the left, equipped with a tool, is actively participating in the process, while his counterpart on the right, holding a clipboard, appears to be overseeing the task. The backdrop of the scene is a bustling street lined with parked motorcycles, adding a dynamic element to the otherwise static scene.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.86, "peak": 118.56, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.92, "min": 12.22}, "VDD_GPU": {"avg": 24.44, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 24.44, "energy_joules_est": 84.65, "sample_count": 34, "duration_seconds": 3.463}, "timestamp": "2026-01-19T12:53:15.404988"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2777.89, "latencies_ms": [2777.89], "images_per_second": 0.36, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Scooter: 2\n2. Motorcycle: 1\n3. Banners: 2\n4. Signboards: 2\n5. People: 2\n6. Posters: 1\n7. Banners: 1\n8. Banners: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.96, "peak": 127.63, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.44, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.08, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.08, "energy_joules_est": 72.46, "sample_count": 27, "duration_seconds": 2.778}, "timestamp": "2026-01-19T12:53:18.210689"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1978.124, "latencies_ms": [1978.124], "images_per_second": 0.506, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The two workers are standing on the sidewalk, with the motorcycles parked on the street behind them. The workers are positioned in the foreground, while the motorcycles are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.83, "peak": 119.44, "min": 31.23}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.24, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.24, "energy_joules_est": 57.86, "sample_count": 19, "duration_seconds": 1.979}, "timestamp": "2026-01-19T12:53:20.193478"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1548.106, "latencies_ms": [1548.106], "images_per_second": 0.646, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two construction workers are standing on a street corner, one of them is holding a metal rod.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.1, "ram_available_mb": 99784.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.12, "peak": 127.39, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.78, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.78, "energy_joules_est": 49.21, "sample_count": 15, "duration_seconds": 1.548}, "timestamp": "2026-01-19T12:53:21.758296"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1957.215, "latencies_ms": [1957.215], "images_per_second": 0.511, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image depicts a rainy day with a gray sky and wet pavement. The two workers are wearing blue uniforms and hard hats, and they are standing on a street corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25988.4, "ram_available_mb": 99783.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.1, "ram_available_mb": 99783.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.29, "peak": 120.1, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.21, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 30.21, "energy_joules_est": 59.15, "sample_count": 19, "duration_seconds": 1.958}, "timestamp": "2026-01-19T12:53:23.736467"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2175.76, "latencies_ms": [2175.76], "images_per_second": 0.46, "prompt_tokens": 1432, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A black plastic container filled with a green and yellow mixture of vegetables and chicken is placed on a white paper plate, accompanied by a fork.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25989.1, "ram_available_mb": 99783.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.1, "ram_available_mb": 99783.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.73, "peak": 127.47, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 30.82, "peak": 41.76, "min": 17.35}}, "power_watts_avg": 30.82, "energy_joules_est": 67.08, "sample_count": 21, "duration_seconds": 2.177}, "timestamp": "2026-01-19T12:53:25.929214"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3095.513, "latencies_ms": [3095.513], "images_per_second": 0.323, "prompt_tokens": 1446, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. black bowl: 1\n2. white plate: 1\n3. fork: 1\n4. shredded chicken: 1\n5. broccoli: 1\n6. mashed potatoes: 1\n7. green sauce: 1\n8. beige carpet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.1, "ram_available_mb": 99783.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.1, "ram_available_mb": 99783.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.92, "peak": 119.44, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.46, "peak": 41.76, "min": 18.14}}, "power_watts_avg": 28.46, "energy_joules_est": 88.11, "sample_count": 30, "duration_seconds": 3.096}, "timestamp": "2026-01-19T12:53:29.055284"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2434.76, "latencies_ms": [2434.76], "images_per_second": 0.411, "prompt_tokens": 1450, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The black bowl of food is on the left side of the plate, and the fork is on the right side. The plate is in the foreground, and the carpet is in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.1, "ram_available_mb": 99783.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.5, "peak": 110.57, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 30.11, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 30.11, "energy_joules_est": 73.32, "sample_count": 24, "duration_seconds": 2.435}, "timestamp": "2026-01-19T12:53:31.550417"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1862.14, "latencies_ms": [1862.14], "images_per_second": 0.537, "prompt_tokens": 1444, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A black plastic container with food is on a white plate with a fork on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.64, "peak": 137.86, "min": 29.97}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.15, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.72, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 32.72, "energy_joules_est": 60.95, "sample_count": 18, "duration_seconds": 1.863}, "timestamp": "2026-01-19T12:53:33.427353"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2179.802, "latencies_ms": [2179.802], "images_per_second": 0.459, "prompt_tokens": 1442, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken in a room with a carpeted floor and a white plate on it. The lighting is natural and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.4, "peak": 128.55, "min": 33.89}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 32.36, "peak": 42.13, "min": 20.89}}, "power_watts_avg": 32.36, "energy_joules_est": 70.57, "sample_count": 21, "duration_seconds": 2.181}, "timestamp": "2026-01-19T12:53:35.617252"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1618.855, "latencies_ms": [1618.855], "images_per_second": 0.618, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man wearing a blue shirt, red tie, and a plaid hat stands in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.91, "peak": 131.5, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.4, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 31.4, "energy_joules_est": 50.85, "sample_count": 16, "duration_seconds": 1.62}, "timestamp": "2026-01-19T12:53:37.289401"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2534.434, "latencies_ms": [2534.434], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. glasses: 1\n3. tie: 1\n4. shirt: 1\n5. cap: 1\n6. building: 1\n7. window: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.03, "peak": 106.49, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.39, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 27.39, "energy_joules_est": 69.43, "sample_count": 25, "duration_seconds": 2.535}, "timestamp": "2026-01-19T12:53:39.885003"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2239.773, "latencies_ms": [2239.773], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The man is positioned in the foreground of the image, with the background consisting of a building and a pool. The man is wearing a blue shirt and a red tie, and he is also wearing a plaid hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.47, "peak": 122.12, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.87, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.87, "energy_joules_est": 62.43, "sample_count": 22, "duration_seconds": 2.24}, "timestamp": "2026-01-19T12:53:42.175242"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1524.913, "latencies_ms": [1524.913], "images_per_second": 0.656, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man wearing a blue shirt and a red tie stands in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 80.91, "peak": 117.86, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.07, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 31.07, "energy_joules_est": 47.4, "sample_count": 15, "duration_seconds": 1.526}, "timestamp": "2026-01-19T12:53:43.740918"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2266.893, "latencies_ms": [2266.893], "images_per_second": 0.441, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The man is wearing a blue shirt and a red tie, and he is standing in front of a building with a green roof. The lighting is bright and sunny, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.85, "peak": 123.27, "min": 30.43}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.46, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 28.46, "energy_joules_est": 64.53, "sample_count": 22, "duration_seconds": 2.267}, "timestamp": "2026-01-19T12:53:46.035232"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2137.261, "latencies_ms": [2137.261], "images_per_second": 0.468, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image is a collage of six photos showing different slices of pizza, each with a different topping, on a white plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.94, "peak": 129.36, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.15, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.36, "peak": 40.97, "min": 14.98}}, "power_watts_avg": 30.36, "energy_joules_est": 64.9, "sample_count": 21, "duration_seconds": 2.138}, "timestamp": "2026-01-19T12:53:48.226067"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1500.406, "latencies_ms": [1500.406], "images_per_second": 0.666, "prompt_tokens": 1446, "response_tokens_est": 4, "n_tiles": 1, "output_text": " pizza: 6", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 1.8, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.07, "peak": 117.77, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 16.36, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 34.12, "peak": 42.54, "min": 17.35}}, "power_watts_avg": 34.12, "energy_joules_est": 51.2, "sample_count": 15, "duration_seconds": 1.501}, "timestamp": "2026-01-19T12:53:49.790217"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2933.641, "latencies_ms": [2933.641], "images_per_second": 0.341, "prompt_tokens": 1450, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The main objects are arranged in a grid pattern, with the largest pizza slice in the top left and the smallest slice in the bottom right. The slices are positioned at varying distances from the camera, with the largest slice closest to the lens and the smallest slice farthest away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.36, "peak": 128.24, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.26, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.45, "peak": 42.94, "min": 17.73}}, "power_watts_avg": 29.45, "energy_joules_est": 86.42, "sample_count": 29, "duration_seconds": 2.934}, "timestamp": "2026-01-19T12:53:52.804841"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1785.961, "latencies_ms": [1785.961], "images_per_second": 0.56, "prompt_tokens": 1444, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The image is a collage of six photos showing different slices of pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.86, "peak": 121.66, "min": 27.12}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.26, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.39, "peak": 42.15, "min": 15.38}}, "power_watts_avg": 32.39, "energy_joules_est": 57.86, "sample_count": 18, "duration_seconds": 1.786}, "timestamp": "2026-01-19T12:53:54.680216"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2160.996, "latencies_ms": [2160.996], "images_per_second": 0.463, "prompt_tokens": 1442, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The pizza has a golden brown crust and is topped with melted cheese and various ingredients. The lighting is warm and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.4, "peak": 117.68, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 16.05, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 17.32, "min": 12.21}, "VDD_GPU": {"avg": 31.74, "peak": 42.15, "min": 18.92}}, "power_watts_avg": 31.74, "energy_joules_est": 68.61, "sample_count": 21, "duration_seconds": 2.161}, "timestamp": "2026-01-19T12:53:56.876890"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1555.46, "latencies_ms": [1555.46], "images_per_second": 0.643, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " Two young girls pet a goat in a fenced area, with a third goat in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.77, "peak": 126.99, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.86, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.86, "energy_joules_est": 49.58, "sample_count": 15, "duration_seconds": 1.556}, "timestamp": "2026-01-19T12:53:58.452055"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2187.422, "latencies_ms": [2187.422], "images_per_second": 0.457, "prompt_tokens": 1113, "response_tokens_est": 43, "n_tiles": 1, "output_text": " goat: 2, girl: 2, fence: 1, girl's dress: 1, girl's hair: 1, girl's hair clip: 1, girl's shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.95, "peak": 114.9, "min": 37.07}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.34, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 29.34, "energy_joules_est": 64.19, "sample_count": 21, "duration_seconds": 2.188}, "timestamp": "2026-01-19T12:54:00.643878"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1900.883, "latencies_ms": [1900.883], "images_per_second": 0.526, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The goat is in the foreground, with the two girls standing behind it. The girls are positioned to the left of the goat, with the fence in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.07, "peak": 119.83, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.61, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.61, "energy_joules_est": 56.29, "sample_count": 19, "duration_seconds": 1.901}, "timestamp": "2026-01-19T12:54:02.624991"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1627.536, "latencies_ms": [1627.536], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two young girls pet a goat in a fenced area, with a grassy field and trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.96, "peak": 115.96, "min": 30.32}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.8, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.8, "energy_joules_est": 50.14, "sample_count": 16, "duration_seconds": 1.628}, "timestamp": "2026-01-19T12:54:04.296435"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1860.224, "latencies_ms": [1860.224], "images_per_second": 0.538, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image is taken in bright daylight with a clear blue sky in the background. The girls are wearing colorful dresses, and the goats are black and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.05, "peak": 123.13, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.4, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 30.4, "energy_joules_est": 56.56, "sample_count": 18, "duration_seconds": 1.861}, "timestamp": "2026-01-19T12:54:06.177725"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2454.351, "latencies_ms": [2454.351], "images_per_second": 0.407, "prompt_tokens": 1099, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image captures a nighttime scene at an intersection, where a traffic light is illuminated by a green light, and a street sign is visible in the background, indicating the intersection of \"AVENUE OF THE DOLLS\" and \"SANTA FE DRIVE\".", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.26, "peak": 118.68, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.61, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 27.61, "energy_joules_est": 67.79, "sample_count": 24, "duration_seconds": 2.455}, "timestamp": "2026-01-19T12:54:08.682260"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1981.551, "latencies_ms": [1981.551], "images_per_second": 0.505, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. traffic light\n2. street sign\n3. street light\n4. car\n5. road\n6. mountain\n7. building\n8. sky", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.39, "peak": 123.91, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.68, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.68, "energy_joules_est": 56.84, "sample_count": 20, "duration_seconds": 1.982}, "timestamp": "2026-01-19T12:54:10.767218"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2366.05, "latencies_ms": [2366.05], "images_per_second": 0.423, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The traffic light is on the left side of the image, while the street sign is on the right side. The traffic light is closer to the viewer than the street sign. The street sign is in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.49, "peak": 117.46, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.49, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.49, "energy_joules_est": 65.06, "sample_count": 23, "duration_seconds": 2.367}, "timestamp": "2026-01-19T12:54:13.158350"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2725.98, "latencies_ms": [2725.98], "images_per_second": 0.367, "prompt_tokens": 1111, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image captures a quiet night at an intersection in a city, where the traffic lights are glowing green, signaling the start of a new day. The sky is painted with hues of blue and orange, reflecting the setting sun. The streetlights cast a warm glow on the buildings and the road, creating a serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.8, "peak": 120.53, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.5, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.5, "energy_joules_est": 72.26, "sample_count": 27, "duration_seconds": 2.727}, "timestamp": "2026-01-19T12:54:15.968447"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2958.078, "latencies_ms": [2958.078], "images_per_second": 0.338, "prompt_tokens": 1109, "response_tokens_est": 72, "n_tiles": 1, "output_text": " The image captures a tranquil night scene at an intersection, where the sky is painted in shades of deep blue, and the moon casts a soft glow. The streetlights and traffic signals, glowing with a cool green hue, stand out against the dark backdrop. The buildings in the distance are silhouetted against the night sky, their details obscured by the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.13, "peak": 126.08, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 25.72, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 25.72, "energy_joules_est": 76.09, "sample_count": 29, "duration_seconds": 2.959}, "timestamp": "2026-01-19T12:54:18.985348"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1854.333, "latencies_ms": [1854.333], "images_per_second": 0.539, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " In the image, a woman is standing in front of a building, smiling at the camera, with a bunch of bananas placed in front of her.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.81, "peak": 123.35, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.44, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.44, "energy_joules_est": 54.62, "sample_count": 18, "duration_seconds": 1.855}, "timestamp": "2026-01-19T12:54:20.874783"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2563.878, "latencies_ms": [2563.878], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. woman: 1\n2. bananas: 2\n3. building: 1\n4. door: 1\n5. wall: 1\n6. sign: 1\n7. window: 1\n8. door frame: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.88, "peak": 113.3, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.22, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 27.22, "energy_joules_est": 69.8, "sample_count": 25, "duration_seconds": 2.564}, "timestamp": "2026-01-19T12:54:23.480360"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1937.726, "latencies_ms": [1937.726], "images_per_second": 0.516, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The bananas are in the foreground, with the woman standing behind them. The woman is positioned to the right of the bananas, and the building is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.04, "peak": 113.84, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.17, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.17, "energy_joules_est": 56.54, "sample_count": 19, "duration_seconds": 1.938}, "timestamp": "2026-01-19T12:54:25.460447"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1489.016, "latencies_ms": [1489.016], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman is standing in front of a building with bananas in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.65, "peak": 109.3, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.41, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.41, "energy_joules_est": 46.78, "sample_count": 15, "duration_seconds": 1.489}, "timestamp": "2026-01-19T12:54:27.031546"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2102.647, "latencies_ms": [2102.647], "images_per_second": 0.476, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image features a woman standing in front of a building with a yellow exterior, and the bananas she is selling are yellow and ripe. The lighting is natural and bright, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.36, "peak": 123.98, "min": 27.56}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.82, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 28.82, "energy_joules_est": 60.61, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T12:54:29.222622"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4922.996, "latencies_ms": [4922.996], "images_per_second": 0.203, "prompt_tokens": 1432, "response_tokens_est": 128, "n_tiles": 1, "output_text": " The image captures a vibrant urban scene featuring a red brick building with two shuttered storefronts, each adorned with a variety of graffiti tags. The building's green fire escape, a common sight in many cityscapes, adds a touch of color and contrast to the scene. A solitary fire hydrant stands guard on the sidewalk, while a tree with lush green leaves adds a touch of nature to the urban landscape. The building's address, \"127,\" is prominently displayed on the door, hinting at the building's identity. The overall composition of the image suggests a bustling city life, where art and architecture coexist in harmony.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.33, "peak": 124.53, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.26, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 24.44, "peak": 41.76, "min": 15.38}}, "power_watts_avg": 24.44, "energy_joules_est": 120.33, "sample_count": 48, "duration_seconds": 4.924}, "timestamp": "2026-01-19T12:54:34.219536"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3024.552, "latencies_ms": [3024.552], "images_per_second": 0.331, "prompt_tokens": 1446, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. fire hydrant: 1\n2. tree: 1\n3. building: 1\n4. door: 1\n5. window: 2\n6. fire escape: 1\n7. graffiti: 2\n8. sidewalk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.47, "peak": 115.71, "min": 27.37}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.36, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 27.86, "peak": 41.76, "min": 15.77}}, "power_watts_avg": 27.86, "energy_joules_est": 84.28, "sample_count": 30, "duration_seconds": 3.025}, "timestamp": "2026-01-19T12:54:37.341575"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3054.796, "latencies_ms": [3054.796], "images_per_second": 0.327, "prompt_tokens": 1450, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The green door is located in the middle of the building, with the fire escape above it. The fire hydrant is on the left side of the building, and the tree is on the right side. The sidewalk is in front of the building, and the street is in front of the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.58, "peak": 126.56, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.26, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 27.82, "peak": 41.76, "min": 14.98}}, "power_watts_avg": 27.82, "energy_joules_est": 85.0, "sample_count": 30, "duration_seconds": 3.055}, "timestamp": "2026-01-19T12:54:40.455962"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2657.534, "latencies_ms": [2657.534], "images_per_second": 0.376, "prompt_tokens": 1444, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image captures a vibrant urban scene, where a red brick building stands proudly on a city street corner. The building's green fire escape and shuttered storefronts add a touch of color to the otherwise monochrome setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.22, "peak": 130.66, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 16.36, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 29.11, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 29.11, "energy_joules_est": 77.37, "sample_count": 26, "duration_seconds": 2.658}, "timestamp": "2026-01-19T12:54:43.167175"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2855.607, "latencies_ms": [2855.607], "images_per_second": 0.35, "prompt_tokens": 1442, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image depicts a red brick building with green shutters and a fire escape. The building is covered in graffiti, and the sidewalk is lined with a fire hydrant. The weather appears to be overcast, as the sky is gray and the lighting is flat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.76, "peak": 134.01, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.36, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 28.55, "peak": 42.15, "min": 16.56}}, "power_watts_avg": 28.55, "energy_joules_est": 81.55, "sample_count": 28, "duration_seconds": 2.856}, "timestamp": "2026-01-19T12:54:46.088029"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1547.01, "latencies_ms": [1547.01], "images_per_second": 0.646, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man wearing a blue beanie is holding a yellow frisbee in his hand.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.18, "peak": 127.97, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.94, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.94, "energy_joules_est": 47.89, "sample_count": 15, "duration_seconds": 1.548}, "timestamp": "2026-01-19T12:54:47.659416"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2612.537, "latencies_ms": [2612.537], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. head: 1\n3. hair: 1\n4. shirt: 1\n5. frisbee: 1\n6. hand: 1\n7. wrist: 1\n8. wristwatch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.85, "peak": 121.69, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.38, "peak": 40.57, "min": 17.35}}, "power_watts_avg": 27.38, "energy_joules_est": 71.54, "sample_count": 26, "duration_seconds": 2.613}, "timestamp": "2026-01-19T12:54:50.365479"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2085.042, "latencies_ms": [2085.042], "images_per_second": 0.48, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The yellow frisbee is in the foreground, close to the camera. The person is in the background, far from the camera. The crowd is in the background, behind the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.16, "peak": 132.42, "min": 33.92}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.39, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.39, "energy_joules_est": 59.21, "sample_count": 20, "duration_seconds": 2.086}, "timestamp": "2026-01-19T12:54:52.457034"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1522.309, "latencies_ms": [1522.309], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man is throwing a frisbee in a room with a crowd of people watching.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.68, "peak": 127.61, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.65, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 31.65, "energy_joules_est": 48.19, "sample_count": 15, "duration_seconds": 1.523}, "timestamp": "2026-01-19T12:54:54.023214"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1935.113, "latencies_ms": [1935.113], "images_per_second": 0.517, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image is a black and white photo with a sepia tone, and the lighting is dim. The subject is wearing a black shirt and a blue beanie.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.6, "peak": 128.34, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.04, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 30.04, "energy_joules_est": 58.14, "sample_count": 19, "duration_seconds": 1.935}, "timestamp": "2026-01-19T12:54:56.010539"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1549.55, "latencies_ms": [1549.55], "images_per_second": 0.645, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A group of people are sitting around a table with laptops and books, working on their projects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.49, "peak": 129.6, "min": 30.39}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.41, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.41, "energy_joules_est": 48.69, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T12:54:57.587649"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1944.41, "latencies_ms": [1944.41], "images_per_second": 0.514, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " laptop: 3, keyboard: 1, mouse: 1, cup: 1, bottle: 1, book: 1, person: 7", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.4, "peak": 126.5, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.19, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 30.19, "energy_joules_est": 58.72, "sample_count": 19, "duration_seconds": 1.945}, "timestamp": "2026-01-19T12:54:59.570870"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2264.235, "latencies_ms": [2264.235], "images_per_second": 0.442, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The laptops are positioned on the left side of the table, with the keyboard and mouse on the right side. The group of people is gathered around the table, with some standing closer to the foreground and others further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.17, "peak": 117.12, "min": 30.73}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.21, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.21, "energy_joules_est": 63.9, "sample_count": 22, "duration_seconds": 2.265}, "timestamp": "2026-01-19T12:55:01.855562"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1568.51, "latencies_ms": [1568.51], "images_per_second": 0.638, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are sitting around a table with laptops and books, working on a project together.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.3, "ram_available_mb": 99782.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.87, "peak": 113.53, "min": 28.44}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.8, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.8, "energy_joules_est": 48.32, "sample_count": 16, "duration_seconds": 1.569}, "timestamp": "2026-01-19T12:55:03.524847"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1696.963, "latencies_ms": [1696.963], "images_per_second": 0.589, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The room is dimly lit with yellow and green lighting, and the walls are covered in black and white tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.84, "peak": 104.18, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.55, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.55, "energy_joules_est": 51.86, "sample_count": 17, "duration_seconds": 1.697}, "timestamp": "2026-01-19T12:55:05.295927"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1974.475, "latencies_ms": [1974.475], "images_per_second": 0.506, "prompt_tokens": 1432, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A young girl wearing a pink jacket and blue jeans is holding a blue umbrella and a brown cookie.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.19, "peak": 130.05, "min": 33.07}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.26, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 31.73, "peak": 42.54, "min": 17.35}}, "power_watts_avg": 31.73, "energy_joules_est": 62.68, "sample_count": 19, "duration_seconds": 1.975}, "timestamp": "2026-01-19T12:55:07.284023"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3254.403, "latencies_ms": [3254.403], "images_per_second": 0.307, "prompt_tokens": 1446, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. girl: 1\n2. blue umbrella: 1\n3. girl's hand: 1\n4. girl's leg: 1\n5. girl's foot: 1\n6. girl's shoe: 1\n7. girl's pant: 1\n8. girl's shirt: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.92, "peak": 123.67, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 28.09, "peak": 42.15, "min": 18.13}}, "power_watts_avg": 28.09, "energy_joules_est": 91.43, "sample_count": 32, "duration_seconds": 3.255}, "timestamp": "2026-01-19T12:55:10.610800"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2196.295, "latencies_ms": [2196.295], "images_per_second": 0.455, "prompt_tokens": 1450, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The girl is standing in the foreground of the image, holding the umbrella in her right hand. The umbrella is positioned above her, providing shade.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.41, "peak": 120.12, "min": 28.07}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 16.36, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 30.6, "peak": 42.15, "min": 15.38}}, "power_watts_avg": 30.6, "energy_joules_est": 67.22, "sample_count": 22, "duration_seconds": 2.197}, "timestamp": "2026-01-19T12:55:12.909607"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1964.87, "latencies_ms": [1964.87], "images_per_second": 0.509, "prompt_tokens": 1444, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A young girl wearing a pink jacket and blue jeans is holding a blue umbrella and a brown cookie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.78, "peak": 109.0, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 16.26, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.14, "peak": 42.15, "min": 16.56}}, "power_watts_avg": 32.14, "energy_joules_est": 63.17, "sample_count": 19, "duration_seconds": 1.965}, "timestamp": "2026-01-19T12:55:14.893076"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2123.209, "latencies_ms": [2123.209], "images_per_second": 0.471, "prompt_tokens": 1442, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The girl is wearing a pink jacket and blue jeans, and the umbrella is blue. The lighting is natural and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.31, "peak": 133.07, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.14, "peak": 42.13, "min": 19.7}}, "power_watts_avg": 32.14, "energy_joules_est": 68.25, "sample_count": 21, "duration_seconds": 2.123}, "timestamp": "2026-01-19T12:55:17.080221"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1465.938, "latencies_ms": [1465.938], "images_per_second": 0.682, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man stands in front of a window with a computer setup on a desk.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.21, "peak": 120.08, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.49, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.49, "energy_joules_est": 46.19, "sample_count": 15, "duration_seconds": 1.467}, "timestamp": "2026-01-19T12:55:18.647122"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2125.98, "latencies_ms": [2125.98], "images_per_second": 0.47, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " man: 1, chair: 1, computer: 2, monitor: 1, keyboard: 1, mouse: 1, desk: 1, window: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.99, "peak": 118.08, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.78, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 28.78, "energy_joules_est": 61.21, "sample_count": 21, "duration_seconds": 2.127}, "timestamp": "2026-01-19T12:55:20.845813"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2711.566, "latencies_ms": [2711.566], "images_per_second": 0.369, "prompt_tokens": 1117, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The man is standing to the left of the desk, which is in the foreground of the image. The desk is located in the middle of the image, with the computer monitors and other equipment placed on it. The window is located behind the desk, and the reflection of the man can be seen in the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.25, "peak": 114.73, "min": 27.77}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.38, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.38, "energy_joules_est": 71.54, "sample_count": 27, "duration_seconds": 2.712}, "timestamp": "2026-01-19T12:55:23.658713"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1481.308, "latencies_ms": [1481.308], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man stands in front of a window with a computer setup on a desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.73, "peak": 123.15, "min": 27.47}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.68, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 30.68, "energy_joules_est": 45.47, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T12:55:25.227013"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1712.613, "latencies_ms": [1712.613], "images_per_second": 0.584, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming in through a window. The man is wearing a black suit and tie.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.52, "peak": 125.24, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.85, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.85, "energy_joules_est": 52.86, "sample_count": 17, "duration_seconds": 1.713}, "timestamp": "2026-01-19T12:55:27.001361"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1735.811, "latencies_ms": [1735.811], "images_per_second": 0.576, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A group of people are sitting around a wooden table in a room with wooden walls and a wooden ceiling, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.74, "peak": 132.49, "min": 30.26}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.54, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 30.54, "energy_joules_est": 53.04, "sample_count": 17, "duration_seconds": 1.737}, "timestamp": "2026-01-19T12:55:28.785280"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2398.987, "latencies_ms": [2398.987], "images_per_second": 0.417, "prompt_tokens": 1113, "response_tokens_est": 49, "n_tiles": 1, "output_text": " table: 1\nclock: 1\nwindow: 1\ncurtain: 1\nbottle: 1\ncup: 1\nplate: 1\nbowl: 1\nteapot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.82, "peak": 119.76, "min": 28.12}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.5, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 27.5, "energy_joules_est": 65.98, "sample_count": 24, "duration_seconds": 2.399}, "timestamp": "2026-01-19T12:55:31.285493"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2152.474, "latencies_ms": [2152.474], "images_per_second": 0.465, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the table is in the foreground, with the people sitting around it. The window is in the background, and the clock is on the wall above the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.71, "peak": 123.12, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.53, "min": 12.22}, "VDD_GPU": {"avg": 28.12, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 28.12, "energy_joules_est": 60.54, "sample_count": 21, "duration_seconds": 2.153}, "timestamp": "2026-01-19T12:55:33.474876"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2001.171, "latencies_ms": [2001.171], "images_per_second": 0.5, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " A group of friends are gathered around a wooden table in a cozy room with a large window. They are enjoying a meal together, with plates of food and drinks on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.77, "peak": 113.83, "min": 27.48}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.98, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.98, "energy_joules_est": 58.01, "sample_count": 20, "duration_seconds": 2.002}, "timestamp": "2026-01-19T12:55:35.558563"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1567.636, "latencies_ms": [1567.636], "images_per_second": 0.638, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The room is filled with warm light and the wooden table is adorned with a variety of colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.65, "peak": 125.63, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.53, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.53, "energy_joules_est": 47.88, "sample_count": 16, "duration_seconds": 1.568}, "timestamp": "2026-01-19T12:55:37.229743"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1705.518, "latencies_ms": [1705.518], "images_per_second": 0.586, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A red truck with a snowplow attached to the front is driving down a snowy street, with a person standing nearby.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.46, "peak": 119.13, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.44, "min": 11.34}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 30.47, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.47, "energy_joules_est": 51.99, "sample_count": 17, "duration_seconds": 1.706}, "timestamp": "2026-01-19T12:55:39.017822"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1974.643, "latencies_ms": [1974.643], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. red truck\n2. snowplow\n3. snow\n4. house\n5. tree\n6. person\n7. street\n8. sidewalk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.98, "peak": 108.82, "min": 34.4}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.69, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.69, "energy_joules_est": 58.64, "sample_count": 19, "duration_seconds": 1.975}, "timestamp": "2026-01-19T12:55:40.997061"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2478.152, "latencies_ms": [2478.152], "images_per_second": 0.404, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The red truck is in the foreground, driving down the street. The snowplow is attached to the front of the truck, and it is in the middle of the image. The houses are in the background, and the trees are on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.92, "peak": 133.32, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.81, "peak": 39.78, "min": 18.91}}, "power_watts_avg": 27.81, "energy_joules_est": 68.92, "sample_count": 24, "duration_seconds": 2.478}, "timestamp": "2026-01-19T12:55:43.497273"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2059.367, "latencies_ms": [2059.367], "images_per_second": 0.486, "prompt_tokens": 1111, "response_tokens_est": 39, "n_tiles": 1, "output_text": " A red truck is driving down a snowy street with a snowplow attached to the front. There are houses on both sides of the street and a yellow fire hydrant on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.75, "peak": 117.76, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.09, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.09, "energy_joules_est": 59.92, "sample_count": 20, "duration_seconds": 2.06}, "timestamp": "2026-01-19T12:55:45.575295"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2111.035, "latencies_ms": [2111.035], "images_per_second": 0.474, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image depicts a red truck with a snowplow attached to the front, driving down a snowy street. The sky is overcast, and the snow is piled up on the sides of the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.55, "peak": 113.27, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.67, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 28.67, "energy_joules_est": 60.54, "sample_count": 21, "duration_seconds": 2.111}, "timestamp": "2026-01-19T12:55:47.771380"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2276.972, "latencies_ms": [2276.972], "images_per_second": 0.439, "prompt_tokens": 1432, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a luxurious bathroom with a large mirror reflecting the person taking the photo, a marble countertop, and a television mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.46, "peak": 116.16, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 16.26, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.96, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 29.96, "energy_joules_est": 68.24, "sample_count": 22, "duration_seconds": 2.278}, "timestamp": "2026-01-19T12:55:50.076285"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2948.677, "latencies_ms": [2948.677], "images_per_second": 0.339, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. mirror: 1\n2. television: 1\n3. towel: 2\n4. bath tub: 1\n5. sink: 2\n6. counter: 1\n7. floor: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.2, "peak": 122.49, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 28.61, "peak": 42.13, "min": 17.74}}, "power_watts_avg": 28.61, "energy_joules_est": 84.37, "sample_count": 29, "duration_seconds": 2.949}, "timestamp": "2026-01-19T12:55:53.088259"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3120.928, "latencies_ms": [3120.928], "images_per_second": 0.32, "prompt_tokens": 1450, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The mirror is positioned above the sink, reflecting the person taking the photo. The sink is located to the left of the mirror, and the person is standing in front of the mirror. The towel is hanging on the wall to the right of the sink, and the television is mounted on the wall above the sink.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.5, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.06, "peak": 129.06, "min": 27.94}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 27.73, "peak": 42.13, "min": 16.16}}, "power_watts_avg": 27.73, "energy_joules_est": 86.56, "sample_count": 31, "duration_seconds": 3.121}, "timestamp": "2026-01-19T12:55:56.316905"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1761.957, "latencies_ms": [1761.957], "images_per_second": 0.568, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A person is taking a picture of themselves in a bathroom mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.19, "peak": 117.38, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 16.26, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 17.32, "min": 12.22}, "VDD_GPU": {"avg": 32.54, "peak": 41.76, "min": 14.98}}, "power_watts_avg": 32.54, "energy_joules_est": 57.35, "sample_count": 17, "duration_seconds": 1.762}, "timestamp": "2026-01-19T12:55:58.092814"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2492.822, "latencies_ms": [2492.822], "images_per_second": 0.401, "prompt_tokens": 1442, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The bathroom is well-lit with warm lighting, and the walls are adorned with a striped pattern. The marble countertop is a rich green color, and the floor is made of beige tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.86, "peak": 121.57, "min": 32.83}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 31.24, "peak": 42.54, "min": 20.49}}, "power_watts_avg": 31.24, "energy_joules_est": 77.89, "sample_count": 24, "duration_seconds": 2.493}, "timestamp": "2026-01-19T12:56:00.592012"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1605.8, "latencies_ms": [1605.8], "images_per_second": 0.623, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two men are unloading luggage from a car in a parking garage, with a white SUV parked nearby.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.61, "peak": 106.27, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.15, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 31.15, "energy_joules_est": 50.04, "sample_count": 16, "duration_seconds": 1.607}, "timestamp": "2026-01-19T12:56:02.266290"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2523.315, "latencies_ms": [2523.315], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. luggage: 4\n2. luggage: 2\n3. luggage: 1\n4. luggage: 1\n5. luggage: 1\n6. luggage: 1\n7. luggage: 1\n8. luggage: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.8, "peak": 112.33, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.38, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.38, "energy_joules_est": 69.1, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T12:56:04.866776"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2165.19, "latencies_ms": [2165.19], "images_per_second": 0.462, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man with the luggage is positioned in the foreground, with the car and other luggage items in the background. The luggage cart is located to the left of the man, and the car is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.82, "peak": 131.77, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.26, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.26, "energy_joules_est": 61.21, "sample_count": 21, "duration_seconds": 2.166}, "timestamp": "2026-01-19T12:56:07.050806"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1436.316, "latencies_ms": [1436.316], "images_per_second": 0.696, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Two men are unloading their luggage from a car in a parking garage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.67, "peak": 126.35, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.05, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 32.05, "energy_joules_est": 46.05, "sample_count": 14, "duration_seconds": 1.437}, "timestamp": "2026-01-19T12:56:08.519389"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2095.267, "latencies_ms": [2095.267], "images_per_second": 0.477, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image depicts a man in a parking garage with a white SUV, a black suitcase, and a red luggage cart. The lighting is dim, and the man is wearing a brown shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.56, "peak": 132.26, "min": 27.59}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.44, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 29.44, "energy_joules_est": 61.7, "sample_count": 21, "duration_seconds": 2.096}, "timestamp": "2026-01-19T12:56:10.704809"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1647.371, "latencies_ms": [1647.371], "images_per_second": 0.607, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A plate with a sandwich, fries, and two small bowls of ketchup and mustard sits on a table.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.65, "peak": 131.72, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.44, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.53, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.53, "energy_joules_est": 50.31, "sample_count": 16, "duration_seconds": 1.648}, "timestamp": "2026-01-19T12:56:12.386688"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2517.885, "latencies_ms": [2517.885], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. plate: 1\n2. fries: 1\n3. onion: 1\n4. lettuce: 1\n5. sauce: 2\n6. sandwich: 2\n7. knife: 1\n8. bun: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.99, "peak": 119.94, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.53, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 27.53, "energy_joules_est": 69.34, "sample_count": 25, "duration_seconds": 2.519}, "timestamp": "2026-01-19T12:56:14.987909"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2368.778, "latencies_ms": [2368.778], "images_per_second": 0.422, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The plate is located in the foreground of the image, with the sandwich, fries, and ketchup in the foreground. The sandwich is in the center of the plate, with the fries to the right and the ketchup to the left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.77, "peak": 126.01, "min": 30.41}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.51, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.51, "energy_joules_est": 65.18, "sample_count": 23, "duration_seconds": 2.369}, "timestamp": "2026-01-19T12:56:17.377193"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1537.532, "latencies_ms": [1537.532], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A plate of food with a sandwich, fries, and ketchup is on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.72, "peak": 125.5, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.59, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.59, "energy_joules_est": 48.58, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T12:56:18.941793"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1615.722, "latencies_ms": [1615.722], "images_per_second": 0.619, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The plate is white and the food is colorful. The lighting is bright and the food is well-lit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.69, "peak": 113.97, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.88, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.88, "energy_joules_est": 51.52, "sample_count": 16, "duration_seconds": 1.616}, "timestamp": "2026-01-19T12:56:20.613643"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1848.469, "latencies_ms": [1848.469], "images_per_second": 0.541, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image depicts a bedroom with a bed covered by a green mosquito net, a wooden table with a candle, and a painting on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.8, "peak": 118.54, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.21, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 30.21, "energy_joules_est": 55.86, "sample_count": 18, "duration_seconds": 1.849}, "timestamp": "2026-01-19T12:56:22.493185"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2522.086, "latencies_ms": [2522.086], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 1\n2. curtains: 2\n3. table: 1\n4. candle: 1\n5. floor: 1\n6. wall: 1\n7. ceiling: 1\n8. roof: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.96, "peak": 113.73, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.34, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 27.34, "energy_joules_est": 68.96, "sample_count": 25, "duration_seconds": 2.522}, "timestamp": "2026-01-19T12:56:25.089338"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2282.577, "latencies_ms": [2282.577], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The bed is positioned in the center of the room, with the mosquito net draped over it. The table and chairs are located to the right of the bed, while the windows are on the left side of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.19, "peak": 128.31, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.83, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.83, "energy_joules_est": 63.53, "sample_count": 22, "duration_seconds": 2.283}, "timestamp": "2026-01-19T12:56:27.385816"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1811.358, "latencies_ms": [1811.358], "images_per_second": 0.552, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is of a bedroom with a bed covered by a green mosquito net, a candle on a table, and a painting on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.9, "peak": 125.08, "min": 28.44}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.85, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.85, "energy_joules_est": 54.08, "sample_count": 18, "duration_seconds": 1.812}, "timestamp": "2026-01-19T12:56:29.261835"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1711.42, "latencies_ms": [1711.42], "images_per_second": 0.584, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is painted yellow and has a thatched roof. The room is lit by natural light coming through the windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.36, "peak": 119.46, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.25, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 30.25, "energy_joules_est": 51.78, "sample_count": 17, "duration_seconds": 1.712}, "timestamp": "2026-01-19T12:56:31.044089"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1638.909, "latencies_ms": [1638.909], "images_per_second": 0.61, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A gray and black cat with a blue collar is standing on the hood of a black car in a garage.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.22, "peak": 126.38, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.1, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.1, "energy_joules_est": 50.98, "sample_count": 16, "duration_seconds": 1.639}, "timestamp": "2026-01-19T12:56:32.723729"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2598.364, "latencies_ms": [2598.364], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 1\n2. car: 1\n3. lamp: 1\n4. box: 1\n5. chair: 1\n6. bicycle: 1\n7. floor: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.41, "peak": 103.76, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.39, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 27.39, "energy_joules_est": 71.18, "sample_count": 25, "duration_seconds": 2.599}, "timestamp": "2026-01-19T12:56:35.332111"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2063.627, "latencies_ms": [2063.627], "images_per_second": 0.485, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The cat is positioned in the foreground, near the front left corner of the car, while the lamp is located in the background, towards the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.37, "peak": 126.03, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.52, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 28.52, "energy_joules_est": 58.86, "sample_count": 20, "duration_seconds": 2.064}, "timestamp": "2026-01-19T12:56:37.424266"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1422.942, "latencies_ms": [1422.942], "images_per_second": 0.703, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A cat is standing on the hood of a car in a garage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.76, "peak": 117.23, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.71, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.71, "energy_joules_est": 45.14, "sample_count": 14, "duration_seconds": 1.423}, "timestamp": "2026-01-19T12:56:38.887842"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1398.757, "latencies_ms": [1398.757], "images_per_second": 0.715, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The cat is gray and white, and the car is black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.55, "peak": 115.45, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 33.18, "peak": 40.57, "min": 20.89}}, "power_watts_avg": 33.18, "energy_joules_est": 46.43, "sample_count": 14, "duration_seconds": 1.399}, "timestamp": "2026-01-19T12:56:40.349998"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1849.95, "latencies_ms": [1849.95], "images_per_second": 0.541, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A delicious plate of food with a knife on it is sitting on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.74, "peak": 114.64, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 16.15, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 33.05, "peak": 42.54, "min": 20.5}}, "power_watts_avg": 33.05, "energy_joules_est": 61.17, "sample_count": 18, "duration_seconds": 1.851}, "timestamp": "2026-01-19T12:56:42.232971"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2915.224, "latencies_ms": [2915.224], "images_per_second": 0.343, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. plate: 1\n2. knife: 1\n3. food: 1\n4. table: 1\n5. fork: 1\n6. sandwich: 1\n7. sauce: 1\n8. bread: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.14, "peak": 128.73, "min": 32.57}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.41, "peak": 42.13, "min": 20.1}}, "power_watts_avg": 29.41, "energy_joules_est": 85.75, "sample_count": 28, "duration_seconds": 2.916}, "timestamp": "2026-01-19T12:56:45.151666"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2911.597, "latencies_ms": [2911.597], "images_per_second": 0.343, "prompt_tokens": 1450, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The main object, the plate of food, is in the foreground and is the closest object to the camera. The knife is located near the plate, but not on it. The background is out of focus, indicating that the main focus of the image is the plate of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.9, "ram_available_mb": 99782.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.54, "peak": 118.11, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.36, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 28.62, "peak": 41.76, "min": 17.34}}, "power_watts_avg": 28.62, "energy_joules_est": 83.34, "sample_count": 29, "duration_seconds": 2.912}, "timestamp": "2026-01-19T12:56:48.150363"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1749.299, "latencies_ms": [1749.299], "images_per_second": 0.572, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A delicious looking meal is served on a plate with a knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.7, "peak": 126.41, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 16.26, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.81, "peak": 41.74, "min": 15.38}}, "power_watts_avg": 32.81, "energy_joules_est": 57.42, "sample_count": 17, "duration_seconds": 1.75}, "timestamp": "2026-01-19T12:56:49.931505"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2832.535, "latencies_ms": [2832.535], "images_per_second": 0.353, "prompt_tokens": 1442, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image features a plate of food with a brown sauce, white gravy, and a slice of bread. The food is placed on a white plate, and the background is a black metal table. The lighting is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.87, "peak": 122.68, "min": 28.34}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 42.13, "min": 18.14}}, "power_watts_avg": 29.53, "energy_joules_est": 83.65, "sample_count": 28, "duration_seconds": 2.833}, "timestamp": "2026-01-19T12:56:52.848812"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1852.26, "latencies_ms": [1852.26], "images_per_second": 0.54, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " In the image, there are three men in a living room, one of them is holding a camera, and they are all sitting on a couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.93, "peak": 126.33, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.46, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.46, "energy_joules_est": 54.59, "sample_count": 18, "duration_seconds": 1.853}, "timestamp": "2026-01-19T12:56:54.737218"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2153.027, "latencies_ms": [2153.027], "images_per_second": 0.464, "prompt_tokens": 1113, "response_tokens_est": 42, "n_tiles": 1, "output_text": " table: 1\nsofa: 1\nbed: 1\nwindow: 1\nlamp: 1\ntable: 1\ncouch: 1\nfloor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.06, "peak": 119.73, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.76, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.76, "energy_joules_est": 61.93, "sample_count": 21, "duration_seconds": 2.153}, "timestamp": "2026-01-19T12:56:56.925719"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3774.897, "latencies_ms": [3774.897], "images_per_second": 0.265, "prompt_tokens": 1117, "response_tokens_est": 102, "n_tiles": 1, "output_text": " The man in the white shirt is standing near the couch, while the man in the blue shirt is sitting on the couch. The man in the blue shirt is holding a can of beer, and the man in the white shirt is holding a phone. The man in the blue shirt is also holding a bag of chips. The man in the white shirt is standing near the table, which has a red tablecloth. The man in the blue shirt is sitting on the couch, which is next to the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.67, "peak": 122.91, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 24.52, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 24.52, "energy_joules_est": 92.57, "sample_count": 37, "duration_seconds": 3.775}, "timestamp": "2026-01-19T12:57:00.776918"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1590.535, "latencies_ms": [1590.535], "images_per_second": 0.629, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " Three men are sitting on a couch in a living room, drinking beer and having a good time.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.67, "peak": 129.96, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.26, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.26, "energy_joules_est": 48.14, "sample_count": 16, "duration_seconds": 1.591}, "timestamp": "2026-01-19T12:57:02.449973"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1549.163, "latencies_ms": [1549.163], "images_per_second": 0.646, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The room is warmly lit with a yellow hue, and the furniture is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.98, "peak": 124.42, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.57, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.57, "energy_joules_est": 48.92, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T12:57:04.017099"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1832.376, "latencies_ms": [1832.376], "images_per_second": 0.546, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A baseball catcher in a white and black uniform is squatting in the batter's box, wearing a black helmet and a black and yellow glove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.73, "peak": 118.08, "min": 30.76}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.66, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 30.66, "energy_joules_est": 56.21, "sample_count": 18, "duration_seconds": 1.833}, "timestamp": "2026-01-19T12:57:05.905161"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3364.899, "latencies_ms": [3364.899], "images_per_second": 0.297, "prompt_tokens": 1113, "response_tokens_est": 87, "n_tiles": 1, "output_text": " 1. catcher's mitt: 1\n2. catcher's mask: 1\n3. catcher's chest protector: 1\n4. catcher's leg guards: 1\n5. catcher's leg pads: 1\n6. catcher's shin guards: 1\n7. catcher's chest protector: 1\n8. catcher's chest protector: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.01, "peak": 123.03, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.2, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 25.2, "energy_joules_est": 84.81, "sample_count": 33, "duration_seconds": 3.365}, "timestamp": "2026-01-19T12:57:09.339621"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2271.141, "latencies_ms": [2271.141], "images_per_second": 0.44, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The catcher is positioned in the foreground, squatting in front of the batter, with the pitcher and umpire in the background. The catcher is closer to the camera than the batter, who is positioned further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.05, "peak": 112.93, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.81, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.81, "energy_joules_est": 63.17, "sample_count": 22, "duration_seconds": 2.272}, "timestamp": "2026-01-19T12:57:11.634603"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1679.596, "latencies_ms": [1679.596], "images_per_second": 0.595, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A baseball catcher is squatting in the batter's box, wearing a black and white uniform and a black helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.4, "peak": 128.29, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 30.31, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.31, "energy_joules_est": 50.92, "sample_count": 17, "duration_seconds": 1.68}, "timestamp": "2026-01-19T12:57:13.406604"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3416.454, "latencies_ms": [3416.454], "images_per_second": 0.293, "prompt_tokens": 1109, "response_tokens_est": 89, "n_tiles": 1, "output_text": " The image captures a moment of intense focus and concentration, with the catcher crouched in anticipation, his body poised in readiness to spring into action. The vibrant green of the field contrasts sharply with the brown dirt of the batter's box, while the catcher's black and white uniform stands out against the lush greenery. The lighting is natural and bright, casting sharp shadows and highlighting the textures of the catcher's gear and the grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.64, "peak": 112.82, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.2, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 25.2, "energy_joules_est": 86.11, "sample_count": 33, "duration_seconds": 3.417}, "timestamp": "2026-01-19T12:57:16.837139"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1851.85, "latencies_ms": [1851.85], "images_per_second": 0.54, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a bathroom with a pink and brown color scheme, featuring a bathtub, a toilet, and a wooden vanity with a mirror above it.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.46, "peak": 120.13, "min": 27.39}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.66, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 29.66, "energy_joules_est": 54.93, "sample_count": 18, "duration_seconds": 1.852}, "timestamp": "2026-01-19T12:57:18.715024"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2853.532, "latencies_ms": [2853.532], "images_per_second": 0.35, "prompt_tokens": 1113, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. Bathtub: 1\n2. Toilet: 1\n3. Window: 1\n4. Shower curtain: 1\n5. Bathroom sink: 1\n6. Bathroom door: 1\n7. Bathroom cabinet: 1\n8. Bathroom floor: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.77, "peak": 110.5, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.37, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 26.37, "energy_joules_est": 75.27, "sample_count": 28, "duration_seconds": 2.854}, "timestamp": "2026-01-19T12:57:21.620987"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2354.885, "latencies_ms": [2354.885], "images_per_second": 0.425, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The toilet is located in the foreground, to the right of the sink. The bathtub is situated in the background, to the left of the sink. The shower curtain is hanging in the middle of the bathroom, between the bathtub and the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.36, "peak": 124.33, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 27.48, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.48, "energy_joules_est": 64.73, "sample_count": 23, "duration_seconds": 2.355}, "timestamp": "2026-01-19T12:57:24.019901"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1453.957, "latencies_ms": [1453.957], "images_per_second": 0.688, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A bathroom with pink tiles and a white toilet is shown in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.18, "peak": 115.75, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.77, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.77, "energy_joules_est": 46.21, "sample_count": 14, "duration_seconds": 1.454}, "timestamp": "2026-01-19T12:57:25.485377"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1636.162, "latencies_ms": [1636.162], "images_per_second": 0.611, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The bathroom has pink tiles, a white bathtub, and a wooden vanity. The lighting is bright and natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.27, "peak": 112.88, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.3, "peak": 40.97, "min": 22.07}}, "power_watts_avg": 32.3, "energy_joules_est": 52.86, "sample_count": 16, "duration_seconds": 1.637}, "timestamp": "2026-01-19T12:57:27.155487"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1946.232, "latencies_ms": [1946.232], "images_per_second": 0.514, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image depicts a bedroom with a bed covered in a yellow and white plaid blanket, a nightstand with a lamp on it, and a window with red curtains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.67, "peak": 112.89, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.8, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.8, "energy_joules_est": 58.01, "sample_count": 19, "duration_seconds": 1.947}, "timestamp": "2026-01-19T12:57:29.140795"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2589.434, "latencies_ms": [2589.434], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. bed: 1\n2. window: 1\n3. curtains: 2\n4. wall: 1\n5. lamp: 1\n6. bedside table: 1\n7. bed: 1\n8. blanket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.03, "peak": 109.42, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.15, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.15, "energy_joules_est": 70.31, "sample_count": 25, "duration_seconds": 2.59}, "timestamp": "2026-01-19T12:57:31.748571"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2262.877, "latencies_ms": [2262.877], "images_per_second": 0.442, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The bed is located in the foreground of the image, with the window and curtains in the background. The lamp is positioned on the nightstand to the right of the bed, and the window is to the left of the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.49, "peak": 130.1, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.1, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.1, "energy_joules_est": 63.59, "sample_count": 22, "duration_seconds": 2.263}, "timestamp": "2026-01-19T12:57:34.045825"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2295.867, "latencies_ms": [2295.867], "images_per_second": 0.436, "prompt_tokens": 1111, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image depicts a bedroom with a bed covered in a yellow and white plaid blanket, positioned in the center of the room. The room is dimly lit, with a window on the left side allowing natural light to filter in.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.74, "peak": 116.46, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.65, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.65, "energy_joules_est": 63.49, "sample_count": 23, "duration_seconds": 2.296}, "timestamp": "2026-01-19T12:57:36.438408"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1674.689, "latencies_ms": [1674.689], "images_per_second": 0.597, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The room is dimly lit with a yellow and white plaid bedspread, and the window has red curtains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.47, "peak": 124.43, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.83, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.83, "energy_joules_est": 49.97, "sample_count": 17, "duration_seconds": 1.675}, "timestamp": "2026-01-19T12:57:38.212840"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2135.022, "latencies_ms": [2135.022], "images_per_second": 0.468, "prompt_tokens": 1432, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A woman in a black dress with a beaded bodice is adjusting a boutonniere on a man's lapel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.27, "peak": 134.14, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.15, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.62, "peak": 41.76, "min": 16.95}}, "power_watts_avg": 30.62, "energy_joules_est": 65.39, "sample_count": 21, "duration_seconds": 2.135}, "timestamp": "2026-01-19T12:57:40.404897"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3195.394, "latencies_ms": [3195.394], "images_per_second": 0.313, "prompt_tokens": 1446, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. tie: 1\n2. suit: 1\n3. woman: 1\n4. man: 1\n5. woman's hand: 1\n6. woman's dress: 1\n7. woman's hair: 1\n8. woman's earring: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.08, "peak": 126.52, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.26, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 27.92, "peak": 41.76, "min": 17.74}}, "power_watts_avg": 27.92, "energy_joules_est": 89.23, "sample_count": 31, "duration_seconds": 3.196}, "timestamp": "2026-01-19T12:57:43.624640"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2734.382, "latencies_ms": [2734.382], "images_per_second": 0.366, "prompt_tokens": 1450, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The woman is standing to the right of the man, and she is closer to the camera than the man. The woman is wearing a black dress with a beige ribbon, and the man is wearing a black suit with a beige tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.23, "peak": 121.46, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 29.05, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 29.05, "energy_joules_est": 79.45, "sample_count": 27, "duration_seconds": 2.735}, "timestamp": "2026-01-19T12:57:46.437624"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1982.956, "latencies_ms": [1982.956], "images_per_second": 0.504, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A woman is helping a man put on a boutonniere. They are at a formal event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.32, "peak": 121.35, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.26, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.85, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 31.85, "energy_joules_est": 63.17, "sample_count": 19, "duration_seconds": 1.983}, "timestamp": "2026-01-19T12:57:48.440554"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2560.904, "latencies_ms": [2560.904], "images_per_second": 0.39, "prompt_tokens": 1442, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image is well-lit with natural light, and the colors are vibrant and bright. The man is wearing a black suit with a gold tie, and the woman is wearing a black dress with a gold belt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.4, "peak": 127.02, "min": 30.58}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 16.26, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 30.34, "peak": 42.13, "min": 19.7}}, "power_watts_avg": 30.34, "energy_joules_est": 77.71, "sample_count": 25, "duration_seconds": 2.561}, "timestamp": "2026-01-19T12:57:51.044288"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1865.782, "latencies_ms": [1865.782], "images_per_second": 0.536, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image features a chain-link fence with a stop sign attached to it, situated in a grassy area with palm trees and a building in the background.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.66, "peak": 135.38, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 29.92, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.92, "energy_joules_est": 55.85, "sample_count": 18, "duration_seconds": 1.867}, "timestamp": "2026-01-19T12:57:52.927841"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2042.165, "latencies_ms": [2042.165], "images_per_second": 0.49, "prompt_tokens": 1113, "response_tokens_est": 38, "n_tiles": 1, "output_text": " 1. chain link fence\n2. stop sign\n3. palm trees\n4. building\n5. bushes\n6. trash\n7. sidewalk\n8. chain link fence", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.57, "peak": 113.8, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 29.35, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 29.35, "energy_joules_est": 59.95, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T12:57:55.018949"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2410.955, "latencies_ms": [2410.955], "images_per_second": 0.415, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The stop sign is located in the foreground of the image, on the left side, and is partially obscured by the chain link fence. The background of the image features a grassy area with palm trees and a building, which is far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.02, "peak": 116.06, "min": 27.32}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 27.2, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 27.2, "energy_joules_est": 65.59, "sample_count": 24, "duration_seconds": 2.411}, "timestamp": "2026-01-19T12:57:57.536424"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2580.995, "latencies_ms": [2580.995], "images_per_second": 0.387, "prompt_tokens": 1111, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a scene of a chain-link fence with a stop sign attached to it, set against a backdrop of a grassy area and palm trees. The stop sign, with its bold red color and octagonal shape, stands out prominently against the green foliage and the blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.24, "peak": 128.28, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.74, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.74, "energy_joules_est": 69.02, "sample_count": 25, "duration_seconds": 2.581}, "timestamp": "2026-01-19T12:58:00.139527"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2203.974, "latencies_ms": [2203.974], "images_per_second": 0.454, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image depicts a chain-link fence with a stop sign attached to it, set against a backdrop of a grassy area with palm trees and a building in the distance. The sky is clear, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.49, "peak": 127.32, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.05, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.05, "energy_joules_est": 61.83, "sample_count": 22, "duration_seconds": 2.204}, "timestamp": "2026-01-19T12:58:02.435595"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1876.152, "latencies_ms": [1876.152], "images_per_second": 0.533, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A man wearing a gray shirt and khaki shorts is standing next to a black bicycle with a basket on the back, while a motorcycle is parked nearby.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.35, "peak": 129.97, "min": 27.49}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.87, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.87, "energy_joules_est": 54.19, "sample_count": 19, "duration_seconds": 1.877}, "timestamp": "2026-01-19T12:58:04.420514"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1989.424, "latencies_ms": [1989.424], "images_per_second": 0.503, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. yellow bicycle\n2. black bicycle\n3. motorcycle\n4. person\n5. tree\n6. fence\n7. trash bag\n8. graffiti", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.4, "peak": 126.63, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.68, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.68, "energy_joules_est": 57.07, "sample_count": 20, "duration_seconds": 1.99}, "timestamp": "2026-01-19T12:58:06.499208"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2284.143, "latencies_ms": [2284.143], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The yellow bicycle is positioned to the left of the black bicycle, which is in the foreground. The person standing next to the black bicycle is positioned in the background, while the motorcycle is positioned to the left of the yellow bicycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.69, "peak": 121.43, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.01, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.01, "energy_joules_est": 63.99, "sample_count": 22, "duration_seconds": 2.285}, "timestamp": "2026-01-19T12:58:08.792311"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1537.389, "latencies_ms": [1537.389], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man is standing next to a bike and a motorcycle. There is graffiti on the ground.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.2, "peak": 118.36, "min": 27.61}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 31.73, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 31.73, "energy_joules_est": 48.8, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T12:58:10.356169"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2551.434, "latencies_ms": [2551.434], "images_per_second": 0.392, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image features a street scene with a yellow bicycle, a black bicycle, and a motorcycle parked on the side of the road. The sky is clear and blue, indicating a sunny day. The ground is covered with green grass and leaves, suggesting that it is either spring or summer.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.98, "peak": 132.5, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.55, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.55, "energy_joules_est": 70.3, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T12:58:12.969693"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2407.213, "latencies_ms": [2407.213], "images_per_second": 0.415, "prompt_tokens": 1099, "response_tokens_est": 51, "n_tiles": 1, "output_text": " In the image, a man is standing on the sidewalk, looking at a street sign that reads \"PROCTER 2\" and \"PROCTER 3\". The street is busy with cars and people, and there are trees lining the sidewalk.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.19, "peak": 115.27, "min": 27.35}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.14, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.14, "energy_joules_est": 65.35, "sample_count": 24, "duration_seconds": 2.408}, "timestamp": "2026-01-19T12:58:15.467644"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2074.323, "latencies_ms": [2074.323], "images_per_second": 0.482, "prompt_tokens": 1113, "response_tokens_est": 38, "n_tiles": 1, "output_text": " 1. black pole\n2. yellow sign\n3. black trash can\n4. traffic light\n5. street sign\n6. person\n7. bicycle\n8. car", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.24, "peak": 117.07, "min": 33.25}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.58, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.58, "energy_joules_est": 59.3, "sample_count": 20, "duration_seconds": 2.075}, "timestamp": "2026-01-19T12:58:17.549081"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2334.056, "latencies_ms": [2334.056], "images_per_second": 0.428, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The man is standing on the sidewalk, which is adjacent to the street. The traffic lights are positioned above the street, and the street signs are mounted on the poles. The man is closer to the camera than the traffic lights and street signs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.83, "peak": 124.25, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.06, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 28.06, "energy_joules_est": 65.5, "sample_count": 23, "duration_seconds": 2.334}, "timestamp": "2026-01-19T12:58:19.941089"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2515.895, "latencies_ms": [2515.895], "images_per_second": 0.397, "prompt_tokens": 1111, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image captures a bustling city intersection, where pedestrians and vehicles coexist. The street is lined with buildings, and a traffic light hangs overhead, guiding the flow of traffic. A man stands on the sidewalk, perhaps waiting for the light to change or simply enjoying the urban scenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.79, "peak": 127.88, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.96, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 26.96, "energy_joules_est": 67.84, "sample_count": 25, "duration_seconds": 2.516}, "timestamp": "2026-01-19T12:58:22.538855"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2264.556, "latencies_ms": [2264.556], "images_per_second": 0.442, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image depicts a city street with a mix of colors, including the red of the car and the black of the street lamp. The lighting is natural, coming from the sky, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.29, "peak": 118.88, "min": 30.95}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.78, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.78, "energy_joules_est": 62.92, "sample_count": 22, "duration_seconds": 2.265}, "timestamp": "2026-01-19T12:58:24.826588"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2522.93, "latencies_ms": [2522.93], "images_per_second": 0.396, "prompt_tokens": 1099, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image captures a bronze statue of two individuals seated on a stone bench, with a handbag resting beside them. The statue is situated on a sidewalk, and in the background, a group of people can be seen standing, possibly admiring the artwork or engaging in conversation.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.02, "peak": 124.59, "min": 28.31}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.95, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.95, "energy_joules_est": 68.01, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T12:58:27.426126"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2546.303, "latencies_ms": [2546.303], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. statue: 2\n2. bench: 1\n3. bag: 1\n4. person: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.51, "peak": 113.52, "min": 32.54}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.76, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.76, "energy_joules_est": 68.15, "sample_count": 25, "duration_seconds": 2.547}, "timestamp": "2026-01-19T12:58:30.030831"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2673.925, "latencies_ms": [2673.925], "images_per_second": 0.374, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The sculpture is located on the left side of the image, with the bench and bag placed in the foreground. The sculpture is positioned in the middle of the image, with the bench and bag placed in the foreground. The sculpture is located in the background, with the bench and bag placed in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.03, "peak": 128.84, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.64, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.64, "energy_joules_est": 71.25, "sample_count": 26, "duration_seconds": 2.674}, "timestamp": "2026-01-19T12:58:32.734235"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2400.658, "latencies_ms": [2400.658], "images_per_second": 0.417, "prompt_tokens": 1111, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image captures a bronze statue of three individuals seated on a stone bench, with a handbag resting on the ground beside them. The setting appears to be an outdoor urban environment, possibly a park or a public square, with people walking in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.36, "peak": 116.12, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.28, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.28, "energy_joules_est": 65.5, "sample_count": 24, "duration_seconds": 2.401}, "timestamp": "2026-01-19T12:58:35.231085"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1808.448, "latencies_ms": [1808.448], "images_per_second": 0.553, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The sculpture is made of bronze and is situated on a stone bench. The sculpture is bathed in sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.93, "peak": 119.72, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.33, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.33, "energy_joules_est": 53.05, "sample_count": 18, "duration_seconds": 1.809}, "timestamp": "2026-01-19T12:58:37.106699"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2373.63, "latencies_ms": [2373.63], "images_per_second": 0.421, "prompt_tokens": 1100, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image shows a series of road signs, including a blue sign with white text and symbols, a green sign with white text and symbols, a red and white circular sign with a truck symbol, and a blue and white rectangular sign with a truck symbol.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.71, "peak": 107.13, "min": 35.81}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.82, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.82, "energy_joules_est": 66.04, "sample_count": 23, "duration_seconds": 2.374}, "timestamp": "2026-01-19T12:58:39.490611"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2178.165, "latencies_ms": [2178.165], "images_per_second": 0.459, "prompt_tokens": 1114, "response_tokens_est": 44, "n_tiles": 1, "output_text": " 1. signpost\n2. blue sign\n3. green sign\n4. white sign\n5. red and white sign\n6. blue and white sign\n7. white arrow\n8. black arrow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.12, "peak": 120.38, "min": 33.13}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 28.63, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 28.63, "energy_joules_est": 62.37, "sample_count": 21, "duration_seconds": 2.179}, "timestamp": "2026-01-19T12:58:41.673844"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2336.458, "latencies_ms": [2336.458], "images_per_second": 0.428, "prompt_tokens": 1118, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The blue sign is located to the left of the green sign, which is in turn positioned to the right of the red and blue circular signs. The red and blue circular signs are located in the foreground, while the blue sign is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.7, "peak": 122.52, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.93, "peak": 39.77, "min": 18.13}}, "power_watts_avg": 27.93, "energy_joules_est": 65.27, "sample_count": 23, "duration_seconds": 2.337}, "timestamp": "2026-01-19T12:58:44.053021"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2070.196, "latencies_ms": [2070.196], "images_per_second": 0.483, "prompt_tokens": 1112, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image captures a scene of a road signpost in a rural area, with a clear sky in the background. The signpost is adorned with multiple signs, each indicating different directions and destinations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.09, "peak": 128.53, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.25, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.25, "energy_joules_est": 58.5, "sample_count": 20, "duration_seconds": 2.071}, "timestamp": "2026-01-19T12:58:46.132727"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2500.092, "latencies_ms": [2500.092], "images_per_second": 0.4, "prompt_tokens": 1110, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image depicts a collection of road signs with a mix of colors including blue, green, red, and white. The signs are attached to a pole and are positioned in an outdoor setting with trees in the background. The sky appears to be overcast, suggesting a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.52, "peak": 109.07, "min": 30.22}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.96}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.16, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.16, "energy_joules_est": 67.91, "sample_count": 25, "duration_seconds": 2.501}, "timestamp": "2026-01-19T12:58:48.733178"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2023.046, "latencies_ms": [2023.046], "images_per_second": 0.494, "prompt_tokens": 1100, "response_tokens_est": 37, "n_tiles": 1, "output_text": " In the image, there are two women standing next to each other, with one of them holding a black suitcase. They are both smiling and appear to be enjoying their time together.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.03, "peak": 123.9, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.23, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 28.23, "energy_joules_est": 57.12, "sample_count": 20, "duration_seconds": 2.023}, "timestamp": "2026-01-19T12:58:50.824679"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2680.37, "latencies_ms": [2680.37], "images_per_second": 0.373, "prompt_tokens": 1114, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. suitcase: 1\n2. woman: 2\n3. girl: 1\n4. backpack: 1\n5. suitcase handle: 1\n6. suitcase strap: 1\n7. train: 1\n8. platform: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.7, "peak": 130.32, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.52, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.52, "energy_joules_est": 71.09, "sample_count": 26, "duration_seconds": 2.681}, "timestamp": "2026-01-19T12:58:53.541199"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2288.001, "latencies_ms": [2288.001], "images_per_second": 0.437, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The woman on the left is standing closer to the camera than the woman on the right. The woman on the right is standing in front of the woman on the left. The woman on the right is standing in front of the luggage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.08, "peak": 126.87, "min": 32.97}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.96, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.96, "energy_joules_est": 63.98, "sample_count": 22, "duration_seconds": 2.288}, "timestamp": "2026-01-19T12:58:55.835303"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1355.731, "latencies_ms": [1355.731], "images_per_second": 0.738, "prompt_tokens": 1112, "response_tokens_est": 12, "n_tiles": 1, "output_text": " Two women are standing on a train platform with their luggage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.54, "peak": 115.05, "min": 43.76}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.97, "peak": 39.39, "min": 17.34}}, "power_watts_avg": 31.97, "energy_joules_est": 43.36, "sample_count": 13, "duration_seconds": 1.356}, "timestamp": "2026-01-19T12:58:57.197559"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2042.551, "latencies_ms": [2042.551], "images_per_second": 0.49, "prompt_tokens": 1110, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image is taken during the day with natural light illuminating the scene. The colors in the image are vibrant, with the red of the train station and the blue of the sky standing out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.25, "peak": 110.81, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.98, "peak": 40.97, "min": 19.32}}, "power_watts_avg": 29.98, "energy_joules_est": 61.25, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T12:58:59.288978"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1646.522, "latencies_ms": [1646.522], "images_per_second": 0.607, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " Three zebras with black and white stripes are walking on a dirt road in a forest with purple flowers.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.78, "peak": 128.07, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.58, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 30.58, "energy_joules_est": 50.37, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T12:59:00.969998"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1155.018, "latencies_ms": [1155.018], "images_per_second": 0.866, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 1.9, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.15, "peak": 124.6, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 33.42, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 33.42, "energy_joules_est": 38.62, "sample_count": 12, "duration_seconds": 1.155}, "timestamp": "2026-01-19T12:59:02.232853"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2171.791, "latencies_ms": [2171.791], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the trees and bushes serving as the background. The zebras are facing the camera, with the tree branches and purple flowers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.43, "peak": 125.81, "min": 30.09}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.44, "min": 11.26}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.83, "peak": 41.36, "min": 20.1}}, "power_watts_avg": 29.83, "energy_joules_est": 64.8, "sample_count": 21, "duration_seconds": 2.172}, "timestamp": "2026-01-19T12:59:04.423549"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1504.308, "latencies_ms": [1504.308], "images_per_second": 0.665, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Three zebras are walking on a dirt road in a wooded area with purple flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.3, "peak": 118.64, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.44, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 31.44, "energy_joules_est": 47.31, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T12:59:05.992446"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1842.452, "latencies_ms": [1842.452], "images_per_second": 0.543, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The zebras are black and white striped, and the trees are green and purple. The sun is shining brightly, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.56, "peak": 125.65, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.42, "peak": 40.97, "min": 19.32}}, "power_watts_avg": 30.42, "energy_joules_est": 56.06, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T12:59:07.872399"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1679.541, "latencies_ms": [1679.541], "images_per_second": 0.595, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A tripod with a camera on it is in a room with a laptop and a vending machine in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.82, "peak": 122.39, "min": 27.98}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.4}, "VDD_GPU": {"avg": 30.29, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.29, "energy_joules_est": 50.89, "sample_count": 17, "duration_seconds": 1.68}, "timestamp": "2026-01-19T12:59:09.648456"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1202.878, "latencies_ms": [1202.878], "images_per_second": 0.831, "prompt_tokens": 1114, "response_tokens_est": 5, "n_tiles": 1, "output_text": " tripod: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.12, "peak": 127.65, "min": 29.92}, "VIN_SYS_5V0": {"avg": 13.94, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 32.37, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 32.37, "energy_joules_est": 38.95, "sample_count": 12, "duration_seconds": 1.203}, "timestamp": "2026-01-19T12:59:10.904044"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2540.806, "latencies_ms": [2540.806], "images_per_second": 0.394, "prompt_tokens": 1118, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The camera is positioned to the left of the laptop, which is placed on the tripod stand. The laptop is situated in the foreground, while the camera is in the background. The tripod stand is positioned near the laptop, and the camera is positioned far from the laptop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.74, "peak": 121.35, "min": 31.24}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.37, "peak": 42.15, "min": 18.13}}, "power_watts_avg": 28.37, "energy_joules_est": 72.1, "sample_count": 25, "duration_seconds": 2.541}, "timestamp": "2026-01-19T12:59:13.503306"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1851.723, "latencies_ms": [1851.723], "images_per_second": 0.54, "prompt_tokens": 1112, "response_tokens_est": 30, "n_tiles": 1, "output_text": " In a room with a camera on a tripod, a laptop is open on a table, and a vending machine is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.31, "peak": 106.76, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.09, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.09, "energy_joules_est": 53.88, "sample_count": 18, "duration_seconds": 1.852}, "timestamp": "2026-01-19T12:59:15.379134"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1875.823, "latencies_ms": [1875.823], "images_per_second": 0.533, "prompt_tokens": 1110, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a blue and white color scheme. The camera on the tripod is black and the laptop is black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.31, "peak": 130.27, "min": 36.81}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.01, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 30.01, "energy_joules_est": 56.31, "sample_count": 18, "duration_seconds": 1.876}, "timestamp": "2026-01-19T12:59:17.261380"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2022.886, "latencies_ms": [2022.886], "images_per_second": 0.494, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " In the image, a sheep is seen grazing on a pile of wool, which is being sheared off by a person who is partially visible behind a metal fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.42, "peak": 128.61, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 28.88, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 28.88, "energy_joules_est": 58.44, "sample_count": 20, "duration_seconds": 2.024}, "timestamp": "2026-01-19T12:59:19.335049"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2819.098, "latencies_ms": [2819.098], "images_per_second": 0.355, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. sheep: 1\n2. fence: 1\n3. metal: 1\n4. wool: 1\n5. sheep's head: 1\n6. wool's edge: 1\n7. wool's edge: 1\n8. wool's edge: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.33, "peak": 129.94, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.96, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 25.96, "energy_joules_est": 73.2, "sample_count": 28, "duration_seconds": 2.82}, "timestamp": "2026-01-19T12:59:22.247265"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.548, "latencies_ms": [2395.548], "images_per_second": 0.417, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The sheep is positioned in the foreground, with the metal fence serving as a boundary between the sheep and the background. The sheep is facing towards the camera, with its head lowered to the ground, indicating it is engaged in the act of shearing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.7, "peak": 121.35, "min": 31.57}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.31, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 27.31, "energy_joules_est": 65.44, "sample_count": 23, "duration_seconds": 2.396}, "timestamp": "2026-01-19T12:59:24.648633"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2101.664, "latencies_ms": [2101.664], "images_per_second": 0.476, "prompt_tokens": 1111, "response_tokens_est": 40, "n_tiles": 1, "output_text": " In a fenced enclosure, a sheep is seen eating a pile of wool, which is a common practice for shearing. The sheep is white and appears to be enjoying the fresh, clean wool.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.95, "peak": 128.22, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.59, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.59, "energy_joules_est": 60.1, "sample_count": 21, "duration_seconds": 2.102}, "timestamp": "2026-01-19T12:59:26.826097"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1613.18, "latencies_ms": [1613.18], "images_per_second": 0.62, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The sheep is white and the wool is gray. The lighting is natural and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.31, "peak": 125.4, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.36, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.36, "energy_joules_est": 48.99, "sample_count": 16, "duration_seconds": 1.613}, "timestamp": "2026-01-19T12:59:28.496075"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2616.641, "latencies_ms": [2616.641], "images_per_second": 0.382, "prompt_tokens": 1099, "response_tokens_est": 59, "n_tiles": 1, "output_text": " In the image, a tennis match is taking place on a blue court, with players dressed in red and purple uniforms, and a scoreboard displaying the scores of the players. The court is surrounded by a crowd of spectators, and various advertisements are visible on the walls and banners surrounding the court.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.2, "peak": 128.38, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.1, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 27.1, "energy_joules_est": 70.93, "sample_count": 26, "duration_seconds": 2.617}, "timestamp": "2026-01-19T12:59:31.186611"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2100.5, "latencies_ms": [2100.5], "images_per_second": 0.476, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " 1. tennis court\n2. tennis ball\n3. tennis racket\n4. net\n5. ball boy\n6. net post\n7. scoreboard\n8. audience", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.81, "peak": 123.77, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.07, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.07, "energy_joules_est": 58.98, "sample_count": 21, "duration_seconds": 2.101}, "timestamp": "2026-01-19T12:59:33.372769"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2436.569, "latencies_ms": [2436.569], "images_per_second": 0.41, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The tennis player is positioned near the net, with the ball in the air between her and the opposing team. The camera is positioned in the foreground, capturing the action from a low angle. The spectators are in the background, watching the match from their seats.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.63, "peak": 107.13, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.23, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.23, "energy_joules_est": 66.36, "sample_count": 24, "duration_seconds": 2.437}, "timestamp": "2026-01-19T12:59:35.866747"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2229.744, "latencies_ms": [2229.744], "images_per_second": 0.448, "prompt_tokens": 1111, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a tennis match in progress, with players on a blue court, surrounded by spectators in the stands. The scoreboard in the background displays the current score, indicating that the match is in progress.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.66, "peak": 128.13, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.8, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.8, "energy_joules_est": 62.01, "sample_count": 22, "duration_seconds": 2.231}, "timestamp": "2026-01-19T12:59:38.158100"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1783.753, "latencies_ms": [1783.753], "images_per_second": 0.561, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The court is blue with white lines, and the players are wearing red and purple. The lighting is bright and the weather is clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.31, "peak": 121.2, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.5, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.5, "energy_joules_est": 52.64, "sample_count": 18, "duration_seconds": 1.784}, "timestamp": "2026-01-19T12:59:40.032228"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1632.046, "latencies_ms": [1632.046], "images_per_second": 0.613, "prompt_tokens": 1100, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A person is walking through a large, open, and empty airport terminal with a glass door in the foreground.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.43, "peak": 123.23, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.48, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.48, "energy_joules_est": 49.75, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T12:59:41.712538"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2857.868, "latencies_ms": [2857.868], "images_per_second": 0.35, "prompt_tokens": 1114, "response_tokens_est": 69, "n_tiles": 1, "output_text": " 1. person: 1\n2. suitcase: 1\n3. suitcase handle: 1\n4. suitcase wheels: 1\n5. suitcase wheels: 1\n6. suitcase wheels: 1\n7. suitcase wheels: 1\n8. suitcase wheels: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.7, "peak": 107.18, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.5, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 26.5, "energy_joules_est": 75.75, "sample_count": 28, "duration_seconds": 2.858}, "timestamp": "2026-01-19T12:59:44.630489"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2227.174, "latencies_ms": [2227.174], "images_per_second": 0.449, "prompt_tokens": 1118, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The main object is the glass door, which is located in the foreground of the image. The person is standing in the background, near the glass door. The escalator is located in the background, behind the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.38, "peak": 129.76, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.79, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.79, "energy_joules_est": 61.91, "sample_count": 22, "duration_seconds": 2.228}, "timestamp": "2026-01-19T12:59:46.907488"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1430.942, "latencies_ms": [1430.942], "images_per_second": 0.699, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person is walking through a modern airport terminal with a suitcase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.8, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.87, "peak": 121.79, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.95, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.95, "energy_joules_est": 44.3, "sample_count": 14, "duration_seconds": 1.431}, "timestamp": "2026-01-19T12:59:48.368138"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2041.868, "latencies_ms": [2041.868], "images_per_second": 0.49, "prompt_tokens": 1110, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image depicts a modern airport terminal with a glass door entrance, where a person is walking with luggage. The lighting is bright and natural, and the materials appear to be concrete and glass.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.39, "peak": 125.74, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.62, "peak": 40.57, "min": 19.31}}, "power_watts_avg": 29.62, "energy_joules_est": 60.49, "sample_count": 20, "duration_seconds": 2.042}, "timestamp": "2026-01-19T12:59:50.459868"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1827.614, "latencies_ms": [1827.614], "images_per_second": 0.547, "prompt_tokens": 1100, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image captures a cozy dining scene with a large pizza on a table, surrounded by empty glasses and a fork, with a television in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.02, "peak": 119.68, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.68, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 29.68, "energy_joules_est": 54.26, "sample_count": 18, "duration_seconds": 1.828}, "timestamp": "2026-01-19T12:59:52.343578"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1978.717, "latencies_ms": [1978.717], "images_per_second": 0.505, "prompt_tokens": 1114, "response_tokens_est": 35, "n_tiles": 1, "output_text": " pizza: 2, glasses: 2, forks: 2, knives: 1, spoons: 1, television: 1, tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.54, "peak": 125.42, "min": 35.21}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 29.4, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.4, "energy_joules_est": 58.19, "sample_count": 19, "duration_seconds": 1.979}, "timestamp": "2026-01-19T12:59:54.327456"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2323.249, "latencies_ms": [2323.249], "images_per_second": 0.43, "prompt_tokens": 1118, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The pizza boxes are placed on the left side of the table, with the glasses of water positioned in the middle. The person sitting in the background is relatively far from the camera, while the television is in the background, closer to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.41, "peak": 119.12, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 27.89, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 27.89, "energy_joules_est": 64.81, "sample_count": 23, "duration_seconds": 2.324}, "timestamp": "2026-01-19T12:59:56.730191"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1520.17, "latencies_ms": [1520.17], "images_per_second": 0.658, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A group of people are sitting around a table with pizza boxes and glasses of soda.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.93, "peak": 127.72, "min": 30.55}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.26, "peak": 40.18, "min": 15.38}}, "power_watts_avg": 30.26, "energy_joules_est": 46.01, "sample_count": 15, "duration_seconds": 1.52}, "timestamp": "2026-01-19T12:59:58.294985"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2251.639, "latencies_ms": [2251.639], "images_per_second": 0.444, "prompt_tokens": 1110, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image is taken in a room with a warm and cozy atmosphere, with the colors of the pizza boxes and the tablecloth being the main focus. The lighting is soft and natural, coming from the window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.61, "peak": 122.62, "min": 28.34}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.6, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 28.6, "energy_joules_est": 64.42, "sample_count": 22, "duration_seconds": 2.252}, "timestamp": "2026-01-19T13:00:00.587871"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1580.619, "latencies_ms": [1580.619], "images_per_second": 0.633, "prompt_tokens": 1100, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A young boy is standing at home plate in a baseball game, wearing a helmet and holding a bat.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.62, "peak": 130.52, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 30.48, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.48, "energy_joules_est": 48.2, "sample_count": 16, "duration_seconds": 1.581}, "timestamp": "2026-01-19T13:00:02.265628"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2662.043, "latencies_ms": [2662.043], "images_per_second": 0.376, "prompt_tokens": 1114, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. baseball bat: 1\n2. helmet: 1\n3. catcher's mitt: 1\n4. baseball: 1\n5. boy: 1\n6. grass: 1\n7. tree: 1\n8. bench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.57, "peak": 124.19, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.88, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.88, "energy_joules_est": 71.56, "sample_count": 26, "duration_seconds": 2.662}, "timestamp": "2026-01-19T13:00:04.972906"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2005.223, "latencies_ms": [2005.223], "images_per_second": 0.499, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire in the background. The batter is closer to the camera than the catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.51, "peak": 113.61, "min": 30.11}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.57, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.57, "energy_joules_est": 57.3, "sample_count": 20, "duration_seconds": 2.006}, "timestamp": "2026-01-19T13:00:07.047345"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1528.505, "latencies_ms": [1528.505], "images_per_second": 0.654, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young boy is playing baseball in a park with a catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.2, "peak": 123.71, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.57, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 30.57, "energy_joules_est": 46.74, "sample_count": 15, "duration_seconds": 1.529}, "timestamp": "2026-01-19T13:00:08.615646"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2024.952, "latencies_ms": [2024.952], "images_per_second": 0.494, "prompt_tokens": 1110, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is taken during a sunny day with clear blue skies. The colors in the image are vibrant, with the green grass and trees providing a natural backdrop to the baseball game.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.21, "peak": 129.93, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.23, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 29.23, "energy_joules_est": 59.2, "sample_count": 20, "duration_seconds": 2.025}, "timestamp": "2026-01-19T13:00:10.706095"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1639.452, "latencies_ms": [1639.452], "images_per_second": 0.61, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A banana is placed on top of a black Philips phone, which is connected to a coiled cord.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.89, "peak": 128.64, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.71, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.71, "energy_joules_est": 50.37, "sample_count": 16, "duration_seconds": 1.64}, "timestamp": "2026-01-19T13:00:12.374767"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1985.571, "latencies_ms": [1985.571], "images_per_second": 0.504, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " banana: 1, phone: 1, pen: 1, paper: 1, computer: 1, coiled cord: 1, desk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.22, "peak": 122.42, "min": 27.72}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.53, "energy_joules_est": 58.65, "sample_count": 20, "duration_seconds": 1.986}, "timestamp": "2026-01-19T13:00:14.459190"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2068.71, "latencies_ms": [2068.71], "images_per_second": 0.483, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The banana is positioned to the right of the phone, which is placed on the left side of the desk. The notebook is located in the foreground, closer to the camera than the banana.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.52, "peak": 129.18, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.72, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.72, "energy_joules_est": 59.44, "sample_count": 20, "duration_seconds": 2.07}, "timestamp": "2026-01-19T13:00:16.544219"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1617.144, "latencies_ms": [1617.144], "images_per_second": 0.618, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A banana is on top of a phone and a piece of paper with writing on it is on the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.33, "peak": 128.52, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.2, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.2, "energy_joules_est": 50.47, "sample_count": 16, "duration_seconds": 1.618}, "timestamp": "2026-01-19T13:00:18.212401"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2003.792, "latencies_ms": [2003.792], "images_per_second": 0.499, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming from the left side. The banana is yellow and ripe, and the phone is black with a blue screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.47, "peak": 113.17, "min": 27.13}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.29, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.29, "energy_joules_est": 58.7, "sample_count": 20, "duration_seconds": 2.004}, "timestamp": "2026-01-19T13:00:20.300039"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2201.719, "latencies_ms": [2201.719], "images_per_second": 0.454, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image captures a bustling scene of a crowd of people, with individuals of various ages and attires, including a person holding a large teddy bear, all engaged in different activities, creating a lively and dynamic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.43, "peak": 112.11, "min": 27.22}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.87, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.87, "energy_joules_est": 61.38, "sample_count": 22, "duration_seconds": 2.203}, "timestamp": "2026-01-19T13:00:22.596084"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2685.222, "latencies_ms": [2685.222], "images_per_second": 0.372, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. crowd: 10\n2. person: 10\n3. backpack: 5\n4. handbag: 5\n5. purse: 5\n6. purse: 5\n7. handbag: 5\n8. handbag: 5", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.74, "peak": 116.01, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.55, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.55, "energy_joules_est": 71.3, "sample_count": 26, "duration_seconds": 2.686}, "timestamp": "2026-01-19T13:00:25.304268"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2523.758, "latencies_ms": [2523.758], "images_per_second": 0.396, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The main objects are the crowd of people, the teddy bear, and the telephone booth. The crowd of people is in the foreground, with the teddy bear and telephone booth in the background. The teddy bear is near the crowd of people, while the telephone booth is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.4, "ram_available_mb": 99782.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.36, "peak": 126.16, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.03, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.03, "energy_joules_est": 68.23, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T13:00:27.911884"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1476.925, "latencies_ms": [1476.925], "images_per_second": 0.677, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A large crowd of people are walking down a busy street in a city.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.59, "peak": 130.4, "min": 28.0}, "VIN_SYS_5V0": {"avg": 14.05, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.63, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.63, "energy_joules_est": 45.25, "sample_count": 15, "duration_seconds": 1.477}, "timestamp": "2026-01-19T13:00:29.477621"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1575.965, "latencies_ms": [1575.965], "images_per_second": 0.635, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The image is taken during the day with natural lighting, and the colors are vibrant and varied.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.33, "peak": 117.04, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.03, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.32, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 31.32, "energy_joules_est": 49.37, "sample_count": 16, "duration_seconds": 1.576}, "timestamp": "2026-01-19T13:00:31.147975"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1996.569, "latencies_ms": [1996.569], "images_per_second": 0.501, "prompt_tokens": 1100, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a man wearing a red shirt is holding a baby in his arms, while a brown horse is leaning its head over a wooden fence to nuzzle the man.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.66, "peak": 121.57, "min": 27.24}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.98, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 28.98, "energy_joules_est": 57.88, "sample_count": 20, "duration_seconds": 1.997}, "timestamp": "2026-01-19T13:00:33.241612"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2744.183, "latencies_ms": [2744.183], "images_per_second": 0.364, "prompt_tokens": 1114, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. horse: 1\n2. man: 1\n3. child: 1\n4. brick wall: 1\n5. wooden beams: 1\n6. horse's nose: 1\n7. man's hand: 1\n8. child's hand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.11, "peak": 130.23, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.19, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.19, "energy_joules_est": 71.88, "sample_count": 27, "duration_seconds": 2.745}, "timestamp": "2026-01-19T13:00:36.050051"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2184.632, "latencies_ms": [2184.632], "images_per_second": 0.458, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The man is standing to the left of the horse, and the baby is in his arms, which are positioned in the foreground. The horse is in the background, and the man is in front of the horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.8, "peak": 118.0, "min": 32.57}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 28.16, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.16, "energy_joules_est": 61.53, "sample_count": 21, "duration_seconds": 2.185}, "timestamp": "2026-01-19T13:00:38.239052"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1404.01, "latencies_ms": [1404.01], "images_per_second": 0.712, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man and a child are petting a horse in a stable.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.92, "peak": 122.26, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.52, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.52, "energy_joules_est": 44.27, "sample_count": 14, "duration_seconds": 1.404}, "timestamp": "2026-01-19T13:00:39.701698"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3079.673, "latencies_ms": [3079.673], "images_per_second": 0.325, "prompt_tokens": 1110, "response_tokens_est": 77, "n_tiles": 1, "output_text": " The image features a brown horse with a white blaze on its face, standing in a stable with a man in a red shirt holding a baby. The horse is looking at the baby, who is wearing a white shirt with blue stars. The stable has a wooden roof and a brick wall. The lighting is natural, coming from the sun, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.04, "peak": 130.57, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.15, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 26.15, "energy_joules_est": 80.55, "sample_count": 30, "duration_seconds": 3.08}, "timestamp": "2026-01-19T13:00:42.826006"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1303.595, "latencies_ms": [1303.595], "images_per_second": 0.767, "prompt_tokens": 1100, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A banana and peanut butter are on a plate.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.04, "peak": 121.31, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.73, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.73, "energy_joules_est": 41.39, "sample_count": 13, "duration_seconds": 1.304}, "timestamp": "2026-01-19T13:00:44.188667"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1439.679, "latencies_ms": [1439.679], "images_per_second": 0.695, "prompt_tokens": 1114, "response_tokens_est": 15, "n_tiles": 1, "output_text": " banana: 1, plate: 1, peanut butter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.72, "peak": 121.09, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 33.26, "peak": 40.56, "min": 21.67}}, "power_watts_avg": 33.26, "energy_joules_est": 47.89, "sample_count": 14, "duration_seconds": 1.44}, "timestamp": "2026-01-19T13:00:45.656260"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1985.854, "latencies_ms": [1985.854], "images_per_second": 0.504, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The banana is on the left side of the plate, and the peanut butter is on the right side. The plate is in the foreground, and the table is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.7, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.91, "peak": 120.93, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.77, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 29.77, "energy_joules_est": 59.13, "sample_count": 20, "duration_seconds": 1.986}, "timestamp": "2026-01-19T13:00:47.743961"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1406.714, "latencies_ms": [1406.714], "images_per_second": 0.711, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A banana and peanut butter are on a plate on a table.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.82, "peak": 112.51, "min": 27.38}, "VIN_SYS_5V0": {"avg": 14.02, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.79, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.79, "energy_joules_est": 43.33, "sample_count": 14, "duration_seconds": 1.407}, "timestamp": "2026-01-19T13:00:49.206849"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1708.249, "latencies_ms": [1708.249], "images_per_second": 0.585, "prompt_tokens": 1110, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image shows a white plate with a banana and peanut butter on it. The plate is placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.46, "peak": 106.43, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.87, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 30.87, "energy_joules_est": 52.75, "sample_count": 17, "duration_seconds": 1.709}, "timestamp": "2026-01-19T13:00:50.982834"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1667.743, "latencies_ms": [1667.743], "images_per_second": 0.6, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man in a green shirt and dark pants is working on a bicycle wheel on the ground next to a motorcycle.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.76, "peak": 106.95, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.45, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 30.45, "energy_joules_est": 50.81, "sample_count": 17, "duration_seconds": 1.669}, "timestamp": "2026-01-19T13:00:52.756596"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2588.372, "latencies_ms": [2588.372], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. motorcycle: 1\n3. bicycle: 1\n4. scooter: 1\n5. wheel: 1\n6. tire: 1\n7. tool: 1\n8. pavement: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.36, "peak": 131.19, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.17, "energy_joules_est": 70.34, "sample_count": 25, "duration_seconds": 2.589}, "timestamp": "2026-01-19T13:00:55.362820"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2613.912, "latencies_ms": [2613.912], "images_per_second": 0.383, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The man is kneeling on the ground, which is in the foreground of the image. He is working on a bicycle wheel, which is near the center of the image. In the background, there are other bicycles and a motorcycle parked, suggesting that this is a busy area with multiple vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.6, "ram_available_mb": 99782.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.32, "peak": 134.95, "min": 27.1}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.74, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 26.74, "energy_joules_est": 69.91, "sample_count": 26, "duration_seconds": 2.615}, "timestamp": "2026-01-19T13:00:58.071606"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1605.982, "latencies_ms": [1605.982], "images_per_second": 0.623, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man is fixing a bicycle wheel on the ground. There are other bicycles and motorcycles parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.45, "peak": 130.46, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.11, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 30.11, "energy_joules_est": 48.37, "sample_count": 16, "duration_seconds": 1.606}, "timestamp": "2026-01-19T13:00:59.740611"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2859.3, "latencies_ms": [2859.3], "images_per_second": 0.35, "prompt_tokens": 1109, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image depicts a man wearing a green shirt and blue pants, kneeling on the ground while working on a bicycle wheel. The scene is set in an outdoor environment with a brick pavement and a motorcycle parked in the background. The lighting appears to be natural, possibly from the sun, and the weather seems to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.81, "peak": 121.19, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.34, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.23, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.23, "energy_joules_est": 75.01, "sample_count": 28, "duration_seconds": 2.86}, "timestamp": "2026-01-19T13:01:02.680706"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1555.701, "latencies_ms": [1555.701], "images_per_second": 0.643, "prompt_tokens": 1100, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A skateboarder with dreadlocks is performing a trick on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.62, "peak": 120.85, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.55, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 30.55, "energy_joules_est": 47.54, "sample_count": 15, "duration_seconds": 1.556}, "timestamp": "2026-01-19T13:01:04.253583"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2568.151, "latencies_ms": [2568.151], "images_per_second": 0.389, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. skateboard: 1\n2. person: 1\n3. fence: 1\n4. grass: 1\n5. building: 1\n6. tree: 1\n7. wristband: 1\n8. camera: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.44, "peak": 129.01, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.61, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 27.61, "energy_joules_est": 70.92, "sample_count": 25, "duration_seconds": 2.569}, "timestamp": "2026-01-19T13:01:06.857098"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2515.706, "latencies_ms": [2515.706], "images_per_second": 0.398, "prompt_tokens": 1118, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick on a ramp. The ramp is located in the middle ground, with the skateboarder's shadow visible on the ramp. The background features a fence and trees, indicating that the skatepark is outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.06, "peak": 122.56, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.7, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.7, "energy_joules_est": 67.18, "sample_count": 25, "duration_seconds": 2.516}, "timestamp": "2026-01-19T13:01:09.469195"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1469.375, "latencies_ms": [1469.375], "images_per_second": 0.681, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A skateboarder is performing a trick on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.18, "peak": 127.85, "min": 29.18}, "VIN_SYS_5V0": {"avg": 13.94, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.94, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 29.94, "energy_joules_est": 44.01, "sample_count": 15, "duration_seconds": 1.47}, "timestamp": "2026-01-19T13:01:11.039108"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2198.628, "latencies_ms": [2198.628], "images_per_second": 0.455, "prompt_tokens": 1110, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick on a concrete ramp, with the skateboarder wearing a black t-shirt and black pants. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25989.9, "ram_available_mb": 99782.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.45, "peak": 129.46, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.44, "min": 11.36}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 28.17, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.17, "energy_joules_est": 61.94, "sample_count": 22, "duration_seconds": 2.199}, "timestamp": "2026-01-19T13:01:13.330565"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4429.202, "latencies_ms": [4429.202], "images_per_second": 0.226, "prompt_tokens": 1099, "response_tokens_est": 128, "n_tiles": 1, "output_text": " In the image, a group of five individuals are gathered on a grassy field, each holding a frisbee. They are arranged in a line, with the person on the far left holding a white frisbee, followed by a person in a blue shirt, a person in a red shirt, a person in a white shirt, and finally a person in a blue shirt on the far right. The frisbees are of different colors, with the person in the blue shirt holding a green frisbee, the person in the red shirt holding a white frisbee, the person in the white shirt holding a red frisbee", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.22, "peak": 124.23, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 23.5, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 23.5, "energy_joules_est": 104.09, "sample_count": 43, "duration_seconds": 4.429}, "timestamp": "2026-01-19T13:01:17.797930"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2628.271, "latencies_ms": [2628.271], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. frisbee: 5\n2. person: 5\n3. headband: 1\n4. shorts: 4\n5. shirt: 4\n6. shoes: 4\n7. grass: 1\n8. goal post: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.28, "peak": 123.4, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.75, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.75, "energy_joules_est": 70.31, "sample_count": 26, "duration_seconds": 2.629}, "timestamp": "2026-01-19T13:01:20.486437"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4328.25, "latencies_ms": [4328.25], "images_per_second": 0.231, "prompt_tokens": 1117, "response_tokens_est": 123, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground, with the person on the left holding the frisbee closest to the camera, and the person on the right holding the frisbee farthest from the camera. The frisbees are arranged in a line, with the person on the left holding the first frisbee, and the person on the right holding the last frisbee. The frisbees are held at different heights, with the person on the left holding the frisbee closest to the ground, and the person on the right holding the frisbee closest to the person in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.16, "peak": 124.49, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 23.72, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 23.72, "energy_joules_est": 102.68, "sample_count": 42, "duration_seconds": 4.329}, "timestamp": "2026-01-19T13:01:24.843871"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1482.512, "latencies_ms": [1482.512], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are posing for a picture on a grassy field at sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.16, "peak": 127.43, "min": 27.19}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 31.12, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.12, "energy_joules_est": 46.14, "sample_count": 15, "duration_seconds": 1.483}, "timestamp": "2026-01-19T13:01:26.403443"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2607.028, "latencies_ms": [2607.028], "images_per_second": 0.384, "prompt_tokens": 1109, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a group of five people posing for a photo on a grassy field at sunset. The sky is painted with hues of orange and blue, and the sun is setting in the background. The people are holding frisbees, which are white with green and red designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.5, "peak": 128.25, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.94, "peak": 40.57, "min": 16.95}}, "power_watts_avg": 26.94, "energy_joules_est": 70.24, "sample_count": 26, "duration_seconds": 2.607}, "timestamp": "2026-01-19T13:01:29.107710"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3943.885, "latencies_ms": [3943.885], "images_per_second": 0.254, "prompt_tokens": 1099, "response_tokens_est": 109, "n_tiles": 1, "output_text": " The image captures a moment at an airport where a large white airplane adorned with red and black accents is parked at a gate, ready for its next journey. The airplane, bearing the logo of Japan Airlines, is surrounded by a bustling airport environment, with various airport vehicles and personnel diligently attending to their tasks. The sky above is a clear blue, dotted with fluffy white clouds, providing a serene backdrop to the scene. The image is taken from a high vantage point, offering a comprehensive view of the airport's operations and the airplane's prominent presence.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.83, "peak": 123.79, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 23.87, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 23.87, "energy_joules_est": 94.16, "sample_count": 39, "duration_seconds": 3.945}, "timestamp": "2026-01-19T13:01:33.155280"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2646.798, "latencies_ms": [2646.798], "images_per_second": 0.378, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. airplane: 1\n2. tail: 1\n3. wing: 1\n4. engine: 1\n5. jet bridge: 1\n6. luggage cart: 1\n7. ground crew: 1\n8. terminal building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.94, "peak": 123.0, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.37, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.37, "energy_joules_est": 69.81, "sample_count": 26, "duration_seconds": 2.647}, "timestamp": "2026-01-19T13:01:35.866385"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2452.65, "latencies_ms": [2452.65], "images_per_second": 0.408, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The airplane is positioned on the left side of the image, with the terminal building located on the right side. The airplane is in the foreground, while the terminal building is in the background. The airplane is closer to the viewer than the terminal building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.1, "ram_available_mb": 99782.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.4, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.38, "peak": 120.14, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.06, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.06, "energy_joules_est": 66.39, "sample_count": 24, "duration_seconds": 2.453}, "timestamp": "2026-01-19T13:01:38.347875"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3251.296, "latencies_ms": [3251.296], "images_per_second": 0.308, "prompt_tokens": 1111, "response_tokens_est": 83, "n_tiles": 1, "output_text": " The image captures a moment at an airport where a large white airplane with red and white accents is parked at a gate. The airplane is adorned with the logo of Japan Airlines, and the word \"JAL\" is prominently displayed on its side. The sky above is a clear blue, dotted with fluffy white clouds, and the airport tarmac is marked with yellow arrows and lines, guiding the flow of airport operations.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.4, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25990.4, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.56, "peak": 129.64, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.32, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 25.32, "energy_joules_est": 82.33, "sample_count": 32, "duration_seconds": 3.252}, "timestamp": "2026-01-19T13:01:41.686821"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1584.725, "latencies_ms": [1584.725], "images_per_second": 0.631, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The airplane is white with red and black accents, and the sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.4, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.08, "peak": 110.02, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.09, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.09, "energy_joules_est": 47.7, "sample_count": 16, "duration_seconds": 1.585}, "timestamp": "2026-01-19T13:01:43.346885"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1603.922, "latencies_ms": [1603.922], "images_per_second": 0.623, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A young man wearing a yellow shirt and black pants is skateboarding on a ramp with graffiti on it.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.48, "peak": 123.25, "min": 30.42}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.2, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.2, "energy_joules_est": 50.06, "sample_count": 16, "duration_seconds": 1.604}, "timestamp": "2026-01-19T13:01:45.017543"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.573, "latencies_ms": [2590.573], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. skateboard: 1\n3. bench: 1\n4. trash can: 1\n5. fence: 1\n6. car: 1\n7. grass: 1\n8. road: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.68, "peak": 112.21, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.19, "peak": 40.57, "min": 17.34}}, "power_watts_avg": 27.19, "energy_joules_est": 70.45, "sample_count": 26, "duration_seconds": 2.591}, "timestamp": "2026-01-19T13:01:47.703683"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2295.61, "latencies_ms": [2295.61], "images_per_second": 0.436, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The skateboarder is positioned to the right of the graffiti-covered ramp, with the ramp situated in the foreground of the image. The skateboarder is also relatively close to the camera, while the ramp is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.25, "peak": 120.47, "min": 27.54}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.31, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.31, "energy_joules_est": 62.71, "sample_count": 23, "duration_seconds": 2.296}, "timestamp": "2026-01-19T13:01:50.085691"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1467.702, "latencies_ms": [1467.702], "images_per_second": 0.681, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A skateboarder is performing a trick on a ramp in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.05, "peak": 128.62, "min": 27.55}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.84, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.84, "energy_joules_est": 45.28, "sample_count": 15, "duration_seconds": 1.468}, "timestamp": "2026-01-19T13:01:51.643220"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2699.589, "latencies_ms": [2699.589], "images_per_second": 0.37, "prompt_tokens": 1109, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick on a concrete ramp, with vibrant colors of the ramp and the skateboard contrasting against the natural green grass and the clear blue sky. The lighting is bright and sunny, casting sharp shadows on the ground and highlighting the skateboarder's movements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.08, "peak": 114.61, "min": 28.2}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 26.66, "peak": 40.57, "min": 16.95}}, "power_watts_avg": 26.66, "energy_joules_est": 71.98, "sample_count": 27, "duration_seconds": 2.7}, "timestamp": "2026-01-19T13:01:54.435450"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1672.688, "latencies_ms": [1672.688], "images_per_second": 0.598, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A slice of chocolate cake with caramel drizzle sits on a white plate with gold floral designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.76, "peak": 128.75, "min": 29.73}, "VIN_SYS_5V0": {"avg": 13.9, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.27, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 29.27, "energy_joules_est": 48.98, "sample_count": 17, "duration_seconds": 1.673}, "timestamp": "2026-01-19T13:01:56.201393"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2586.618, "latencies_ms": [2586.618], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. plate: 1\n2. cake: 1\n3. syrup: 1\n4. chocolate: 1\n5. plate's design: 1\n6. table: 1\n7. background: 1\n8. light: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.69, "peak": 123.75, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.34, "min": 10.96}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 11.82}, "VDD_GPU": {"avg": 27.03, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.03, "energy_joules_est": 69.92, "sample_count": 25, "duration_seconds": 2.587}, "timestamp": "2026-01-19T13:01:58.805048"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2174.79, "latencies_ms": [2174.79], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The slice of chocolate cake is positioned on the right side of the plate, which is placed on a wooden table. The cake is in the foreground, with the plate and the table being the background elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.85, "peak": 118.24, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.94, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.94, "energy_joules_est": 60.77, "sample_count": 22, "duration_seconds": 2.175}, "timestamp": "2026-01-19T13:02:01.083344"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1445.065, "latencies_ms": [1445.065], "images_per_second": 0.692, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A slice of chocolate cake is on a plate with a gold rim.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 80.07, "peak": 111.85, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.93, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 31.21, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.21, "energy_joules_est": 45.12, "sample_count": 14, "duration_seconds": 1.446}, "timestamp": "2026-01-19T13:02:02.547047"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2080.731, "latencies_ms": [2080.731], "images_per_second": 0.481, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The slice of chocolate cake is on a white plate with gold designs, and the plate is on a wooden table. The lighting is bright and natural, and the cake is shiny and moist.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.95, "peak": 119.35, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.59, "peak": 40.97, "min": 18.14}}, "power_watts_avg": 29.59, "energy_joules_est": 61.58, "sample_count": 21, "duration_seconds": 2.081}, "timestamp": "2026-01-19T13:02:04.720934"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2224.54, "latencies_ms": [2224.54], "images_per_second": 0.45, "prompt_tokens": 1099, "response_tokens_est": 44, "n_tiles": 1, "output_text": " In the image, a man is seated at a desk in a cluttered office space, surrounded by various electronic devices and equipment, while a group of people are gathered around a table, engaged in a discussion or activity.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.26, "peak": 122.55, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 27.74, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.74, "energy_joules_est": 61.73, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T13:02:07.017758"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2526.249, "latencies_ms": [2526.249], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. laptop: 2\n2. chair: 3\n3. person: 3\n4. table: 2\n5. box: 1\n6. monitor: 1\n7. keyboard: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.95, "peak": 105.54, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.92, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.92, "energy_joules_est": 68.01, "sample_count": 25, "duration_seconds": 2.527}, "timestamp": "2026-01-19T13:02:09.637147"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2148.131, "latencies_ms": [2148.131], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The man is sitting in the foreground, working on his laptop. The other people are in the background, working on their laptops. The boxes are in the background, and the chairs are in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.15, "peak": 112.54, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.11, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.11, "energy_joules_est": 60.39, "sample_count": 21, "duration_seconds": 2.148}, "timestamp": "2026-01-19T13:02:11.822510"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1467.195, "latencies_ms": [1467.195], "images_per_second": 0.682, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are working in a cluttered office with computers and laptops.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.74, "peak": 130.01, "min": 27.04}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.28, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.28, "energy_joules_est": 45.91, "sample_count": 15, "duration_seconds": 1.468}, "timestamp": "2026-01-19T13:02:13.389882"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1654.715, "latencies_ms": [1654.715], "images_per_second": 0.604, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The room is lit with fluorescent lighting, and the walls are painted white. The floor is covered with yellow paint.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.87, "peak": 119.99, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.44, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 31.35, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.35, "energy_joules_est": 51.89, "sample_count": 16, "duration_seconds": 1.655}, "timestamp": "2026-01-19T13:02:15.060441"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2989.968, "latencies_ms": [2989.968], "images_per_second": 0.334, "prompt_tokens": 1099, "response_tokens_est": 75, "n_tiles": 1, "output_text": " In the image, a group of people are gathered in a living room, engaging in a video game session. The room is furnished with a blue couch, a wooden coffee table, and a wooden entertainment center. The individuals are holding Wii remotes, indicating that they are playing a video game. The room is well-lit, with natural light coming from the windows.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.12, "peak": 119.16, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 26.49, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 26.49, "energy_joules_est": 79.22, "sample_count": 29, "duration_seconds": 2.991}, "timestamp": "2026-01-19T13:02:18.083387"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2565.148, "latencies_ms": [2565.148], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. Wii remote: 1\n2. Woman: 1\n3. Man: 2\n4. Man: 1\n5. Woman: 1\n6. Woman: 1\n7. Woman: 1\n8. Woman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.53, "peak": 126.45, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.0, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.0, "energy_joules_est": 69.27, "sample_count": 25, "duration_seconds": 2.565}, "timestamp": "2026-01-19T13:02:20.695341"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3367.955, "latencies_ms": [3367.955], "images_per_second": 0.297, "prompt_tokens": 1117, "response_tokens_est": 88, "n_tiles": 1, "output_text": " The woman is holding a Wii remote in her right hand and is standing to the left of the couch. The couch is located in the middle of the room, with the woman standing to its right. The man in the green shirt is standing to the right of the couch, while the man in the blue shirt is standing to the left of the couch. The woman is standing closer to the camera than the man in the green shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.17, "peak": 128.14, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 25.06, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 25.06, "energy_joules_est": 84.41, "sample_count": 33, "duration_seconds": 3.368}, "timestamp": "2026-01-19T13:02:24.136896"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1418.446, "latencies_ms": [1418.446], "images_per_second": 0.705, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A group of people are playing a video game in a living room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.48, "peak": 133.33, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.29, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.29, "energy_joules_est": 44.39, "sample_count": 14, "duration_seconds": 1.419}, "timestamp": "2026-01-19T13:02:25.611352"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1597.129, "latencies_ms": [1597.129], "images_per_second": 0.626, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is lit by natural light coming from the windows, and the carpet is a light grey color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.15, "peak": 119.53, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.81, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 31.81, "energy_joules_est": 50.82, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T13:02:27.291333"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1569.047, "latencies_ms": [1569.047], "images_per_second": 0.637, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A person stands on a beach at sunset, with a frisbee in the air above them.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.93, "peak": 121.41, "min": 28.25}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.07, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.07, "energy_joules_est": 48.77, "sample_count": 16, "duration_seconds": 1.57}, "timestamp": "2026-01-19T13:02:28.971339"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2584.355, "latencies_ms": [2584.355], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. sun: 1\n3. water: 1\n4. ice: 1\n5. sky: 1\n6. sand: 1\n7. frisbee: 1\n8. reflection: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.5, "peak": 133.85, "min": 29.84}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 27.22, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.22, "energy_joules_est": 70.37, "sample_count": 25, "duration_seconds": 2.585}, "timestamp": "2026-01-19T13:02:31.581082"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2164.27, "latencies_ms": [2164.27], "images_per_second": 0.462, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The person is standing on the left side of the image, with the sun in the center and the water on the right. The person is closer to the camera than the sun, and the water is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.86, "peak": 120.78, "min": 30.36}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.57, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.57, "energy_joules_est": 61.85, "sample_count": 21, "duration_seconds": 2.165}, "timestamp": "2026-01-19T13:02:33.773409"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2613.159, "latencies_ms": [2613.159], "images_per_second": 0.383, "prompt_tokens": 1111, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image captures a serene beach scene at sunset, where a lone figure stands on the shore, silhouetted against the vibrant orange and yellow hues of the setting sun. The water is calm, reflecting the warm colors of the sky, and the sand is wet, hinting at the recent tide.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.0, "peak": 107.73, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.85, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 26.85, "energy_joules_est": 70.17, "sample_count": 26, "duration_seconds": 2.613}, "timestamp": "2026-01-19T13:02:36.483210"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2392.617, "latencies_ms": [2392.617], "images_per_second": 0.418, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image captures a serene beach scene during sunset, with the sun casting a warm glow over the water and sand. The sky is painted with hues of orange and yellow, while the water reflects the vibrant colors, creating a beautiful contrast with the surrounding landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.34, "peak": 115.58, "min": 33.9}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.36, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 27.36, "energy_joules_est": 65.48, "sample_count": 23, "duration_seconds": 2.393}, "timestamp": "2026-01-19T13:02:38.885225"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2266.584, "latencies_ms": [2266.584], "images_per_second": 0.441, "prompt_tokens": 1099, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image depicts a modern living room with a white sofa, a black coffee table, a dining table with four chairs, a television on a stand, a floor lamp, a vase with flowers, and a variety of wall decorations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.0, "peak": 123.35, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.17, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.17, "energy_joules_est": 63.86, "sample_count": 22, "duration_seconds": 2.267}, "timestamp": "2026-01-19T13:02:41.179279"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2812.303, "latencies_ms": [2812.303], "images_per_second": 0.356, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. white sofa: 1\n2. red chairs: 2\n3. white table: 1\n4. black and white rug: 1\n5. television: 1\n6. vase with flowers: 1\n7. wall art: 4\n8. floor lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.33, "peak": 109.75, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.23, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.23, "energy_joules_est": 73.78, "sample_count": 28, "duration_seconds": 2.813}, "timestamp": "2026-01-19T13:02:44.096096"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2763.48, "latencies_ms": [2763.48], "images_per_second": 0.362, "prompt_tokens": 1117, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The living room is situated in the foreground of the image, with the dining table and chairs placed in the center. The television is mounted on the wall in the background, and the sofa is positioned to the left of the television. The window is located behind the sofa, allowing natural light to flood the room.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.9, "peak": 129.24, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.14, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.14, "energy_joules_est": 72.25, "sample_count": 27, "duration_seconds": 2.764}, "timestamp": "2026-01-19T13:02:46.905433"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2223.5, "latencies_ms": [2223.5], "images_per_second": 0.45, "prompt_tokens": 1111, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image depicts a modern living room with a white sofa, a coffee table, and a dining table with chairs. The room is decorated with various pieces of art and has a large window that lets in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.94, "peak": 125.48, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.86, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.86, "energy_joules_est": 61.96, "sample_count": 22, "duration_seconds": 2.224}, "timestamp": "2026-01-19T13:02:49.199502"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1811.954, "latencies_ms": [1811.954], "images_per_second": 0.552, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The room is well lit with natural light coming in from the windows. The walls are painted white and the furniture is mostly black and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.02, "peak": 126.14, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.48, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.48, "energy_joules_est": 53.43, "sample_count": 18, "duration_seconds": 1.812}, "timestamp": "2026-01-19T13:02:51.077941"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1510.909, "latencies_ms": [1510.909], "images_per_second": 0.662, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A cat is standing on top of a blue refrigerator, looking up at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.05, "peak": 124.03, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 31.41, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 31.41, "energy_joules_est": 47.47, "sample_count": 15, "duration_seconds": 1.511}, "timestamp": "2026-01-19T13:02:52.648189"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2626.742, "latencies_ms": [2626.742], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. cat: 1\n2. refrigerator: 1\n3. cabinet: 1\n4. door: 1\n5. light: 1\n6. shelf: 1\n7. cupboard: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.79, "peak": 120.64, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 27.12, "energy_joules_est": 71.25, "sample_count": 26, "duration_seconds": 2.627}, "timestamp": "2026-01-19T13:02:55.350429"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2324.785, "latencies_ms": [2324.785], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The cat is positioned in the foreground, on top of the refrigerator, while the refrigerator is situated in the middle ground, between the cat and the wall. The wall is in the background, providing a sense of depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.39, "peak": 117.39, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.31, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.31, "energy_joules_est": 63.5, "sample_count": 23, "duration_seconds": 2.325}, "timestamp": "2026-01-19T13:02:57.749243"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1406.915, "latencies_ms": [1406.915], "images_per_second": 0.711, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A cat is standing on top of a refrigerator in a kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.85, "peak": 124.99, "min": 27.27}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.35, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 31.35, "energy_joules_est": 44.12, "sample_count": 14, "duration_seconds": 1.407}, "timestamp": "2026-01-19T13:02:59.212932"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2101.07, "latencies_ms": [2101.07], "images_per_second": 0.476, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a cat standing on top of a blue refrigerator, with a white cabinet in the background. The lighting is natural, coming from a window on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.3, "ram_available_mb": 99781.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.5, "peak": 122.73, "min": 27.89}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.0, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 29.0, "energy_joules_est": 60.94, "sample_count": 21, "duration_seconds": 2.101}, "timestamp": "2026-01-19T13:03:01.407985"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3566.09, "latencies_ms": [3566.09], "images_per_second": 0.28, "prompt_tokens": 1099, "response_tokens_est": 94, "n_tiles": 1, "output_text": " The image depicts a cozy, well-decorated living space with a mix of modern and vintage elements, featuring a white refrigerator adorned with various photos and magnets, a wooden bookshelf filled with books and decorative items, a wooden desk with a computer setup, and a comfortable couch with a throw blanket. The room is festively decorated with balloons in blue, green, and yellow, and a chandelier hangs from the ceiling, adding to the warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.74, "peak": 116.56, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 24.5, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 24.5, "energy_joules_est": 87.39, "sample_count": 35, "duration_seconds": 3.567}, "timestamp": "2026-01-19T13:03:05.058519"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4094.473, "latencies_ms": [4094.473], "images_per_second": 0.244, "prompt_tokens": 1113, "response_tokens_est": 114, "n_tiles": 1, "output_text": " refrigerator: 1, microwave: 1, toaster: 1, oven: 1, television: 1, bookshelf: 1, chair: 1, table: 1, rug: 1, balloon: 4, wall: 1, mirror: 1, curtain: 1, light: 1, wall art: 1, plant: 1, wall: 1, door: 1, floor: 1, ceiling: 1, light fixture: 1, balloon string: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.64, "peak": 125.38, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 23.85, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 23.85, "energy_joules_est": 97.66, "sample_count": 40, "duration_seconds": 4.095}, "timestamp": "2026-01-19T13:03:09.219823"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2430.234, "latencies_ms": [2430.234], "images_per_second": 0.411, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The refrigerator is on the left side of the room, the couch is on the right side, and the desk is in the middle. The balloons are in the foreground, the bookshelves are in the background, and the mirror is on the wall.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.91, "peak": 126.34, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.07, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.07, "energy_joules_est": 65.8, "sample_count": 24, "duration_seconds": 2.431}, "timestamp": "2026-01-19T13:03:11.718487"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1420.159, "latencies_ms": [1420.159], "images_per_second": 0.704, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A room with a fridge, a table, and a couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.08, "peak": 109.9, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.26, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 31.26, "energy_joules_est": 44.42, "sample_count": 14, "duration_seconds": 1.421}, "timestamp": "2026-01-19T13:03:13.177648"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1427.082, "latencies_ms": [1427.082], "images_per_second": 0.701, "prompt_tokens": 1109, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The room is lit by a chandelier and has wooden floors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.04, "peak": 116.84, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 33.17, "peak": 40.57, "min": 20.89}}, "power_watts_avg": 33.17, "energy_joules_est": 47.35, "sample_count": 14, "duration_seconds": 1.427}, "timestamp": "2026-01-19T13:03:14.646197"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1415.774, "latencies_ms": [1415.774], "images_per_second": 0.706, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man wearing headphones is sitting in a train and using a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.45, "peak": 129.34, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 33.01, "peak": 40.57, "min": 20.49}}, "power_watts_avg": 33.01, "energy_joules_est": 46.75, "sample_count": 14, "duration_seconds": 1.416}, "timestamp": "2026-01-19T13:03:16.122162"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2550.935, "latencies_ms": [2550.935], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. laptop: 1\n3. headset: 1\n4. chair: 1\n5. window: 1\n6. train: 1\n7. table: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.26, "peak": 134.71, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.77, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.77, "energy_joules_est": 70.85, "sample_count": 25, "duration_seconds": 2.551}, "timestamp": "2026-01-19T13:03:18.723716"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2201.25, "latencies_ms": [2201.25], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The laptop is in the foreground, on the table, and the man is sitting in the middle of the image. The window is on the left side of the image, and the train tracks are in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.42, "peak": 121.79, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 27.78, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.78, "energy_joules_est": 61.16, "sample_count": 22, "duration_seconds": 2.202}, "timestamp": "2026-01-19T13:03:21.024664"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1422.786, "latencies_ms": [1422.786], "images_per_second": 0.703, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man wearing headphones is sitting in a train and using a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.05, "peak": 120.26, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.4, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.4, "energy_joules_est": 44.69, "sample_count": 14, "duration_seconds": 1.423}, "timestamp": "2026-01-19T13:03:22.483674"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2389.691, "latencies_ms": [2389.691], "images_per_second": 0.418, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image depicts a man wearing headphones and using a laptop on a train. The train has a window with a view of the tracks outside. The lighting is natural, coming from the window, and the colors are muted due to the overcast weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.08, "peak": 127.5, "min": 27.95}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.23, "peak": 40.57, "min": 17.34}}, "power_watts_avg": 28.23, "energy_joules_est": 67.47, "sample_count": 24, "duration_seconds": 2.39}, "timestamp": "2026-01-19T13:03:24.971958"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2433.211, "latencies_ms": [2433.211], "images_per_second": 0.411, "prompt_tokens": 1099, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image captures a bustling train station with a striking white bridge arching gracefully over the tracks, while a sleek train glides smoothly along the tracks, and a red car is parked on the road, all under a clear blue sky dotted with fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.8, "ram_available_mb": 99781.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.77, "peak": 118.02, "min": 30.59}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 27.04, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.04, "energy_joules_est": 65.82, "sample_count": 24, "duration_seconds": 2.434}, "timestamp": "2026-01-19T13:03:27.459717"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.427, "latencies_ms": [2590.427], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. bridge: 1\n2. train tracks: 4\n3. train: 1\n4. train station: 1\n5. road: 1\n6. sky: 1\n7. clouds: 1\n8. buildings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.8, "ram_available_mb": 99781.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.24, "peak": 127.94, "min": 30.87}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 26.61, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.61, "energy_joules_est": 68.94, "sample_count": 26, "duration_seconds": 2.591}, "timestamp": "2026-01-19T13:03:30.149594"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2255.511, "latencies_ms": [2255.511], "images_per_second": 0.443, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The train station is located in the foreground, with the bridge stretching across the image, and the city skyline is visible in the background. The train tracks are parallel to each other, and the bridge is positioned above them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.92, "peak": 124.17, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.67, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.67, "energy_joules_est": 62.42, "sample_count": 22, "duration_seconds": 2.256}, "timestamp": "2026-01-19T13:03:32.452756"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2576.353, "latencies_ms": [2576.353], "images_per_second": 0.388, "prompt_tokens": 1111, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a bustling train station nestled amidst a serene landscape. The station, a marvel of modern engineering, boasts a striking white truss bridge that arches gracefully over the tracks. The sky above is a clear blue, dotted with fluffy white clouds, providing a beautiful backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.17, "peak": 130.76, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.98, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 26.98, "energy_joules_est": 69.52, "sample_count": 25, "duration_seconds": 2.577}, "timestamp": "2026-01-19T13:03:35.049701"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1631.349, "latencies_ms": [1631.349], "images_per_second": 0.613, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image features a white bridge with a unique triangular design, and the sky is filled with fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.73, "peak": 122.7, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.75, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 30.75, "energy_joules_est": 50.18, "sample_count": 16, "duration_seconds": 1.632}, "timestamp": "2026-01-19T13:03:36.713224"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2029.708, "latencies_ms": [2029.708], "images_per_second": 0.493, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a group of people are enjoying a day at the park, with a kite flying high in the sky, and a man in a white shirt is flying it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.8, "peak": 119.32, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.29, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 29.29, "energy_joules_est": 59.47, "sample_count": 20, "duration_seconds": 2.03}, "timestamp": "2026-01-19T13:03:38.801556"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2091.485, "latencies_ms": [2091.485], "images_per_second": 0.478, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " kite: 1, person: 2, grass: 1, person: 2, person: 2, person: 2, person: 2, person: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.66, "peak": 107.85, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.42, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 28.42, "energy_joules_est": 59.45, "sample_count": 21, "duration_seconds": 2.092}, "timestamp": "2026-01-19T13:03:40.983824"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2289.281, "latencies_ms": [2289.281], "images_per_second": 0.437, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the people are in the background, walking around the park. The kite is positioned to the left of the people, and the park is spread out behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.52, "peak": 121.11, "min": 34.84}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.92, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.92, "energy_joules_est": 63.93, "sample_count": 22, "duration_seconds": 2.29}, "timestamp": "2026-01-19T13:03:43.276303"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1383.44, "latencies_ms": [1383.44], "images_per_second": 0.723, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A group of people are flying a large kite in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.28, "peak": 120.81, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.08, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 32.08, "energy_joules_est": 44.4, "sample_count": 14, "duration_seconds": 1.384}, "timestamp": "2026-01-19T13:03:44.738505"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1495.998, "latencies_ms": [1495.998], "images_per_second": 0.668, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The kite is blue, purple, and yellow, and the sky is clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.45, "peak": 122.67, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 32.07, "peak": 40.97, "min": 19.71}}, "power_watts_avg": 32.07, "energy_joules_est": 48.0, "sample_count": 15, "duration_seconds": 1.497}, "timestamp": "2026-01-19T13:03:46.304422"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2362.452, "latencies_ms": [2362.452], "images_per_second": 0.423, "prompt_tokens": 1099, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image captures a miniature model of a red and black train with the Virgin logo on its side, traveling on a track alongside a group of workers dressed in orange uniforms, all set against a backdrop of a green landscape with trees and a fence.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.4, "peak": 127.67, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.09, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 28.09, "energy_joules_est": 66.39, "sample_count": 23, "duration_seconds": 2.363}, "timestamp": "2026-01-19T13:03:48.707989"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1962.282, "latencies_ms": [1962.282], "images_per_second": 0.51, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " train: 1, workers: 4, railway: 2, tracks: 2, wires: 2, fence: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.07, "peak": 116.38, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.32, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.32, "energy_joules_est": 57.54, "sample_count": 19, "duration_seconds": 1.963}, "timestamp": "2026-01-19T13:03:50.690878"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2358.665, "latencies_ms": [2358.665], "images_per_second": 0.424, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The red and black train is positioned on the left side of the image, with the workers on the right side. The train is in the foreground, while the workers are in the background. The train is closer to the viewer than the workers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.53, "peak": 121.12, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.92, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 27.92, "energy_joules_est": 65.88, "sample_count": 23, "duration_seconds": 2.359}, "timestamp": "2026-01-19T13:03:53.094174"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1842.54, "latencies_ms": [1842.54], "images_per_second": 0.543, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A model train is on a track with workers in orange uniforms. The train is red and black and has the word \"Virgin\" on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.74, "peak": 114.35, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.85, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.64, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 29.64, "energy_joules_est": 54.62, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T13:03:54.972457"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2828.37, "latencies_ms": [2828.37], "images_per_second": 0.354, "prompt_tokens": 1109, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image features a vibrant red and black model train with the Virgin logo on its side, traveling on a set of tracks. The train is surrounded by a lush green landscape with trees and bushes, and the sky is clear and blue. The lighting in the image is bright and natural, suggesting that the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.38, "peak": 114.29, "min": 27.82}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.36, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.36, "energy_joules_est": 74.56, "sample_count": 28, "duration_seconds": 2.829}, "timestamp": "2026-01-19T13:03:57.896907"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1735.046, "latencies_ms": [1735.046], "images_per_second": 0.576, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image shows a close-up view of a cat's back, with its fur displaying a mix of brown and white colors.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.21, "peak": 127.61, "min": 27.37}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.6, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 29.6, "energy_joules_est": 51.38, "sample_count": 17, "duration_seconds": 1.736}, "timestamp": "2026-01-19T13:03:59.677502"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2589.048, "latencies_ms": [2589.048], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 1\n2. fur: 1\n3. tail: 1\n4. fabric: 1\n5. blanket: 1\n6. pattern: 1\n7. texture: 1\n8. color: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.51, "peak": 133.49, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 27.01, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 27.01, "energy_joules_est": 69.94, "sample_count": 25, "duration_seconds": 2.59}, "timestamp": "2026-01-19T13:04:02.289913"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1999.643, "latencies_ms": [1999.643], "images_per_second": 0.5, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The cat's tail is in the foreground, while the background is a textured beige blanket. The cat's fur is brown and white, and it appears to be lying down.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.45, "peak": 127.09, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.58, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.58, "energy_joules_est": 57.16, "sample_count": 20, "duration_seconds": 2.0}, "timestamp": "2026-01-19T13:04:04.382518"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1620.832, "latencies_ms": [1620.832], "images_per_second": 0.617, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A cat's back is shown in a close-up photo, with a textured blanket in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 126.47, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.74, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.74, "energy_joules_est": 48.23, "sample_count": 16, "duration_seconds": 1.622}, "timestamp": "2026-01-19T13:04:06.052094"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1808.091, "latencies_ms": [1808.091], "images_per_second": 0.553, "prompt_tokens": 1110, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image features a cat's fur with a mix of brown and white colors, and the lighting appears to be natural, possibly from a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.22, "peak": 111.65, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.03, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 30.03, "energy_joules_est": 54.31, "sample_count": 18, "duration_seconds": 1.809}, "timestamp": "2026-01-19T13:04:07.928957"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2103.24, "latencies_ms": [2103.24], "images_per_second": 0.475, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " In the image, a cow is seen milking itself using a milking machine, with its udder being the main focus, and the machine is attached to the cow's udder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.66, "peak": 109.61, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.48, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.48, "energy_joules_est": 59.92, "sample_count": 21, "duration_seconds": 2.104}, "timestamp": "2026-01-19T13:04:10.124377"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2251.308, "latencies_ms": [2251.308], "images_per_second": 0.444, "prompt_tokens": 1113, "response_tokens_est": 45, "n_tiles": 1, "output_text": " cow: 1, black: 1, white: 1, red: 1, black and white: 1, black carbon fiber: 1, yellow sticker: 1, metal pipe: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.7, "peak": 127.38, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.88, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.88, "energy_joules_est": 62.78, "sample_count": 22, "duration_seconds": 2.252}, "timestamp": "2026-01-19T13:04:12.422798"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2677.633, "latencies_ms": [2677.633], "images_per_second": 0.373, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The cow's udder is positioned in the foreground, with the black and white cow's body extending into the background. The red and white milking tubes are attached to the udder, with one tube on the left side and the other on the right side, both extending towards the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.22, "peak": 106.24, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.76, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.76, "energy_joules_est": 71.67, "sample_count": 26, "duration_seconds": 2.678}, "timestamp": "2026-01-19T13:04:15.128902"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1492.236, "latencies_ms": [1492.236], "images_per_second": 0.67, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A cow is standing in a barn and milking herself using a milking machine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.66, "peak": 133.92, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 31.2, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.2, "energy_joules_est": 46.57, "sample_count": 15, "duration_seconds": 1.493}, "timestamp": "2026-01-19T13:04:16.695088"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2087.4, "latencies_ms": [2087.4], "images_per_second": 0.479, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image features a black and white cow with red and white udders, standing on a black and yellow mat. The lighting is natural and bright, illuminating the scene with a warm glow.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.62, "peak": 131.72, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.93, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 28.93, "energy_joules_est": 60.41, "sample_count": 21, "duration_seconds": 2.088}, "timestamp": "2026-01-19T13:04:18.882912"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1859.717, "latencies_ms": [1859.717], "images_per_second": 0.538, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image features a plate with a sandwich cut in half, revealing its filling of red berries, placed on a green and white patterned tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.48, "peak": 122.31, "min": 27.24}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.4, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.4, "energy_joules_est": 54.69, "sample_count": 18, "duration_seconds": 1.86}, "timestamp": "2026-01-19T13:04:20.763042"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1690.226, "latencies_ms": [1690.226], "images_per_second": 0.592, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " plate: 1, sandwich: 1, knife: 1, bread: 2, fruit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.35, "peak": 117.57, "min": 27.74}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.5, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 30.5, "energy_joules_est": 51.57, "sample_count": 17, "duration_seconds": 1.691}, "timestamp": "2026-01-19T13:04:22.538303"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2274.71, "latencies_ms": [2274.71], "images_per_second": 0.44, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The sandwich is located in the foreground of the image, with the plate and knife placed to the right of it. The plate is positioned in the center of the image, with the sandwich and knife placed on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.34, "peak": 126.81, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.13, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.13, "energy_joules_est": 64.01, "sample_count": 22, "duration_seconds": 2.276}, "timestamp": "2026-01-19T13:04:24.828222"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1426.983, "latencies_ms": [1426.983], "images_per_second": 0.701, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A plate with a sandwich and a knife on a green tablecloth.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.93, "peak": 117.14, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.91, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.91, "energy_joules_est": 45.55, "sample_count": 14, "duration_seconds": 1.427}, "timestamp": "2026-01-19T13:04:26.290872"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2010.56, "latencies_ms": [2010.56], "images_per_second": 0.497, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The sandwich is on a plate with a white and brown pattern, and the plate is on a green tablecloth. The lighting is dim and the sandwich is lit from the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.4, "peak": 126.48, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.78, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 29.78, "energy_joules_est": 59.89, "sample_count": 20, "duration_seconds": 2.011}, "timestamp": "2026-01-19T13:04:28.373536"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1994.1, "latencies_ms": [1994.1], "images_per_second": 0.501, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image depicts a colorful and well-organized meal in a purple divided container, featuring a variety of food items including a salad, pasta with sauce, grapes, and carrots.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.78, "peak": 113.95, "min": 27.46}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.7, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.7, "energy_joules_est": 57.26, "sample_count": 20, "duration_seconds": 1.995}, "timestamp": "2026-01-19T13:04:30.463916"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2135.183, "latencies_ms": [2135.183], "images_per_second": 0.468, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " 1. purple tray\n2. orange tray\n3. green tray\n4. red tray\n5. purple container\n6. green container\n7. orange container\n8. blue container", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.46, "peak": 116.21, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.26, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.26, "energy_joules_est": 60.35, "sample_count": 21, "duration_seconds": 2.136}, "timestamp": "2026-01-19T13:04:32.648280"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3329.628, "latencies_ms": [3329.628], "images_per_second": 0.3, "prompt_tokens": 1117, "response_tokens_est": 81, "n_tiles": 1, "output_text": " The main objects are arranged in a grid-like pattern, with the salad in the top left, the pasta in the bottom left, the carrots in the top right, and the grapes in the bottom right. The salad is positioned in the top left corner, while the pasta is in the bottom left corner. The carrots are in the top right corner, and the grapes are in the bottom right corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.37, "peak": 127.22, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 24.72, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 24.72, "energy_joules_est": 82.32, "sample_count": 33, "duration_seconds": 3.33}, "timestamp": "2026-01-19T13:04:36.082696"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1822.015, "latencies_ms": [1822.015], "images_per_second": 0.549, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A colorful and healthy meal is served in a purple divided container. The meal includes a salad, pasta with sauce, grapes, and carrots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.48, "peak": 119.55, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.13, "peak": 39.39, "min": 14.19}}, "power_watts_avg": 29.13, "energy_joules_est": 53.1, "sample_count": 18, "duration_seconds": 1.823}, "timestamp": "2026-01-19T13:04:37.960427"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2330.592, "latencies_ms": [2330.592], "images_per_second": 0.429, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image features a vibrant and colorful meal in a purple divided container, with a variety of food items including a salad, pasta with sauce, and grapes. The lighting is bright and even, and the container appears to be made of plastic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.54, "peak": 117.45, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.87, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.87, "energy_joules_est": 64.97, "sample_count": 23, "duration_seconds": 2.331}, "timestamp": "2026-01-19T13:04:40.355918"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2366.622, "latencies_ms": [2366.622], "images_per_second": 0.423, "prompt_tokens": 1100, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image captures a serene scene of a cherry blossom tree in full bloom, with its branches adorned with delicate pink and white flowers, while a traffic light with a red light hangs in the foreground, adding a touch of urban life to the tranquil setting.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.41, "peak": 128.71, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.34, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.34, "energy_joules_est": 64.71, "sample_count": 23, "duration_seconds": 2.367}, "timestamp": "2026-01-19T13:04:42.757221"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2679.507, "latencies_ms": [2679.507], "images_per_second": 0.373, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. traffic light: 4\n2. cherry blossoms: 1\n3. streetlight: 1\n4. building: 1\n5. tree branches: 1\n6. sky: 1\n7. ground: 1\n8. traffic light signal: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.06, "peak": 124.65, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.67, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.67, "energy_joules_est": 71.47, "sample_count": 26, "duration_seconds": 2.68}, "timestamp": "2026-01-19T13:04:45.464479"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2154.749, "latencies_ms": [2154.749], "images_per_second": 0.464, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The traffic light is positioned above the cherry blossoms, with the traffic light on the left and the cherry blossoms on the right. The traffic light is in the foreground, while the cherry blossoms are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.83, "peak": 114.32, "min": 29.77}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.27, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.27, "energy_joules_est": 60.93, "sample_count": 21, "duration_seconds": 2.155}, "timestamp": "2026-01-19T13:04:47.654336"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2459.676, "latencies_ms": [2459.676], "images_per_second": 0.407, "prompt_tokens": 1112, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a serene scene of a cherry blossom tree in full bloom, with its branches adorned with delicate pink and white flowers. The tree stands in front of a traffic light, which is currently displaying a red signal, indicating a halt in the flow of traffic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.4, "peak": 105.81, "min": 29.94}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.76, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.76, "energy_joules_est": 65.83, "sample_count": 24, "duration_seconds": 2.46}, "timestamp": "2026-01-19T13:04:50.166397"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1677.076, "latencies_ms": [1677.076], "images_per_second": 0.596, "prompt_tokens": 1110, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image features a cherry blossom tree with a traffic light hanging from its branches, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.45, "peak": 111.53, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.78, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.78, "energy_joules_est": 49.96, "sample_count": 17, "duration_seconds": 1.678}, "timestamp": "2026-01-19T13:04:51.927982"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1757.638, "latencies_ms": [1757.638], "images_per_second": 0.569, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image shows a plate with a serving of broccoli and a piece of grilled salmon, both of which are placed on a white plate.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.09, "peak": 118.6, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.44, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.5, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.5, "energy_joules_est": 53.62, "sample_count": 17, "duration_seconds": 1.758}, "timestamp": "2026-01-19T13:04:53.710648"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2746.169, "latencies_ms": [2746.169], "images_per_second": 0.364, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. plate: 1\n2. broccoli: 12\n3. salmon: 1\n4. sauce: 1\n5. seasoning: 1\n6. broccoli stem: 1\n7. broccoli floret: 1\n8. broccoli leaf: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.06, "peak": 114.04, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.7, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 26.7, "energy_joules_est": 73.33, "sample_count": 27, "duration_seconds": 2.746}, "timestamp": "2026-01-19T13:04:56.526844"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1884.422, "latencies_ms": [1884.422], "images_per_second": 0.531, "prompt_tokens": 1117, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The broccoli is in the foreground, while the salmon is in the background. The broccoli is near the plate, while the salmon is far from the plate.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.5, "peak": 115.85, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.99, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.99, "energy_joules_est": 54.65, "sample_count": 19, "duration_seconds": 1.885}, "timestamp": "2026-01-19T13:04:58.491697"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2356.113, "latencies_ms": [2356.113], "images_per_second": 0.424, "prompt_tokens": 1111, "response_tokens_est": 48, "n_tiles": 1, "output_text": " In the image, there is a plate of food that includes broccoli and salmon. The broccoli is green and appears to be cooked, while the salmon is orange and appears to be grilled. The plate is white and is placed on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.91, "peak": 114.54, "min": 30.25}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.61, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 27.61, "energy_joules_est": 65.06, "sample_count": 23, "duration_seconds": 2.356}, "timestamp": "2026-01-19T13:05:00.888275"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2525.083, "latencies_ms": [2525.083], "images_per_second": 0.396, "prompt_tokens": 1109, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a plate of food with a vibrant green broccoli dish and a piece of grilled salmon. The broccoli is cooked to a bright green color, while the salmon has a golden-brown crust. The plate is white, and the food is placed on a white tablecloth.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.64, "peak": 122.19, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.97, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.97, "energy_joules_est": 68.11, "sample_count": 25, "duration_seconds": 2.525}, "timestamp": "2026-01-19T13:05:03.481898"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1566.117, "latencies_ms": [1566.117], "images_per_second": 0.639, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are sitting at a table in a restaurant, with one person eating a sandwich.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25990.5, "ram_available_mb": 99781.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.71, "peak": 126.99, "min": 26.85}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.44, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.44, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.44, "energy_joules_est": 47.7, "sample_count": 16, "duration_seconds": 1.567}, "timestamp": "2026-01-19T13:05:05.156419"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2546.773, "latencies_ms": [2546.773], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 3\n2. chair: 1\n3. table: 1\n4. person: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.03, "peak": 126.75, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 27.33, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 27.33, "energy_joules_est": 69.62, "sample_count": 25, "duration_seconds": 2.547}, "timestamp": "2026-01-19T13:05:07.747100"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2455.837, "latencies_ms": [2455.837], "images_per_second": 0.407, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The person in the middle is sitting between the two people on either side of them. The person on the left is sitting closer to the camera than the person on the right. The person on the right is sitting closer to the background than the person on the left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.11, "peak": 127.2, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.32, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.32, "energy_joules_est": 67.11, "sample_count": 24, "duration_seconds": 2.456}, "timestamp": "2026-01-19T13:05:10.230425"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1521.053, "latencies_ms": [1521.053], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Three people are sitting at a table in a restaurant. One person is eating a sandwich.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.04, "peak": 118.17, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.36, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.36, "energy_joules_est": 47.72, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T13:05:11.797069"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1622.755, "latencies_ms": [1622.755], "images_per_second": 0.616, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with warm lighting, and the subjects are wearing casual clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.49, "peak": 114.71, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.84, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 31.84, "energy_joules_est": 51.68, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T13:05:13.457011"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2083.234, "latencies_ms": [2083.234], "images_per_second": 0.48, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image captures a bustling urban scene with a yellow bus parked on the side of a street, a white bus in the distance, and a modern building with a grid of windows in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.38, "peak": 123.08, "min": 27.8}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.89, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 28.89, "energy_joules_est": 60.2, "sample_count": 21, "duration_seconds": 2.084}, "timestamp": "2026-01-19T13:05:15.640381"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2575.577, "latencies_ms": [2575.577], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bus: 1\n2. street: 1\n3. building: 1\n4. tree: 1\n5. sidewalk: 1\n6. pole: 1\n7. streetlight: 1\n8. curb: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.05, "peak": 110.12, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.89, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.89, "energy_joules_est": 69.27, "sample_count": 25, "duration_seconds": 2.576}, "timestamp": "2026-01-19T13:05:18.228679"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2109.674, "latencies_ms": [2109.674], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The yellow bus is parked on the side of the road, with the white bus driving on the road in the background. The trees are located in the foreground, while the buildings are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.64, "peak": 123.2, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.42, "peak": 39.39, "min": 16.55}}, "power_watts_avg": 28.42, "energy_joules_est": 59.96, "sample_count": 21, "duration_seconds": 2.11}, "timestamp": "2026-01-19T13:05:20.418650"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2638.157, "latencies_ms": [2638.157], "images_per_second": 0.379, "prompt_tokens": 1111, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The image captures a bustling urban scene with a yellow bus parked on the side of the road, a white bus driving down the street, and a large building in the background. The setting appears to be a city with a mix of modern architecture and greenery, suggesting a well-developed urban area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.8, "peak": 135.76, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.53, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.53, "energy_joules_est": 70.01, "sample_count": 26, "duration_seconds": 2.639}, "timestamp": "2026-01-19T13:05:23.121232"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1975.084, "latencies_ms": [1975.084], "images_per_second": 0.506, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a modern building with a glass facade, reflecting the sunlight and casting shadows on the ground. The sky is clear and blue, indicating a bright and sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.14, "peak": 124.54, "min": 28.05}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.58, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.58, "energy_joules_est": 56.46, "sample_count": 20, "duration_seconds": 1.976}, "timestamp": "2026-01-19T13:05:25.202876"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2190.22, "latencies_ms": [2190.22], "images_per_second": 0.457, "prompt_tokens": 1100, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image captures a serene street scene bathed in the warm glow of the setting sun, with a stop sign standing prominently in the foreground, its bold red color contrasting sharply with the muted tones of the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.65, "peak": 131.81, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.72, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.72, "energy_joules_est": 60.73, "sample_count": 22, "duration_seconds": 2.191}, "timestamp": "2026-01-19T13:05:27.484263"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.725, "latencies_ms": [2590.725], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. stop sign: 1\n2. pole: 1\n3. railing: 1\n4. street: 1\n5. building: 1\n6. car: 1\n7. grass: 1\n8. sidewalk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.18, "peak": 125.34, "min": 29.97}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.76, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.76, "energy_joules_est": 69.34, "sample_count": 25, "duration_seconds": 2.591}, "timestamp": "2026-01-19T13:05:30.088536"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2226.506, "latencies_ms": [2226.506], "images_per_second": 0.449, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The stop sign is positioned to the left of the metal fence, which is situated in the foreground of the image. In the background, there is a street with parked cars and buildings, which are relatively far from the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.11, "peak": 119.64, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.96, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.96, "energy_joules_est": 62.27, "sample_count": 22, "duration_seconds": 2.227}, "timestamp": "2026-01-19T13:05:32.381698"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2767.311, "latencies_ms": [2767.311], "images_per_second": 0.361, "prompt_tokens": 1112, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image captures a serene street scene bathed in the warm glow of the setting sun. A stop sign stands prominently in the foreground, its bold red color contrasting sharply with the muted tones of the surroundings. In the distance, a row of apartment buildings rises against the skyline, their windows reflecting the fading light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.62, "peak": 119.78, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.92, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 25.92, "energy_joules_est": 71.74, "sample_count": 27, "duration_seconds": 2.768}, "timestamp": "2026-01-19T13:05:35.174391"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1994.096, "latencies_ms": [1994.096], "images_per_second": 0.501, "prompt_tokens": 1110, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features a stop sign with a red octagonal shape and white letters, standing on a metal pole. The sun is shining brightly, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.96, "peak": 127.87, "min": 27.85}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.57, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.57, "energy_joules_est": 56.98, "sample_count": 20, "duration_seconds": 1.994}, "timestamp": "2026-01-19T13:05:37.259836"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1691.039, "latencies_ms": [1691.039], "images_per_second": 0.591, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A cat with a white and brown coat is lying on a black couch, with a white computer mouse next to it.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.41, "peak": 128.28, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.97, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.97, "energy_joules_est": 50.71, "sample_count": 17, "duration_seconds": 1.692}, "timestamp": "2026-01-19T13:05:39.032036"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1990.331, "latencies_ms": [1990.331], "images_per_second": 0.502, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " cat: 1, mouse: 1, cord: 1, black: 1, white: 1, brown: 1, black background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.45, "peak": 129.91, "min": 27.9}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.06, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.06, "energy_joules_est": 57.85, "sample_count": 20, "duration_seconds": 1.991}, "timestamp": "2026-01-19T13:05:41.114802"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2146.588, "latencies_ms": [2146.588], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The cat is in the foreground, lying on a black couch. The mouse is in the foreground, to the left of the cat. The cord is in the foreground, to the right of the cat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.12, "peak": 131.12, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.27, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.27, "energy_joules_est": 60.7, "sample_count": 21, "duration_seconds": 2.147}, "timestamp": "2026-01-19T13:05:43.307763"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1478.511, "latencies_ms": [1478.511], "images_per_second": 0.676, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A cat is laying on a black couch with a computer mouse next to it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.51, "peak": 119.01, "min": 27.74}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.33, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.33, "energy_joules_est": 46.34, "sample_count": 15, "duration_seconds": 1.479}, "timestamp": "2026-01-19T13:05:44.865204"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1501.946, "latencies_ms": [1501.946], "images_per_second": 0.666, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The cat is white and brown with green eyes, and the mouse is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.6, "peak": 111.81, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.94, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 31.94, "energy_joules_est": 47.99, "sample_count": 15, "duration_seconds": 1.502}, "timestamp": "2026-01-19T13:05:46.427660"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2335.901, "latencies_ms": [2335.901], "images_per_second": 0.428, "prompt_tokens": 1099, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a bustling cityscape with a large bus terminal bustling with activity, surrounded by a variety of buildings, including a prominent white building with a distinctive red and white tower, and a clear blue sky dotted with fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.39, "peak": 125.44, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.34, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.09, "peak": 40.57, "min": 18.52}}, "power_watts_avg": 28.09, "energy_joules_est": 65.64, "sample_count": 23, "duration_seconds": 2.337}, "timestamp": "2026-01-19T13:05:48.812580"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2540.35, "latencies_ms": [2540.35], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Bus: 3\n2. Bus: 2\n3. Bus: 1\n4. Bus: 1\n5. Bus: 1\n6. Bus: 1\n7. Bus: 1\n8. Bus: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.92, "peak": 129.76, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 26.89, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.89, "energy_joules_est": 68.32, "sample_count": 25, "duration_seconds": 2.541}, "timestamp": "2026-01-19T13:05:51.418891"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2409.937, "latencies_ms": [2409.937], "images_per_second": 0.415, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The buses are parked in the bus station, which is located in the foreground of the image. The buildings are in the background, with the skyline extending into the distance. The trees are in the foreground, providing a natural contrast to the urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.35, "peak": 122.45, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.2, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.2, "energy_joules_est": 65.57, "sample_count": 24, "duration_seconds": 2.411}, "timestamp": "2026-01-19T13:05:53.902126"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2487.321, "latencies_ms": [2487.321], "images_per_second": 0.402, "prompt_tokens": 1111, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image captures a bustling cityscape with a large bus terminal nestled amidst towering buildings. The sky above is a clear blue, dotted with fluffy white clouds, while the ground below is a mix of concrete and greenery, with trees and roads crisscrossing the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.67, "peak": 104.67, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 27.2, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.2, "energy_joules_est": 67.68, "sample_count": 24, "duration_seconds": 2.488}, "timestamp": "2026-01-19T13:05:56.399596"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1455.508, "latencies_ms": [1455.508], "images_per_second": 0.687, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The sky is blue with white clouds, and the buildings are white and gray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.41, "peak": 106.38, "min": 30.77}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 32.14, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 32.14, "energy_joules_est": 46.79, "sample_count": 14, "duration_seconds": 1.456}, "timestamp": "2026-01-19T13:05:57.863995"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1655.335, "latencies_ms": [1655.335], "images_per_second": 0.604, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man wearing a cowboy hat and shorts is skateboarding on a ramp, with a large tent in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.28, "peak": 121.6, "min": 28.34}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.4, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.4, "energy_joules_est": 52.0, "sample_count": 17, "duration_seconds": 1.656}, "timestamp": "2026-01-19T13:05:59.628437"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2631.765, "latencies_ms": [2631.765], "images_per_second": 0.38, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. hat: 1\n3. shorts: 1\n4. skateboard: 1\n5. ramp: 1\n6. tent: 2\n7. shadow: 1\n8. bicycle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.56, "peak": 129.04, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 26.47, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.47, "energy_joules_est": 69.67, "sample_count": 26, "duration_seconds": 2.632}, "timestamp": "2026-01-19T13:06:02.332689"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2551.659, "latencies_ms": [2551.659], "images_per_second": 0.392, "prompt_tokens": 1118, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The skateboarder is positioned in the foreground, performing a trick on a ramp. The tents are located in the background, providing a contrast to the skateboarder's action. The shadow of the skateboarder is cast on the ramp, indicating the direction of the light source.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.38, "peak": 128.0, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.68, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.68, "energy_joules_est": 68.09, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T13:06:04.929428"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1447.59, "latencies_ms": [1447.59], "images_per_second": 0.691, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man is skateboarding on a ramp in a desert-like setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.91, "peak": 113.75, "min": 32.07}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 31.07, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.07, "energy_joules_est": 44.98, "sample_count": 14, "duration_seconds": 1.448}, "timestamp": "2026-01-19T13:06:06.385142"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3328.296, "latencies_ms": [3328.296], "images_per_second": 0.3, "prompt_tokens": 1110, "response_tokens_est": 87, "n_tiles": 1, "output_text": " The image features a man wearing a cowboy hat and shorts, performing a skateboard trick on a ramp. The skateboarder is wearing a white cowboy hat and black shorts. The ramp is made of wood and has a red stripe painted on it. The skateboarder is in the middle of a trick, with his skateboard in the air. The background of the image shows a clear blue sky and a large tent.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.45, "peak": 128.6, "min": 27.62}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 25.8, "peak": 40.97, "min": 17.35}}, "power_watts_avg": 25.8, "energy_joules_est": 85.88, "sample_count": 33, "duration_seconds": 3.329}, "timestamp": "2026-01-19T13:06:09.793966"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1470.611, "latencies_ms": [1470.611], "images_per_second": 0.68, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is windsurfing on the ocean with a kite in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.01, "peak": 113.75, "min": 28.15}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.81, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.81, "energy_joules_est": 45.34, "sample_count": 15, "duration_seconds": 1.471}, "timestamp": "2026-01-19T13:06:11.355396"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2654.848, "latencies_ms": [2654.848], "images_per_second": 0.377, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. person: 1\n2. windsurf board: 1\n3. kite: 2\n4. kiteboard: 1\n5. ocean: 1\n6. sky: 2\n7. waves: 1\n8. water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.92, "peak": 129.03, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.02, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 27.02, "energy_joules_est": 71.74, "sample_count": 26, "duration_seconds": 2.655}, "timestamp": "2026-01-19T13:06:14.062730"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2224.384, "latencies_ms": [2224.384], "images_per_second": 0.45, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The kiteboarder is positioned in the foreground, with the ocean and kiteboarders in the background. The kiteboarder is near the water's edge, while the kiteboarders are further out in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.78, "peak": 112.67, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.96, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.96, "energy_joules_est": 62.21, "sample_count": 22, "duration_seconds": 2.225}, "timestamp": "2026-01-19T13:06:16.336946"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1596.056, "latencies_ms": [1596.056], "images_per_second": 0.627, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man is windsurfing in the ocean with a kite. The kite is yellow and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.7, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.19, "peak": 113.34, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.85, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.56, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 30.56, "energy_joules_est": 48.78, "sample_count": 16, "duration_seconds": 1.596}, "timestamp": "2026-01-19T13:06:18.002992"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2163.293, "latencies_ms": [2163.293], "images_per_second": 0.462, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a person windsurfing on the ocean with a clear blue sky and white clouds in the background. The water is a deep blue color, and the windsurfer is wearing a red shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.98, "peak": 122.64, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.95, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 28.95, "energy_joules_est": 62.64, "sample_count": 21, "duration_seconds": 2.164}, "timestamp": "2026-01-19T13:06:20.177560"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1724.027, "latencies_ms": [1724.027], "images_per_second": 0.58, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, there is a red fire hydrant situated in a grassy area, with a house and trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.23, "peak": 106.84, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.24, "energy_joules_est": 52.15, "sample_count": 17, "duration_seconds": 1.725}, "timestamp": "2026-01-19T13:06:21.955614"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2634.0, "latencies_ms": [2634.0], "images_per_second": 0.38, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. Fire hydrant: 1\n2. Grass: 1\n3. Flowers: 1\n4. Tree: 1\n5. House: 1\n6. Window: 1\n7. Flowers: 1\n8. Dandelion: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.1, "peak": 128.9, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.98, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 26.98, "energy_joules_est": 71.07, "sample_count": 26, "duration_seconds": 2.634}, "timestamp": "2026-01-19T13:06:24.644308"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2165.619, "latencies_ms": [2165.619], "images_per_second": 0.462, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The fire hydrant is located in the foreground of the image, with the house and trees in the background. The hydrant is positioned to the left of the house, and the grass is in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.56, "peak": 126.84, "min": 30.75}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.18, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.18, "energy_joules_est": 61.04, "sample_count": 21, "duration_seconds": 2.166}, "timestamp": "2026-01-19T13:06:26.832935"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1456.557, "latencies_ms": [1456.557], "images_per_second": 0.687, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A red fire hydrant is in the grass in front of a house.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.62, "peak": 127.9, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.68, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.68, "energy_joules_est": 44.7, "sample_count": 15, "duration_seconds": 1.457}, "timestamp": "2026-01-19T13:06:28.387587"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1615.919, "latencies_ms": [1615.919], "images_per_second": 0.619, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The fire hydrant is red and black, and it is in a grassy area with dandelions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.74, "peak": 123.26, "min": 28.89}, "VIN_SYS_5V0": {"avg": 13.98, "peak": 15.54, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 30.88, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.88, "energy_joules_est": 49.91, "sample_count": 16, "duration_seconds": 1.616}, "timestamp": "2026-01-19T13:06:30.047413"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1591.327, "latencies_ms": [1591.327], "images_per_second": 0.628, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A bird is flying over a roof with a blue hue, while other birds are on the roof.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.44, "peak": 122.58, "min": 28.24}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.2, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.2, "energy_joules_est": 49.66, "sample_count": 16, "duration_seconds": 1.592}, "timestamp": "2026-01-19T13:06:31.730018"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2545.853, "latencies_ms": [2545.853], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bird: 1\n2. roof: 1\n3. bird: 1\n4. roof: 1\n5. bird: 1\n6. roof: 1\n7. bird: 1\n8. roof: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.49, "peak": 126.92, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.3, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 27.3, "energy_joules_est": 69.51, "sample_count": 25, "duration_seconds": 2.546}, "timestamp": "2026-01-19T13:06:34.336365"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2201.752, "latencies_ms": [2201.752], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The bird is flying towards the right side of the image, while the ducks are located in the background, near the edge of the roof. The bird is in the foreground, closer to the camera than the ducks.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.67, "peak": 120.03, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 27.9, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.9, "energy_joules_est": 61.44, "sample_count": 22, "duration_seconds": 2.202}, "timestamp": "2026-01-19T13:06:36.613742"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1629.811, "latencies_ms": [1629.811], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A bird is flying over a roof with a blue hue, while other birds are on the ground nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.9, "peak": 124.95, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.51, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 30.51, "energy_joules_est": 49.74, "sample_count": 16, "duration_seconds": 1.63}, "timestamp": "2026-01-19T13:06:38.285271"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2411.445, "latencies_ms": [2411.445], "images_per_second": 0.415, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a bird perched on a weathered wooden roof, with a vibrant blue sky in the background. The bird's feathers are a mix of black and white, and it appears to be in motion, possibly in the process of landing or taking off.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.23, "peak": 126.3, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.89, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.89, "energy_joules_est": 67.27, "sample_count": 24, "duration_seconds": 2.412}, "timestamp": "2026-01-19T13:06:40.767622"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1729.325, "latencies_ms": [1729.325], "images_per_second": 0.578, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, a woman is walking a horse in a barn, with a red door and a window visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.92, "peak": 126.02, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.97, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.97, "energy_joules_est": 51.84, "sample_count": 17, "duration_seconds": 1.73}, "timestamp": "2026-01-19T13:06:42.551954"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2592.48, "latencies_ms": [2592.48], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. horse: 1\n2. person: 1\n3. door: 1\n4. window: 1\n5. ladder: 1\n6. bucket: 1\n7. stool: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.17, "peak": 127.3, "min": 29.97}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.12, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.12, "energy_joules_est": 70.32, "sample_count": 25, "duration_seconds": 2.593}, "timestamp": "2026-01-19T13:06:45.163396"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1982.75, "latencies_ms": [1982.75], "images_per_second": 0.504, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The horse is positioned in the foreground, with the woman walking towards it. The barn is located in the background, with the horse and woman positioned in the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.17, "peak": 128.43, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.96, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 28.96, "energy_joules_est": 57.43, "sample_count": 20, "duration_seconds": 1.983}, "timestamp": "2026-01-19T13:06:47.241499"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1322.27, "latencies_ms": [1322.27], "images_per_second": 0.756, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A woman is walking a horse in a barn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.29, "peak": 125.87, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.97, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.97, "energy_joules_est": 42.29, "sample_count": 13, "duration_seconds": 1.323}, "timestamp": "2026-01-19T13:06:48.596542"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1745.654, "latencies_ms": [1745.654], "images_per_second": 0.573, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image is taken in a stable with a wooden interior, the horse is brown and white, and the woman is wearing blue jeans.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.09, "peak": 124.43, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.1, "peak": 40.97, "min": 22.07}}, "power_watts_avg": 32.1, "energy_joules_est": 56.05, "sample_count": 17, "duration_seconds": 1.746}, "timestamp": "2026-01-19T13:06:50.366578"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1883.116, "latencies_ms": [1883.116], "images_per_second": 0.531, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, a group of zebras and a group of sheep are grazing on a grassy field, with a tree and a pond visible in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.94, "peak": 113.79, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.75, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 29.75, "energy_joules_est": 56.03, "sample_count": 19, "duration_seconds": 1.883}, "timestamp": "2026-01-19T13:06:52.352309"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2608.338, "latencies_ms": [2608.338], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. tree: 1\n2. zebras: 2\n3. rhinoceros: 1\n4. rocks: 1\n5. grass: 1\n6. water: 1\n7. trees: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.73, "peak": 121.14, "min": 28.07}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.58, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.58, "energy_joules_est": 69.34, "sample_count": 26, "duration_seconds": 2.609}, "timestamp": "2026-01-19T13:06:55.049970"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2261.923, "latencies_ms": [2261.923], "images_per_second": 0.442, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground, grazing on the grass, while the sheep are in the background, grazing on the grass as well. The trees are located in the background, providing a natural backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.66, "peak": 118.84, "min": 31.08}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.8, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.8, "energy_joules_est": 62.89, "sample_count": 22, "duration_seconds": 2.262}, "timestamp": "2026-01-19T13:06:57.340134"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1685.621, "latencies_ms": [1685.621], "images_per_second": 0.593, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " In this image, we can see a herd of animals grazing in a grassy field. There are trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.64, "peak": 117.8, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.27, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 30.27, "energy_joules_est": 51.03, "sample_count": 17, "duration_seconds": 1.686}, "timestamp": "2026-01-19T13:06:59.114672"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2329.978, "latencies_ms": [2329.978], "images_per_second": 0.429, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a vibrant green field with a variety of animals, including zebras, rhinos, and antelopes. The sky is a clear blue with a few clouds, and the trees in the background are tall and green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.22, "peak": 122.33, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.68, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.68, "energy_joules_est": 64.51, "sample_count": 23, "duration_seconds": 2.331}, "timestamp": "2026-01-19T13:07:01.501067"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1918.486, "latencies_ms": [1918.486], "images_per_second": 0.521, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A horse-drawn trolley with a green and red body and gold accents is being pulled by a white horse in a park with trees and people in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.1, "peak": 127.28, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.11, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.11, "energy_joules_est": 55.87, "sample_count": 19, "duration_seconds": 1.919}, "timestamp": "2026-01-19T13:07:03.490997"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1986.159, "latencies_ms": [1986.159], "images_per_second": 0.503, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " trolley: 1, horse: 1, umbrella: 1, bench: 1, people: 1, trees: 1, sign: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.96, "peak": 129.83, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.92, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 28.92, "energy_joules_est": 57.46, "sample_count": 20, "duration_seconds": 1.987}, "timestamp": "2026-01-19T13:07:05.566133"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2227.746, "latencies_ms": [2227.746], "images_per_second": 0.449, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The horse-drawn trolley is positioned in the foreground, with the crowd of people in the background. The trolley is moving towards the right side of the image, while the people are standing on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.27, "peak": 125.23, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.94, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 27.94, "energy_joules_est": 62.25, "sample_count": 22, "duration_seconds": 2.228}, "timestamp": "2026-01-19T13:07:07.856424"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1582.683, "latencies_ms": [1582.683], "images_per_second": 0.632, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A horse drawn trolley is being pulled by a white horse down a street in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.89, "peak": 118.57, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.56, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 30.56, "energy_joules_est": 48.38, "sample_count": 16, "duration_seconds": 1.583}, "timestamp": "2026-01-19T13:07:09.519042"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2608.832, "latencies_ms": [2608.832], "images_per_second": 0.383, "prompt_tokens": 1109, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a vibrant scene with a horse-drawn trolley in the foreground, with a clear blue sky and lush green trees in the background. The trolley is adorned with a green and gold color scheme, and the horse is white with a black mane and tail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.32, "peak": 113.29, "min": 27.87}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.69, "peak": 40.57, "min": 16.56}}, "power_watts_avg": 26.69, "energy_joules_est": 69.65, "sample_count": 26, "duration_seconds": 2.609}, "timestamp": "2026-01-19T13:07:12.233843"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1993.015, "latencies_ms": [1993.015], "images_per_second": 0.502, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, there are several people sitting on benches in an outdoor area, with one person reading a newspaper, and a few others standing and talking.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.75, "peak": 118.24, "min": 28.26}, "VIN_SYS_5V0": {"avg": 14.05, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 27.99, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 27.99, "energy_joules_est": 55.81, "sample_count": 20, "duration_seconds": 1.994}, "timestamp": "2026-01-19T13:07:14.324598"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2529.319, "latencies_ms": [2529.319], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 2\n2. bench: 4\n3. man: 1\n4. man: 1\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.14, "peak": 125.28, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.24, "min": 10.84}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 11.82}, "VDD_GPU": {"avg": 26.57, "peak": 39.0, "min": 14.19}}, "power_watts_avg": 26.57, "energy_joules_est": 67.21, "sample_count": 25, "duration_seconds": 2.53}, "timestamp": "2026-01-19T13:07:16.928087"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2589.983, "latencies_ms": [2589.983], "images_per_second": 0.386, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The man on the left is sitting on a green bench, while the man on the right is sitting on a red bench. The man on the left is closer to the camera than the man on the right. The man on the left is sitting in front of the man on the right.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.7, "peak": 120.14, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.54, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.89, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.89, "energy_joules_est": 69.65, "sample_count": 25, "duration_seconds": 2.59}, "timestamp": "2026-01-19T13:07:19.533112"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2148.777, "latencies_ms": [2148.777], "images_per_second": 0.465, "prompt_tokens": 1111, "response_tokens_est": 42, "n_tiles": 1, "output_text": " In a busy city square, a group of people are enjoying a sunny day on a bench. The man in the foreground is reading a newspaper, while the man in the background is talking on his cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.14, "peak": 129.31, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.61, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.61, "energy_joules_est": 61.48, "sample_count": 21, "duration_seconds": 2.149}, "timestamp": "2026-01-19T13:07:21.705326"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2597.543, "latencies_ms": [2597.543], "images_per_second": 0.385, "prompt_tokens": 1109, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The image depicts a group of people sitting on green metal benches in an outdoor setting, with the sun shining brightly and casting shadows on the ground. The colors in the image are vibrant, with the green of the benches contrasting against the gray of the pavement and the red of the metal railing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.64, "peak": 133.06, "min": 27.91}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.84, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.84, "energy_joules_est": 69.72, "sample_count": 26, "duration_seconds": 2.598}, "timestamp": "2026-01-19T13:07:24.406301"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1531.805, "latencies_ms": [1531.805], "images_per_second": 0.653, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A desk with a laptop, a lamp, and a glass of orange juice on it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.85, "peak": 131.61, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.71, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 30.71, "energy_joules_est": 47.07, "sample_count": 15, "duration_seconds": 1.533}, "timestamp": "2026-01-19T13:07:25.976426"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2185.96, "latencies_ms": [2185.96], "images_per_second": 0.457, "prompt_tokens": 1113, "response_tokens_est": 43, "n_tiles": 1, "output_text": " 1. black laptop\n2. black lamp\n3. glass of orange juice\n4. white telephone\n5. black trash can\n6. black drawer\n7. black book\n8. black pen", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.57, "peak": 131.47, "min": 38.45}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.25, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 29.25, "energy_joules_est": 63.95, "sample_count": 21, "duration_seconds": 2.186}, "timestamp": "2026-01-19T13:07:28.172979"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2408.055, "latencies_ms": [2408.055], "images_per_second": 0.415, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The lamp is positioned to the right of the laptop, and the glass of orange juice is placed on the desk in front of the laptop. The telephone is located to the left of the laptop, and the framed picture is hanging on the wall above the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.67, "peak": 108.32, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.53, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 27.53, "energy_joules_est": 66.3, "sample_count": 24, "duration_seconds": 2.408}, "timestamp": "2026-01-19T13:07:30.665215"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1414.015, "latencies_ms": [1414.015], "images_per_second": 0.707, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A desk with a laptop, lamp, and phone on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.13, "peak": 122.76, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.32, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 31.32, "energy_joules_est": 44.3, "sample_count": 14, "duration_seconds": 1.414}, "timestamp": "2026-01-19T13:07:32.114896"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1571.327, "latencies_ms": [1571.327], "images_per_second": 0.636, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is well-lit with a warm yellow light, and the desk is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.77, "peak": 112.87, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 32.11, "peak": 40.57, "min": 20.89}}, "power_watts_avg": 32.11, "energy_joules_est": 50.47, "sample_count": 16, "duration_seconds": 1.572}, "timestamp": "2026-01-19T13:07:33.788583"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2612.631, "latencies_ms": [2612.631], "images_per_second": 0.383, "prompt_tokens": 1432, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a serene beach scene with a blue and white striped towel, a blue surfboard, and a red cooler on the sand, with a person in the water and a beach umbrella in the background.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.92, "peak": 121.42, "min": 27.39}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 16.15, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 12.21}, "VDD_GPU": {"avg": 28.71, "peak": 41.74, "min": 17.35}}, "power_watts_avg": 28.71, "energy_joules_est": 75.02, "sample_count": 26, "duration_seconds": 2.613}, "timestamp": "2026-01-19T13:07:36.503201"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2405.409, "latencies_ms": [2405.409], "images_per_second": 0.416, "prompt_tokens": 1446, "response_tokens_est": 37, "n_tiles": 1, "output_text": " surfboard: 2, beach chair: 1, umbrella: 1, towel: 1, bag: 1, cooler: 1, beach umbrella: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.95, "peak": 133.86, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 16.05, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.81, "peak": 41.36, "min": 15.38}}, "power_watts_avg": 29.81, "energy_joules_est": 71.72, "sample_count": 24, "duration_seconds": 2.406}, "timestamp": "2026-01-19T13:07:39.005951"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3146.344, "latencies_ms": [3146.344], "images_per_second": 0.318, "prompt_tokens": 1450, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The blue and white striped towel is to the left of the blue surfboard, which is in front of the red cooler. The pink and white surfboard is to the right of the blue surfboard, and the umbrella is behind the chairs. The beachgoers are in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.17, "peak": 125.55, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 16.15, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 27.77, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 27.77, "energy_joules_est": 87.38, "sample_count": 31, "duration_seconds": 3.147}, "timestamp": "2026-01-19T13:07:42.228111"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2922.186, "latencies_ms": [2922.186], "images_per_second": 0.342, "prompt_tokens": 1444, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image captures a serene beach scene with a clear blue sky and calm ocean waves. The beach is adorned with various beach items, including a blue and white striped towel, a blue surfboard, and a red cooler. A person can be seen enjoying the water in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.48, "peak": 126.29, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.26, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.15, "peak": 41.76, "min": 15.38}}, "power_watts_avg": 28.15, "energy_joules_est": 82.27, "sample_count": 29, "duration_seconds": 2.923}, "timestamp": "2026-01-19T13:07:45.243491"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1710.812, "latencies_ms": [1710.812], "images_per_second": 0.585, "prompt_tokens": 1442, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The beach is covered in sand and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.38, "peak": 133.35, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.93, "peak": 41.36, "min": 15.38}}, "power_watts_avg": 32.93, "energy_joules_est": 56.35, "sample_count": 17, "duration_seconds": 1.711}, "timestamp": "2026-01-19T13:07:47.017509"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1610.748, "latencies_ms": [1610.748], "images_per_second": 0.621, "prompt_tokens": 1100, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A sheep with a black face and white wool is standing on a rocky cliff, looking down at the ground.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.82, "peak": 124.09, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.94, "peak": 40.57, "min": 20.89}}, "power_watts_avg": 31.94, "energy_joules_est": 51.46, "sample_count": 16, "duration_seconds": 1.611}, "timestamp": "2026-01-19T13:07:48.689744"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2676.838, "latencies_ms": [2676.838], "images_per_second": 0.374, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. sheep: 1\n2. rock: 1\n3. grass: 1\n4. cloud: 1\n5. sky: 1\n6. sheep's wool: 1\n7. sheep's head: 1\n8. sheep's legs: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.36, "peak": 115.92, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.03, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 27.03, "energy_joules_est": 72.37, "sample_count": 26, "duration_seconds": 2.677}, "timestamp": "2026-01-19T13:07:51.391777"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2477.228, "latencies_ms": [2477.228], "images_per_second": 0.404, "prompt_tokens": 1118, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The sheep is positioned on the left side of the image, with the sky occupying the majority of the background. The foreground features a rocky terrain with patches of green grass, while the sheep is standing on a higher elevation, providing a sense of depth and perspective to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.46, "peak": 105.55, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.33, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.33, "energy_joules_est": 67.71, "sample_count": 24, "duration_seconds": 2.477}, "timestamp": "2026-01-19T13:07:53.882685"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1615.627, "latencies_ms": [1615.627], "images_per_second": 0.619, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A sheep with a black face and white wool is standing on a rocky cliff, looking down at the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25990.6, "ram_available_mb": 99781.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.11, "peak": 118.34, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 30.33, "peak": 39.39, "min": 17.34}}, "power_watts_avg": 30.33, "energy_joules_est": 49.02, "sample_count": 16, "duration_seconds": 1.616}, "timestamp": "2026-01-19T13:07:55.548186"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1759.298, "latencies_ms": [1759.298], "images_per_second": 0.568, "prompt_tokens": 1110, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The sheep is white with a black face and legs, standing on a rocky cliff with a blue sky and white clouds in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.17, "peak": 117.8, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.57, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 30.57, "energy_joules_est": 53.8, "sample_count": 17, "duration_seconds": 1.76}, "timestamp": "2026-01-19T13:07:57.319976"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1412.046, "latencies_ms": [1412.046], "images_per_second": 0.708, "prompt_tokens": 1100, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person with blue hair is taking a selfie in a mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.26, "peak": 126.3, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.22, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 32.22, "energy_joules_est": 45.51, "sample_count": 14, "duration_seconds": 1.413}, "timestamp": "2026-01-19T13:07:58.791127"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2602.562, "latencies_ms": [2602.562], "images_per_second": 0.384, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. phone: 1\n3. shirt: 1\n4. tie: 1\n5. wall: 1\n6. mirror: 1\n7. ring: 1\n8. text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.78, "peak": 123.07, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.48, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 27.48, "energy_joules_est": 71.53, "sample_count": 25, "duration_seconds": 2.603}, "timestamp": "2026-01-19T13:08:01.404212"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1971.58, "latencies_ms": [1971.58], "images_per_second": 0.507, "prompt_tokens": 1118, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The person is in the foreground of the image, taking a selfie with a smartphone. The smartphone is held in their right hand, and the mirror is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.39, "peak": 126.57, "min": 34.9}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.2, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.2, "energy_joules_est": 57.58, "sample_count": 19, "duration_seconds": 1.972}, "timestamp": "2026-01-19T13:08:03.385301"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1444.085, "latencies_ms": [1444.085], "images_per_second": 0.692, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person with blue hair is taking a selfie in a bathroom mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.78, "peak": 125.18, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.51, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 31.51, "energy_joules_est": 45.52, "sample_count": 14, "duration_seconds": 1.445}, "timestamp": "2026-01-19T13:08:04.851996"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2182.054, "latencies_ms": [2182.054], "images_per_second": 0.458, "prompt_tokens": 1110, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image is a selfie taken in a bathroom with a plain wall in the background. The lighting is natural, coming from a window out of frame. The subject is wearing a blue shirt and a black tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.54, "peak": 113.87, "min": 31.78}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.21, "peak": 40.18, "min": 20.1}}, "power_watts_avg": 29.21, "energy_joules_est": 63.75, "sample_count": 21, "duration_seconds": 2.183}, "timestamp": "2026-01-19T13:08:07.040239"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1916.532, "latencies_ms": [1916.532], "images_per_second": 0.522, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image depicts a room with a fireplace, a wooden chair, a table, and a bookshelf, all arranged in a way that suggests a study or library.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.54, "peak": 125.19, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.4, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.4, "energy_joules_est": 56.37, "sample_count": 19, "duration_seconds": 1.917}, "timestamp": "2026-01-19T13:08:09.033053"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2581.111, "latencies_ms": [2581.111], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. fireplace: 1\n2. chair: 2\n3. table: 1\n4. bookshelf: 1\n5. painting: 2\n6. vase: 2\n7. lamp: 1\n8. rug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.38, "peak": 125.44, "min": 31.07}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.04, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.04, "energy_joules_est": 69.8, "sample_count": 25, "duration_seconds": 2.582}, "timestamp": "2026-01-19T13:08:11.643704"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2150.146, "latencies_ms": [2150.146], "images_per_second": 0.465, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The fireplace is located on the left side of the room, with the wooden chair positioned in the foreground. The wooden table is situated in the middle of the room, with the bookshelf in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.47, "peak": 127.94, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.35, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.35, "energy_joules_est": 60.97, "sample_count": 21, "duration_seconds": 2.151}, "timestamp": "2026-01-19T13:08:13.841483"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2019.059, "latencies_ms": [2019.059], "images_per_second": 0.495, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image depicts a room with a fireplace, a wooden chair, and a table. The room is decorated with various items, including vases, a painting, and a book.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.0, "peak": 107.1, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.4}, "VDD_GPU": {"avg": 28.88, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.88, "energy_joules_est": 58.32, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T13:08:15.927052"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1965.197, "latencies_ms": [1965.197], "images_per_second": 0.509, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the walls are painted white. The furniture is made of wood, and the floor is covered with a patterned carpet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.4, "peak": 122.62, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.34, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 29.34, "energy_joules_est": 57.67, "sample_count": 19, "duration_seconds": 1.966}, "timestamp": "2026-01-19T13:08:17.908203"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1841.856, "latencies_ms": [1841.856], "images_per_second": 0.543, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A black dog is jumping in the air to catch a red frisbee.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.63, "peak": 130.6, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.85, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 32.35, "peak": 41.76, "min": 17.74}}, "power_watts_avg": 32.35, "energy_joules_est": 59.61, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T13:08:19.797643"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2267.732, "latencies_ms": [2267.732], "images_per_second": 0.441, "prompt_tokens": 1446, "response_tokens_est": 31, "n_tiles": 1, "output_text": " dog: 1, frisbee: 1, car: 2, tree: 1, grass: 1, mulch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.48, "peak": 122.88, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 31.61, "peak": 42.13, "min": 19.71}}, "power_watts_avg": 31.61, "energy_joules_est": 71.69, "sample_count": 22, "duration_seconds": 2.268}, "timestamp": "2026-01-19T13:08:22.086020"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2596.748, "latencies_ms": [2596.748], "images_per_second": 0.385, "prompt_tokens": 1450, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The red frisbee is in the air, slightly to the left of the black dog, which is in the foreground of the image. The black car is in the background, parked on the street behind the dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.38, "peak": 123.68, "min": 27.63}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 29.64, "peak": 42.13, "min": 17.73}}, "power_watts_avg": 29.64, "energy_joules_est": 76.98, "sample_count": 26, "duration_seconds": 2.597}, "timestamp": "2026-01-19T13:08:24.781056"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1947.951, "latencies_ms": [1947.951], "images_per_second": 0.513, "prompt_tokens": 1444, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A black dog is jumping in the air to catch a red frisbee in a grassy yard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 80.46, "peak": 120.17, "min": 27.59}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.26, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.02, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 32.02, "energy_joules_est": 62.39, "sample_count": 19, "duration_seconds": 1.948}, "timestamp": "2026-01-19T13:08:26.769752"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3231.325, "latencies_ms": [3231.325], "images_per_second": 0.309, "prompt_tokens": 1442, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image is a vibrant depiction of a black dog in mid-air, leaping to catch a red frisbee. The scene is bathed in natural daylight, with the lush green grass and trees providing a serene backdrop. The dog's fur is a rich, dark brown, contrasting with the bright red of the frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.18, "peak": 132.25, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 28.01, "peak": 42.15, "min": 17.74}}, "power_watts_avg": 28.01, "energy_joules_est": 90.52, "sample_count": 32, "duration_seconds": 3.232}, "timestamp": "2026-01-19T13:08:30.086866"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1897.238, "latencies_ms": [1897.238], "images_per_second": 0.527, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, a giraffe with a brown and white coat stands in a lush green forest, its head turned towards the camera, its eyes wide and curious.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.42, "peak": 117.55, "min": 28.19}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.03, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.03, "energy_joules_est": 55.09, "sample_count": 19, "duration_seconds": 1.898}, "timestamp": "2026-01-19T13:08:32.067952"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2797.093, "latencies_ms": [2797.093], "images_per_second": 0.358, "prompt_tokens": 1113, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. giraffe head: 1\n2. giraffe neck: 1\n3. giraffe ears: 2\n4. giraffe eyes: 2\n5. giraffe mouth: 1\n6. giraffe tongue: 1\n7. giraffe mane: 1\n8. giraffe horns: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.9, "peak": 106.83, "min": 34.77}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.43, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.43, "energy_joules_est": 73.94, "sample_count": 27, "duration_seconds": 2.798}, "timestamp": "2026-01-19T13:08:34.872237"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1919.402, "latencies_ms": [1919.402], "images_per_second": 0.521, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The giraffe is in the foreground, with its head and neck prominently displayed. The background is filled with green foliage, suggesting that the giraffe is in a natural habitat.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.63, "peak": 125.87, "min": 28.15}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.28, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.28, "energy_joules_est": 56.21, "sample_count": 19, "duration_seconds": 1.92}, "timestamp": "2026-01-19T13:08:36.863393"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2419.344, "latencies_ms": [2419.344], "images_per_second": 0.413, "prompt_tokens": 1111, "response_tokens_est": 51, "n_tiles": 1, "output_text": " In the image, a giraffe is standing in a lush green forest, its head and neck prominently displayed as it gazes directly at the camera. The giraffe's brown and white spotted coat stands out against the verdant backdrop of the trees and foliage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.26, "peak": 120.67, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.3, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 27.3, "energy_joules_est": 66.06, "sample_count": 24, "duration_seconds": 2.42}, "timestamp": "2026-01-19T13:08:39.362928"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1577.31, "latencies_ms": [1577.31], "images_per_second": 0.634, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The giraffe is brown and white with a black nose, and the background is green with trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.82, "peak": 122.1, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.24, "energy_joules_est": 47.72, "sample_count": 16, "duration_seconds": 1.578}, "timestamp": "2026-01-19T13:08:41.034962"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2085.525, "latencies_ms": [2085.525], "images_per_second": 0.479, "prompt_tokens": 1100, "response_tokens_est": 39, "n_tiles": 1, "output_text": " In the image, there are two zebras standing side by side, their black and white striped bodies facing away from the camera, with a chain link fence and a rocky ground in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.35, "peak": 105.86, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.54, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 28.54, "energy_joules_est": 59.53, "sample_count": 21, "duration_seconds": 2.086}, "timestamp": "2026-01-19T13:08:43.226137"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1172.88, "latencies_ms": [1172.88], "images_per_second": 0.853, "prompt_tokens": 1114, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 1.9, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.42, "peak": 123.41, "min": 29.74}, "VIN_SYS_5V0": {"avg": 13.94, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.81, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 31.81, "energy_joules_est": 37.32, "sample_count": 12, "duration_seconds": 1.173}, "timestamp": "2026-01-19T13:08:44.485462"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2293.559, "latencies_ms": [2293.559], "images_per_second": 0.436, "prompt_tokens": 1118, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The zebras are positioned on the left side of the image, with the fence serving as a boundary between them and the background. The zebras are in the foreground, with the fence and the background providing depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.33, "peak": 115.73, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.54, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.21}, "VDD_GPU": {"avg": 28.92, "peak": 41.36, "min": 17.74}}, "power_watts_avg": 28.92, "energy_joules_est": 66.34, "sample_count": 23, "duration_seconds": 2.294}, "timestamp": "2026-01-19T13:08:46.868723"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1684.863, "latencies_ms": [1684.863], "images_per_second": 0.594, "prompt_tokens": 1112, "response_tokens_est": 23, "n_tiles": 1, "output_text": " Two zebras are standing in a fenced enclosure, their black and white stripes contrasting against the brown dirt ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.61, "peak": 128.83, "min": 27.0}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.13, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.13, "energy_joules_est": 49.09, "sample_count": 17, "duration_seconds": 1.685}, "timestamp": "2026-01-19T13:08:48.641024"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2518.479, "latencies_ms": [2518.479], "images_per_second": 0.397, "prompt_tokens": 1110, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image features two zebras standing side by side in a fenced enclosure, with their distinctive black and white stripes clearly visible. The lighting is natural, suggesting daytime, and the zebras are standing on a patch of grass, with a fence and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.01, "peak": 129.68, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.92, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.92, "energy_joules_est": 67.81, "sample_count": 25, "duration_seconds": 2.519}, "timestamp": "2026-01-19T13:08:51.235008"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2502.948, "latencies_ms": [2502.948], "images_per_second": 0.4, "prompt_tokens": 1099, "response_tokens_est": 54, "n_tiles": 1, "output_text": " In the image, a silver sports car is parked on the right side of a narrow, tree-lined street, while a group of horses, including a brown and white one, are standing on the left side of the street, seemingly grazing or interacting with each other.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.13, "peak": 127.36, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.7, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.7, "energy_joules_est": 66.84, "sample_count": 25, "duration_seconds": 2.503}, "timestamp": "2026-01-19T13:08:53.839761"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2634.773, "latencies_ms": [2634.773], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. horse: 4\n2. horse: 3\n3. horse: 2\n4. horse: 1\n5. horse: 1\n6. horse: 1\n7. horse: 1\n8. horse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.83, "peak": 128.23, "min": 29.97}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.44, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.23, "peak": 39.0, "min": 14.98}}, "power_watts_avg": 26.23, "energy_joules_est": 69.12, "sample_count": 26, "duration_seconds": 2.635}, "timestamp": "2026-01-19T13:08:56.536061"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2506.762, "latencies_ms": [2506.762], "images_per_second": 0.399, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The horses are positioned in the middle of the road, with the car parked on the right side of the street. The car is relatively close to the camera, while the horses are farther away. The horses are near the car, but not directly in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.17, "peak": 126.44, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.71, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.71, "energy_joules_est": 66.96, "sample_count": 25, "duration_seconds": 2.507}, "timestamp": "2026-01-19T13:08:59.136909"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2684.094, "latencies_ms": [2684.094], "images_per_second": 0.373, "prompt_tokens": 1111, "response_tokens_est": 58, "n_tiles": 1, "output_text": " In a serene setting, a group of horses, their coats a mix of brown and black, are seen grazing on the lush grass along a tree-lined road. A silver car is parked on the side of the road, its presence adding a touch of modernity to the otherwise tranquil scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.09, "peak": 123.18, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.28, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.28, "energy_joules_est": 70.55, "sample_count": 26, "duration_seconds": 2.685}, "timestamp": "2026-01-19T13:09:01.838021"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2472.785, "latencies_ms": [2472.785], "images_per_second": 0.404, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a silver car parked on the side of a road, with a group of horses standing on the road. The horses are brown and black, and the car is parked next to a wooden fence. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.36, "peak": 126.11, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.23, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.23, "energy_joules_est": 67.35, "sample_count": 24, "duration_seconds": 2.473}, "timestamp": "2026-01-19T13:09:04.345463"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1868.835, "latencies_ms": [1868.835], "images_per_second": 0.535, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " In the image, there is a wooden desk with a blackboard behind it, a bell on the desk, and a chair in front of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.37, "peak": 118.19, "min": 34.81}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.77, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.77, "energy_joules_est": 55.65, "sample_count": 18, "duration_seconds": 1.869}, "timestamp": "2026-01-19T13:09:06.223555"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2635.784, "latencies_ms": [2635.784], "images_per_second": 0.379, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. wooden desk: 1\n2. bookshelf: 1\n3. chair: 1\n4. blackboard: 1\n5. bell: 1\n6. picture frame: 1\n7. wall: 1\n8. door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.11, "peak": 126.16, "min": 27.85}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 27.0, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.0, "energy_joules_est": 71.17, "sample_count": 26, "duration_seconds": 2.636}, "timestamp": "2026-01-19T13:09:08.940851"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2135.39, "latencies_ms": [2135.39], "images_per_second": 0.468, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The desk is located in the foreground of the image, with the chalkboard and bookshelf in the background. The bell is positioned near the desk, while the chair is situated further back in the room.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.51, "peak": 126.99, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 28.18, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.18, "energy_joules_est": 60.19, "sample_count": 21, "duration_seconds": 2.136}, "timestamp": "2026-01-19T13:09:11.114095"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1649.491, "latencies_ms": [1649.491], "images_per_second": 0.606, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A classroom with a blackboard, books, and a bell sits in a room with a picture on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.48, "peak": 122.04, "min": 30.32}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.95, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 30.95, "energy_joules_est": 51.06, "sample_count": 16, "duration_seconds": 1.65}, "timestamp": "2026-01-19T13:09:12.780064"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1698.564, "latencies_ms": [1698.564], "images_per_second": 0.589, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming from a window, and the walls are painted a light brown color.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.25, "peak": 106.44, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.01, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 31.01, "energy_joules_est": 52.68, "sample_count": 17, "duration_seconds": 1.699}, "timestamp": "2026-01-19T13:09:14.548346"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1552.675, "latencies_ms": [1552.675], "images_per_second": 0.644, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A yellow and white bus with a red and white license plate is driving down a busy street.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.59, "peak": 126.24, "min": 30.61}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.44, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.52, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.52, "energy_joules_est": 48.95, "sample_count": 15, "duration_seconds": 1.553}, "timestamp": "2026-01-19T13:09:16.120240"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2668.405, "latencies_ms": [2668.405], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. bus: 1\n2. white van: 1\n3. motor bike: 1\n4. road: 1\n5. white line: 1\n6. white sign: 1\n7. white text: 1\n8. red text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.79, "peak": 121.61, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.53, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 27.53, "energy_joules_est": 73.47, "sample_count": 26, "duration_seconds": 2.669}, "timestamp": "2026-01-19T13:09:18.807899"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2221.502, "latencies_ms": [2221.502], "images_per_second": 0.45, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The bus is in the foreground, parked on the side of the road, with a white van in the background. The bus is parked on the left side of the road, while the white van is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.35, "peak": 128.43, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.12, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 28.12, "energy_joules_est": 62.48, "sample_count": 22, "duration_seconds": 2.222}, "timestamp": "2026-01-19T13:09:21.094566"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1458.762, "latencies_ms": [1458.762], "images_per_second": 0.686, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A yellow bus is driving down a busy street with a white van behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.31, "peak": 107.26, "min": 42.62}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.82, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.82, "energy_joules_est": 46.43, "sample_count": 14, "duration_seconds": 1.459}, "timestamp": "2026-01-19T13:09:22.558197"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2182.75, "latencies_ms": [2182.75], "images_per_second": 0.458, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image shows a yellow and white bus with red and green writing on the back, parked on a street with a white van behind it. The sky is overcast, and the street is wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.0, "peak": 120.72, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.33, "peak": 40.95, "min": 18.13}}, "power_watts_avg": 29.33, "energy_joules_est": 64.03, "sample_count": 22, "duration_seconds": 2.183}, "timestamp": "2026-01-19T13:09:24.834800"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1838.425, "latencies_ms": [1838.425], "images_per_second": 0.544, "prompt_tokens": 1100, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image depicts a bathroom with a large mirror reflecting a television screen showing a football game, two sinks with faucets, and a trash can.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.38, "peak": 129.28, "min": 27.99}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.2, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.2, "energy_joules_est": 53.7, "sample_count": 18, "duration_seconds": 1.839}, "timestamp": "2026-01-19T13:09:26.713949"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2573.046, "latencies_ms": [2573.046], "images_per_second": 0.389, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. television: 1\n2. sink: 2\n3. mirror: 1\n4. door: 1\n5. trash can: 1\n6. countertop: 1\n7. wall: 1\n8. tile: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.0, "peak": 116.9, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.19, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.19, "energy_joules_est": 69.97, "sample_count": 25, "duration_seconds": 2.573}, "timestamp": "2026-01-19T13:09:29.302093"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2118.157, "latencies_ms": [2118.157], "images_per_second": 0.472, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The television is positioned above the sinks, which are located on the right side of the mirror. The trash can is situated near the television, and the sinks are positioned on the left side of the mirror.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.27, "peak": 127.42, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.4, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.4, "energy_joules_est": 60.17, "sample_count": 21, "duration_seconds": 2.119}, "timestamp": "2026-01-19T13:09:31.490793"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2145.144, "latencies_ms": [2145.144], "images_per_second": 0.466, "prompt_tokens": 1112, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image captures a bathroom scene with a large mirror reflecting a television screen showing a football game. The bathroom features two sinks, one on each side of the mirror, and a trash can in the corner.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.78, "peak": 119.05, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.84, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 27.84, "energy_joules_est": 59.74, "sample_count": 21, "duration_seconds": 2.146}, "timestamp": "2026-01-19T13:09:33.664231"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1761.219, "latencies_ms": [1761.219], "images_per_second": 0.568, "prompt_tokens": 1110, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The bathroom has a beige tiled wall and a brown granite countertop. The mirror reflects a television screen showing a football game.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.2, "peak": 118.91, "min": 35.23}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.24, "energy_joules_est": 53.27, "sample_count": 17, "duration_seconds": 1.762}, "timestamp": "2026-01-19T13:09:35.433946"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1838.128, "latencies_ms": [1838.128], "images_per_second": 0.544, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man sits on a bench in a park with a church in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.05, "peak": 134.3, "min": 30.65}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 32.59, "peak": 41.74, "min": 18.52}}, "power_watts_avg": 32.59, "energy_joules_est": 59.93, "sample_count": 18, "duration_seconds": 1.839}, "timestamp": "2026-01-19T13:09:37.319460"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2977.52, "latencies_ms": [2977.52], "images_per_second": 0.336, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. man: 1\n2. bench: 1\n3. streetlamp: 1\n4. tree: 2\n5. building: 1\n6. clock: 1\n7. sky: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.94, "peak": 132.01, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.36, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 28.86, "peak": 42.13, "min": 19.31}}, "power_watts_avg": 28.86, "energy_joules_est": 85.94, "sample_count": 29, "duration_seconds": 2.978}, "timestamp": "2026-01-19T13:09:40.326769"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2311.981, "latencies_ms": [2311.981], "images_per_second": 0.433, "prompt_tokens": 1450, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The man is sitting on the bench in the foreground, with the church in the background. The church is located behind the man, and there are trees in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.51, "peak": 118.68, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 30.52, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 30.52, "energy_joules_est": 70.58, "sample_count": 23, "duration_seconds": 2.313}, "timestamp": "2026-01-19T13:09:42.727076"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1856.487, "latencies_ms": [1856.487], "images_per_second": 0.539, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man sits on a bench in a park with a church in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.17, "peak": 130.92, "min": 27.21}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 16.36, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.63, "peak": 42.15, "min": 16.16}}, "power_watts_avg": 32.63, "energy_joules_est": 60.6, "sample_count": 18, "duration_seconds": 1.857}, "timestamp": "2026-01-19T13:09:44.603833"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2408.839, "latencies_ms": [2408.839], "images_per_second": 0.415, "prompt_tokens": 1442, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is in black and white, with a clear contrast between the man and the background. The sky is cloudy, and the man is sitting on a bench in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.34, "peak": 132.37, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 30.71, "peak": 42.15, "min": 18.52}}, "power_watts_avg": 30.71, "energy_joules_est": 73.99, "sample_count": 24, "duration_seconds": 2.409}, "timestamp": "2026-01-19T13:09:47.106331"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2234.758, "latencies_ms": [2234.758], "images_per_second": 0.447, "prompt_tokens": 1099, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image captures a bustling street scene in a city, with a row of parked cars lining the street, including a black car, a white car, and a silver car, all parked in front of a large stone wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.23, "peak": 110.95, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 27.96, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.96, "energy_joules_est": 62.51, "sample_count": 22, "duration_seconds": 2.236}, "timestamp": "2026-01-19T13:09:49.399759"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2547.697, "latencies_ms": [2547.697], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. car: 5\n2. street light: 1\n3. sign: 1\n4. building: 1\n5. wall: 1\n6. tree: 1\n7. street: 1\n8. road: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.7, "peak": 108.2, "min": 30.15}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.53, "min": 13.4}, "VDD_GPU": {"avg": 27.0, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.0, "energy_joules_est": 68.8, "sample_count": 25, "duration_seconds": 2.548}, "timestamp": "2026-01-19T13:09:52.003029"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2499.827, "latencies_ms": [2499.827], "images_per_second": 0.4, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The cars are parked on the side of the road, with the street curving to the left. The cars are parked in front of a large stone wall, which is in the background. The cars are parked near the street, with the street curving to the left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.29, "peak": 125.1, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.9, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.9, "energy_joules_est": 67.26, "sample_count": 25, "duration_seconds": 2.5}, "timestamp": "2026-01-19T13:09:54.606756"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2505.7, "latencies_ms": [2505.7], "images_per_second": 0.399, "prompt_tokens": 1111, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image captures a bustling street scene in a city, with several cars parked on the side of the road. In the background, there is a large stone wall, possibly part of a historical or cultural site. The sky above is clear and blue, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.32, "peak": 125.9, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.7, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.7, "energy_joules_est": 66.92, "sample_count": 25, "duration_seconds": 2.506}, "timestamp": "2026-01-19T13:09:57.215298"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1608.355, "latencies_ms": [1608.355], "images_per_second": 0.622, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The image features a clear blue sky with a few clouds, and the cars are parked on the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.31, "peak": 125.99, "min": 26.91}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.29, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.29, "energy_joules_est": 48.72, "sample_count": 16, "duration_seconds": 1.609}, "timestamp": "2026-01-19T13:09:58.884081"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4639.186, "latencies_ms": [4639.186], "images_per_second": 0.216, "prompt_tokens": 1099, "response_tokens_est": 128, "n_tiles": 1, "output_text": " The image captures a serene breakfast scene set on a wooden table, bathed in the soft glow of sunlight. A white plate cradles a half-eaten omelette, its golden hue contrasting with the vibrant colors of the fruit and vegetables that accompany it. A cup of tea, its contents a deep red, sits next to the plate, while a small bowl of salt adds a touch of simplicity to the tableau. The table itself is a study in contrasts - the white tiles beneath it are pristine and clean, while the wooden table adds a rustic charm. The shadows cast by the objects on the table hint at the presence", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.68, "peak": 122.28, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 23.15, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 23.15, "energy_joules_est": 107.42, "sample_count": 45, "duration_seconds": 4.64}, "timestamp": "2026-01-19T13:10:03.555087"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2117.758, "latencies_ms": [2117.758], "images_per_second": 0.472, "prompt_tokens": 1113, "response_tokens_est": 41, "n_tiles": 1, "output_text": " plate: 1, cup: 1, fork: 1, knife: 1, banana: 1, watermelon: 1, cup of tea: 1, sugar: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.72, "peak": 129.25, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.41, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.41, "energy_joules_est": 60.18, "sample_count": 21, "duration_seconds": 2.118}, "timestamp": "2026-01-19T13:10:05.737972"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3159.484, "latencies_ms": [3159.484], "images_per_second": 0.317, "prompt_tokens": 1117, "response_tokens_est": 80, "n_tiles": 1, "output_text": " The plate of food is located in the foreground, to the left of the cup of coffee, which is positioned in the middle of the table. The knife and fork are placed on the plate, with the knife on the left and the fork on the right. The shadow of the table is cast on the floor, indicating that the table is positioned in front of a light source, likely the sun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.63, "peak": 117.5, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 25.47, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 25.47, "energy_joules_est": 80.48, "sample_count": 31, "duration_seconds": 3.16}, "timestamp": "2026-01-19T13:10:08.956053"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2813.978, "latencies_ms": [2813.978], "images_per_second": 0.355, "prompt_tokens": 1111, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image captures a serene breakfast scene set on a wooden table with a white marble floor. A plate of food, including a slice of omelette and a bowl of fruit, is placed on the table. A cup of coffee and a small bowl of sugar are also present, suggesting a moment of quiet enjoyment before the day begins.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.38, "peak": 127.85, "min": 27.28}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.03, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.03, "energy_joules_est": 73.26, "sample_count": 28, "duration_seconds": 2.814}, "timestamp": "2026-01-19T13:10:11.865220"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2656.422, "latencies_ms": [2656.422], "images_per_second": 0.376, "prompt_tokens": 1109, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image features a wooden table with a white plate of food, a cup of coffee, and a bowl of fruit. The table is bathed in sunlight, casting shadows on the floor. The colors are vibrant, with the white plate of food contrasting against the brown table and the red and yellow fruit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.87, "peak": 125.87, "min": 28.3}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.44, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.44, "energy_joules_est": 70.24, "sample_count": 26, "duration_seconds": 2.657}, "timestamp": "2026-01-19T13:10:14.572178"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2105.353, "latencies_ms": [2105.353], "images_per_second": 0.475, "prompt_tokens": 1100, "response_tokens_est": 41, "n_tiles": 1, "output_text": " An elderly woman wearing a green and white striped shirt and pink apron is preparing food on a table with a variety of baked goods, including cookies, bread, and pastries, while a child sits nearby.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.54, "peak": 128.77, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.11, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.11, "energy_joules_est": 59.2, "sample_count": 21, "duration_seconds": 2.106}, "timestamp": "2026-01-19T13:10:16.761684"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2541.637, "latencies_ms": [2541.637], "images_per_second": 0.393, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. table: 1\n3. chair: 1\n4. glass: 2\n5. cup: 1\n6. bottle: 1\n7. food: 10\n8. mat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.13, "peak": 120.67, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.82, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.82, "energy_joules_est": 68.17, "sample_count": 25, "duration_seconds": 2.542}, "timestamp": "2026-01-19T13:10:19.364420"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2016.593, "latencies_ms": [2016.593], "images_per_second": 0.496, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The person is standing to the left of the table, with the table being in the foreground. The food items are placed on the table, with the person's hands reaching towards them.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.39, "peak": 129.96, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.56, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.56, "energy_joules_est": 57.6, "sample_count": 20, "duration_seconds": 2.017}, "timestamp": "2026-01-19T13:10:21.450477"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1714.057, "latencies_ms": [1714.057], "images_per_second": 0.583, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " An elderly woman is preparing food in a kitchen. She is wearing a green and white striped shirt and a pink apron.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.33, "peak": 124.75, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 29.64, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.64, "energy_joules_est": 50.82, "sample_count": 17, "duration_seconds": 1.715}, "timestamp": "2026-01-19T13:10:23.218279"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2463.789, "latencies_ms": [2463.789], "images_per_second": 0.406, "prompt_tokens": 1110, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image depicts an elderly woman in a green and white striped shirt and pink apron, standing in a kitchen with a wooden table. The lighting in the room is natural, coming from a window in the background, and the table is covered with a colorful placemat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.23, "peak": 92.07, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.41, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.41, "energy_joules_est": 67.54, "sample_count": 24, "duration_seconds": 2.464}, "timestamp": "2026-01-19T13:10:25.726978"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1919.25, "latencies_ms": [1919.25], "images_per_second": 0.521, "prompt_tokens": 1100, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A man stands in front of a traffic light with a sign that says \"AUSTRALIA TRAFFIC LIGHT\" and a red traffic light on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.92, "peak": 127.62, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 29.01, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.01, "energy_joules_est": 55.69, "sample_count": 19, "duration_seconds": 1.92}, "timestamp": "2026-01-19T13:10:27.716945"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1963.213, "latencies_ms": [1963.213], "images_per_second": 0.509, "prompt_tokens": 1114, "response_tokens_est": 35, "n_tiles": 1, "output_text": " person: 1, traffic light: 1, sign: 1, pole: 1, ground: 1, rocks: 1, plants: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.24, "peak": 94.24, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.26, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.26, "energy_joules_est": 57.45, "sample_count": 19, "duration_seconds": 1.963}, "timestamp": "2026-01-19T13:10:29.699706"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2057.426, "latencies_ms": [2057.426], "images_per_second": 0.486, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The man is standing to the left of the traffic light, which is positioned in the middle of the image. The traffic light is located in the background, while the man is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.26, "peak": 128.58, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.12, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.12, "energy_joules_est": 59.92, "sample_count": 20, "duration_seconds": 2.058}, "timestamp": "2026-01-19T13:10:31.780745"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1538.184, "latencies_ms": [1538.184], "images_per_second": 0.65, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A man stands in front of a traffic light and a sign that says \"Australia Traffic Light\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.59, "peak": 128.29, "min": 27.61}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.97, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.97, "energy_joules_est": 47.65, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T13:10:33.346896"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1583.617, "latencies_ms": [1583.617], "images_per_second": 0.631, "prompt_tokens": 1110, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The man is wearing a white t-shirt and khaki shorts, and the traffic light is red.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.24, "peak": 129.4, "min": 28.06}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.47, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 31.47, "energy_joules_est": 49.86, "sample_count": 16, "duration_seconds": 1.584}, "timestamp": "2026-01-19T13:10:35.011961"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1761.16, "latencies_ms": [1761.16], "images_per_second": 0.568, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A large group of people are gathered in a park to fly a variety of kites, including a large red and black fish kite.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.79, "peak": 128.52, "min": 34.29}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.68, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.68, "energy_joules_est": 54.05, "sample_count": 17, "duration_seconds": 1.762}, "timestamp": "2026-01-19T13:10:36.787289"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2887.365, "latencies_ms": [2887.365], "images_per_second": 0.346, "prompt_tokens": 1113, "response_tokens_est": 70, "n_tiles": 1, "output_text": " 1. Kites: 12\n2. People: 10\n3. Grass: 10\n4. Trees: 10\n5. Buildings: 1\n6. Kite tails: 10\n7. Kite strings: 10\n8. Kite tails: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.99, "peak": 114.22, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.64, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 26.64, "energy_joules_est": 76.93, "sample_count": 28, "duration_seconds": 2.888}, "timestamp": "2026-01-19T13:10:39.699935"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2786.984, "latencies_ms": [2786.984], "images_per_second": 0.359, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The kites are positioned in the foreground, with the largest and most prominent one being in the center. The kites are arranged in a loose formation, with some flying higher than others, creating a sense of depth and dimension. The background is filled with people and buildings, providing a sense of scale and context to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.61, "peak": 109.57, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.51, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.51, "energy_joules_est": 73.89, "sample_count": 27, "duration_seconds": 2.787}, "timestamp": "2026-01-19T13:10:42.508918"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1759.288, "latencies_ms": [1759.288], "images_per_second": 0.568, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A group of people are flying kites in a park. The kites are shaped like fish and are flying high in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.82, "peak": 117.27, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.22, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.22, "energy_joules_est": 53.17, "sample_count": 17, "duration_seconds": 1.76}, "timestamp": "2026-01-19T13:10:44.283048"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2923.015, "latencies_ms": [2923.015], "images_per_second": 0.342, "prompt_tokens": 1109, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a kite festival, with a multitude of kites soaring in the sky, predominantly in hues of red, blue, and purple. The sky is a canvas of overcast clouds, casting a soft light over the festivities below. The ground is a lush green, dotted with people and kites, creating a lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.61, "peak": 122.28, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.25, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.25, "energy_joules_est": 76.74, "sample_count": 29, "duration_seconds": 2.923}, "timestamp": "2026-01-19T13:10:47.301955"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1640.328, "latencies_ms": [1640.328], "images_per_second": 0.61, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man in a blue hoodie is sitting on the floor with a child, sharing a slice of pizza.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.96, "peak": 128.41, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.29, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 30.29, "energy_joules_est": 49.71, "sample_count": 16, "duration_seconds": 1.641}, "timestamp": "2026-01-19T13:10:48.975022"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2518.414, "latencies_ms": [2518.414], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. boy: 1\n3. pizza: 1\n4. box: 1\n5. blanket: 1\n6. chair: 1\n7. wall: 1\n8. floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.79, "peak": 116.07, "min": 27.44}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.55, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 27.55, "energy_joules_est": 69.41, "sample_count": 25, "duration_seconds": 2.519}, "timestamp": "2026-01-19T13:10:51.576012"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2203.242, "latencies_ms": [2203.242], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The man is sitting on the left side of the image, while the boy is sitting on the right side. The pizza is in the middle of the image, and the man is holding it with his right hand.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.43, "peak": 131.76, "min": 27.34}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.76, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.76, "energy_joules_est": 61.17, "sample_count": 22, "duration_seconds": 2.204}, "timestamp": "2026-01-19T13:10:53.864382"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1420.716, "latencies_ms": [1420.716], "images_per_second": 0.704, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man and a boy are sitting on the floor eating pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.58, "peak": 123.87, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.32, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 31.32, "energy_joules_est": 44.51, "sample_count": 14, "duration_seconds": 1.421}, "timestamp": "2026-01-19T13:10:55.326615"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1923.246, "latencies_ms": [1923.246], "images_per_second": 0.52, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image is taken in a room with warm lighting and the colors are vibrant. The man is wearing a blue hoodie and the boy is wearing a blue shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.26, "peak": 115.16, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.21, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 30.21, "energy_joules_est": 58.12, "sample_count": 19, "duration_seconds": 1.924}, "timestamp": "2026-01-19T13:10:57.318552"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1586.031, "latencies_ms": [1586.031], "images_per_second": 0.631, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A woman wearing a striped shirt is eating a piece of pizza while sitting in a blue folding chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.02, "peak": 124.01, "min": 30.69}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.63, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.63, "energy_joules_est": 48.61, "sample_count": 16, "duration_seconds": 1.587}, "timestamp": "2026-01-19T13:10:58.994466"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2607.535, "latencies_ms": [2607.535], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. chair: 1\n3. plate: 1\n4. food: 1\n5. backpack: 1\n6. ground: 1\n7. fire: 1\n8. tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.17, "peak": 120.14, "min": 27.18}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.81, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.81, "energy_joules_est": 69.93, "sample_count": 26, "duration_seconds": 2.608}, "timestamp": "2026-01-19T13:11:01.702740"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2613.055, "latencies_ms": [2613.055], "images_per_second": 0.383, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The person is sitting on the left side of the image, with the plate of food placed in front of them on the right side. The plate of food is positioned closer to the camera than the person, and the background is darker, suggesting that the scene is taking place outdoors at night.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.73, "peak": 93.46, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.35, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 26.35, "energy_joules_est": 68.88, "sample_count": 26, "duration_seconds": 2.614}, "timestamp": "2026-01-19T13:11:04.412127"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1575.813, "latencies_ms": [1575.813], "images_per_second": 0.635, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A woman is sitting in a camping chair and eating a hotdog while sitting on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.03, "peak": 126.48, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.14, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 30.14, "energy_joules_est": 47.51, "sample_count": 16, "duration_seconds": 1.576}, "timestamp": "2026-01-19T13:11:06.088508"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2072.977, "latencies_ms": [2072.977], "images_per_second": 0.482, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image is taken at night, with the subject illuminated by a dim light source, and the background is dark. The subject is wearing a striped shirt and is seated on a folding chair.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.42, "peak": 130.37, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.15, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 29.15, "energy_joules_est": 60.44, "sample_count": 20, "duration_seconds": 2.073}, "timestamp": "2026-01-19T13:11:08.176053"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1938.417, "latencies_ms": [1938.417], "images_per_second": 0.516, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A family of 12 is gathered around a long table at a dinner party, with the table filled with food and drinks, and the room decorated with various items.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.29, "peak": 104.45, "min": 30.48}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.53, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 29.53, "energy_joules_est": 57.26, "sample_count": 19, "duration_seconds": 1.939}, "timestamp": "2026-01-19T13:11:10.165504"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1931.612, "latencies_ms": [1931.612], "images_per_second": 0.518, "prompt_tokens": 1113, "response_tokens_est": 33, "n_tiles": 1, "output_text": " table: 1, chairs: 1, glasses: 10, plates: 10, food: 10, people: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.44, "peak": 127.56, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.53, "energy_joules_est": 57.05, "sample_count": 19, "duration_seconds": 1.932}, "timestamp": "2026-01-19T13:11:12.144267"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2498.444, "latencies_ms": [2498.444], "images_per_second": 0.4, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the table is in the foreground, with the people sitting around it. The people are positioned in the middle of the image, with the table in front of them. The background includes a doorway and a painting on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.02, "peak": 134.37, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.11, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.11, "energy_joules_est": 67.74, "sample_count": 25, "duration_seconds": 2.499}, "timestamp": "2026-01-19T13:11:14.748264"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1610.005, "latencies_ms": [1610.005], "images_per_second": 0.621, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of people are gathered around a long table in a dining room, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.14, "peak": 115.98, "min": 27.6}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.92, "min": 12.21}, "VDD_GPU": {"avg": 30.04, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.04, "energy_joules_est": 48.38, "sample_count": 16, "duration_seconds": 1.61}, "timestamp": "2026-01-19T13:11:16.419653"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2008.281, "latencies_ms": [2008.281], "images_per_second": 0.498, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming from the windows. The colors in the image are vibrant and the materials are mostly wooden and fabric.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.42, "peak": 110.03, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 29.1, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 29.1, "energy_joules_est": 58.45, "sample_count": 20, "duration_seconds": 2.009}, "timestamp": "2026-01-19T13:11:18.503346"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1760.517, "latencies_ms": [1760.517], "images_per_second": 0.568, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A baseball game is taking place with a player sliding into home plate, and the catcher and umpire are standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.24, "peak": 98.99, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.99, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.99, "energy_joules_est": 52.82, "sample_count": 17, "duration_seconds": 1.761}, "timestamp": "2026-01-19T13:11:20.284188"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2707.158, "latencies_ms": [2707.158], "images_per_second": 0.369, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. baseball player: 2\n2. catcher: 1\n3. umpire: 1\n4. baseball bat: 1\n5. baseball glove: 1\n6. baseball field: 1\n7. fence: 1\n8. bench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.32, "peak": 128.06, "min": 35.48}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 27.17, "energy_joules_est": 73.57, "sample_count": 26, "duration_seconds": 2.708}, "timestamp": "2026-01-19T13:11:22.995661"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2104.744, "latencies_ms": [2104.744], "images_per_second": 0.475, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The baseball player is in the foreground, sliding into the base. The catcher is in the background, behind the batter. The umpire is also in the background, behind the catcher.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.86, "peak": 123.27, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 28.44, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 28.44, "energy_joules_est": 59.87, "sample_count": 21, "duration_seconds": 2.105}, "timestamp": "2026-01-19T13:11:25.185000"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1501.168, "latencies_ms": [1501.168], "images_per_second": 0.666, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A baseball game is taking place on a field with a fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.39, "peak": 122.77, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.84, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.84, "energy_joules_est": 46.3, "sample_count": 15, "duration_seconds": 1.501}, "timestamp": "2026-01-19T13:11:26.755824"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2451.091, "latencies_ms": [2451.091], "images_per_second": 0.408, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a baseball game, with the vibrant green of the field contrasting against the brown dirt of the infield. The sun casts a warm glow on the scene, illuminating the players' uniforms and the white lines marking the base paths.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.62, "peak": 117.47, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.81, "peak": 40.57, "min": 18.52}}, "power_watts_avg": 27.81, "energy_joules_est": 68.17, "sample_count": 24, "duration_seconds": 2.451}, "timestamp": "2026-01-19T13:11:29.250751"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2076.6, "latencies_ms": [2076.6], "images_per_second": 0.482, "prompt_tokens": 1432, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A skateboarder wearing a black helmet and black clothing is performing a trick on a concrete ramp with graffiti on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.27, "peak": 111.79, "min": 32.57}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 30.95, "peak": 41.76, "min": 15.77}}, "power_watts_avg": 30.95, "energy_joules_est": 64.3, "sample_count": 20, "duration_seconds": 2.077}, "timestamp": "2026-01-19T13:11:31.340658"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2968.923, "latencies_ms": [2968.923], "images_per_second": 0.337, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. helmet: 1\n3. knee pads: 2\n4. skateboard: 1\n5. rail: 1\n6. grass: 1\n7. fence: 1\n8. shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.4, "peak": 122.38, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.87, "peak": 42.13, "min": 18.91}}, "power_watts_avg": 28.87, "energy_joules_est": 85.73, "sample_count": 29, "duration_seconds": 2.969}, "timestamp": "2026-01-19T13:11:34.348067"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3012.156, "latencies_ms": [3012.156], "images_per_second": 0.332, "prompt_tokens": 1450, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The skateboarder is positioned in the foreground, jumping over the concrete ledge of the skatepark. The shadow of the skateboarder is cast on the ground, indicating the direction of the light source. The skatepark is situated in the background, surrounded by a grassy area and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.18, "peak": 132.75, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 16.36, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 28.41, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 28.41, "energy_joules_est": 85.59, "sample_count": 29, "duration_seconds": 3.013}, "timestamp": "2026-01-19T13:11:37.373543"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2011.344, "latencies_ms": [2011.344], "images_per_second": 0.497, "prompt_tokens": 1444, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A skateboarder wearing a black helmet and black clothing is performing a trick on a concrete ramp in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.07, "peak": 129.7, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 31.87, "peak": 41.74, "min": 16.95}}, "power_watts_avg": 31.87, "energy_joules_est": 64.12, "sample_count": 20, "duration_seconds": 2.012}, "timestamp": "2026-01-19T13:11:39.454764"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3461.377, "latencies_ms": [3461.377], "images_per_second": 0.289, "prompt_tokens": 1442, "response_tokens_est": 75, "n_tiles": 1, "output_text": " The image captures a dynamic scene of a skateboarder in mid-air, executing a trick on a concrete ramp. The skateboarder is dressed in black attire, including a helmet and knee pads, and is surrounded by a vibrant green park setting. The sun casts a bright light on the scene, highlighting the skateboarder's shadow and the graffiti on the ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.01, "peak": 120.29, "min": 30.49}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.15, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 27.2, "peak": 42.13, "min": 17.73}}, "power_watts_avg": 27.2, "energy_joules_est": 94.17, "sample_count": 34, "duration_seconds": 3.462}, "timestamp": "2026-01-19T13:11:43.008897"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1765.512, "latencies_ms": [1765.512], "images_per_second": 0.566, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A plate of food is on a table with a glass of water, a glass of soda, and a bottle of ketchup.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.16, "peak": 127.58, "min": 30.61}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.78, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.78, "energy_joules_est": 52.6, "sample_count": 17, "duration_seconds": 1.766}, "timestamp": "2026-01-19T13:11:44.800728"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2122.816, "latencies_ms": [2122.816], "images_per_second": 0.471, "prompt_tokens": 1113, "response_tokens_est": 41, "n_tiles": 1, "output_text": " crab cake: 1, fries: 1, tomato: 1, lettuce: 1, pickles: 1, tartar sauce: 1, lemon: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.23, "peak": 113.73, "min": 30.96}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.93, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 28.93, "energy_joules_est": 61.43, "sample_count": 21, "duration_seconds": 2.123}, "timestamp": "2026-01-19T13:11:46.993520"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2498.876, "latencies_ms": [2498.876], "images_per_second": 0.4, "prompt_tokens": 1117, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The plate of food is located in the foreground of the image, with the glass of water and condiments in the background. The plate of food is to the left of the glass of water, and the condiments are to the right of the glass of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.03, "peak": 128.27, "min": 27.34}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.08, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.08, "energy_joules_est": 67.68, "sample_count": 25, "duration_seconds": 2.499}, "timestamp": "2026-01-19T13:11:49.593484"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1686.457, "latencies_ms": [1686.457], "images_per_second": 0.593, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A meal is served on a table with a glass of water, a glass of wine, and a plate of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.08, "peak": 129.1, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.73, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.73, "energy_joules_est": 50.15, "sample_count": 17, "duration_seconds": 1.687}, "timestamp": "2026-01-19T13:11:51.376237"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2905.937, "latencies_ms": [2905.937], "images_per_second": 0.344, "prompt_tokens": 1109, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The image features a plate of food with a variety of colors, including the golden brown of the fries, the red of the tomato, and the green of the lettuce. The lighting is bright and even, illuminating the food and creating a warm and inviting atmosphere. The table is made of wood, and the overall setting suggests a casual dining experience.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.18, "peak": 124.78, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.24, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.24, "energy_joules_est": 76.27, "sample_count": 28, "duration_seconds": 2.906}, "timestamp": "2026-01-19T13:11:54.299877"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1468.429, "latencies_ms": [1468.429], "images_per_second": 0.681, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A red motorcycle is parked on a sandy beach with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.84, "peak": 125.88, "min": 27.83}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.02, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.02, "energy_joules_est": 45.56, "sample_count": 15, "duration_seconds": 1.469}, "timestamp": "2026-01-19T13:11:55.873630"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3150.511, "latencies_ms": [3150.511], "images_per_second": 0.317, "prompt_tokens": 1113, "response_tokens_est": 78, "n_tiles": 1, "output_text": " 1. Motorcycle: 1\n2. Motorcycle seat: 1\n3. Motorcycle windshield: 1\n4. Motorcycle handlebars: 1\n5. Motorcycle mirrors: 2\n6. Motorcycle saddlebags: 2\n7. Motorcycle rear fender: 1\n8. Motorcycle front fender: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.4, "peak": 120.47, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.69, "peak": 40.57, "min": 17.34}}, "power_watts_avg": 25.69, "energy_joules_est": 80.95, "sample_count": 31, "duration_seconds": 3.151}, "timestamp": "2026-01-19T13:11:59.100766"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2163.501, "latencies_ms": [2163.501], "images_per_second": 0.462, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The motorcycle is positioned on the left side of the image, with the beach and palm trees in the background. The motorcycle is in the foreground, with the beach and palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.3, "peak": 130.48, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.82, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.82, "energy_joules_est": 60.2, "sample_count": 21, "duration_seconds": 2.164}, "timestamp": "2026-01-19T13:12:01.297446"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1498.881, "latencies_ms": [1498.881], "images_per_second": 0.667, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A red motorcycle is parked on a sandy beach with palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.79, "peak": 126.62, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.1, "energy_joules_est": 46.62, "sample_count": 15, "duration_seconds": 1.499}, "timestamp": "2026-01-19T13:12:02.867178"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1770.799, "latencies_ms": [1770.799], "images_per_second": 0.565, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The motorcycle is red and black, parked on a sandy beach with palm trees in the background. The sky is clear and blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.25, "peak": 123.66, "min": 32.67}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.68, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.68, "energy_joules_est": 54.35, "sample_count": 17, "duration_seconds": 1.771}, "timestamp": "2026-01-19T13:12:04.644530"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1467.741, "latencies_ms": [1467.741], "images_per_second": 0.681, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man in a suit and tie is standing in front of a dark wall.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.3, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.19, "peak": 126.7, "min": 27.16}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.8, "peak": 39.78, "min": 18.53}}, "power_watts_avg": 31.8, "energy_joules_est": 46.69, "sample_count": 15, "duration_seconds": 1.468}, "timestamp": "2026-01-19T13:12:06.214786"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2626.601, "latencies_ms": [2626.601], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. suit: 1\n2. tie: 1\n3. shirt: 1\n4. tie clip: 1\n5. wall: 1\n6. door: 1\n7. light switch: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.6, "peak": 127.17, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 26.96, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 26.96, "energy_joules_est": 70.84, "sample_count": 26, "duration_seconds": 2.627}, "timestamp": "2026-01-19T13:12:08.917803"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2515.56, "latencies_ms": [2515.56], "images_per_second": 0.398, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The man is positioned in the foreground, with his face and upper body prominently displayed. The background is relatively dark, with a wall and a light switch visible. The man's tie is positioned to the right of his face, and his suit jacket is to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.99, "peak": 127.35, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.63, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.63, "energy_joules_est": 67.0, "sample_count": 25, "duration_seconds": 2.516}, "timestamp": "2026-01-19T13:12:11.522263"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1479.638, "latencies_ms": [1479.638], "images_per_second": 0.676, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man in a suit and tie is standing in front of a door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.02, "peak": 122.12, "min": 30.82}, "VIN_SYS_5V0": {"avg": 14.01, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.57, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 30.57, "energy_joules_est": 45.26, "sample_count": 15, "duration_seconds": 1.48}, "timestamp": "2026-01-19T13:12:13.097503"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1773.818, "latencies_ms": [1773.818], "images_per_second": 0.564, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The man is wearing a dark suit with a white shirt and a patterned tie. The lighting is dim and the background is dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.28, "peak": 126.35, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.54, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.16, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.16, "energy_joules_est": 53.51, "sample_count": 18, "duration_seconds": 1.774}, "timestamp": "2026-01-19T13:12:14.976386"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1336.762, "latencies_ms": [1336.762], "images_per_second": 0.748, "prompt_tokens": 1099, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A cat is sleeping on a pair of shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.99, "peak": 116.37, "min": 29.84}, "VIN_SYS_5V0": {"avg": 14.03, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 32.09, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 32.09, "energy_joules_est": 42.92, "sample_count": 13, "duration_seconds": 1.338}, "timestamp": "2026-01-19T13:12:16.340318"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2622.106, "latencies_ms": [2622.106], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. Cat: 1\n2. Shoes: 3\n3. Wall: 1\n4. Floor: 1\n5. Sneakers: 2\n6. White: 1\n7. Black: 1\n8. Blue: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.74, "peak": 108.05, "min": 27.05}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.7, "peak": 40.97, "min": 17.35}}, "power_watts_avg": 27.7, "energy_joules_est": 72.65, "sample_count": 26, "duration_seconds": 2.623}, "timestamp": "2026-01-19T13:12:19.060528"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2072.748, "latencies_ms": [2072.748], "images_per_second": 0.482, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The cat is sleeping on the left side of the shoe, which is on the right side of the image. The shoe is in the foreground, while the wall is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.19, "peak": 122.5, "min": 30.17}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.39, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 28.39, "energy_joules_est": 58.86, "sample_count": 20, "duration_seconds": 2.073}, "timestamp": "2026-01-19T13:12:21.147626"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1311.781, "latencies_ms": [1311.781], "images_per_second": 0.762, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A cat is sleeping on a pair of shoes.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.92, "peak": 134.14, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.6, "peak": 39.77, "min": 17.34}}, "power_watts_avg": 32.6, "energy_joules_est": 42.77, "sample_count": 13, "duration_seconds": 1.312}, "timestamp": "2026-01-19T13:12:22.509575"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1617.604, "latencies_ms": [1617.604], "images_per_second": 0.618, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The cat is brown and white, the shoes are blue and white, and the wall is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.71, "peak": 120.61, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.2, "peak": 40.97, "min": 22.07}}, "power_watts_avg": 32.2, "energy_joules_est": 52.1, "sample_count": 16, "duration_seconds": 1.618}, "timestamp": "2026-01-19T13:12:24.180824"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2024.077, "latencies_ms": [2024.077], "images_per_second": 0.494, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A green Isuzu truck with the number 213171 on the front is parked on the street, with a man in a green uniform standing next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.7, "peak": 95.05, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.19, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.19, "energy_joules_est": 59.11, "sample_count": 20, "duration_seconds": 2.025}, "timestamp": "2026-01-19T13:12:26.282874"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2464.992, "latencies_ms": [2464.992], "images_per_second": 0.406, "prompt_tokens": 1113, "response_tokens_est": 53, "n_tiles": 1, "output_text": " 1. green truck\n2. yellow light\n3. red and white striped bar\n4. person in green uniform\n5. person in yellow vest\n6. person in green vest\n7. person in green uniform\n8. person in green vest", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.69, "peak": 123.26, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.25, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.25, "energy_joules_est": 67.19, "sample_count": 24, "duration_seconds": 2.466}, "timestamp": "2026-01-19T13:12:28.783320"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2217.598, "latencies_ms": [2217.598], "images_per_second": 0.451, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The green truck is in the foreground, parked on the street. The red and white striped barrier is in the middle ground, separating the truck from the sidewalk. The building in the background is far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.53, "peak": 120.38, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.95, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.95, "energy_joules_est": 62.01, "sample_count": 22, "duration_seconds": 2.218}, "timestamp": "2026-01-19T13:12:31.079245"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1743.159, "latencies_ms": [1743.159], "images_per_second": 0.574, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A green truck with a white logo on the front is driving down a street. There are two people on top of the truck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.2, "ram_available_mb": 99777.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.41, "peak": 121.78, "min": 28.14}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.06, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.06, "energy_joules_est": 52.42, "sample_count": 17, "duration_seconds": 1.744}, "timestamp": "2026-01-19T13:12:32.855695"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1671.09, "latencies_ms": [1671.09], "images_per_second": 0.598, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The truck is green and white, and the license plate is yellow. The sky is blue and the sun is shining.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.32, "peak": 129.9, "min": 27.85}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.73, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 30.73, "energy_joules_est": 51.37, "sample_count": 17, "duration_seconds": 1.672}, "timestamp": "2026-01-19T13:12:34.628743"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1610.453, "latencies_ms": [1610.453], "images_per_second": 0.621, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person is standing in a river with rocks and water, with a bridge and greenery in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.44, "peak": 122.24, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 30.73, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 30.73, "energy_joules_est": 49.51, "sample_count": 16, "duration_seconds": 1.611}, "timestamp": "2026-01-19T13:12:36.306317"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2564.561, "latencies_ms": [2564.561], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. rocks: 20\n3. water: 1\n4. bridge: 1\n5. vegetation: 1\n6. car: 1\n7. bridge: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.77, "peak": 115.63, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.45, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 27.45, "energy_joules_est": 70.42, "sample_count": 25, "duration_seconds": 2.565}, "timestamp": "2026-01-19T13:12:38.904780"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2507.117, "latencies_ms": [2507.117], "images_per_second": 0.399, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The person is standing on the left side of the image, near the riverbank. The bridge is located in the background, far away from the person. The rocks are scattered throughout the river, with some closer to the person and others further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.29, "peak": 110.18, "min": 27.39}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.49, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.49, "energy_joules_est": 66.42, "sample_count": 25, "duration_seconds": 2.507}, "timestamp": "2026-01-19T13:12:41.511623"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1397.259, "latencies_ms": [1397.259], "images_per_second": 0.716, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A person is standing in a river with rocks and water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 80.9, "peak": 131.06, "min": 29.81}, "VIN_SYS_5V0": {"avg": 13.96, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 16.53, "min": 12.22}, "VDD_GPU": {"avg": 30.84, "peak": 39.39, "min": 14.19}}, "power_watts_avg": 30.84, "energy_joules_est": 43.1, "sample_count": 14, "duration_seconds": 1.398}, "timestamp": "2026-01-19T13:12:42.972195"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2036.224, "latencies_ms": [2036.224], "images_per_second": 0.491, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image features a person standing in a river with rocks and water, with a bridge in the background. The sky is clear and blue, and the water is a light blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.04, "peak": 114.25, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.86, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 29.86, "energy_joules_est": 60.81, "sample_count": 20, "duration_seconds": 2.036}, "timestamp": "2026-01-19T13:12:45.053532"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4397.98, "latencies_ms": [4397.98], "images_per_second": 0.227, "prompt_tokens": 1099, "response_tokens_est": 128, "n_tiles": 1, "output_text": " The image captures a bustling street scene in Paris, France, where a row of parked motorcycles, adorned in various colors, lines the sidewalk in front of a row of shops. The motorcycles, each unique in design and color, are neatly aligned, creating a striking visual contrast against the backdrop of the city's architecture. The shops, with their vibrant red awnings and green shutters, add a splash of color to the scene. The buildings, with their ornate details and classic French architecture, stand tall, exuding an air of timeless elegance. The street itself is lined with cobblestones, adding to the charm of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.18, "peak": 124.2, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 23.7, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 23.7, "energy_joules_est": 104.24, "sample_count": 43, "duration_seconds": 4.398}, "timestamp": "2026-01-19T13:12:49.519975"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2690.243, "latencies_ms": [2690.243], "images_per_second": 0.372, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. Motorbikes: 12\n2. Street: 1\n3. Building: 1\n4. Sign: 1\n5. Window: 1\n6. Door: 1\n7. Streetlamp: 1\n8. Pedestrian: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.66, "peak": 124.79, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.28, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.28, "energy_joules_est": 70.71, "sample_count": 27, "duration_seconds": 2.691}, "timestamp": "2026-01-19T13:12:52.309288"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1917.591, "latencies_ms": [1917.591], "images_per_second": 0.521, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The motorcycles are parked on the left side of the street, while the buildings are on the right side. The motorcycles are closer to the viewer than the buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.92, "peak": 126.61, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.84, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.84, "energy_joules_est": 55.32, "sample_count": 19, "duration_seconds": 1.918}, "timestamp": "2026-01-19T13:12:54.283189"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2981.694, "latencies_ms": [2981.694], "images_per_second": 0.335, "prompt_tokens": 1111, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The image captures a bustling street corner in Paris, France, where a row of parked motorcycles lines the sidewalk. The motorcycles, predominantly black and silver, are parked in front of a row of shops, including a bar and a caf\u00e9. The buildings are adorned with ornate details, and the street is lined with trees, adding a touch of greenery to the urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.52, "peak": 122.91, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.14, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.14, "energy_joules_est": 77.95, "sample_count": 29, "duration_seconds": 2.982}, "timestamp": "2026-01-19T13:12:57.279449"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2493.281, "latencies_ms": [2493.281], "images_per_second": 0.401, "prompt_tokens": 1109, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image depicts a vibrant street scene with a variety of motorcycles parked in front of a row of shops. The motorcycles are predominantly black and silver, with some featuring red and green accents. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.25, "peak": 130.23, "min": 27.41}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 27.09, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.09, "energy_joules_est": 67.56, "sample_count": 25, "duration_seconds": 2.494}, "timestamp": "2026-01-19T13:12:59.871135"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1377.531, "latencies_ms": [1377.531], "images_per_second": 0.726, "prompt_tokens": 1099, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A person is holding a bunch of broccoli in their hand.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.87, "peak": 121.6, "min": 27.89}, "VIN_SYS_5V0": {"avg": 14.01, "peak": 15.34, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.98, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 30.98, "energy_joules_est": 42.69, "sample_count": 14, "duration_seconds": 1.378}, "timestamp": "2026-01-19T13:13:01.329918"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2155.655, "latencies_ms": [2155.655], "images_per_second": 0.464, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " broccoli: 1, hand: 1, broccoli bud: 1, broccoli stem: 1, broccoli leaves: 1, broccoli flower: 1, broccoli flower bud: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.86, "peak": 116.2, "min": 30.52}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.44, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.12, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 29.12, "energy_joules_est": 62.78, "sample_count": 21, "duration_seconds": 2.156}, "timestamp": "2026-01-19T13:13:03.520070"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1610.825, "latencies_ms": [1610.825], "images_per_second": 0.621, "prompt_tokens": 1117, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The broccoli is in the foreground, held by a hand, while the background is a tiled wall.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.78, "peak": 129.57, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.85, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.85, "energy_joules_est": 49.7, "sample_count": 16, "duration_seconds": 1.611}, "timestamp": "2026-01-19T13:13:05.185749"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1368.555, "latencies_ms": [1368.555], "images_per_second": 0.731, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A person is holding a bunch of broccoli in their hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.75, "peak": 125.84, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.56, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 32.56, "energy_joules_est": 44.57, "sample_count": 14, "duration_seconds": 1.369}, "timestamp": "2026-01-19T13:13:06.639646"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1320.158, "latencies_ms": [1320.158], "images_per_second": 0.757, "prompt_tokens": 1109, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The broccoli is green and the hand is pink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.95, "peak": 125.38, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 33.55, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 33.55, "energy_joules_est": 44.3, "sample_count": 13, "duration_seconds": 1.32}, "timestamp": "2026-01-19T13:13:07.997591"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1931.199, "latencies_ms": [1931.199], "images_per_second": 0.518, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " Two people are standing close to each other, one of them is wearing a black jacket with a hood and the other is wearing a black jacket with a red zipper.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.91, "peak": 128.04, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.83, "peak": 40.97, "min": 20.1}}, "power_watts_avg": 30.83, "energy_joules_est": 59.56, "sample_count": 19, "duration_seconds": 1.932}, "timestamp": "2026-01-19T13:13:09.978909"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2518.076, "latencies_ms": [2518.076], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 2\n2. jacket: 2\n3. hood: 1\n4. hat: 1\n5. person: 1\n6. person: 1\n7. person: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.92, "peak": 130.28, "min": 27.46}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.23, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.23, "energy_joules_est": 68.58, "sample_count": 25, "duration_seconds": 2.518}, "timestamp": "2026-01-19T13:13:12.580663"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2552.95, "latencies_ms": [2552.95], "images_per_second": 0.392, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The person on the left is closer to the camera than the person on the right. The person on the right is in the background, while the person on the left is in the foreground. The person on the right is also farther away from the camera than the person on the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.54, "peak": 121.89, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.89, "peak": 39.0, "min": 14.98}}, "power_watts_avg": 26.89, "energy_joules_est": 68.66, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T13:13:15.163913"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1669.306, "latencies_ms": [1669.306], "images_per_second": 0.599, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " Two people are standing close together in a crowded area, one of them is laughing and the other is covering their face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.5, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.57, "peak": 123.82, "min": 28.24}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.18, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 30.18, "energy_joules_est": 50.39, "sample_count": 17, "duration_seconds": 1.67}, "timestamp": "2026-01-19T13:13:16.940111"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1665.974, "latencies_ms": [1665.974], "images_per_second": 0.6, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image is taken in a dimly lit environment with a yellowish hue, and the subjects are wearing winter clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.57, "peak": 129.97, "min": 27.35}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 30.29, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.29, "energy_joules_est": 50.47, "sample_count": 17, "duration_seconds": 1.666}, "timestamp": "2026-01-19T13:13:18.708013"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1377.004, "latencies_ms": [1377.004], "images_per_second": 0.726, "prompt_tokens": 1099, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man with long hair is playing tennis on a blue court.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.76, "peak": 129.22, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 31.91, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.91, "energy_joules_est": 43.95, "sample_count": 14, "duration_seconds": 1.377}, "timestamp": "2026-01-19T13:13:20.179789"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2731.701, "latencies_ms": [2731.701], "images_per_second": 0.366, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. man: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. chair: 1\n5. white chair: 1\n6. green chair: 1\n7. blue tennis court: 1\n8. white line: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.33, "peak": 123.89, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.11, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.11, "energy_joules_est": 74.06, "sample_count": 27, "duration_seconds": 2.732}, "timestamp": "2026-01-19T13:13:22.969796"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2388.044, "latencies_ms": [2388.044], "images_per_second": 0.419, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis ball in the center and the empty chairs on the right side. The player is closer to the camera than the chairs, and the ball is in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.83, "peak": 129.31, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.24, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.24, "energy_joules_est": 65.06, "sample_count": 24, "duration_seconds": 2.388}, "timestamp": "2026-01-19T13:13:25.460548"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1324.831, "latencies_ms": [1324.831], "images_per_second": 0.755, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A man is playing tennis on a blue court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.69, "peak": 121.58, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.67, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.67, "energy_joules_est": 41.97, "sample_count": 13, "duration_seconds": 1.325}, "timestamp": "2026-01-19T13:13:26.822077"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1555.194, "latencies_ms": [1555.194], "images_per_second": 0.643, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The tennis court is blue, the man is wearing a white shirt, and the ball is yellow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.51, "peak": 121.87, "min": 46.59}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 33.33, "peak": 40.97, "min": 22.47}}, "power_watts_avg": 33.33, "energy_joules_est": 51.86, "sample_count": 15, "duration_seconds": 1.556}, "timestamp": "2026-01-19T13:13:28.380917"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1848.706, "latencies_ms": [1848.706], "images_per_second": 0.541, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A red glass vase with a floral design sits on a glass coaster on a wooden shelf, flanked by a white candle and a string of lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.57, "peak": 122.27, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.82, "peak": 40.57, "min": 20.49}}, "power_watts_avg": 30.82, "energy_joules_est": 56.99, "sample_count": 18, "duration_seconds": 1.849}, "timestamp": "2026-01-19T13:13:30.262105"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.136, "latencies_ms": [2552.136], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. candle: 1\n2. glass: 1\n3. mirror: 1\n4. wood: 1\n5. string lights: 1\n6. vase: 1\n7. wall: 1\n8. table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.73, "peak": 114.27, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.34, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.34, "energy_joules_est": 69.79, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T13:13:32.864211"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2449.878, "latencies_ms": [2449.878], "images_per_second": 0.408, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The red glass vase is positioned in the center of the image, with the white candle to its left and the string of lights to its right. The vase is placed on a round glass coaster, which is positioned in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.55, "peak": 122.66, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.09, "peak": 39.39, "min": 15.76}}, "power_watts_avg": 27.09, "energy_joules_est": 66.38, "sample_count": 24, "duration_seconds": 2.45}, "timestamp": "2026-01-19T13:13:35.357885"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1733.114, "latencies_ms": [1733.114], "images_per_second": 0.577, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A red glass vase sits on a glass coaster on a wooden shelf, with a white candle and string lights in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.38, "peak": 122.67, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.17, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.17, "energy_joules_est": 52.3, "sample_count": 17, "duration_seconds": 1.734}, "timestamp": "2026-01-19T13:13:37.119667"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1306.541, "latencies_ms": [1306.541], "images_per_second": 0.765, "prompt_tokens": 1109, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The vase is red and the candle is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.84, "peak": 124.51, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.94, "peak": 40.16, "min": 18.52}}, "power_watts_avg": 32.94, "energy_joules_est": 43.05, "sample_count": 13, "duration_seconds": 1.307}, "timestamp": "2026-01-19T13:13:38.480579"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1833.0, "latencies_ms": [1833.0], "images_per_second": 0.546, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A man in a gray shirt is standing in front of a mirror, while another man in a green shirt is standing in front of a couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.68, "peak": 132.47, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.19, "peak": 40.97, "min": 20.49}}, "power_watts_avg": 31.19, "energy_joules_est": 57.18, "sample_count": 18, "duration_seconds": 1.833}, "timestamp": "2026-01-19T13:13:40.356136"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2543.557, "latencies_ms": [2543.557], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. suitcase: 1\n2. jacket: 1\n3. laptop: 1\n4. man: 2\n5. man: 1\n6. man: 1\n7. man: 1\n8. man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.07, "peak": 123.08, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.34, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.34, "energy_joules_est": 69.55, "sample_count": 25, "duration_seconds": 2.544}, "timestamp": "2026-01-19T13:13:42.939501"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1982.575, "latencies_ms": [1982.575], "images_per_second": 0.504, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The man in the foreground is bending over, while the man in the background is standing upright. The man in the foreground is closer to the camera than the man in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.39, "peak": 128.49, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.9, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.9, "energy_joules_est": 57.3, "sample_count": 20, "duration_seconds": 1.983}, "timestamp": "2026-01-19T13:13:45.015430"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1479.578, "latencies_ms": [1479.578], "images_per_second": 0.676, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is filming another man in a room with a couch and a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.55, "peak": 122.09, "min": 27.57}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.12, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.12, "energy_joules_est": 46.06, "sample_count": 15, "duration_seconds": 1.48}, "timestamp": "2026-01-19T13:13:46.578668"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1646.825, "latencies_ms": [1646.825], "images_per_second": 0.607, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image is taken in a room with a brown couch, a silver suitcase, and a white light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.43, "peak": 127.7, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.64, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 31.64, "energy_joules_est": 52.12, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T13:13:48.236970"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1864.537, "latencies_ms": [1864.537], "images_per_second": 0.536, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman wearing a hat and a striped shirt is holding a cigarette in her hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.69, "peak": 123.51, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.88, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 32.85, "peak": 42.15, "min": 19.31}}, "power_watts_avg": 32.85, "energy_joules_est": 61.27, "sample_count": 18, "duration_seconds": 1.865}, "timestamp": "2026-01-19T13:13:50.114949"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2966.018, "latencies_ms": [2966.018], "images_per_second": 0.337, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. hat: 1\n2. cigarette: 1\n3. woman: 1\n4. necklace: 1\n5. bracelet: 1\n6. shirt: 1\n7. tie: 1\n8. earring: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.28, "peak": 116.21, "min": 30.81}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.13, "peak": 42.13, "min": 19.31}}, "power_watts_avg": 29.13, "energy_joules_est": 86.41, "sample_count": 29, "duration_seconds": 2.966}, "timestamp": "2026-01-19T13:13:53.114760"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2534.945, "latencies_ms": [2534.945], "images_per_second": 0.394, "prompt_tokens": 1450, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The woman is positioned in the foreground of the image, with her hat and cigarette in the foreground. The background is a plain white wall, which provides a stark contrast to the woman's dark attire and hair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.65, "peak": 134.55, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 29.67, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 29.67, "energy_joules_est": 75.22, "sample_count": 25, "duration_seconds": 2.535}, "timestamp": "2026-01-19T13:13:55.713997"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1826.822, "latencies_ms": [1826.822], "images_per_second": 0.547, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman wearing a hat and a striped shirt is holding a cigarette and smiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.85, "peak": 131.97, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.57, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 32.57, "energy_joules_est": 59.52, "sample_count": 18, "duration_seconds": 1.827}, "timestamp": "2026-01-19T13:13:57.592455"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2357.704, "latencies_ms": [2357.704], "images_per_second": 0.424, "prompt_tokens": 1442, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is in black and white, with a high contrast and a soft focus. The lighting is natural, and the subject is wearing a black hat and a striped shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.6, "peak": 129.48, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 31.21, "peak": 42.92, "min": 20.11}}, "power_watts_avg": 31.21, "energy_joules_est": 73.6, "sample_count": 23, "duration_seconds": 2.358}, "timestamp": "2026-01-19T13:13:59.985744"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1502.837, "latencies_ms": [1502.837], "images_per_second": 0.665, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two zebras are grazing in a grassy enclosure with trees and rocks in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.75, "peak": 116.75, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.49, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.49, "energy_joules_est": 47.35, "sample_count": 15, "duration_seconds": 1.504}, "timestamp": "2026-01-19T13:14:01.556167"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1609.686, "latencies_ms": [1609.686], "images_per_second": 0.621, "prompt_tokens": 1113, "response_tokens_est": 20, "n_tiles": 1, "output_text": " zebra: 2\ngrass: 1\nrocks: 2\ntrees: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.2, "peak": 130.81, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.39, "peak": 40.97, "min": 19.32}}, "power_watts_avg": 31.39, "energy_joules_est": 50.54, "sample_count": 16, "duration_seconds": 1.61}, "timestamp": "2026-01-19T13:14:03.228366"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2140.756, "latencies_ms": [2140.756], "images_per_second": 0.467, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the rocky wall and trees in the background. The zebras are grazing on the grass, which is in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.51, "peak": 134.13, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.82, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 28.82, "energy_joules_est": 61.71, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T13:14:05.424064"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1534.252, "latencies_ms": [1534.252], "images_per_second": 0.652, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Two zebras are grazing in a grassy enclosure with trees and rocks in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.4, "ram_available_mb": 99777.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.06, "peak": 123.38, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.99, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.99, "energy_joules_est": 47.56, "sample_count": 15, "duration_seconds": 1.535}, "timestamp": "2026-01-19T13:14:06.991671"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2436.753, "latencies_ms": [2436.753], "images_per_second": 0.41, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features two zebras grazing in a grassy enclosure with a rocky wall in the background. The zebras are black and white with distinctive stripes, and the grass is green with patches of moss. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.75, "peak": 131.42, "min": 30.62}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.96, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.96, "energy_joules_est": 68.14, "sample_count": 24, "duration_seconds": 2.437}, "timestamp": "2026-01-19T13:14:09.488395"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1580.0, "latencies_ms": [1580.0], "images_per_second": 0.633, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " An old, rusted fire hydrant with a chain attached to it is sitting on a sidewalk.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.85, "peak": 125.04, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.21, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.21, "energy_joules_est": 47.76, "sample_count": 16, "duration_seconds": 1.581}, "timestamp": "2026-01-19T13:14:11.160362"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1654.787, "latencies_ms": [1654.787], "images_per_second": 0.604, "prompt_tokens": 1114, "response_tokens_est": 22, "n_tiles": 1, "output_text": " hydrant: 1\nstone: 1\nchain: 1\npink flowers: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.11, "peak": 121.36, "min": 30.4}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.95, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.95, "energy_joules_est": 51.23, "sample_count": 16, "duration_seconds": 1.655}, "timestamp": "2026-01-19T13:14:12.832159"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2528.421, "latencies_ms": [2528.421], "images_per_second": 0.396, "prompt_tokens": 1118, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The fire hydrant is located in the foreground of the image, with a concrete sidewalk and a stone wall in the background. The fire hydrant is positioned to the left of the stone wall, and there are some plants and flowers to the left of the hydrant.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.7, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.83, "peak": 128.19, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.22, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 27.22, "energy_joules_est": 68.84, "sample_count": 25, "duration_seconds": 2.529}, "timestamp": "2026-01-19T13:14:15.436041"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1735.28, "latencies_ms": [1735.28], "images_per_second": 0.576, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A rusted fire hydrant sits on the ground next to a stone wall, surrounded by a garden with flowers and plants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.84, "peak": 118.43, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.11, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.11, "energy_joules_est": 50.53, "sample_count": 17, "duration_seconds": 1.736}, "timestamp": "2026-01-19T13:14:17.211029"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1506.718, "latencies_ms": [1506.718], "images_per_second": 0.664, "prompt_tokens": 1110, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The fire hydrant is rusted and chained, with a faded pink color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.29, "peak": 129.16, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.49, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.49, "energy_joules_est": 47.47, "sample_count": 15, "duration_seconds": 1.507}, "timestamp": "2026-01-19T13:14:18.775814"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1802.045, "latencies_ms": [1802.045], "images_per_second": 0.555, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " In the image, there are two brown bears walking on a rocky path, with one bear in the foreground and the other in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.09, "peak": 133.03, "min": 27.88}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.12, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.12, "energy_joules_est": 54.29, "sample_count": 18, "duration_seconds": 1.803}, "timestamp": "2026-01-19T13:14:20.676763"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2589.08, "latencies_ms": [2589.08], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bear: 2\n2. road: 1\n3. grass: 1\n4. dirt: 1\n5. rocks: 1\n6. dirt road: 1\n7. bear: 1\n8. bear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.07, "peak": 111.24, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.93, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.93, "energy_joules_est": 69.74, "sample_count": 25, "duration_seconds": 2.59}, "timestamp": "2026-01-19T13:14:23.288524"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2225.706, "latencies_ms": [2225.706], "images_per_second": 0.449, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The main bear is in the foreground, walking towards the camera, while the second bear is in the background, walking away from the camera. The bears are on a gravel road, with the road stretching out behind them.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.97, "peak": 113.61, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.97, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.97, "energy_joules_est": 62.27, "sample_count": 22, "duration_seconds": 2.226}, "timestamp": "2026-01-19T13:14:25.586734"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2000.261, "latencies_ms": [2000.261], "images_per_second": 0.5, "prompt_tokens": 1111, "response_tokens_est": 35, "n_tiles": 1, "output_text": " In the image, two brown bears are walking on a gravel road. The bear in the foreground is walking towards the camera, while the other bear is walking away from it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.72, "peak": 118.75, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.56, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.56, "energy_joules_est": 57.15, "sample_count": 20, "duration_seconds": 2.001}, "timestamp": "2026-01-19T13:14:27.671483"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1724.67, "latencies_ms": [1724.67], "images_per_second": 0.58, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The bears are brown with a white underbelly, and the image is taken in natural daylight with a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.97, "peak": 126.14, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.89, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.89, "energy_joules_est": 51.56, "sample_count": 17, "duration_seconds": 1.725}, "timestamp": "2026-01-19T13:14:29.448397"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1898.341, "latencies_ms": [1898.341], "images_per_second": 0.527, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A young child with curly blonde hair is kneeling on the ground next to a metal bowl filled with soil, wearing a white shirt and a colorful tie.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.14, "peak": 123.18, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.32, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 29.32, "energy_joules_est": 55.68, "sample_count": 19, "duration_seconds": 1.899}, "timestamp": "2026-01-19T13:14:31.436727"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2541.753, "latencies_ms": [2541.753], "images_per_second": 0.393, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. child: 1\n2. shirt: 1\n3. tie: 1\n4. pants: 1\n5. shoes: 1\n6. bowl: 1\n7. gravel: 1\n8. leaves: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.29, "peak": 121.07, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.86, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 26.86, "energy_joules_est": 68.28, "sample_count": 25, "duration_seconds": 2.542}, "timestamp": "2026-01-19T13:14:34.036826"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2563.494, "latencies_ms": [2563.494], "images_per_second": 0.39, "prompt_tokens": 1118, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The child is in the foreground of the image, kneeling on the ground next to a bucket of soil. The bucket is placed on the ground, which is covered with gravel. The child is holding a handful of soil in their hand, and there is a plant in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.45, "peak": 125.91, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.84, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.84, "energy_joules_est": 68.82, "sample_count": 25, "duration_seconds": 2.564}, "timestamp": "2026-01-19T13:14:36.643001"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1398.822, "latencies_ms": [1398.822], "images_per_second": 0.715, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A young child is playing in a dirt area with a bucket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.94, "peak": 132.38, "min": 30.34}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.93, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.93, "energy_joules_est": 43.28, "sample_count": 14, "duration_seconds": 1.399}, "timestamp": "2026-01-19T13:14:38.109231"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2108.665, "latencies_ms": [2108.665], "images_per_second": 0.474, "prompt_tokens": 1110, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image is in black and white, with the exception of the child's tie, which is colorful. The child is wearing a white shirt and beige pants. The ground is covered with gravel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.44, "peak": 126.24, "min": 27.45}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.98, "peak": 40.97, "min": 18.52}}, "power_watts_avg": 28.98, "energy_joules_est": 61.12, "sample_count": 21, "duration_seconds": 2.109}, "timestamp": "2026-01-19T13:14:40.303374"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1679.402, "latencies_ms": [1679.402], "images_per_second": 0.595, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A table with bottles and flowers sits in the middle of a dirt field with stuffed animals and a red cross on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.2, "peak": 123.3, "min": 27.57}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.01, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.01, "energy_joules_est": 50.42, "sample_count": 17, "duration_seconds": 1.68}, "timestamp": "2026-01-19T13:14:42.078474"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1872.55, "latencies_ms": [1872.55], "images_per_second": 0.534, "prompt_tokens": 1113, "response_tokens_est": 31, "n_tiles": 1, "output_text": " teddy bear: 2\nbottle: 3\nflowers: 1\ntable: 1\nbench: 1\ncloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.82, "peak": 126.65, "min": 32.19}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.03, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.03, "energy_joules_est": 56.26, "sample_count": 18, "duration_seconds": 1.873}, "timestamp": "2026-01-19T13:14:43.959673"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2196.226, "latencies_ms": [2196.226], "images_per_second": 0.455, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The teddy bears are positioned to the left of the table, which is located in the foreground of the image. The table is situated near the center of the image, with the teddy bears and other objects arranged around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.22, "peak": 107.19, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.51, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 28.51, "energy_joules_est": 62.62, "sample_count": 22, "duration_seconds": 2.197}, "timestamp": "2026-01-19T13:14:46.248190"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2031.14, "latencies_ms": [2031.14], "images_per_second": 0.492, "prompt_tokens": 1111, "response_tokens_est": 37, "n_tiles": 1, "output_text": " In a barren, dry landscape, a makeshift memorial is set up. A makeshift bed, adorned with a red cross, holds stuffed animals and bottles of alcohol, suggesting a poignant tribute.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.92, "peak": 127.68, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.62, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.62, "energy_joules_est": 58.14, "sample_count": 20, "duration_seconds": 2.032}, "timestamp": "2026-01-19T13:14:48.333760"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2679.405, "latencies_ms": [2679.405], "images_per_second": 0.373, "prompt_tokens": 1109, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image depicts a barren, dry landscape with a makeshift table made of wood and metal. The table is adorned with a variety of stuffed animals, including teddy bears and a lion, and a vase filled with yellow flowers. The sky is clear, and the sun is shining brightly, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.85, "peak": 114.18, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.87, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.87, "energy_joules_est": 72.01, "sample_count": 26, "duration_seconds": 2.68}, "timestamp": "2026-01-19T13:14:51.037882"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4784.129, "latencies_ms": [4784.129], "images_per_second": 0.209, "prompt_tokens": 1432, "response_tokens_est": 123, "n_tiles": 1, "output_text": " The image captures a serene harbor scene, where several fishing boats are docked at a pier. The boats, painted in hues of green and white, are adorned with orange buoys and life jackets, hinting at their purpose and the safety measures in place. The harbor is nestled amidst lush green hills, providing a picturesque backdrop to the scene. The sky overhead is overcast, casting a soft light over the entire setting. In the distance, other boats can be seen, adding to the sense of a bustling harbor. The image exudes a sense of tranquility and the quiet beauty of a day at the harbor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.6, "ram_available_mb": 99777.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.0, "peak": 122.76, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 24.64, "peak": 41.36, "min": 16.16}}, "power_watts_avg": 24.64, "energy_joules_est": 117.9, "sample_count": 47, "duration_seconds": 4.785}, "timestamp": "2026-01-19T13:14:55.928695"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2338.812, "latencies_ms": [2338.812], "images_per_second": 0.428, "prompt_tokens": 1446, "response_tokens_est": 34, "n_tiles": 1, "output_text": " boat: 1, dock: 1, water: 1, mountains: 1, people: 1, ropes: 1, metal: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.31, "peak": 133.77, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 16.26, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.08, "peak": 41.76, "min": 14.98}}, "power_watts_avg": 30.08, "energy_joules_est": 70.36, "sample_count": 23, "duration_seconds": 2.339}, "timestamp": "2026-01-19T13:14:58.324245"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2588.865, "latencies_ms": [2588.865], "images_per_second": 0.386, "prompt_tokens": 1450, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The fishing boats are positioned on the left side of the image, with the nearest boat being closest to the viewer. The background features a calm body of water, with hills and other boats visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.52, "peak": 114.71, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 16.36, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 29.7, "peak": 42.13, "min": 16.95}}, "power_watts_avg": 29.7, "energy_joules_est": 76.9, "sample_count": 25, "duration_seconds": 2.589}, "timestamp": "2026-01-19T13:15:00.929481"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3133.418, "latencies_ms": [3133.418], "images_per_second": 0.319, "prompt_tokens": 1444, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image captures a serene harbor scene where several fishing boats are docked at a pier. The boats, painted in hues of green and white, are adorned with orange buoys and life jackets, indicating their readiness for the sea. The harbor is nestled amidst lush green hills, providing a picturesque backdrop to this tranquil setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.99, "peak": 120.17, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 27.87, "peak": 41.76, "min": 17.35}}, "power_watts_avg": 27.87, "energy_joules_est": 87.34, "sample_count": 31, "duration_seconds": 3.134}, "timestamp": "2026-01-19T13:15:04.158318"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2025.836, "latencies_ms": [2025.836], "images_per_second": 0.494, "prompt_tokens": 1442, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image features a harbor with boats docked at the pier, the water is calm and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.43, "peak": 123.82, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 16.36, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 31.42, "peak": 42.15, "min": 14.98}}, "power_watts_avg": 31.42, "energy_joules_est": 63.68, "sample_count": 20, "duration_seconds": 2.027}, "timestamp": "2026-01-19T13:15:06.242605"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1434.495, "latencies_ms": [1434.495], "images_per_second": 0.697, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A woman is eating a hot dog with mustard and has her tongue out.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.11, "peak": 113.94, "min": 30.27}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 32.42, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 32.42, "energy_joules_est": 46.53, "sample_count": 14, "duration_seconds": 1.435}, "timestamp": "2026-01-19T13:15:07.707111"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2551.43, "latencies_ms": [2551.43], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. food: 1\n3. hand: 1\n4. mouth: 1\n5. nose: 1\n6. eyes: 1\n7. hair: 1\n8. scarf: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.29, "peak": 109.1, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.94, "peak": 40.56, "min": 18.52}}, "power_watts_avg": 27.94, "energy_joules_est": 71.31, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T13:15:10.308746"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2183.983, "latencies_ms": [2183.983], "images_per_second": 0.458, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The woman is in the foreground of the image, with the hot dog in her mouth being the main object in the foreground. The background is blurred, indicating that the focus is on the woman and the hot dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.93, "peak": 122.6, "min": 36.77}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.35, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.35, "energy_joules_est": 61.92, "sample_count": 21, "duration_seconds": 2.184}, "timestamp": "2026-01-19T13:15:12.497445"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1298.074, "latencies_ms": [1298.074], "images_per_second": 0.77, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " A woman is eating a hot dog at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.2, "peak": 124.84, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 32.61, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 32.61, "energy_joules_est": 42.34, "sample_count": 13, "duration_seconds": 1.299}, "timestamp": "2026-01-19T13:15:13.853679"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1896.944, "latencies_ms": [1896.944], "images_per_second": 0.527, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is taken at night, with the subject's face illuminated by a warm light source. The subject is wearing a black jacket and has short dark hair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.13, "peak": 114.37, "min": 27.68}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.48, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 30.48, "energy_joules_est": 57.83, "sample_count": 19, "duration_seconds": 1.897}, "timestamp": "2026-01-19T13:15:15.835938"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1816.962, "latencies_ms": [1816.962], "images_per_second": 0.55, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man and a woman are standing in a room, the man is holding a glass of champagne and the woman is looking at him.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.64, "peak": 128.13, "min": 27.28}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.75, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.35, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 29.35, "energy_joules_est": 53.34, "sample_count": 18, "duration_seconds": 1.817}, "timestamp": "2026-01-19T13:15:17.712959"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2528.575, "latencies_ms": [2528.575], "images_per_second": 0.395, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 1\n3. glass: 1\n4. curtain: 1\n5. door: 1\n6. wall: 1\n7. floor: 1\n8. table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.81, "peak": 126.62, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.0, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 27.0, "energy_joules_est": 68.28, "sample_count": 25, "duration_seconds": 2.529}, "timestamp": "2026-01-19T13:15:20.312834"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1947.754, "latencies_ms": [1947.754], "images_per_second": 0.513, "prompt_tokens": 1118, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The man is standing to the right of the woman, and they are both standing in the foreground of the image. The woman is closer to the camera than the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.75, "peak": 123.41, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.07, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 29.07, "energy_joules_est": 56.63, "sample_count": 19, "duration_seconds": 1.948}, "timestamp": "2026-01-19T13:15:22.285819"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1782.639, "latencies_ms": [1782.639], "images_per_second": 0.561, "prompt_tokens": 1112, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man and a woman are standing in a room, the man is holding a glass of champagne and the woman is looking at him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.9, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.79, "peak": 117.33, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.37, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.37, "energy_joules_est": 52.36, "sample_count": 18, "duration_seconds": 1.783}, "timestamp": "2026-01-19T13:15:24.168232"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1952.516, "latencies_ms": [1952.516], "images_per_second": 0.512, "prompt_tokens": 1110, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming from the window. The man is wearing a black suit and the woman is wearing a black dress.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.4, "peak": 126.49, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.15, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 29.15, "energy_joules_est": 56.93, "sample_count": 19, "duration_seconds": 1.953}, "timestamp": "2026-01-19T13:15:26.147531"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3914.695, "latencies_ms": [3914.695], "images_per_second": 0.255, "prompt_tokens": 1099, "response_tokens_est": 102, "n_tiles": 1, "output_text": " The image depicts a blue wooden cabinet with a white top, which is situated in a room with a concrete floor. The cabinet has two open shelves and a closed cabinet door. On the top shelf, there are several items including a silver teapot, a silver pitcher, and a few small metal objects. The bottom shelf contains a green plate and a white bowl. The cabinet is positioned next to a white wall, and there are other items in the background, such as a wooden chair and a basket.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.7, "peak": 127.57, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 23.99, "peak": 39.77, "min": 17.34}}, "power_watts_avg": 23.99, "energy_joules_est": 93.93, "sample_count": 38, "duration_seconds": 3.915}, "timestamp": "2026-01-19T13:15:30.100648"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2100.387, "latencies_ms": [2100.387], "images_per_second": 0.476, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " 1. blue cabinet\n2. silver vase\n3. silver bowl\n4. silver cup\n5. silver spoon\n6. silver knife\n7. silver fork\n8. silver spoon", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.82, "peak": 123.0, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.31, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.31, "energy_joules_est": 59.47, "sample_count": 21, "duration_seconds": 2.101}, "timestamp": "2026-01-19T13:15:32.288602"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2113.136, "latencies_ms": [2113.136], "images_per_second": 0.473, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The blue cabinet is positioned in the foreground, with the green and white table to its left. The green and white table is situated in the background, while the blue cabinet is in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.1, "peak": 120.63, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.27, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.27, "energy_joules_est": 59.74, "sample_count": 21, "duration_seconds": 2.113}, "timestamp": "2026-01-19T13:15:34.483008"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4253.41, "latencies_ms": [4253.41], "images_per_second": 0.235, "prompt_tokens": 1111, "response_tokens_est": 121, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a flea market, where a blue wooden cabinet stands as the centerpiece. The cabinet, adorned with a green and white striped top, is brimming with an array of items, each with its own story to tell. Amidst the clutter, a silver teapot and a white vase add a touch of elegance, while a green and white striped plate and a blue and white striped bowl add a dash of color. The background is a blur of other stalls and people, each with their own tales to tell, creating a lively atmosphere that is quintessentially flea market.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.96, "peak": 123.22, "min": 27.87}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 23.76, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 23.76, "energy_joules_est": 101.08, "sample_count": 42, "duration_seconds": 4.254}, "timestamp": "2026-01-19T13:15:38.839311"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2734.368, "latencies_ms": [2734.368], "images_per_second": 0.366, "prompt_tokens": 1109, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image depicts a blue wooden cabinet with a white top, situated in a room with a concrete floor. The cabinet is adorned with various items, including a silver teapot, a silver pitcher, and a green vase. The lighting in the room is natural, coming from a window in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.56, "peak": 130.16, "min": 28.17}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.99, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 25.99, "energy_joules_est": 71.08, "sample_count": 27, "duration_seconds": 2.735}, "timestamp": "2026-01-19T13:15:41.644451"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1577.285, "latencies_ms": [1577.285], "images_per_second": 0.634, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A plate of cheese and butter sandwiches is on a desk with a keyboard and mouse in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.57, "peak": 124.82, "min": 27.28}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.26, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.26, "energy_joules_est": 47.75, "sample_count": 16, "duration_seconds": 1.578}, "timestamp": "2026-01-19T13:15:43.317868"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2003.296, "latencies_ms": [2003.296], "images_per_second": 0.499, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " plate: 1, butter: 1, bread: 4, butter knife: 1, keyboard: 1, mouse: 1, mousepad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.96, "peak": 116.6, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.23, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 29.23, "energy_joules_est": 58.56, "sample_count": 20, "duration_seconds": 2.004}, "timestamp": "2026-01-19T13:15:45.395034"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2473.528, "latencies_ms": [2473.528], "images_per_second": 0.404, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The plate is located in the foreground, with the sandwich slices placed on top of it. The sandwich slices are positioned in a diagonal arrangement, with the top left slice being the closest to the camera. The plate is situated on a desk, which is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25994.8, "ram_available_mb": 99777.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.51, "peak": 129.52, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.4, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.4, "energy_joules_est": 67.79, "sample_count": 24, "duration_seconds": 2.474}, "timestamp": "2026-01-19T13:15:47.892019"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1480.868, "latencies_ms": [1480.868], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A plate of cheese and butter sandwiches is on a desk with a keyboard and mouse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.33, "peak": 103.08, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.41, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.41, "energy_joules_est": 46.52, "sample_count": 15, "duration_seconds": 1.481}, "timestamp": "2026-01-19T13:15:49.457406"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2082.142, "latencies_ms": [2082.142], "images_per_second": 0.48, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image shows a plate of cheese and butter sandwiches on a white plate, with a keyboard and mouse in the background. The lighting is natural, and the sandwiches are placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.98, "peak": 125.3, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.95, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.95, "energy_joules_est": 60.28, "sample_count": 21, "duration_seconds": 2.082}, "timestamp": "2026-01-19T13:15:51.644805"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2107.58, "latencies_ms": [2107.58], "images_per_second": 0.474, "prompt_tokens": 1432, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man in a suit and tie is adjusting his tie, which has a pattern of red, green, and yellow lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.06, "peak": 121.55, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 16.26, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.32, "peak": 41.76, "min": 15.38}}, "power_watts_avg": 30.32, "energy_joules_est": 63.92, "sample_count": 21, "duration_seconds": 2.108}, "timestamp": "2026-01-19T13:15:53.830999"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2084.656, "latencies_ms": [2084.656], "images_per_second": 0.48, "prompt_tokens": 1446, "response_tokens_est": 24, "n_tiles": 1, "output_text": " tie: 1, glasses: 1, suit: 1, shirt: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.7, "peak": 118.04, "min": 33.77}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.36, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.85, "peak": 42.13, "min": 17.35}}, "power_watts_avg": 31.85, "energy_joules_est": 66.41, "sample_count": 20, "duration_seconds": 2.085}, "timestamp": "2026-01-19T13:15:55.920752"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2605.741, "latencies_ms": [2605.741], "images_per_second": 0.384, "prompt_tokens": 1450, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The man is standing in the foreground, with the tie being the main object in the image. The tie is positioned in the middle of the image, with the man's face and glasses being the closest objects to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.8, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.9, "peak": 133.87, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.82, "peak": 41.76, "min": 18.14}}, "power_watts_avg": 29.82, "energy_joules_est": 77.71, "sample_count": 26, "duration_seconds": 2.606}, "timestamp": "2026-01-19T13:15:58.621843"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1803.635, "latencies_ms": [1803.635], "images_per_second": 0.554, "prompt_tokens": 1444, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man is adjusting his tie with a glowing red light on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.33, "peak": 121.36, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 16.26, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 32.17, "peak": 41.74, "min": 15.38}}, "power_watts_avg": 32.17, "energy_joules_est": 58.04, "sample_count": 18, "duration_seconds": 1.804}, "timestamp": "2026-01-19T13:16:00.502929"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2211.782, "latencies_ms": [2211.782], "images_per_second": 0.452, "prompt_tokens": 1442, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The man is wearing a black suit with a tie that has a pattern of red and green lights. The lighting is dim and the background is dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.97, "peak": 134.97, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 16.26, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 31.53, "peak": 42.53, "min": 19.71}}, "power_watts_avg": 31.53, "energy_joules_est": 69.76, "sample_count": 22, "duration_seconds": 2.212}, "timestamp": "2026-01-19T13:16:02.792458"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1618.584, "latencies_ms": [1618.584], "images_per_second": 0.618, "prompt_tokens": 1100, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man is walking across the street in front of a building with a sign that says \"TABU\".", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.35, "peak": 113.55, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.68, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.68, "energy_joules_est": 49.68, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T13:16:04.468239"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2633.637, "latencies_ms": [2633.637], "images_per_second": 0.38, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. traffic light: 1\n3. street light: 1\n4. building: 2\n5. sign: 1\n6. car: 1\n7. sidewalk: 1\n8. street: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25996.2, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.3, "peak": 130.96, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.79, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 26.79, "energy_joules_est": 70.56, "sample_count": 26, "duration_seconds": 2.634}, "timestamp": "2026-01-19T13:16:07.184899"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2242.173, "latencies_ms": [2242.173], "images_per_second": 0.446, "prompt_tokens": 1118, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The man is walking on the sidewalk in front of the building, which is located on the corner of the street. The traffic light is positioned on the street corner, and the building is situated on the corner of the intersection.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25996.2, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.09, "peak": 127.44, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.58, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.58, "energy_joules_est": 61.85, "sample_count": 22, "duration_seconds": 2.242}, "timestamp": "2026-01-19T13:16:09.477030"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1678.652, "latencies_ms": [1678.652], "images_per_second": 0.596, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man is walking across the street at night in front of a building with a sign that says \"TABU\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.93, "peak": 125.96, "min": 27.21}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.53, "energy_joules_est": 49.58, "sample_count": 17, "duration_seconds": 1.679}, "timestamp": "2026-01-19T13:16:11.248533"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1904.064, "latencies_ms": [1904.064], "images_per_second": 0.525, "prompt_tokens": 1110, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is taken at night, with the sky being dark blue and the street being lit by streetlights. The building is white and has a curved facade.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.81, "peak": 123.08, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 29.15, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 29.15, "energy_joules_est": 55.51, "sample_count": 19, "duration_seconds": 1.904}, "timestamp": "2026-01-19T13:16:13.228279"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2236.729, "latencies_ms": [2236.729], "images_per_second": 0.447, "prompt_tokens": 1099, "response_tokens_est": 44, "n_tiles": 1, "output_text": " In the image, a young girl is skillfully riding a wave on a blue surfboard, while a group of people are enjoying the ocean, with one person holding a blue surfboard and another person swimming in the water.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.18, "peak": 125.63, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.92, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 27.92, "energy_joules_est": 62.47, "sample_count": 22, "duration_seconds": 2.237}, "timestamp": "2026-01-19T13:16:15.523246"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2657.99, "latencies_ms": [2657.99], "images_per_second": 0.376, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. surfboard: 1\n2. person: 3\n3. surfboard: 1\n4. person: 1\n5. surfboard: 1\n6. person: 1\n7. surfboard: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25996.1, "ram_available_mb": 99776.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.03, "peak": 129.94, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.67, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 26.67, "energy_joules_est": 70.9, "sample_count": 26, "duration_seconds": 2.658}, "timestamp": "2026-01-19T13:16:18.227795"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2865.121, "latencies_ms": [2865.121], "images_per_second": 0.349, "prompt_tokens": 1117, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The main object, a girl, is in the foreground, riding a wave on a blue surfboard. The surfboard is in the middle ground, with the girl positioned slightly to the left of it. The background features other people, including a man in a black wetsuit, who are swimming and paddling on their surfboards.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.37, "peak": 133.15, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.09, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.09, "energy_joules_est": 74.76, "sample_count": 28, "duration_seconds": 2.865}, "timestamp": "2026-01-19T13:16:21.146519"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3301.612, "latencies_ms": [3301.612], "images_per_second": 0.303, "prompt_tokens": 1111, "response_tokens_est": 84, "n_tiles": 1, "output_text": " In the vast expanse of the ocean, a group of people are enjoying a day of surfing. The surfer in the foreground, clad in a blue wetsuit, is skillfully riding a wave on a blue surfboard. In the background, other surfers can be seen paddling and waiting for their turn to catch a wave. The scene is a vibrant display of the ocean's power and the thrill of surfing.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.75, "peak": 118.03, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.13, "peak": 39.78, "min": 15.37}}, "power_watts_avg": 25.13, "energy_joules_est": 82.99, "sample_count": 32, "duration_seconds": 3.302}, "timestamp": "2026-01-19T13:16:24.482004"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2854.15, "latencies_ms": [2854.15], "images_per_second": 0.35, "prompt_tokens": 1109, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a group of people enjoying a day at the beach. The sky is a clear blue, and the sun is shining brightly, casting a warm glow on the water. The waves are a beautiful shade of blue, and the surfers are wearing colorful swimwear, adding a splash of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.3, "peak": 122.5, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 26.07, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.07, "energy_joules_est": 74.42, "sample_count": 28, "duration_seconds": 2.855}, "timestamp": "2026-01-19T13:16:27.394993"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2062.34, "latencies_ms": [2062.34], "images_per_second": 0.485, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " In the image, a man wearing a white shirt and beige pants is feeding an elephant with his hand, while the elephant is standing on a concrete platform with a fence in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.95, "peak": 124.39, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.62, "peak": 39.39, "min": 15.37}}, "power_watts_avg": 28.62, "energy_joules_est": 59.05, "sample_count": 20, "duration_seconds": 2.063}, "timestamp": "2026-01-19T13:16:29.483765"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2341.042, "latencies_ms": [2341.042], "images_per_second": 0.427, "prompt_tokens": 1113, "response_tokens_est": 49, "n_tiles": 1, "output_text": " elephant: 1, man: 1, fence: 1, elephant's trunk: 1, man's hand: 1, man's pants: 1, man's shirt: 1, man's belt: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.78, "peak": 128.78, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.85, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.85, "energy_joules_est": 65.21, "sample_count": 23, "duration_seconds": 2.341}, "timestamp": "2026-01-19T13:16:31.888004"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2212.864, "latencies_ms": [2212.864], "images_per_second": 0.452, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, while the elephant is on the right side. The man is closer to the camera than the elephant. The elephant is in the background, behind the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.05, "peak": 129.91, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.83, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.83, "energy_joules_est": 61.6, "sample_count": 22, "duration_seconds": 2.213}, "timestamp": "2026-01-19T13:16:34.178963"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1515.277, "latencies_ms": [1515.277], "images_per_second": 0.66, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man is feeding an elephant in a fenced area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.69, "peak": 122.27, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.92, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.92, "energy_joules_est": 46.88, "sample_count": 15, "duration_seconds": 1.516}, "timestamp": "2026-01-19T13:16:35.748650"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1515.254, "latencies_ms": [1515.254], "images_per_second": 0.66, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The elephant is gray, the man is wearing white, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.64, "peak": 123.42, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.12, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 32.12, "energy_joules_est": 48.68, "sample_count": 15, "duration_seconds": 1.516}, "timestamp": "2026-01-19T13:16:37.316750"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1626.527, "latencies_ms": [1626.527], "images_per_second": 0.615, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A brown and white dog sits on a bed covered in clothes and a box, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.38, "peak": 118.86, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.39, "peak": 40.97, "min": 19.32}}, "power_watts_avg": 31.39, "energy_joules_est": 51.08, "sample_count": 16, "duration_seconds": 1.627}, "timestamp": "2026-01-19T13:16:38.991148"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1963.282, "latencies_ms": [1963.282], "images_per_second": 0.509, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " dog: 1, box: 1, clothes: 1, pillow: 1, blanket: 1, bag: 1, window: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.58, "peak": 128.57, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.76, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 29.76, "energy_joules_est": 58.45, "sample_count": 19, "duration_seconds": 1.964}, "timestamp": "2026-01-19T13:16:40.990424"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1972.808, "latencies_ms": [1972.808], "images_per_second": 0.507, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The dog is sitting on the bed, which is in the foreground of the image. The bed is located near the window, which is in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.18, "peak": 123.3, "min": 31.01}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.53, "energy_joules_est": 58.28, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T13:16:42.971981"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1478.844, "latencies_ms": [1478.844], "images_per_second": 0.676, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A brown and white dog sits on a bed covered in clothes and a box.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.34, "peak": 129.39, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 31.59, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.59, "energy_joules_est": 46.73, "sample_count": 15, "duration_seconds": 1.479}, "timestamp": "2026-01-19T13:16:44.540717"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1619.015, "latencies_ms": [1619.015], "images_per_second": 0.618, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The dog is brown and white, and the room is lit by natural light coming through a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.67, "peak": 122.74, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.05, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.22, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.22, "energy_joules_est": 50.56, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T13:16:46.212198"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1781.442, "latencies_ms": [1781.442], "images_per_second": 0.561, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man in a blue tie and white shirt is sitting at a desk with a laptop and a pen in his hand, looking thoughtful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25997.7, "ram_available_mb": 99774.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.2, "ram_available_mb": 99773.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.08, "peak": 127.07, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.18, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 30.18, "energy_joules_est": 53.78, "sample_count": 18, "duration_seconds": 1.782}, "timestamp": "2026-01-19T13:16:48.092013"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1719.708, "latencies_ms": [1719.708], "images_per_second": 0.581, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " laptop: 1, pen: 1, glasses: 1, notebook: 1, man: 1", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25999.2, "ram_available_mb": 99773.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.78, "peak": 121.1, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.06, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.06, "energy_joules_est": 51.71, "sample_count": 17, "duration_seconds": 1.72}, "timestamp": "2026-01-19T13:16:49.865672"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2401.693, "latencies_ms": [2401.693], "images_per_second": 0.416, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The laptop is on the left side of the man, the pen is on the right side of the man, and the glasses are on the man's head. The man is in the foreground, and the background is a blurred image of a building.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.6, "peak": 107.34, "min": 27.2}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.51, "peak": 40.57, "min": 17.35}}, "power_watts_avg": 27.51, "energy_joules_est": 66.08, "sample_count": 24, "duration_seconds": 2.402}, "timestamp": "2026-01-19T13:16:52.363873"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1714.121, "latencies_ms": [1714.121], "images_per_second": 0.583, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man in a blue tie and white shirt is sitting at a desk with a laptop and a pen in his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.23, "peak": 112.96, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.71, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.71, "energy_joules_est": 50.94, "sample_count": 17, "duration_seconds": 1.715}, "timestamp": "2026-01-19T13:16:54.139555"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1653.384, "latencies_ms": [1653.384], "images_per_second": 0.605, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The man is wearing a blue tie and a white shirt. The laptop is black and the pen is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.0, "peak": 120.11, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.1, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.1, "energy_joules_est": 51.43, "sample_count": 16, "duration_seconds": 1.654}, "timestamp": "2026-01-19T13:16:55.812384"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1832.735, "latencies_ms": [1832.735], "images_per_second": 0.546, "prompt_tokens": 1432, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A large airplane is flying in the sky with a moon in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25999.1, "ram_available_mb": 99773.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25999.4, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.76, "peak": 130.8, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.77, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 32.61, "peak": 42.15, "min": 19.31}}, "power_watts_avg": 32.61, "energy_joules_est": 59.79, "sample_count": 18, "duration_seconds": 1.834}, "timestamp": "2026-01-19T13:16:57.694426"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1924.061, "latencies_ms": [1924.061], "images_per_second": 0.52, "prompt_tokens": 1446, "response_tokens_est": 19, "n_tiles": 1, "output_text": " airplane: 1, moon: 1, clouds: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.4, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.42, "peak": 120.61, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.26, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.86, "peak": 42.13, "min": 19.71}}, "power_watts_avg": 32.86, "energy_joules_est": 63.25, "sample_count": 19, "duration_seconds": 1.925}, "timestamp": "2026-01-19T13:16:59.677140"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2228.948, "latencies_ms": [2228.948], "images_per_second": 0.449, "prompt_tokens": 1450, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The airplane is flying in the sky, which is in the background. The moon is in the foreground, and the airplane is flying above it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.12, "peak": 108.45, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 31.28, "peak": 42.53, "min": 19.32}}, "power_watts_avg": 31.28, "energy_joules_est": 69.73, "sample_count": 22, "duration_seconds": 2.229}, "timestamp": "2026-01-19T13:17:01.968992"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1819.881, "latencies_ms": [1819.881], "images_per_second": 0.549, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A plane is flying in the sky and a moon is in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.28, "peak": 115.49, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.59, "peak": 42.15, "min": 16.95}}, "power_watts_avg": 32.59, "energy_joules_est": 59.32, "sample_count": 18, "duration_seconds": 1.82}, "timestamp": "2026-01-19T13:17:03.847557"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2190.023, "latencies_ms": [2190.023], "images_per_second": 0.457, "prompt_tokens": 1442, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The airplane is white with red and blue stripes on the tail, and the moon is orange. The sky is blue and the lighting is bright.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.01, "peak": 111.01, "min": 27.85}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 31.5, "peak": 42.92, "min": 19.31}}, "power_watts_avg": 31.5, "energy_joules_est": 68.99, "sample_count": 22, "duration_seconds": 2.19}, "timestamp": "2026-01-19T13:17:06.137201"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2104.728, "latencies_ms": [2104.728], "images_per_second": 0.475, "prompt_tokens": 1432, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A young man wearing a tie-dye shirt and black pants is performing a skateboard trick in a skate park.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.32, "peak": 133.99, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 16.26, "min": 11.26}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.39, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 30.39, "energy_joules_est": 63.98, "sample_count": 21, "duration_seconds": 2.105}, "timestamp": "2026-01-19T13:17:08.327802"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2409.02, "latencies_ms": [2409.02], "images_per_second": 0.415, "prompt_tokens": 1446, "response_tokens_est": 36, "n_tiles": 1, "output_text": " skateboard: 1, person: 1, palm tree: 1, building: 1, bench: 1, sky: 1, clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.68, "peak": 123.12, "min": 27.96}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 16.26, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.97, "peak": 42.15, "min": 16.95}}, "power_watts_avg": 29.97, "energy_joules_est": 72.21, "sample_count": 24, "duration_seconds": 2.409}, "timestamp": "2026-01-19T13:17:10.827912"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2401.825, "latencies_ms": [2401.825], "images_per_second": 0.416, "prompt_tokens": 1450, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick in the middle of the image. The skate park is in the background, with palm trees and buildings visible beyond it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.93, "peak": 122.16, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 16.26, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 29.84, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 29.84, "energy_joules_est": 71.68, "sample_count": 24, "duration_seconds": 2.402}, "timestamp": "2026-01-19T13:17:13.328286"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1872.442, "latencies_ms": [1872.442], "images_per_second": 0.534, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A young man wearing a tie dye shirt is skateboarding in a skate park.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.59, "peak": 120.9, "min": 31.09}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 16.26, "min": 11.16}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 12.21}, "VDD_GPU": {"avg": 32.28, "peak": 42.15, "min": 16.16}}, "power_watts_avg": 32.28, "energy_joules_est": 60.45, "sample_count": 18, "duration_seconds": 1.873}, "timestamp": "2026-01-19T13:17:15.204756"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2156.127, "latencies_ms": [2156.127], "images_per_second": 0.464, "prompt_tokens": 1442, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The skateboarder is wearing a tie-dye shirt and black pants, and the skate park is surrounded by palm trees and buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.3, "peak": 122.25, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 32.43, "peak": 42.53, "min": 21.28}}, "power_watts_avg": 32.43, "energy_joules_est": 69.94, "sample_count": 21, "duration_seconds": 2.157}, "timestamp": "2026-01-19T13:17:17.395013"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2046.595, "latencies_ms": [2046.595], "images_per_second": 0.489, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " In the image, a sheep with a fluffy white coat is standing in a grassy field, looking directly at the camera, while a wire fence with a metal gate is visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.85, "peak": 124.76, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 29.17, "peak": 40.16, "min": 18.14}}, "power_watts_avg": 29.17, "energy_joules_est": 59.72, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T13:17:19.491030"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2714.849, "latencies_ms": [2714.849], "images_per_second": 0.368, "prompt_tokens": 1113, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. sheep: 1\n2. wire fence: 1\n3. grass: 1\n4. trees: 1\n5. wire: 1\n6. sheep's wool: 1\n7. sheep's face: 1\n8. sheep's body: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.87, "peak": 123.49, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.56, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.56, "energy_joules_est": 72.12, "sample_count": 27, "duration_seconds": 2.715}, "timestamp": "2026-01-19T13:17:22.302174"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2028.867, "latencies_ms": [2028.867], "images_per_second": 0.493, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The sheep is in the foreground, behind a wire fence, and the trees are in the background. The sheep is looking directly at the camera, while the trees are behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.2, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.0, "peak": 119.07, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.43, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.43, "energy_joules_est": 57.69, "sample_count": 20, "duration_seconds": 2.029}, "timestamp": "2026-01-19T13:17:24.392551"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1493.91, "latencies_ms": [1493.91], "images_per_second": 0.669, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A sheep is standing in a field behind a fence, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.2, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.2, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.79, "peak": 135.17, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.2, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.2, "energy_joules_est": 46.62, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T13:17:25.960477"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1410.701, "latencies_ms": [1410.701], "images_per_second": 0.709, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The sheep is white and fluffy, and the grass is green.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.2, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.2, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.55, "peak": 132.32, "min": 31.17}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 32.59, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 32.59, "energy_joules_est": 45.98, "sample_count": 14, "duration_seconds": 1.411}, "timestamp": "2026-01-19T13:17:27.421643"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1773.053, "latencies_ms": [1773.053], "images_per_second": 0.564, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image shows a close-up of a white electronic device with a red logo and a circular button with a red symbol on it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25999.2, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25998.6, "ram_available_mb": 99773.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.09, "peak": 119.82, "min": 28.24}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.99}, "VDD_GPU": {"avg": 30.73, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 30.73, "energy_joules_est": 54.51, "sample_count": 18, "duration_seconds": 1.774}, "timestamp": "2026-01-19T13:17:29.303083"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2311.099, "latencies_ms": [2311.099], "images_per_second": 0.433, "prompt_tokens": 1113, "response_tokens_est": 47, "n_tiles": 1, "output_text": " 1. white iPod with red logo\n2. red button\n3. white button\n4. black button\n5. silver button\n6. black and white button\n7. green button\n8. black and white button", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.6, "ram_available_mb": 99773.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.45, "peak": 126.01, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 27.58, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.58, "energy_joules_est": 63.76, "sample_count": 23, "duration_seconds": 2.312}, "timestamp": "2026-01-19T13:17:31.701302"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2319.108, "latencies_ms": [2319.108], "images_per_second": 0.431, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The main object is located in the foreground, with the buttons being the focal point of the image. The buttons are positioned on the left side of the device, with the \"WALKMAN\" logo being the most prominent feature.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.24, "peak": 128.68, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.44, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.44, "energy_joules_est": 63.65, "sample_count": 23, "duration_seconds": 2.32}, "timestamp": "2026-01-19T13:17:34.096240"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2392.323, "latencies_ms": [2392.323], "images_per_second": 0.418, "prompt_tokens": 1111, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image captures a close-up view of a white electronic device, possibly a portable media player, with a prominent red logo on its side. The device features a circular button with a red outline and a white center, surrounded by a green ring.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.26, "peak": 119.93, "min": 26.95}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.09, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.09, "energy_joules_est": 64.82, "sample_count": 24, "duration_seconds": 2.393}, "timestamp": "2026-01-19T13:17:36.594298"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1840.06, "latencies_ms": [1840.06], "images_per_second": 0.543, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The device is white with green accents and has a glossy finish. It is illuminated by a bright light source, casting a shadow on the surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.28, "peak": 111.89, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.29, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 29.29, "energy_joules_est": 53.9, "sample_count": 18, "duration_seconds": 1.84}, "timestamp": "2026-01-19T13:17:38.475008"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1990.88, "latencies_ms": [1990.88], "images_per_second": 0.502, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A woman wearing a black dress and black shoes is standing in a kitchen holding a glass of champagne.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.9, "ram_available_mb": 99773.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.32, "peak": 108.46, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 31.26, "peak": 41.74, "min": 17.74}}, "power_watts_avg": 31.26, "energy_joules_est": 62.26, "sample_count": 20, "duration_seconds": 1.992}, "timestamp": "2026-01-19T13:17:40.566607"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2979.641, "latencies_ms": [2979.641], "images_per_second": 0.336, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. woman: 1\n2. dress: 1\n3. shoes: 1\n4. refrigerator: 1\n5. counter: 1\n6. cabinet: 1\n7. bottle: 1\n8. mop: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.36, "peak": 123.16, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.26, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 28.45, "peak": 42.13, "min": 17.35}}, "power_watts_avg": 28.45, "energy_joules_est": 84.79, "sample_count": 29, "duration_seconds": 2.98}, "timestamp": "2026-01-19T13:17:43.587689"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2520.479, "latencies_ms": [2520.479], "images_per_second": 0.397, "prompt_tokens": 1450, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The woman is standing in the foreground of the image, with the refrigerator in the background. The woman is positioned to the left of the refrigerator, and the sink is located to the right of the refrigerator.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.57, "peak": 132.4, "min": 28.36}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 29.58, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 29.58, "energy_joules_est": 74.57, "sample_count": 25, "duration_seconds": 2.521}, "timestamp": "2026-01-19T13:17:46.195124"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1904.316, "latencies_ms": [1904.316], "images_per_second": 0.525, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman wearing a black dress is standing in a kitchen holding a glass of champagne.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.9, "peak": 129.46, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 16.36, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 31.95, "peak": 41.74, "min": 15.77}}, "power_watts_avg": 31.95, "energy_joules_est": 60.86, "sample_count": 19, "duration_seconds": 1.905}, "timestamp": "2026-01-19T13:17:48.177704"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2087.201, "latencies_ms": [2087.201], "images_per_second": 0.479, "prompt_tokens": 1442, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The woman is wearing a black dress with sparkles and black shoes. The kitchen has wooden cabinets and a stainless steel refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25998.5, "ram_available_mb": 99773.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26000.5, "ram_available_mb": 99771.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.44, "peak": 119.27, "min": 33.09}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 32.19, "peak": 42.53, "min": 18.92}}, "power_watts_avg": 32.19, "energy_joules_est": 67.2, "sample_count": 20, "duration_seconds": 2.087}, "timestamp": "2026-01-19T13:17:50.274284"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1958.316, "latencies_ms": [1958.316], "images_per_second": 0.511, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image captures a round convex mirror mounted on a pole, reflecting the scene of a yellow school bus and a silver car on a street with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.66, "peak": 124.83, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.09, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 30.09, "energy_joules_est": 58.94, "sample_count": 19, "duration_seconds": 1.959}, "timestamp": "2026-01-19T13:17:52.260479"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1982.568, "latencies_ms": [1982.568], "images_per_second": 0.504, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " mirror: 1\nbus: 1\ntraffic light: 1\npole: 1\nbuilding: 1\ncar: 1\nroad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.64, "peak": 122.09, "min": 32.33}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.61, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 29.61, "energy_joules_est": 58.71, "sample_count": 19, "duration_seconds": 1.983}, "timestamp": "2026-01-19T13:17:54.246786"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2726.079, "latencies_ms": [2726.079], "images_per_second": 0.367, "prompt_tokens": 1117, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The mirror is positioned in the foreground, reflecting the yellow school bus and the surrounding environment. The reflection of the school bus is prominently displayed in the mirror, which is mounted on a pole. The background of the image features a building with a sign, indicating that the mirror is likely located near a commercial or industrial area.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.74, "peak": 118.41, "min": 28.09}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 26.8, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 26.8, "energy_joules_est": 73.07, "sample_count": 27, "duration_seconds": 2.727}, "timestamp": "2026-01-19T13:17:57.053504"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2341.719, "latencies_ms": [2341.719], "images_per_second": 0.427, "prompt_tokens": 1111, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a scene of a school bus parked on the side of a road, with its reflection visible in a convex mirror mounted on a pole. The background reveals a commercial area with various signs and advertisements, suggesting a busy urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.13, "peak": 119.86, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 27.39, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.39, "energy_joules_est": 64.15, "sample_count": 23, "duration_seconds": 2.342}, "timestamp": "2026-01-19T13:17:59.447275"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2160.196, "latencies_ms": [2160.196], "images_per_second": 0.463, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a yellow school bus with a red roof, parked on the side of a road. The sky is overcast, and the lighting is dim, casting a soft glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.17, "peak": 128.04, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.37, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 28.37, "energy_joules_est": 61.29, "sample_count": 21, "duration_seconds": 2.16}, "timestamp": "2026-01-19T13:18:01.630096"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2176.277, "latencies_ms": [2176.277], "images_per_second": 0.46, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " In the image, a gray cat is sitting on a wooden table, looking at a dog that is standing outside the window. The dog is brown and black, and there are several potted plants on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.16, "peak": 112.94, "min": 32.42}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.61, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.61, "energy_joules_est": 62.28, "sample_count": 21, "duration_seconds": 2.177}, "timestamp": "2026-01-19T13:18:03.815998"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2563.096, "latencies_ms": [2563.096], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. cat: 1\n2. dog: 2\n3. potted plant: 3\n4. can: 1\n5. window: 1\n6. table: 1\n7. grass: 1\n8. fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.37, "peak": 128.32, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.26, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.26, "energy_joules_est": 69.88, "sample_count": 25, "duration_seconds": 2.564}, "timestamp": "2026-01-19T13:18:06.417933"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1991.893, "latencies_ms": [1991.893], "images_per_second": 0.502, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The cat is in the foreground, looking at the dog, which is in the background. The dog is standing on the grass, while the cat is sitting on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.85, "peak": 121.15, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 28.74, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.74, "energy_joules_est": 57.26, "sample_count": 20, "duration_seconds": 1.992}, "timestamp": "2026-01-19T13:18:08.504027"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1576.88, "latencies_ms": [1576.88], "images_per_second": 0.634, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A gray cat is sitting on a table next to a window, looking outside at a dog.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.01, "peak": 120.89, "min": 27.09}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.39, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.39, "energy_joules_est": 47.93, "sample_count": 16, "duration_seconds": 1.577}, "timestamp": "2026-01-19T13:18:10.176168"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1747.575, "latencies_ms": [1747.575], "images_per_second": 0.572, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image is taken during the day with natural light coming through the window, and the cat is sitting on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.32, "peak": 130.66, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.57, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 30.57, "energy_joules_est": 53.44, "sample_count": 17, "duration_seconds": 1.748}, "timestamp": "2026-01-19T13:18:11.950241"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2195.595, "latencies_ms": [2195.595], "images_per_second": 0.455, "prompt_tokens": 1099, "response_tokens_est": 44, "n_tiles": 1, "output_text": " A female soccer player in a blue jersey with the number 10 and the name \"Marilyn\" on it is dribbling the ball while being closely followed by a player in a yellow jersey.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.97, "peak": 125.75, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.44, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 28.44, "energy_joules_est": 62.47, "sample_count": 22, "duration_seconds": 2.196}, "timestamp": "2026-01-19T13:18:14.255094"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2618.798, "latencies_ms": [2618.798], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. soccer ball: 1\n3. jersey: 1\n4. shorts: 1\n5. headband: 1\n6. hair: 1\n7. background: 1\n8. grass: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.3, "ram_available_mb": 99772.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.77, "peak": 125.96, "min": 30.93}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.46, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.46, "energy_joules_est": 69.31, "sample_count": 26, "duration_seconds": 2.619}, "timestamp": "2026-01-19T13:18:16.970070"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2822.163, "latencies_ms": [2822.163], "images_per_second": 0.354, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The main object is a woman wearing a blue jersey, positioned in the foreground of the image. The woman is holding a soccer ball in her hands, which is located in the middle of the image. In the background, there is another woman wearing a yellow jersey, positioned to the right of the woman in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.29, "peak": 117.87, "min": 27.74}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.84, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 25.84, "energy_joules_est": 72.93, "sample_count": 28, "duration_seconds": 2.822}, "timestamp": "2026-01-19T13:18:19.889152"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1699.388, "latencies_ms": [1699.388], "images_per_second": 0.588, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A female soccer player in a blue jersey is dribbling the ball on a field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.1, "peak": 124.11, "min": 27.37}, "VIN_SYS_5V0": {"avg": 13.88, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.09, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 29.09, "energy_joules_est": 49.45, "sample_count": 17, "duration_seconds": 1.7}, "timestamp": "2026-01-19T13:18:21.662845"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2707.219, "latencies_ms": [2707.219], "images_per_second": 0.369, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image captures a dynamic moment on a grassy field, where a female athlete in a vibrant blue jersey is in the midst of a powerful kick, her body language suggesting intense focus and determination. The lighting is natural and bright, casting soft shadows and highlighting the texture of the grass beneath her feet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.8, "peak": 120.53, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.44, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 26.58, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.58, "energy_joules_est": 71.97, "sample_count": 26, "duration_seconds": 2.708}, "timestamp": "2026-01-19T13:18:24.380811"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1768.058, "latencies_ms": [1768.058], "images_per_second": 0.566, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " In the image, there are two giraffes standing in a grassy area, one of them eating grass while the other is standing near a fence.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.77, "peak": 115.6, "min": 32.91}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.31, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 30.31, "energy_joules_est": 53.61, "sample_count": 17, "duration_seconds": 1.769}, "timestamp": "2026-01-19T13:18:26.162458"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2611.986, "latencies_ms": [2611.986], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. giraffe: 2\n2. fence: 1\n3. grass: 1\n4. tree: 1\n5. tree trunk: 1\n6. tree leaves: 1\n7. tree branches: 1\n8. tree trunk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.74, "peak": 115.49, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 27.15, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 27.15, "energy_joules_est": 70.92, "sample_count": 26, "duration_seconds": 2.612}, "timestamp": "2026-01-19T13:18:28.871125"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2964.19, "latencies_ms": [2964.19], "images_per_second": 0.337, "prompt_tokens": 1117, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The giraffe on the left is positioned closer to the camera than the one on the right, which is farther away. The giraffe on the right is standing near the fence, while the one on the left is grazing near the fence. The giraffe on the right is also positioned in the background, while the one on the left is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.19, "peak": 129.57, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 25.5, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 25.5, "energy_joules_est": 75.6, "sample_count": 29, "duration_seconds": 2.965}, "timestamp": "2026-01-19T13:18:31.889582"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1697.594, "latencies_ms": [1697.594], "images_per_second": 0.589, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " Two giraffes are standing in a grassy area with a fence in the background, one eating grass and the other looking around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.32, "peak": 131.48, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.85, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.85, "energy_joules_est": 50.69, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T13:18:33.663369"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1813.332, "latencies_ms": [1813.332], "images_per_second": 0.551, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The giraffes are brown and white, and the grass is green. The giraffes are standing in a grassy area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.37, "peak": 128.57, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.81, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.81, "energy_joules_est": 54.07, "sample_count": 18, "duration_seconds": 1.814}, "timestamp": "2026-01-19T13:18:35.543618"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1858.725, "latencies_ms": [1858.725], "images_per_second": 0.538, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A suitcase and two bags are on the floor in front of a curtain.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.89, "peak": 127.06, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 16.26, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.07, "peak": 41.76, "min": 16.95}}, "power_watts_avg": 32.07, "energy_joules_est": 59.63, "sample_count": 18, "duration_seconds": 1.859}, "timestamp": "2026-01-19T13:18:37.425100"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1515.186, "latencies_ms": [1515.186], "images_per_second": 0.66, "prompt_tokens": 1446, "response_tokens_est": 5, "n_tiles": 1, "output_text": " suitcase: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 1.8, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.83, "peak": 120.5, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.79, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 35.16, "peak": 42.13, "min": 20.5}}, "power_watts_avg": 35.16, "energy_joules_est": 53.28, "sample_count": 15, "duration_seconds": 1.515}, "timestamp": "2026-01-19T13:18:38.995452"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2022.935, "latencies_ms": [2022.935], "images_per_second": 0.494, "prompt_tokens": 1450, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The suitcase is on the left, the bag is on the right, and the books are in the middle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.29, "peak": 120.38, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.26, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 33.58, "peak": 43.31, "min": 21.67}}, "power_watts_avg": 33.58, "energy_joules_est": 67.95, "sample_count": 20, "duration_seconds": 2.023}, "timestamp": "2026-01-19T13:18:41.080357"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1930.125, "latencies_ms": [1930.125], "images_per_second": 0.518, "prompt_tokens": 1444, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A black and white photo of a suitcase and a bag on the floor next to a curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.27, "peak": 114.51, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.76, "peak": 42.13, "min": 18.53}}, "power_watts_avg": 32.76, "energy_joules_est": 63.25, "sample_count": 19, "duration_seconds": 1.931}, "timestamp": "2026-01-19T13:18:43.062020"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1983.459, "latencies_ms": [1983.459], "images_per_second": 0.504, "prompt_tokens": 1442, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image is in black and white, the lighting is dim, and the material of the luggage is fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.8, "peak": 121.58, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.44, "peak": 42.15, "min": 19.32}}, "power_watts_avg": 32.44, "energy_joules_est": 64.35, "sample_count": 20, "duration_seconds": 1.984}, "timestamp": "2026-01-19T13:18:45.146321"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1935.261, "latencies_ms": [1935.261], "images_per_second": 0.517, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " In the image, a man wearing a blue shirt and an orange bandana is standing on a rocky trail in a forest, observing a group of people riding horses.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.03, "peak": 121.45, "min": 29.3}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.92, "min": 12.21}, "VDD_GPU": {"avg": 29.46, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.46, "energy_joules_est": 57.03, "sample_count": 19, "duration_seconds": 1.936}, "timestamp": "2026-01-19T13:18:47.139893"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2609.632, "latencies_ms": [2609.632], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. horse: 2\n3. saddle: 1\n4. saddle blanket: 1\n5. backpack: 1\n6. rock: 1\n7. tree: 1\n8. person's hand: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 128.57, "min": 30.26}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.96, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 26.96, "energy_joules_est": 70.37, "sample_count": 26, "duration_seconds": 2.61}, "timestamp": "2026-01-19T13:18:49.829329"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2079.412, "latencies_ms": [2079.412], "images_per_second": 0.481, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The man is standing to the left of the horses, which are positioned in the middle of the image. The horses are walking on a rocky trail that is located in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.26, "peak": 117.14, "min": 33.11}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.74, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.74, "energy_joules_est": 59.77, "sample_count": 20, "duration_seconds": 2.08}, "timestamp": "2026-01-19T13:18:51.915891"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1853.858, "latencies_ms": [1853.858], "images_per_second": 0.539, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A man wearing a blue shirt and an orange bandana is standing on a rocky trail in the woods, watching two horses with saddles on their backs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.56, "peak": 128.09, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 30.25, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 30.25, "energy_joules_est": 56.09, "sample_count": 18, "duration_seconds": 1.854}, "timestamp": "2026-01-19T13:18:53.788882"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1960.173, "latencies_ms": [1960.173], "images_per_second": 0.51, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is a photograph taken in a sunny forest with a blue sky and green trees. The ground is covered with rocks and dirt, and the horses are brown and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.45, "peak": 110.26, "min": 37.03}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.88, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 29.88, "energy_joules_est": 58.59, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T13:18:55.754678"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1400.862, "latencies_ms": [1400.862], "images_per_second": 0.714, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man is riding a horse with a blurred background of a building.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.36, "peak": 108.57, "min": 28.08}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 32.25, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 32.25, "energy_joules_est": 45.19, "sample_count": 14, "duration_seconds": 1.401}, "timestamp": "2026-01-19T13:18:57.221754"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2633.056, "latencies_ms": [2633.056], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. horse: 1\n3. saddle: 1\n4. bridle: 1\n5. reins: 1\n6. jacket: 1\n7. building: 1\n8. tent: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.39, "peak": 133.8, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.23, "peak": 40.97, "min": 17.34}}, "power_watts_avg": 27.23, "energy_joules_est": 71.71, "sample_count": 26, "duration_seconds": 2.634}, "timestamp": "2026-01-19T13:18:59.937354"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2480.916, "latencies_ms": [2480.916], "images_per_second": 0.403, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The man is positioned on the left side of the image, with the horse on the right side. The man is in the foreground, while the building is in the background. The horse is in the middle of the image, with the man on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.76, "peak": 123.09, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.06, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.06, "energy_joules_est": 67.14, "sample_count": 24, "duration_seconds": 2.481}, "timestamp": "2026-01-19T13:19:02.439111"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1387.209, "latencies_ms": [1387.209], "images_per_second": 0.721, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man is riding a horse in a black and white photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.59, "peak": 125.65, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.83, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.83, "energy_joules_est": 44.18, "sample_count": 14, "duration_seconds": 1.388}, "timestamp": "2026-01-19T13:19:03.904014"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1777.158, "latencies_ms": [1777.158], "images_per_second": 0.563, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image is in black and white, with the subject in motion, and the background is blurred, suggesting a fast-moving subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.27, "peak": 127.41, "min": 30.42}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.62, "peak": 40.97, "min": 19.31}}, "power_watts_avg": 30.62, "energy_joules_est": 54.43, "sample_count": 18, "duration_seconds": 1.778}, "timestamp": "2026-01-19T13:19:05.786779"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1475.451, "latencies_ms": [1475.451], "images_per_second": 0.678, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of geese are swimming in a pond surrounded by tall grass and trees.", "error": null, "sys_before": {"cpu_percent": 5.9, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.9, "peak": 120.07, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.1, "energy_joules_est": 45.91, "sample_count": 15, "duration_seconds": 1.476}, "timestamp": "2026-01-19T13:19:07.366797"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1425.783, "latencies_ms": [1425.783], "images_per_second": 0.701, "prompt_tokens": 1113, "response_tokens_est": 14, "n_tiles": 1, "output_text": " goose: 4\nwater: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.14, "peak": 127.77, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 32.42, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 32.42, "energy_joules_est": 46.24, "sample_count": 14, "duration_seconds": 1.426}, "timestamp": "2026-01-19T13:19:08.831698"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2147.703, "latencies_ms": [2147.703], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The geese are positioned in the middle of the pond, with the vegetation on the far side of the pond providing a natural backdrop. The geese are relatively close to the camera, while the vegetation is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.19, "peak": 129.16, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.55, "peak": 40.57, "min": 19.31}}, "power_watts_avg": 29.55, "energy_joules_est": 63.47, "sample_count": 21, "duration_seconds": 2.148}, "timestamp": "2026-01-19T13:19:11.020641"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1464.964, "latencies_ms": [1464.964], "images_per_second": 0.683, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of geese are swimming in a pond surrounded by tall grass and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.8, "peak": 122.43, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.38, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.38, "energy_joules_est": 45.98, "sample_count": 15, "duration_seconds": 1.465}, "timestamp": "2026-01-19T13:19:12.585114"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2455.648, "latencies_ms": [2455.648], "images_per_second": 0.407, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image features a serene scene of a group of geese swimming in a calm body of water, with the sun casting a warm glow on the scene. The geese are surrounded by lush green vegetation, and the water reflects the sunlight, creating a peaceful and tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.3, "ram_available_mb": 99772.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.5, "ram_available_mb": 99772.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.01, "peak": 124.34, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.44, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.83, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 27.83, "energy_joules_est": 68.35, "sample_count": 24, "duration_seconds": 2.456}, "timestamp": "2026-01-19T13:19:15.088588"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1722.52, "latencies_ms": [1722.52], "images_per_second": 0.581, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A cat is lying on the hood of a black Mercedes-Benz car, which is parked in front of a house.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.08, "peak": 124.33, "min": 27.17}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.15, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.15, "energy_joules_est": 51.96, "sample_count": 17, "duration_seconds": 1.723}, "timestamp": "2026-01-19T13:19:16.867887"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2517.078, "latencies_ms": [2517.078], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 1\n2. car: 1\n3. house: 1\n4. window: 1\n5. fence: 1\n6. plant: 1\n7. flower: 1\n8. reflection: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.5, "ram_available_mb": 99772.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.38, "peak": 130.37, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.29, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 27.29, "energy_joules_est": 68.7, "sample_count": 25, "duration_seconds": 2.517}, "timestamp": "2026-01-19T13:19:19.467188"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2085.7, "latencies_ms": [2085.7], "images_per_second": 0.479, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The cat is positioned to the left of the car, which is in the foreground of the image. The cat is resting on the hood of the car, which is in front of the house.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.85, "peak": 127.53, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.18, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.18, "energy_joules_est": 58.78, "sample_count": 21, "duration_seconds": 2.086}, "timestamp": "2026-01-19T13:19:21.654730"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1639.688, "latencies_ms": [1639.688], "images_per_second": 0.61, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A cat is laying on top of a black car, and the car is parked in front of a house.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.69, "peak": 122.91, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.49, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.49, "energy_joules_est": 50.01, "sample_count": 16, "duration_seconds": 1.64}, "timestamp": "2026-01-19T13:19:23.324253"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1907.071, "latencies_ms": [1907.071], "images_per_second": 0.524, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The car is black and the cat is orange and white. The car is parked in front of a house and the cat is laying on the hood of the car.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.88, "peak": 133.34, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.06, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.06, "energy_joules_est": 57.34, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T13:19:25.303989"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1794.005, "latencies_ms": [1794.005], "images_per_second": 0.557, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A snowboarder is captured mid-air against a clear blue sky, wearing a brown jacket, yellow pants, and a black helmet.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.36, "peak": 128.57, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 29.79, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.79, "energy_joules_est": 53.45, "sample_count": 18, "duration_seconds": 1.794}, "timestamp": "2026-01-19T13:19:27.179692"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2812.647, "latencies_ms": [2812.647], "images_per_second": 0.356, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " snowboard: 1, snowboarder: 1, snowboarder's pants: 1, snowboarder's jacket: 1, snowboarder's gloves: 1, snowboarder's helmet: 1, snowboarder's goggles: 1, snowboarder's boots: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.03, "peak": 119.75, "min": 34.89}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.47, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.47, "energy_joules_est": 74.46, "sample_count": 27, "duration_seconds": 2.813}, "timestamp": "2026-01-19T13:19:29.997308"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1983.969, "latencies_ms": [1983.969], "images_per_second": 0.504, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The snowboarder is in the foreground, jumping over a snow-covered slope. The snowboarder is in the middle of the image, with the sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.08, "peak": 125.81, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 12.86}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.94, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 28.94, "energy_joules_est": 57.42, "sample_count": 20, "duration_seconds": 1.984}, "timestamp": "2026-01-19T13:19:32.086605"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1518.98, "latencies_ms": [1518.98], "images_per_second": 0.658, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A snowboarder is performing a trick in the air against a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.51, "peak": 125.99, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.89, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.89, "energy_joules_est": 46.94, "sample_count": 15, "duration_seconds": 1.52}, "timestamp": "2026-01-19T13:19:33.652237"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1852.937, "latencies_ms": [1852.937], "images_per_second": 0.54, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The snowboarder is wearing a brown jacket and yellow pants, and the snow is white. The sky is blue and the snow is white.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.68, "peak": 124.13, "min": 29.77}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.49, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 30.49, "energy_joules_est": 56.5, "sample_count": 18, "duration_seconds": 1.853}, "timestamp": "2026-01-19T13:19:35.532335"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1657.759, "latencies_ms": [1657.759], "images_per_second": 0.603, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image depicts a small bathroom with a white bathtub, a white toilet, and a white pipe running along the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.06, "peak": 130.81, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.0, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.0, "energy_joules_est": 51.41, "sample_count": 16, "duration_seconds": 1.658}, "timestamp": "2026-01-19T13:19:37.208749"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2495.134, "latencies_ms": [2495.134], "images_per_second": 0.401, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. toilet: 1\n2. bathtub: 1\n3. pipes: 1\n4. door: 1\n5. towel: 1\n6. wall: 1\n7. floor: 1\n8. ceiling: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.28, "peak": 128.42, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 27.92, "peak": 40.16, "min": 19.31}}, "power_watts_avg": 27.92, "energy_joules_est": 69.68, "sample_count": 24, "duration_seconds": 2.496}, "timestamp": "2026-01-19T13:19:39.722407"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1974.569, "latencies_ms": [1974.569], "images_per_second": 0.506, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The toilet is located to the left of the bathtub, which is situated in the background. The towel is hanging from the pipes above the toilet, which are positioned near the bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.59, "peak": 123.05, "min": 37.41}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 29.42, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.42, "energy_joules_est": 58.11, "sample_count": 19, "duration_seconds": 1.975}, "timestamp": "2026-01-19T13:19:41.703655"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1397.17, "latencies_ms": [1397.17], "images_per_second": 0.716, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A bathroom with a toilet and a bathtub is shown in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.46, "peak": 116.03, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.77, "peak": 39.78, "min": 18.52}}, "power_watts_avg": 31.77, "energy_joules_est": 44.4, "sample_count": 14, "duration_seconds": 1.397}, "timestamp": "2026-01-19T13:19:43.164487"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1460.672, "latencies_ms": [1460.672], "images_per_second": 0.685, "prompt_tokens": 1110, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The bathroom is painted white with a brown floor, and the lighting is dim.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.09, "peak": 122.3, "min": 37.69}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.81, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 32.81, "energy_joules_est": 47.93, "sample_count": 14, "duration_seconds": 1.461}, "timestamp": "2026-01-19T13:19:44.628476"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1759.895, "latencies_ms": [1759.895], "images_per_second": 0.568, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " In the image, there is a statue of two people holding a kite, standing on a pedestal, with a building in the background.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.2, "peak": 133.51, "min": 27.57}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.97, "peak": 40.97, "min": 19.31}}, "power_watts_avg": 30.97, "energy_joules_est": 54.51, "sample_count": 18, "duration_seconds": 1.76}, "timestamp": "2026-01-19T13:19:46.496491"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2640.371, "latencies_ms": [2640.371], "images_per_second": 0.379, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. statue: 2\n2. kite: 1\n3. building: 1\n4. clouds: 1\n5. sky: 1\n6. ground: 1\n7. statue's hand: 1\n8. statue's foot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.13, "peak": 125.96, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.22}, "VDD_GPU": {"avg": 26.61, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 26.61, "energy_joules_est": 70.27, "sample_count": 26, "duration_seconds": 2.641}, "timestamp": "2026-01-19T13:19:49.198864"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2052.198, "latencies_ms": [2052.198], "images_per_second": 0.487, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the statue is in the background, standing on a pedestal. The statue is closer to the viewer than the kite.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.9, "peak": 116.31, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 28.68, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.68, "energy_joules_est": 58.87, "sample_count": 20, "duration_seconds": 2.053}, "timestamp": "2026-01-19T13:19:51.269689"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2495.89, "latencies_ms": [2495.89], "images_per_second": 0.401, "prompt_tokens": 1112, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image captures a scene of a kite flying high in the sky, with a statue of two people standing on a pedestal in the foreground. The statue is positioned on a corner of a building, with the kite soaring above it, creating a sense of height and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.68, "peak": 127.29, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.68, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.68, "energy_joules_est": 66.6, "sample_count": 25, "duration_seconds": 2.496}, "timestamp": "2026-01-19T13:19:53.868298"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2838.151, "latencies_ms": [2838.151], "images_per_second": 0.352, "prompt_tokens": 1110, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image features a statue of two people holding a kite, with the kite being the most prominent object in the scene. The kite is colorful and appears to be made of fabric, while the statue is made of metal and has a shiny, reflective surface. The sky is overcast, and the overall lighting is soft and diffused.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.88, "peak": 122.87, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.79, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 25.79, "energy_joules_est": 73.2, "sample_count": 28, "duration_seconds": 2.838}, "timestamp": "2026-01-19T13:19:56.777701"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2101.435, "latencies_ms": [2101.435], "images_per_second": 0.476, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a vibrant display of fresh produce, including a variety of fruits and vegetables such as strawberries, broccoli, radishes, carrots, and potatoes, all neatly arranged in a wooden crate.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.78, "peak": 115.46, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.12, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.12, "energy_joules_est": 59.12, "sample_count": 21, "duration_seconds": 2.102}, "timestamp": "2026-01-19T13:19:58.971807"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2718.066, "latencies_ms": [2718.066], "images_per_second": 0.368, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. broccoli: 4\n2. radishes: 12\n3. carrots: 4\n4. strawberries: 12\n5. peas: 12\n6. potatoes: 4\n7. asparagus: 1\n8. celery: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.61, "peak": 111.58, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.18, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.18, "energy_joules_est": 71.17, "sample_count": 27, "duration_seconds": 2.718}, "timestamp": "2026-01-19T13:20:01.768655"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2005.479, "latencies_ms": [2005.479], "images_per_second": 0.499, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The strawberries are located in the foreground, while the broccoli is situated in the middle ground. The carrots are positioned in the background, and the radishes are placed near the broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.32, "peak": 126.32, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.62, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.62, "energy_joules_est": 57.4, "sample_count": 20, "duration_seconds": 2.006}, "timestamp": "2026-01-19T13:20:03.839967"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2895.33, "latencies_ms": [2895.33], "images_per_second": 0.345, "prompt_tokens": 1111, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The image captures a vibrant display of fresh produce at a market stall, with a variety of fruits and vegetables arranged in a visually appealing manner. The stall is filled with an assortment of colorful fruits and vegetables, including strawberries, broccoli, radishes, carrots, and potatoes, all arranged in a way that highlights their freshness and abundance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.11, "peak": 116.49, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.96, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 25.96, "energy_joules_est": 75.17, "sample_count": 28, "duration_seconds": 2.896}, "timestamp": "2026-01-19T13:20:06.754334"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2414.789, "latencies_ms": [2414.789], "images_per_second": 0.414, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a vibrant display of fresh produce, with a variety of colors including red, green, and orange. The lighting is bright and natural, highlighting the freshness of the vegetables. The materials are organic and natural, with the produce appearing fresh and healthy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.11, "peak": 119.66, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.35, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.35, "energy_joules_est": 66.06, "sample_count": 24, "duration_seconds": 2.415}, "timestamp": "2026-01-19T13:20:09.236748"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1556.634, "latencies_ms": [1556.634], "images_per_second": 0.642, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Three people are sitting on a couch playing video games with a projector on top of the couch.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.86, "peak": 131.11, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.99, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.99, "energy_joules_est": 48.26, "sample_count": 15, "duration_seconds": 1.557}, "timestamp": "2026-01-19T13:20:10.808261"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1402.59, "latencies_ms": [1402.59], "images_per_second": 0.713, "prompt_tokens": 1113, "response_tokens_est": 14, "n_tiles": 1, "output_text": " projector: 1, couch: 1, person: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.42, "peak": 125.47, "min": 27.21}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 33.2, "peak": 40.57, "min": 20.49}}, "power_watts_avg": 33.2, "energy_joules_est": 46.58, "sample_count": 14, "duration_seconds": 1.403}, "timestamp": "2026-01-19T13:20:12.260570"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2869.924, "latencies_ms": [2869.924], "images_per_second": 0.348, "prompt_tokens": 1117, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The person on the left is in the foreground, the person in the middle is in the background, and the person on the right is in the foreground. The person on the left is closer to the camera than the person in the middle, and the person on the right is closer to the camera than the person in the middle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.63, "peak": 111.92, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.82, "peak": 40.57, "min": 18.52}}, "power_watts_avg": 26.82, "energy_joules_est": 76.98, "sample_count": 28, "duration_seconds": 2.87}, "timestamp": "2026-01-19T13:20:15.155444"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1335.177, "latencies_ms": [1335.177], "images_per_second": 0.749, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " Three people are sitting on a couch playing video games.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.41, "peak": 121.56, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.09, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 32.09, "energy_joules_est": 42.86, "sample_count": 13, "duration_seconds": 1.336}, "timestamp": "2026-01-19T13:20:16.508624"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1616.752, "latencies_ms": [1616.752], "images_per_second": 0.619, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is dimly lit with a projector on the bed, and the people are wearing casual clothes.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.83, "peak": 127.1, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.69, "peak": 41.34, "min": 22.07}}, "power_watts_avg": 32.69, "energy_joules_est": 52.86, "sample_count": 16, "duration_seconds": 1.617}, "timestamp": "2026-01-19T13:20:18.166961"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1734.679, "latencies_ms": [1734.679], "images_per_second": 0.576, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, a white sheep is resting on the grass near a tree, while a group of cows graze in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.11, "peak": 128.95, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.87, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 30.87, "energy_joules_est": 53.56, "sample_count": 17, "duration_seconds": 1.735}, "timestamp": "2026-01-19T13:20:19.939931"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2526.221, "latencies_ms": [2526.221], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. tree: 1\n2. grass: 1\n3. sheep: 2\n4. cow: 2\n5. cow: 2\n6. cow: 2\n7. cow: 2\n8. cow: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.88, "peak": 115.19, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.51, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 27.51, "energy_joules_est": 69.51, "sample_count": 25, "duration_seconds": 2.527}, "timestamp": "2026-01-19T13:20:22.523942"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1795.62, "latencies_ms": [1795.62], "images_per_second": 0.557, "prompt_tokens": 1117, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The sheep is in the foreground, lying on the grass near a tree. In the background, there are other sheep grazing in the field.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.1, "peak": 120.21, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.66, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.66, "energy_joules_est": 53.27, "sample_count": 18, "duration_seconds": 1.796}, "timestamp": "2026-01-19T13:20:24.396449"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2166.047, "latencies_ms": [2166.047], "images_per_second": 0.462, "prompt_tokens": 1111, "response_tokens_est": 42, "n_tiles": 1, "output_text": " In a serene rural setting, a white sheep lies peacefully on a lush green field, while a tree stands nearby. In the distance, other sheep and cows graze on the grass, adding to the tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.05, "peak": 106.38, "min": 31.58}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.63, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 28.63, "energy_joules_est": 62.03, "sample_count": 21, "duration_seconds": 2.166}, "timestamp": "2026-01-19T13:20:26.569039"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2349.562, "latencies_ms": [2349.562], "images_per_second": 0.426, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image features a pastoral scene with a lush green field, a tree with a rough bark, and a herd of cows grazing peacefully. The lighting is natural and soft, suggesting a sunny day, and the colors are vibrant and true to life.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.64, "peak": 129.83, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 27.85, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.85, "energy_joules_est": 65.44, "sample_count": 23, "duration_seconds": 2.35}, "timestamp": "2026-01-19T13:20:28.968242"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2446.581, "latencies_ms": [2446.581], "images_per_second": 0.409, "prompt_tokens": 1099, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image is a black and white photograph of a large group of boys sitting on the ground in front of a building, with the text \"GOODMAYES BOYS' SCHOOL APRIL 1929\" visible at the bottom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.77, "peak": 113.06, "min": 30.34}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.32, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.32, "energy_joules_est": 66.86, "sample_count": 24, "duration_seconds": 2.447}, "timestamp": "2026-01-19T13:20:31.469482"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3357.944, "latencies_ms": [3357.944], "images_per_second": 0.298, "prompt_tokens": 1113, "response_tokens_est": 87, "n_tiles": 1, "output_text": " 1. group of boys: 100\n2. boys' school: 10\n3. boys' school building: 1\n4. boys' school building windows: 1\n5. boys' school building doors: 1\n6. boys' school building roof: 1\n7. boys' school building chimney: 1\n8. boys' school building chimney roof: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.48, "peak": 122.51, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.03, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 25.03, "energy_joules_est": 84.06, "sample_count": 33, "duration_seconds": 3.358}, "timestamp": "2026-01-19T13:20:34.903890"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1935.146, "latencies_ms": [1935.146], "images_per_second": 0.517, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The boys are standing in front of the building, with the building being in the background. The boys are in the foreground, with the building being in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.57, "peak": 132.77, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.99, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.99, "energy_joules_est": 56.11, "sample_count": 19, "duration_seconds": 1.935}, "timestamp": "2026-01-19T13:20:36.883150"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1470.647, "latencies_ms": [1470.647], "images_per_second": 0.68, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A large group of boys are posing for a picture in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.27, "peak": 116.72, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.54, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 31.54, "energy_joules_est": 46.4, "sample_count": 15, "duration_seconds": 1.471}, "timestamp": "2026-01-19T13:20:38.442690"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2012.699, "latencies_ms": [2012.699], "images_per_second": 0.497, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is in black and white, and the lighting is even, with no shadows or highlights. The material of the photograph is paper, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.4, "peak": 127.45, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.39, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 29.39, "energy_joules_est": 59.16, "sample_count": 20, "duration_seconds": 2.013}, "timestamp": "2026-01-19T13:20:40.524398"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1676.174, "latencies_ms": [1676.174], "images_per_second": 0.597, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A colorful kite with a long tail is flying high in the sky, with a park and some buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.27, "peak": 110.78, "min": 28.18}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.01, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.01, "energy_joules_est": 50.32, "sample_count": 17, "duration_seconds": 1.677}, "timestamp": "2026-01-19T13:20:42.306282"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2609.404, "latencies_ms": [2609.404], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. Kite: 1\n2. Buildings: 1\n3. Trees: 2\n4. People: 1\n5. Clouds: 2\n6. Sky: 1\n7. Kite string: 1\n8. Grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.95, "peak": 123.8, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 26.84, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.84, "energy_joules_est": 70.05, "sample_count": 26, "duration_seconds": 2.61}, "timestamp": "2026-01-19T13:20:45.016218"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1909.64, "latencies_ms": [1909.64], "images_per_second": 0.524, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The kite is in the sky, which is above the buildings and trees. The kite is in the foreground, while the buildings and trees are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.54, "peak": 124.4, "min": 30.91}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.97, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.97, "energy_joules_est": 55.33, "sample_count": 19, "duration_seconds": 1.91}, "timestamp": "2026-01-19T13:20:46.994143"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1661.291, "latencies_ms": [1661.291], "images_per_second": 0.602, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A colorful kite with a long tail flies high in the sky over a park with trees and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.91, "peak": 108.38, "min": 30.5}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.78, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.78, "energy_joules_est": 51.14, "sample_count": 16, "duration_seconds": 1.662}, "timestamp": "2026-01-19T13:20:48.678621"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1837.775, "latencies_ms": [1837.775], "images_per_second": 0.544, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The kite is a vibrant rainbow of colors, with a long tail that trails behind it. The sky is a clear blue with fluffy white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.04, "peak": 107.99, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.85, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.42, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 30.42, "energy_joules_est": 55.92, "sample_count": 18, "duration_seconds": 1.838}, "timestamp": "2026-01-19T13:20:50.561039"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1539.72, "latencies_ms": [1539.72], "images_per_second": 0.649, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A large pizza with a generous amount of cheese and tomato sauce is sitting in a cardboard box.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.89, "peak": 114.99, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.65, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.65, "energy_joules_est": 48.75, "sample_count": 15, "duration_seconds": 1.54}, "timestamp": "2026-01-19T13:20:52.140338"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2647.472, "latencies_ms": [2647.472], "images_per_second": 0.378, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. pizza: 1\n2. cardboard box: 1\n3. pizza crust: 1\n4. cheese: 1\n5. sauce: 1\n6. pepperoni: 1\n7. black pepper: 1\n8. sauce can: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.64, "peak": 125.6, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.43, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.43, "energy_joules_est": 72.63, "sample_count": 26, "duration_seconds": 2.648}, "timestamp": "2026-01-19T13:20:54.846700"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2010.495, "latencies_ms": [2010.495], "images_per_second": 0.497, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The pizza is in the foreground, with the cardboard box in the background. The pizza is on the left side of the box, and the box is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.57, "peak": 119.75, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.58, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.58, "energy_joules_est": 57.47, "sample_count": 20, "duration_seconds": 2.011}, "timestamp": "2026-01-19T13:20:56.935068"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1398.174, "latencies_ms": [1398.174], "images_per_second": 0.715, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A pizza with cheese and tomato sauce is in a cardboard box.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.42, "peak": 127.01, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.74, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.74, "energy_joules_est": 44.39, "sample_count": 14, "duration_seconds": 1.399}, "timestamp": "2026-01-19T13:20:58.392187"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1791.367, "latencies_ms": [1791.367], "images_per_second": 0.558, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The pizza is in a cardboard box with a white and red color scheme. The lighting is dim and the pizza is in a dark environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.75, "peak": 126.61, "min": 27.61}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.91, "peak": 40.97, "min": 19.7}}, "power_watts_avg": 30.91, "energy_joules_est": 55.38, "sample_count": 18, "duration_seconds": 1.792}, "timestamp": "2026-01-19T13:21:00.266900"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1866.987, "latencies_ms": [1866.987], "images_per_second": 0.536, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A woman in a brown jacket is sitting on the edge of an open refrigerator, talking on her cell phone, while another woman sits on the sidewalk nearby.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.03, "peak": 119.58, "min": 34.19}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.92, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.92, "energy_joules_est": 55.88, "sample_count": 18, "duration_seconds": 1.868}, "timestamp": "2026-01-19T13:21:02.148991"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2507.786, "latencies_ms": [2507.786], "images_per_second": 0.399, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 2\n2. jacket: 1\n3. pants: 1\n4. shoes: 1\n5. refrigerator: 1\n6. cup: 2\n7. glass: 1\n8. door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.58, "peak": 128.08, "min": 27.3}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.48, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.48, "energy_joules_est": 68.92, "sample_count": 25, "duration_seconds": 2.508}, "timestamp": "2026-01-19T13:21:04.739588"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.172, "latencies_ms": [2395.172], "images_per_second": 0.418, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The woman sitting on the refrigerator is positioned to the right of the woman sitting on the bench, with the refrigerator being in the foreground and the bench being in the background. The woman on the bench is closer to the camera than the woman on the refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.68, "peak": 115.82, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.24, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.24, "energy_joules_est": 65.25, "sample_count": 24, "duration_seconds": 2.395}, "timestamp": "2026-01-19T13:21:07.237371"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1374.608, "latencies_ms": [1374.608], "images_per_second": 0.727, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A woman sits in an open refrigerator on a city street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.4, "peak": 121.7, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.35, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.35, "energy_joules_est": 43.1, "sample_count": 14, "duration_seconds": 1.375}, "timestamp": "2026-01-19T13:21:08.694237"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2521.094, "latencies_ms": [2521.094], "images_per_second": 0.397, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image depicts a woman sitting in an open refrigerator on a street, with a brown jacket and blue jeans. The refrigerator is white and has a yellow interior. The weather appears to be overcast, and the street is wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.29, "peak": 122.35, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.22, "peak": 40.57, "min": 17.34}}, "power_watts_avg": 27.22, "energy_joules_est": 68.63, "sample_count": 25, "duration_seconds": 2.521}, "timestamp": "2026-01-19T13:21:11.298589"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1696.793, "latencies_ms": [1696.793], "images_per_second": 0.589, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man wearing a straw hat and a green shirt is holding a tray of hot dogs with a red sauce on top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.44, "peak": 126.17, "min": 30.43}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.48, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 29.48, "energy_joules_est": 50.04, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T13:21:13.081931"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2568.377, "latencies_ms": [2568.377], "images_per_second": 0.389, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. man: 1\n2. hat: 1\n3. shirt: 1\n4. chair: 1\n5. hotdogs: 12\n6. foil: 1\n7. grass: 1\n8. white: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.14, "peak": 119.73, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.07, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.07, "energy_joules_est": 69.53, "sample_count": 25, "duration_seconds": 2.569}, "timestamp": "2026-01-19T13:21:15.689320"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2145.706, "latencies_ms": [2145.706], "images_per_second": 0.466, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man is standing in the foreground, wearing a straw hat and a green shirt. The hot dogs are placed on a tray in the middle of the image, and the white chair is located in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.25, "peak": 121.9, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.35, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.35, "energy_joules_est": 60.84, "sample_count": 21, "duration_seconds": 2.146}, "timestamp": "2026-01-19T13:21:17.869152"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1698.088, "latencies_ms": [1698.088], "images_per_second": 0.589, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man wearing a straw hat and a green shirt is standing in a grassy area and looking at a tray of hot dogs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.11, "peak": 122.7, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.76, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 29.76, "energy_joules_est": 50.54, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T13:21:19.644311"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2222.763, "latencies_ms": [2222.763], "images_per_second": 0.45, "prompt_tokens": 1110, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features a man wearing a straw hat and a green shirt, standing outdoors on a sunny day. The hot dogs are placed on a tray covered with aluminum foil, and the man is holding a tray of hot dogs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.56, "peak": 131.46, "min": 32.04}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.13, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.13, "energy_joules_est": 62.54, "sample_count": 22, "duration_seconds": 2.223}, "timestamp": "2026-01-19T13:21:21.922156"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1832.042, "latencies_ms": [1832.042], "images_per_second": 0.546, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a room with a desk, a chair, a bookshelf, and a couch, all of which are cluttered with various items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.11, "peak": 120.92, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.75, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.75, "energy_joules_est": 54.53, "sample_count": 18, "duration_seconds": 1.833}, "timestamp": "2026-01-19T13:21:23.802628"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2097.059, "latencies_ms": [2097.059], "images_per_second": 0.477, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " chair: 1, laptop: 1, bookshelf: 1, books: 1, blanket: 1, couch: 1, wall: 1, star: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.32, "peak": 112.68, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.67, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.67, "energy_joules_est": 60.14, "sample_count": 21, "duration_seconds": 2.098}, "timestamp": "2026-01-19T13:21:25.989497"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2291.555, "latencies_ms": [2291.555], "images_per_second": 0.436, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The laptop is on the left side of the room, the bookshelf is in the middle, and the couch is on the right side. The bookshelf is closer to the camera than the laptop, and the couch is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.07, "peak": 124.55, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.53, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.53, "energy_joules_est": 63.1, "sample_count": 23, "duration_seconds": 2.292}, "timestamp": "2026-01-19T13:21:28.387059"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1583.371, "latencies_ms": [1583.371], "images_per_second": 0.632, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A room with a blue couch, a bookshelf, and a desk with a laptop on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.93, "peak": 119.49, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.34, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.34, "energy_joules_est": 48.06, "sample_count": 16, "duration_seconds": 1.584}, "timestamp": "2026-01-19T13:21:30.057235"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1920.085, "latencies_ms": [1920.085], "images_per_second": 0.521, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The room is lit by a single light bulb and has a warm yellow light. The walls are painted a light beige color and the furniture is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.28, "peak": 129.1, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.69, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 29.69, "energy_joules_est": 57.02, "sample_count": 19, "duration_seconds": 1.921}, "timestamp": "2026-01-19T13:21:32.043684"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1844.784, "latencies_ms": [1844.784], "images_per_second": 0.542, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " In the image, two elephants are seen in a grassy field, their trunks intertwined as they face each other, with a bird flying in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.24, "peak": 127.18, "min": 30.31}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.79, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.79, "energy_joules_est": 54.98, "sample_count": 18, "duration_seconds": 1.846}, "timestamp": "2026-01-19T13:21:33.931794"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1946.234, "latencies_ms": [1946.234], "images_per_second": 0.514, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " elephant: 2, grass: 1, tree: 1, bird: 1, bush: 1, hill: 1, sky: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.77, "peak": 123.59, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.67, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.67, "energy_joules_est": 57.75, "sample_count": 19, "duration_seconds": 1.947}, "timestamp": "2026-01-19T13:21:35.904690"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3133.879, "latencies_ms": [3133.879], "images_per_second": 0.319, "prompt_tokens": 1117, "response_tokens_est": 79, "n_tiles": 1, "output_text": " The two elephants are positioned in the foreground of the image, with the one on the left slightly closer to the camera than the one on the right. The elephants are facing each other, with the one on the left appearing to be slightly larger in size. The background of the image features a hazy, misty landscape with a few trees and bushes, providing a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.64, "peak": 114.46, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 25.7, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 25.7, "energy_joules_est": 80.55, "sample_count": 31, "duration_seconds": 3.134}, "timestamp": "2026-01-19T13:21:39.124694"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2351.934, "latencies_ms": [2351.934], "images_per_second": 0.425, "prompt_tokens": 1111, "response_tokens_est": 49, "n_tiles": 1, "output_text": " In the heart of a verdant savanna, two majestic elephants engage in a tender moment of affection. The elephants, their skin a rich brown, stand side by side in the lush green grass, their trunks intertwined in a display of affection.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.24, "peak": 126.56, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.34, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.34, "energy_joules_est": 64.31, "sample_count": 23, "duration_seconds": 2.352}, "timestamp": "2026-01-19T13:21:41.521990"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1503.315, "latencies_ms": [1503.315], "images_per_second": 0.665, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The elephants are brown, the grass is green, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.52, "peak": 114.74, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.15, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.15, "energy_joules_est": 46.84, "sample_count": 15, "duration_seconds": 1.504}, "timestamp": "2026-01-19T13:21:43.086901"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1786.353, "latencies_ms": [1786.353], "images_per_second": 0.56, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A shirtless man wearing a baseball cap and sunglasses is holding a frisbee and a bottle of beer while standing on a grassy field.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.76, "peak": 118.59, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.49, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.49, "energy_joules_est": 54.48, "sample_count": 18, "duration_seconds": 1.787}, "timestamp": "2026-01-19T13:21:44.972548"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2593.639, "latencies_ms": [2593.639], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. man: 1\n2. cap: 1\n3. shirt: 1\n4. shorts: 1\n5. frisbee: 1\n6. bottle: 1\n7. grass: 1\n8. trees: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.89, "peak": 109.61, "min": 27.13}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.72, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.72, "energy_joules_est": 69.31, "sample_count": 26, "duration_seconds": 2.594}, "timestamp": "2026-01-19T13:21:47.669143"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2446.833, "latencies_ms": [2446.833], "images_per_second": 0.409, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The man is in the foreground, holding a frisbee and a bottle, while the other man is in the background, walking on the field. The frisbee is held in the man's right hand, and the bottle is in his left hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.74, "peak": 110.07, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 26.94, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.94, "energy_joules_est": 65.92, "sample_count": 24, "duration_seconds": 2.447}, "timestamp": "2026-01-19T13:21:50.168846"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1567.358, "latencies_ms": [1567.358], "images_per_second": 0.638, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A shirtless man is playing frisbee in a grassy field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.67, "peak": 124.21, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.53, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.53, "energy_joules_est": 47.87, "sample_count": 16, "duration_seconds": 1.568}, "timestamp": "2026-01-19T13:21:51.838665"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1990.12, "latencies_ms": [1990.12], "images_per_second": 0.502, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a man in a grassy field, wearing a white baseball cap, sunglasses, and khaki shorts. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.23, "peak": 124.74, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.06, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 29.06, "energy_joules_est": 57.85, "sample_count": 20, "duration_seconds": 1.991}, "timestamp": "2026-01-19T13:21:53.923313"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1922.718, "latencies_ms": [1922.718], "images_per_second": 0.52, "prompt_tokens": 1100, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A young boy wearing a blue sports jersey is cutting a large chocolate cake decorated with red and yellow icing and chocolate logs on a table with a colorful tablecloth.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.61, "peak": 126.83, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.01, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.01, "energy_joules_est": 55.79, "sample_count": 19, "duration_seconds": 1.923}, "timestamp": "2026-01-19T13:21:55.903286"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2546.11, "latencies_ms": [2546.11], "images_per_second": 0.393, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. boy: 1\n2. knife: 1\n3. cake: 1\n4. tablecloth: 1\n5. plate: 1\n6. toy: 1\n7. candle: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.78, "peak": 119.99, "min": 30.34}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.98, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.98, "energy_joules_est": 68.72, "sample_count": 25, "duration_seconds": 2.547}, "timestamp": "2026-01-19T13:21:58.515338"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2130.466, "latencies_ms": [2130.466], "images_per_second": 0.469, "prompt_tokens": 1118, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The boy is in the foreground, leaning over the table with a knife in his hand. The cake is in the middle of the table, and the plate is on the right side of the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.13, "peak": 128.85, "min": 28.44}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.18, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.18, "energy_joules_est": 60.04, "sample_count": 21, "duration_seconds": 2.131}, "timestamp": "2026-01-19T13:22:00.694665"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1811.993, "latencies_ms": [1811.993], "images_per_second": 0.552, "prompt_tokens": 1112, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A young boy wearing a blue sports jersey is cutting a large chocolate cake shaped like a log on a table covered with a colorful tablecloth.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26000.2, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26000.1, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.13, "peak": 109.6, "min": 28.44}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.0, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.0, "energy_joules_est": 52.57, "sample_count": 18, "duration_seconds": 1.813}, "timestamp": "2026-01-19T13:22:02.577633"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1935.084, "latencies_ms": [1935.084], "images_per_second": 0.517, "prompt_tokens": 1110, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a room with a brown wall and a table covered with a colorful tablecloth. The lighting is natural, coming from a window out of frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.1, "ram_available_mb": 99772.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.08, "peak": 121.0, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.26, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.26, "energy_joules_est": 56.64, "sample_count": 19, "duration_seconds": 1.936}, "timestamp": "2026-01-19T13:22:04.558614"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2097.799, "latencies_ms": [2097.799], "images_per_second": 0.477, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a close-up view of a zebra's face, showcasing its distinctive black and white stripes, with a blurred background that hints at the presence of other zebras and a natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.11, "peak": 121.0, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.57, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.57, "energy_joules_est": 59.95, "sample_count": 21, "duration_seconds": 2.098}, "timestamp": "2026-01-19T13:22:06.753835"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1967.259, "latencies_ms": [1967.259], "images_per_second": 0.508, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " zebra: 2, eye: 1, nose: 1, ear: 1, mouth: 1, tail: 1, fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.98, "peak": 120.18, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.13, "peak": 40.18, "min": 15.38}}, "power_watts_avg": 29.13, "energy_joules_est": 57.31, "sample_count": 19, "duration_seconds": 1.968}, "timestamp": "2026-01-19T13:22:08.737546"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2204.296, "latencies_ms": [2204.296], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The zebra's head is in the foreground, with its body partially visible in the background. The zebra's eye is in the upper left corner of the image, while the zebra's nose is in the lower right corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 127.44, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.35, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.35, "energy_joules_est": 62.5, "sample_count": 22, "duration_seconds": 2.205}, "timestamp": "2026-01-19T13:22:11.038379"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2259.471, "latencies_ms": [2259.471], "images_per_second": 0.443, "prompt_tokens": 1111, "response_tokens_est": 45, "n_tiles": 1, "output_text": " In the image, there are two zebras standing close to each other, with one zebra's face prominently displayed in the foreground. The zebras are in a zoo enclosure, with a white fence separating them from the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.69, "peak": 127.02, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.79, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.79, "energy_joules_est": 62.81, "sample_count": 22, "duration_seconds": 2.26}, "timestamp": "2026-01-19T13:22:13.328136"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2726.87, "latencies_ms": [2726.87], "images_per_second": 0.367, "prompt_tokens": 1109, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The image features a close-up of a zebra's face, with its distinctive black and white stripes standing out against the natural colors of the environment. The lighting is natural and soft, casting gentle shadows on the zebra's face, and the zebra appears to be in a natural setting, possibly a zoo or wildlife reserve.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.19, "peak": 118.83, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.53, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.53, "energy_joules_est": 72.35, "sample_count": 27, "duration_seconds": 2.727}, "timestamp": "2026-01-19T13:22:16.136136"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1842.99, "latencies_ms": [1842.99], "images_per_second": 0.543, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The black and white photo captures a train station with a sign that reads \"La Spezia Centrale\" and a train on the tracks.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.03, "peak": 134.1, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 29.4, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.4, "energy_joules_est": 54.2, "sample_count": 18, "duration_seconds": 1.844}, "timestamp": "2026-01-19T13:22:18.014767"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2571.088, "latencies_ms": [2571.088], "images_per_second": 0.389, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sign: 1\n2. train: 1\n3. platform: 1\n4. bench: 1\n5. train tracks: 2\n6. train station: 1\n7. mountain: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.2, "peak": 110.19, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.4, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 27.4, "energy_joules_est": 70.45, "sample_count": 25, "duration_seconds": 2.571}, "timestamp": "2026-01-19T13:22:20.614342"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2020.266, "latencies_ms": [2020.266], "images_per_second": 0.495, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The train station platform is located in the foreground, with the train tracks extending into the background. The sign is positioned above the platform, indicating the direction to the center of the station.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.66, "peak": 122.06, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.84, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.84, "energy_joules_est": 58.27, "sample_count": 20, "duration_seconds": 2.021}, "timestamp": "2026-01-19T13:22:22.695411"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1640.842, "latencies_ms": [1640.842], "images_per_second": 0.609, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A black and white photo of a train station with a sign that says La Spezia Centrale.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.31, "peak": 130.78, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.85, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.85, "energy_joules_est": 50.63, "sample_count": 16, "duration_seconds": 1.641}, "timestamp": "2026-01-19T13:22:24.360925"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2076.768, "latencies_ms": [2076.768], "images_per_second": 0.482, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image is in black and white, with the station platform and train tracks being the main focus. The lighting is natural, coming from the sky, and the materials are mostly concrete and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.63, "peak": 114.52, "min": 36.0}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.55, "peak": 40.16, "min": 19.31}}, "power_watts_avg": 29.55, "energy_joules_est": 61.38, "sample_count": 20, "duration_seconds": 2.077}, "timestamp": "2026-01-19T13:22:26.443732"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1584.662, "latencies_ms": [1584.662], "images_per_second": 0.631, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person is sitting on a red surfboard in the ocean, with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.83, "peak": 116.99, "min": 28.05}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.1, "energy_joules_est": 49.3, "sample_count": 16, "duration_seconds": 1.585}, "timestamp": "2026-01-19T13:22:28.112464"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2555.914, "latencies_ms": [2555.914], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. ocean: 1\n4. sky: 1\n5. clouds: 1\n6. sun: 1\n7. waves: 1\n8. water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.93, "peak": 119.76, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.34, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.34, "energy_joules_est": 69.89, "sample_count": 25, "duration_seconds": 2.556}, "timestamp": "2026-01-19T13:22:30.714877"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2001.093, "latencies_ms": [2001.093], "images_per_second": 0.5, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The person is sitting on the surfboard, which is in the foreground of the image. The ocean is in the background, and the sky is above the person and the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.43, "peak": 117.14, "min": 27.74}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.84, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.84, "energy_joules_est": 57.73, "sample_count": 20, "duration_seconds": 2.002}, "timestamp": "2026-01-19T13:22:32.798171"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1490.593, "latencies_ms": [1490.593], "images_per_second": 0.671, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person is sitting on a red surfboard in the ocean at sunset.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.28, "peak": 112.84, "min": 27.45}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.94, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.94, "energy_joules_est": 46.13, "sample_count": 15, "duration_seconds": 1.491}, "timestamp": "2026-01-19T13:22:34.359894"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3008.9, "latencies_ms": [3008.9], "images_per_second": 0.332, "prompt_tokens": 1109, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The image captures a solitary figure clad in a black wetsuit, poised on a vibrant red surfboard, as they gaze out at the tranquil ocean. The sky above is a canvas of dark clouds, casting an ominous shadow over the scene, while the sun's warm glow casts a golden hue on the water's surface, creating a striking contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.8, "peak": 122.92, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.23, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 26.23, "energy_joules_est": 78.93, "sample_count": 29, "duration_seconds": 3.009}, "timestamp": "2026-01-19T13:22:37.381216"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1555.567, "latencies_ms": [1555.567], "images_per_second": 0.643, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man and a woman are sitting at a table on a train, eating sushi and other food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.76, "peak": 124.23, "min": 36.46}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.18, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.18, "energy_joules_est": 48.52, "sample_count": 15, "duration_seconds": 1.556}, "timestamp": "2026-01-19T13:22:38.944893"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2527.469, "latencies_ms": [2527.469], "images_per_second": 0.396, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. woman: 1\n3. chopsticks: 2\n4. tray: 1\n5. food: 1\n6. seat: 1\n7. window: 1\n8. bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.23, "peak": 120.54, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.7, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 27.7, "energy_joules_est": 70.02, "sample_count": 25, "duration_seconds": 2.528}, "timestamp": "2026-01-19T13:22:41.548238"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2586.862, "latencies_ms": [2586.862], "images_per_second": 0.387, "prompt_tokens": 1118, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The man is sitting on the left side of the image, while the woman is on the right side. The tray of food is in the middle of the image, and the man is holding chopsticks in his right hand. The woman is holding a bag of chips in her left hand.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.68, "peak": 129.96, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.82, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.82, "energy_joules_est": 69.39, "sample_count": 25, "duration_seconds": 2.587}, "timestamp": "2026-01-19T13:22:44.148679"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1595.96, "latencies_ms": [1595.96], "images_per_second": 0.627, "prompt_tokens": 1112, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A couple is enjoying a meal on a train, with a window in the background showing a cityscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.94, "peak": 123.29, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.11, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 30.11, "energy_joules_est": 48.06, "sample_count": 16, "duration_seconds": 1.596}, "timestamp": "2026-01-19T13:22:45.825655"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1967.984, "latencies_ms": [1967.984], "images_per_second": 0.508, "prompt_tokens": 1110, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken in a well-lit train carriage with natural light coming through the windows. The colors in the image are vibrant and the materials are mostly plastic and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.54, "peak": 129.3, "min": 33.47}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.59, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.59, "energy_joules_est": 58.24, "sample_count": 19, "duration_seconds": 1.968}, "timestamp": "2026-01-19T13:22:47.802086"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1802.375, "latencies_ms": [1802.375], "images_per_second": 0.555, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " Two men are walking down the street at night, one in a white shirt and black tie, and the other in a pink shirt and black tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.42, "peak": 129.79, "min": 28.28}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 30.14, "peak": 39.78, "min": 18.52}}, "power_watts_avg": 30.14, "energy_joules_est": 54.35, "sample_count": 18, "duration_seconds": 1.803}, "timestamp": "2026-01-19T13:22:49.686087"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2522.267, "latencies_ms": [2522.267], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 2\n2. tie: 1\n3. shirt: 1\n4. pants: 2\n5. shoes: 2\n6. building: 1\n7. street: 1\n8. sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.31, "peak": 128.64, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.14, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.14, "energy_joules_est": 68.46, "sample_count": 25, "duration_seconds": 2.523}, "timestamp": "2026-01-19T13:22:52.283163"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2573.567, "latencies_ms": [2573.567], "images_per_second": 0.389, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The two men are standing on the sidewalk, with the man on the left being closer to the camera and the man on the right being farther away. The man on the left is standing in front of the man on the right, and the man on the right is standing on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.0, "peak": 123.8, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.89, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.89, "energy_joules_est": 69.22, "sample_count": 25, "duration_seconds": 2.574}, "timestamp": "2026-01-19T13:22:54.888321"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1303.306, "latencies_ms": [1303.306], "images_per_second": 0.767, "prompt_tokens": 1111, "response_tokens_est": 10, "n_tiles": 1, "output_text": " Two men are walking down a street at night.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.21, "peak": 120.49, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.3, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 32.3, "energy_joules_est": 42.11, "sample_count": 13, "duration_seconds": 1.304}, "timestamp": "2026-01-19T13:22:56.244563"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2005.972, "latencies_ms": [2005.972], "images_per_second": 0.499, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken at night, with the two men walking on the sidewalk. The street is lit by streetlights and the building behind them is lit by a yellow light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.53, "peak": 111.64, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.1, "peak": 40.97, "min": 18.52}}, "power_watts_avg": 30.1, "energy_joules_est": 60.39, "sample_count": 20, "duration_seconds": 2.006}, "timestamp": "2026-01-19T13:22:58.328289"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1879.937, "latencies_ms": [1879.937], "images_per_second": 0.532, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man in glasses and a gray sweater is pouring wine into a glass at a bar, while a person in a blue shirt is holding a wine glass.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.92, "peak": 110.16, "min": 27.88}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.09, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.09, "energy_joules_est": 54.7, "sample_count": 19, "duration_seconds": 1.88}, "timestamp": "2026-01-19T13:23:00.306130"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2741.969, "latencies_ms": [2741.969], "images_per_second": 0.365, "prompt_tokens": 1114, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. person: 1\n2. glasses: 2\n3. wine bottle: 1\n4. wine glass: 1\n5. wine rack: 1\n6. wooden table: 1\n7. wine glass holder: 1\n8. wine bottle holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.76, "peak": 128.19, "min": 30.22}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.35, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.35, "energy_joules_est": 72.26, "sample_count": 27, "duration_seconds": 2.742}, "timestamp": "2026-01-19T13:23:03.112839"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2376.295, "latencies_ms": [2376.295], "images_per_second": 0.421, "prompt_tokens": 1118, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, with the wine glass in his hand on the right side. The wine bottle is placed on the counter in front of him, while the glasses are positioned on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.93, "peak": 122.53, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.46, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.46, "energy_joules_est": 65.26, "sample_count": 23, "duration_seconds": 2.377}, "timestamp": "2026-01-19T13:23:05.507729"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1856.315, "latencies_ms": [1856.315], "images_per_second": 0.539, "prompt_tokens": 1112, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A man in glasses and a gray sweater is standing behind a bar, pouring wine into a glass. There are wine bottles on the shelves behind him.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26000.1, "ram_available_mb": 99772.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.86, "peak": 129.49, "min": 41.7}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.39, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.39, "energy_joules_est": 54.56, "sample_count": 18, "duration_seconds": 1.857}, "timestamp": "2026-01-19T13:23:07.373570"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1709.468, "latencies_ms": [1709.468], "images_per_second": 0.585, "prompt_tokens": 1110, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with warm lighting, and the wooden wine rack is filled with bottles of wine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.1, "ram_available_mb": 99772.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.07, "peak": 132.96, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 30.73, "peak": 39.78, "min": 18.53}}, "power_watts_avg": 30.73, "energy_joules_est": 52.54, "sample_count": 17, "duration_seconds": 1.71}, "timestamp": "2026-01-19T13:23:09.138078"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1785.275, "latencies_ms": [1785.275], "images_per_second": 0.56, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A tennis player is in the middle of a powerful swing with his racket, attempting to hit a tennis ball that is in the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.96, "peak": 124.71, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.87, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 29.87, "energy_joules_est": 53.35, "sample_count": 18, "duration_seconds": 1.786}, "timestamp": "2026-01-19T13:23:11.021940"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2810.233, "latencies_ms": [2810.233], "images_per_second": 0.356, "prompt_tokens": 1114, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. tennis racket: 1\n2. tennis ball: 1\n3. tennis player: 1\n4. grass court: 1\n5. white lines: 1\n6. white shorts: 1\n7. white shirt: 1\n8. white wristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.45, "peak": 116.77, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.13, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 26.13, "energy_joules_est": 73.44, "sample_count": 28, "duration_seconds": 2.811}, "timestamp": "2026-01-19T13:23:13.915896"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2773.925, "latencies_ms": [2773.925], "images_per_second": 0.361, "prompt_tokens": 1118, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, with the tennis ball in the background. The player is holding the tennis racket in his right hand, and the ball is in his left hand. The player is positioned to the left of the image, and the ball is to the right of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.47, "peak": 113.35, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.05, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.05, "energy_joules_est": 72.27, "sample_count": 27, "duration_seconds": 2.774}, "timestamp": "2026-01-19T13:23:16.721885"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1665.574, "latencies_ms": [1665.574], "images_per_second": 0.6, "prompt_tokens": 1112, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A tennis player is playing on a grass court, wearing white clothes and holding a blue and green tennis racket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.0, "peak": 117.9, "min": 33.33}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.96, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.96, "energy_joules_est": 49.91, "sample_count": 16, "duration_seconds": 1.666}, "timestamp": "2026-01-19T13:23:18.393209"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2643.153, "latencies_ms": [2643.153], "images_per_second": 0.378, "prompt_tokens": 1110, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image captures a dynamic moment on a grass court, where a tennis player is in the midst of a powerful swing with his racket. The vibrant green of the grass contrasts with the player's white attire, and the bright blue of the tennis ball stands out against the backdrop of the overcast sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.9, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.77, "peak": 130.02, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.13, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.13, "energy_joules_est": 71.72, "sample_count": 26, "duration_seconds": 2.643}, "timestamp": "2026-01-19T13:23:21.102340"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1504.529, "latencies_ms": [1504.529], "images_per_second": 0.665, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A white and orange cat is walking on a wooden shelf in front of a television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.99, "peak": 112.27, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.89, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.89, "energy_joules_est": 46.48, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T13:23:22.675070"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2599.41, "latencies_ms": [2599.41], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. television: 1\n2. cat: 2\n3. shelf: 1\n4. bookshelf: 1\n5. cup: 1\n6. remote: 1\n7. digital clock: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.82, "peak": 125.94, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.22, "peak": 40.57, "min": 16.95}}, "power_watts_avg": 27.22, "energy_joules_est": 70.77, "sample_count": 26, "duration_seconds": 2.6}, "timestamp": "2026-01-19T13:23:25.379575"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2314.011, "latencies_ms": [2314.011], "images_per_second": 0.432, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The cat is in the foreground, on the right side of the TV, and the TV is on the left side of the shelf. The shelf is in the middle of the room, and the cat is in front of the TV.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.85, "peak": 127.2, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.32, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 27.32, "energy_joules_est": 63.23, "sample_count": 23, "duration_seconds": 2.314}, "timestamp": "2026-01-19T13:23:27.771244"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1535.927, "latencies_ms": [1535.927], "images_per_second": 0.651, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A cat is on top of a TV stand, and a TV is on the stand.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.37, "peak": 125.83, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.99, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.99, "energy_joules_est": 47.61, "sample_count": 15, "duration_seconds": 1.536}, "timestamp": "2026-01-19T13:23:29.340340"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1665.593, "latencies_ms": [1665.593], "images_per_second": 0.6, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The room is well lit with natural light coming in from the window, and the cat is white with orange stripes.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.98, "peak": 116.39, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.96, "peak": 40.18, "min": 19.7}}, "power_watts_avg": 30.96, "energy_joules_est": 51.58, "sample_count": 17, "duration_seconds": 1.666}, "timestamp": "2026-01-19T13:23:31.112144"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2248.185, "latencies_ms": [2248.185], "images_per_second": 0.445, "prompt_tokens": 1099, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features a street sign with a blue circle and a bicycle symbol, and a white sign with Chinese characters, all mounted on a metal pole against a backdrop of a clear blue sky with a few trees in the background.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.3, "peak": 110.23, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 28.1, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.1, "energy_joules_est": 63.19, "sample_count": 22, "duration_seconds": 2.249}, "timestamp": "2026-01-19T13:23:33.411542"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1974.313, "latencies_ms": [1974.313], "images_per_second": 0.507, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. signpost\n2. blue circle\n3. white sign\n4. tree\n5. bicycle\n6. road\n7. sky\n8. sun", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.59, "peak": 108.26, "min": 32.49}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.36, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.36, "energy_joules_est": 57.97, "sample_count": 19, "duration_seconds": 1.975}, "timestamp": "2026-01-19T13:23:35.389046"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2326.487, "latencies_ms": [2326.487], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The sign is located in the foreground of the image, with the bicycle symbol and the Chinese characters being the main focus. The sign is positioned to the left of the pole, and the background consists of a clear blue sky with some trees visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.7, "peak": 128.96, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.09, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 28.09, "energy_joules_est": 65.36, "sample_count": 23, "duration_seconds": 2.327}, "timestamp": "2026-01-19T13:23:37.782963"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3354.724, "latencies_ms": [3354.724], "images_per_second": 0.298, "prompt_tokens": 1111, "response_tokens_est": 87, "n_tiles": 1, "output_text": " The image captures a street scene in China, where a blue circular sign with a white bicycle symbol and the words \"\u4eba\u884c\u9053\" (pedestrian road) is mounted on a metal pole. Below it, a white rectangular sign with the Chinese characters \"\u6c5f\u82cf\u7701\u6dee\u5317\u5e02\" (Jiangsu Province, Hubei City) is visible, indicating the location.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.19, "peak": 120.12, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.04, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 25.04, "energy_joules_est": 84.02, "sample_count": 33, "duration_seconds": 3.355}, "timestamp": "2026-01-19T13:23:41.202689"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2018.71, "latencies_ms": [2018.71], "images_per_second": 0.495, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features a blue circular sign with a white bicycle symbol and a white rectangular sign with black Chinese characters. The sky is clear and blue, and the sun is shining brightly.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.58, "peak": 122.59, "min": 30.78}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.63, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.63, "energy_joules_est": 57.81, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T13:23:43.286167"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1708.439, "latencies_ms": [1708.439], "images_per_second": 0.585, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young girl with black hair and a black dress is sitting at a table in a restaurant, eating a slice of pizza.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.34, "peak": 124.22, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.22, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.22, "energy_joules_est": 51.65, "sample_count": 17, "duration_seconds": 1.709}, "timestamp": "2026-01-19T13:23:45.060490"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1951.983, "latencies_ms": [1951.983], "images_per_second": 0.512, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " table: 1, chair: 1, pizza: 1, glass: 1, book: 1, person: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.91, "peak": 111.05, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.8, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 29.8, "energy_joules_est": 58.18, "sample_count": 19, "duration_seconds": 1.952}, "timestamp": "2026-01-19T13:23:47.032355"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2250.213, "latencies_ms": [2250.213], "images_per_second": 0.444, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The girl is sitting at a table in the foreground, with a glass of water and a book on the table. The pizza is on the table in the background, and the chairs are behind the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.09, "peak": 121.33, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 27.99, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 27.99, "energy_joules_est": 62.99, "sample_count": 22, "duration_seconds": 2.25}, "timestamp": "2026-01-19T13:23:49.312025"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1535.975, "latencies_ms": [1535.975], "images_per_second": 0.651, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young girl is sitting at a table in a restaurant, eating a slice of pizza.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.97, "peak": 121.55, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 31.2, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.2, "energy_joules_est": 47.93, "sample_count": 15, "duration_seconds": 1.536}, "timestamp": "2026-01-19T13:23:50.869619"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2066.59, "latencies_ms": [2066.59], "images_per_second": 0.484, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image is taken in a dimly lit restaurant with warm lighting. The colors in the image are mostly muted with the exception of the girl's yellow hair and the green of the salad.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.7, "peak": 125.87, "min": 43.52}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.9, "peak": 40.57, "min": 20.49}}, "power_watts_avg": 29.9, "energy_joules_est": 61.8, "sample_count": 20, "duration_seconds": 2.067}, "timestamp": "2026-01-19T13:23:52.942440"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2181.732, "latencies_ms": [2181.732], "images_per_second": 0.458, "prompt_tokens": 1099, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image captures a meal spread out on a kitchen counter, featuring a variety of dishes including a bowl of broccoli, a plate of rice, and a plate of bread, all set against the backdrop of a wooden wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.28, "peak": 126.6, "min": 27.9}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.95, "min": 12.96}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 28.4, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.4, "energy_joules_est": 61.98, "sample_count": 22, "duration_seconds": 2.182}, "timestamp": "2026-01-19T13:23:55.224626"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2095.809, "latencies_ms": [2095.809], "images_per_second": 0.477, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " bowl: 1, plate: 2, glass: 1, bowl: 1, plate: 1, bowl: 1, plate: 1, bowl: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.8, "peak": 132.83, "min": 28.18}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.18, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.18, "energy_joules_est": 59.07, "sample_count": 21, "duration_seconds": 2.096}, "timestamp": "2026-01-19T13:23:57.417410"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2665.273, "latencies_ms": [2665.273], "images_per_second": 0.375, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the plates of food are placed in the foreground, with the bowls of vegetables and the glass of water in the background. The plates of food are positioned to the left of the bowls, and the glass of water is placed to the right of the bowls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.38, "peak": 129.23, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.57, "peak": 40.18, "min": 15.38}}, "power_watts_avg": 26.57, "energy_joules_est": 70.82, "sample_count": 26, "duration_seconds": 2.666}, "timestamp": "2026-01-19T13:24:00.127597"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1635.693, "latencies_ms": [1635.693], "images_per_second": 0.611, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A meal is set on a table with a variety of food items, including broccoli, cauliflower, and rice.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.73, "peak": 122.58, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.71, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.71, "energy_joules_est": 50.24, "sample_count": 16, "duration_seconds": 1.636}, "timestamp": "2026-01-19T13:24:01.786844"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1917.259, "latencies_ms": [1917.259], "images_per_second": 0.522, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image is taken in a kitchen with a wooden table and a stainless steel countertop. The lighting is natural and bright, and the colors are vibrant and varied.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.37, "peak": 128.53, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 29.92, "energy_joules_est": 57.39, "sample_count": 19, "duration_seconds": 1.918}, "timestamp": "2026-01-19T13:24:03.771501"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2285.268, "latencies_ms": [2285.268], "images_per_second": 0.438, "prompt_tokens": 1099, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image captures a bustling city street corner, where a green bus is navigating through the traffic, while a white truck and a red car are parked on the side of the road, and a black car is driving down the street.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.27, "peak": 122.79, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 27.53, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.53, "energy_joules_est": 62.94, "sample_count": 23, "duration_seconds": 2.286}, "timestamp": "2026-01-19T13:24:06.156346"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2580.231, "latencies_ms": [2580.231], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. street sign: 1\n2. bus: 1\n3. car: 4\n4. truck: 1\n5. person: 1\n6. tree: 2\n7. building: 2\n8. pole: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.48, "peak": 121.36, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.78, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.78, "energy_joules_est": 69.12, "sample_count": 25, "duration_seconds": 2.581}, "timestamp": "2026-01-19T13:24:08.764412"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2533.026, "latencies_ms": [2533.026], "images_per_second": 0.395, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The bus is positioned in the middle of the street, with the cars on the right side and the truck on the left. The bus is closer to the camera than the cars, and the truck is farther away. The bus is also closer to the street sign than the cars.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.1, "peak": 116.81, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.06, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.06, "energy_joules_est": 68.56, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T13:24:11.350899"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3266.409, "latencies_ms": [3266.409], "images_per_second": 0.306, "prompt_tokens": 1111, "response_tokens_est": 83, "n_tiles": 1, "output_text": " The image captures a bustling city street corner, teeming with life and activity. A green bus, adorned with a yellow sign, is in motion, navigating through the traffic. The street is lined with towering buildings, their windows reflecting the city's hustle and bustle. Cars, including a black one, a red one, and a white one, are parked along the curb, adding to the urban landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.81, "peak": 124.66, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.85, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 25.19, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 25.19, "energy_joules_est": 82.29, "sample_count": 32, "duration_seconds": 3.267}, "timestamp": "2026-01-19T13:24:14.693529"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3113.307, "latencies_ms": [3113.307], "images_per_second": 0.321, "prompt_tokens": 1109, "response_tokens_est": 77, "n_tiles": 1, "output_text": " The image depicts a bustling city street with a variety of vehicles, including cars and buses, and buildings of different heights and designs. The sky is overcast, casting a soft light over the scene. The colors in the image are predominantly muted, with the gray of the buildings and the black of the vehicles standing out against the green of the trees and the blue of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.12, "peak": 124.16, "min": 30.73}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.48, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 25.48, "energy_joules_est": 79.34, "sample_count": 30, "duration_seconds": 3.114}, "timestamp": "2026-01-19T13:24:17.820613"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1822.713, "latencies_ms": [1822.713], "images_per_second": 0.549, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A laptop computer with a Toshiba brand name on it is sitting on a table with a cell phone and a remote control next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.69, "peak": 109.52, "min": 27.96}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.81, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.81, "energy_joules_est": 54.35, "sample_count": 18, "duration_seconds": 1.823}, "timestamp": "2026-01-19T13:24:19.705869"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2090.394, "latencies_ms": [2090.394], "images_per_second": 0.478, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " laptop: 1, phone: 1, remote: 1, phone: 1, remote: 1, phone: 1, remote: 1, phone: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.21, "peak": 114.38, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.44, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.44, "energy_joules_est": 59.46, "sample_count": 21, "duration_seconds": 2.091}, "timestamp": "2026-01-19T13:24:21.881670"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1894.678, "latencies_ms": [1894.678], "images_per_second": 0.528, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The laptop is on the left side of the table, the cell phone is on the right side, and the remote is in front of the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.23, "peak": 121.9, "min": 27.45}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.95, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 28.95, "energy_joules_est": 54.86, "sample_count": 19, "duration_seconds": 1.895}, "timestamp": "2026-01-19T13:24:23.866917"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1592.672, "latencies_ms": [1592.672], "images_per_second": 0.628, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A laptop computer is open on a table with a cell phone and a remote control next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.18, "peak": 112.18, "min": 27.65}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.49, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.49, "energy_joules_est": 48.57, "sample_count": 16, "duration_seconds": 1.593}, "timestamp": "2026-01-19T13:24:25.525809"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1531.609, "latencies_ms": [1531.609], "images_per_second": 0.653, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The laptop is black and silver, the phone is black, and the remote is silver.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.1, "peak": 110.96, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.75, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.75, "energy_joules_est": 48.64, "sample_count": 15, "duration_seconds": 1.532}, "timestamp": "2026-01-19T13:24:27.098524"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1849.333, "latencies_ms": [1849.333], "images_per_second": 0.541, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a cluttered desk with a computer monitor, keyboard, mouse, and laptop, surrounded by books, papers, and other office supplies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.93, "peak": 119.98, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.68, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 30.68, "energy_joules_est": 56.76, "sample_count": 18, "duration_seconds": 1.85}, "timestamp": "2026-01-19T13:24:28.969357"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.999, "latencies_ms": [2552.999], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. monitor: 2\n2. keyboard: 1\n3. mouse: 1\n4. laptop: 1\n5. books: 10\n6. pen: 1\n7. paper: 1\n8. bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.49, "peak": 127.73, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.39, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.39, "energy_joules_est": 69.93, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T13:24:31.581470"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2175.365, "latencies_ms": [2175.365], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The laptop is positioned to the right of the monitor, while the books are stacked on the left side of the desk. The water bottle is placed near the laptop, and the window is located behind the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.24, "peak": 106.55, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.94, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 27.94, "energy_joules_est": 60.79, "sample_count": 22, "duration_seconds": 2.176}, "timestamp": "2026-01-19T13:24:33.860438"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1458.716, "latencies_ms": [1458.716], "images_per_second": 0.686, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A desk with a computer, books, and a laptop on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.34, "peak": 130.44, "min": 27.38}, "VIN_SYS_5V0": {"avg": 13.94, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.47, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.47, "energy_joules_est": 44.46, "sample_count": 15, "duration_seconds": 1.459}, "timestamp": "2026-01-19T13:24:35.417237"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1635.783, "latencies_ms": [1635.783], "images_per_second": 0.611, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.9, "peak": 125.07, "min": 29.28}, "VIN_SYS_5V0": {"avg": 13.98, "peak": 15.44, "min": 11.36}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 31.2, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.2, "energy_joules_est": 51.05, "sample_count": 16, "duration_seconds": 1.636}, "timestamp": "2026-01-19T13:24:37.088737"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1564.597, "latencies_ms": [1564.597], "images_per_second": 0.639, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A skateboarder is performing a trick in the air while being filmed by a group of people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.14, "peak": 109.83, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.32, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 31.32, "energy_joules_est": 49.03, "sample_count": 16, "duration_seconds": 1.565}, "timestamp": "2026-01-19T13:24:38.752833"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2647.336, "latencies_ms": [2647.336], "images_per_second": 0.378, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. person: 1\n2. helmet: 1\n3. rollerblades: 2\n4. skateboard: 1\n5. camera: 1\n6. crowd: 1\n7. banner: 1\n8. railing: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.01, "peak": 126.21, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.85, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 26.85, "energy_joules_est": 71.09, "sample_count": 26, "duration_seconds": 2.648}, "timestamp": "2026-01-19T13:24:41.463255"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2041.284, "latencies_ms": [2041.284], "images_per_second": 0.49, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick in the air, while the spectators are in the background. The skateboarder is closer to the camera than the spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.07, "peak": 121.19, "min": 30.42}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.62, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.62, "energy_joules_est": 58.43, "sample_count": 20, "duration_seconds": 2.042}, "timestamp": "2026-01-19T13:24:43.534360"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1582.285, "latencies_ms": [1582.285], "images_per_second": 0.632, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A skateboarder is performing a trick in the air while being filmed by a group of people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.93, "peak": 110.0, "min": 30.34}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.19, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.19, "energy_joules_est": 47.78, "sample_count": 16, "duration_seconds": 1.583}, "timestamp": "2026-01-19T13:24:45.207232"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2177.094, "latencies_ms": [2177.094], "images_per_second": 0.459, "prompt_tokens": 1110, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a skateboarder performing a trick in an indoor arena with a crowd of spectators. The lighting is artificial, and the skateboarder is wearing protective gear, including a helmet and knee pads.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.79, "peak": 127.73, "min": 32.11}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.57, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.57, "energy_joules_est": 62.22, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T13:24:47.388723"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2035.096, "latencies_ms": [2035.096], "images_per_second": 0.491, "prompt_tokens": 1100, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a vibrant red fire hydrant with a smiley face painted on it, standing on a sidewalk next to a street with a yellow line, and a tree in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.38, "peak": 129.63, "min": 30.46}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.9, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 28.9, "energy_joules_est": 58.83, "sample_count": 20, "duration_seconds": 2.036}, "timestamp": "2026-01-19T13:24:49.482193"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2053.129, "latencies_ms": [2053.129], "images_per_second": 0.487, "prompt_tokens": 1114, "response_tokens_est": 38, "n_tiles": 1, "output_text": " fire hydrant: 1\nsmiley face: 1\ntree: 1\ncar: 1\nbuilding: 1\nstreet: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.71, "peak": 132.86, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.76, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.76, "energy_joules_est": 59.06, "sample_count": 20, "duration_seconds": 2.053}, "timestamp": "2026-01-19T13:24:51.568319"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1782.264, "latencies_ms": [1782.264], "images_per_second": 0.561, "prompt_tokens": 1118, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The fire hydrant is located on the right side of the image, in the foreground, and is positioned near a tree and a building.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.58, "peak": 120.54, "min": 28.44}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.72, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.72, "energy_joules_est": 52.98, "sample_count": 18, "duration_seconds": 1.783}, "timestamp": "2026-01-19T13:24:53.444397"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1604.71, "latencies_ms": [1604.71], "images_per_second": 0.623, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A red fire hydrant with a smiley face painted on it stands on a city street corner.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.73, "peak": 119.32, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.01, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.96, "peak": 39.77, "min": 16.56}}, "power_watts_avg": 29.96, "energy_joules_est": 48.09, "sample_count": 16, "duration_seconds": 1.605}, "timestamp": "2026-01-19T13:24:55.114137"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2362.369, "latencies_ms": [2362.369], "images_per_second": 0.423, "prompt_tokens": 1110, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The fire hydrant is painted in a vibrant shade of orange, with a black nozzle and a smiley face drawn on it. The scene is bathed in natural light, and the fire hydrant is situated on a sidewalk next to a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.14, "peak": 127.33, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.91, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.91, "energy_joules_est": 65.94, "sample_count": 23, "duration_seconds": 2.363}, "timestamp": "2026-01-19T13:24:57.512437"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2017.161, "latencies_ms": [2017.161], "images_per_second": 0.496, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A green trolley is filled with old suitcases of various colors and sizes, including brown, blue, and green, and is parked in front of a building with a green door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.98, "peak": 113.5, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.8, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.8, "energy_joules_est": 58.11, "sample_count": 20, "duration_seconds": 2.018}, "timestamp": "2026-01-19T13:24:59.604224"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2752.551, "latencies_ms": [2752.551], "images_per_second": 0.363, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. suitcase: 6\n2. cart: 1\n3. door: 1\n4. wall: 1\n5. window: 1\n6. green door: 1\n7. green cart: 1\n8. green trolley: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.62, "peak": 123.78, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.15, "peak": 39.77, "min": 16.16}}, "power_watts_avg": 26.15, "energy_joules_est": 71.99, "sample_count": 27, "duration_seconds": 2.753}, "timestamp": "2026-01-19T13:25:02.427141"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2827.199, "latencies_ms": [2827.199], "images_per_second": 0.354, "prompt_tokens": 1117, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The green trolley is positioned in the foreground, with the stack of luggage placed on top of it. The luggage is stacked in a haphazard manner, with some pieces leaning against each other and others stacked on top of one another. The trolley is parked in front of a green door, which is located in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.46, "peak": 128.79, "min": 27.45}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.0, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.0, "energy_joules_est": 73.52, "sample_count": 28, "duration_seconds": 2.828}, "timestamp": "2026-01-19T13:25:05.338869"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1575.612, "latencies_ms": [1575.612], "images_per_second": 0.635, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A green cart is filled with old suitcases and a green cart is parked outside a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.89, "peak": 126.07, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 30.24, "energy_joules_est": 47.66, "sample_count": 16, "duration_seconds": 1.576}, "timestamp": "2026-01-19T13:25:07.008144"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1703.706, "latencies_ms": [1703.706], "images_per_second": 0.587, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The luggage is a mix of brown, green, and blue with a worn look, and the lighting is natural daylight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.43, "peak": 129.24, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.65, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.57, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 30.57, "energy_joules_est": 52.09, "sample_count": 17, "duration_seconds": 1.704}, "timestamp": "2026-01-19T13:25:08.778288"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1528.861, "latencies_ms": [1528.861], "images_per_second": 0.654, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young girl in a pink dress is playing a video game on a Wii console.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.6, "peak": 127.02, "min": 27.07}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.62, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.62, "energy_joules_est": 48.37, "sample_count": 15, "duration_seconds": 1.53}, "timestamp": "2026-01-19T13:25:10.346010"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2617.434, "latencies_ms": [2617.434], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. girl: 1\n2. dress: 1\n3. couch: 1\n4. remote: 1\n5. window: 1\n6. blinds: 1\n7. curtain: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.98, "peak": 121.61, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.14, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 27.14, "energy_joules_est": 71.05, "sample_count": 26, "duration_seconds": 2.618}, "timestamp": "2026-01-19T13:25:13.049678"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2228.404, "latencies_ms": [2228.404], "images_per_second": 0.449, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The girl is standing to the left of the couch, which is positioned in the middle of the image. The window blinds are located behind the couch, and the red curtain is on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.82, "peak": 133.64, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.74, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.74, "energy_joules_est": 61.82, "sample_count": 22, "duration_seconds": 2.229}, "timestamp": "2026-01-19T13:25:15.342231"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1530.603, "latencies_ms": [1530.603], "images_per_second": 0.653, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A young girl is playing a video game on a Wii console in her living room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.11, "peak": 125.14, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.18, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.18, "energy_joules_est": 47.73, "sample_count": 15, "duration_seconds": 1.531}, "timestamp": "2026-01-19T13:25:16.909778"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2342.024, "latencies_ms": [2342.024], "images_per_second": 0.427, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image depicts a young girl in a pink dress with a floral pattern, standing in front of a brown couch. The lighting in the room is natural, coming from a window with white blinds, and the overall atmosphere is warm and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.64, "peak": 121.67, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.39, "peak": 40.57, "min": 18.52}}, "power_watts_avg": 28.39, "energy_joules_est": 66.5, "sample_count": 23, "duration_seconds": 2.342}, "timestamp": "2026-01-19T13:25:19.307037"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1693.354, "latencies_ms": [1693.354], "images_per_second": 0.591, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A cluttered desk with a laptop, keyboard, and headphones sits in front of a window with a trash can beside it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.2, "peak": 120.47, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.15, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.15, "energy_joules_est": 51.08, "sample_count": 17, "duration_seconds": 1.694}, "timestamp": "2026-01-19T13:25:21.085387"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2555.613, "latencies_ms": [2555.613], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. chair: 1\n2. laptop: 1\n3. keyboard: 1\n4. mouse: 1\n5. monitor: 1\n6. trash can: 1\n7. window: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.53, "peak": 123.11, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.31, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.31, "energy_joules_est": 69.8, "sample_count": 25, "duration_seconds": 2.556}, "timestamp": "2026-01-19T13:25:23.679282"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2254.062, "latencies_ms": [2254.062], "images_per_second": 0.444, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The laptop is on the left side of the desk, the chair is on the right side, and the trash can is in the middle of the desk. The desk is in the foreground, and the window is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.45, "peak": 128.82, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.08, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.08, "energy_joules_est": 63.31, "sample_count": 22, "duration_seconds": 2.255}, "timestamp": "2026-01-19T13:25:25.963716"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1540.011, "latencies_ms": [1540.011], "images_per_second": 0.649, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A cluttered office with a computer on a desk, a chair, and a trash can.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.72, "peak": 127.02, "min": 30.4}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.52, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.52, "energy_joules_est": 48.55, "sample_count": 15, "duration_seconds": 1.54}, "timestamp": "2026-01-19T13:25:27.531042"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1698.342, "latencies_ms": [1698.342], "images_per_second": 0.589, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The room is well lit with natural light coming in from the windows. The walls are painted white and the floor is wooden.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.3, "peak": 125.62, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.29, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.29, "energy_joules_est": 53.16, "sample_count": 17, "duration_seconds": 1.699}, "timestamp": "2026-01-19T13:25:29.301643"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1623.378, "latencies_ms": [1623.378], "images_per_second": 0.616, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A person is cutting a pizza with a knife and fork on a red and white checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.03, "peak": 115.36, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.12, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.12, "energy_joules_est": 50.54, "sample_count": 16, "duration_seconds": 1.624}, "timestamp": "2026-01-19T13:25:30.974602"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2822.334, "latencies_ms": [2822.334], "images_per_second": 0.354, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. plate: 1\n2. pizza: 1\n3. fork: 1\n4. person's hand: 1\n5. red and white checkered tablecloth: 1\n6. green peppers: 2\n7. mushrooms: 2\n8. red peppers: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.3, "peak": 117.79, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.58, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.58, "energy_joules_est": 75.04, "sample_count": 28, "duration_seconds": 2.823}, "timestamp": "2026-01-19T13:25:33.891384"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1973.323, "latencies_ms": [1973.323], "images_per_second": 0.507, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The pizza is in the foreground, on a white plate, with a fork and knife in the background. The person's hand is near the pizza, holding a fork.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.8, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.44, "peak": 119.8, "min": 34.93}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.95, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.95, "energy_joules_est": 57.14, "sample_count": 19, "duration_seconds": 1.974}, "timestamp": "2026-01-19T13:25:35.870375"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1613.156, "latencies_ms": [1613.156], "images_per_second": 0.62, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A person is cutting a pizza with a knife and fork on a red and white checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.19, "peak": 126.91, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.42, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 31.42, "energy_joules_est": 50.7, "sample_count": 16, "duration_seconds": 1.614}, "timestamp": "2026-01-19T13:25:37.528920"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1582.033, "latencies_ms": [1582.033], "images_per_second": 0.632, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The pizza is on a white plate with red and white checkered tablecloth in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.59, "peak": 127.82, "min": 28.08}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.47, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.47, "energy_joules_est": 49.8, "sample_count": 16, "duration_seconds": 1.582}, "timestamp": "2026-01-19T13:25:39.194592"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2061.895, "latencies_ms": [2061.895], "images_per_second": 0.485, "prompt_tokens": 1432, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A white and red bus with the words \"Metropolitan Transit System\" on it is parked in front of a building.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.5, "peak": 123.82, "min": 31.73}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.15, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.59, "peak": 41.76, "min": 18.14}}, "power_watts_avg": 31.59, "energy_joules_est": 65.15, "sample_count": 20, "duration_seconds": 2.062}, "timestamp": "2026-01-19T13:25:41.272385"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2901.926, "latencies_ms": [2901.926], "images_per_second": 0.345, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bus: 1\n2. windows: 4\n3. doors: 2\n4. wheels: 2\n5. seats: 2\n6. passengers: 2\n7. trees: 1\n8. building: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.58, "peak": 125.14, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 16.46, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.25, "peak": 41.76, "min": 18.91}}, "power_watts_avg": 29.25, "energy_joules_est": 84.89, "sample_count": 28, "duration_seconds": 2.902}, "timestamp": "2026-01-19T13:25:44.188213"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2383.22, "latencies_ms": [2383.22], "images_per_second": 0.42, "prompt_tokens": 1450, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The bus is parked on the left side of the image, with the sidewalk and trees in the background. The bus is in the foreground, with the building and street in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.95, "peak": 128.63, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.81, "peak": 16.46, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 30.76, "peak": 41.36, "min": 16.95}}, "power_watts_avg": 30.76, "energy_joules_est": 73.32, "sample_count": 23, "duration_seconds": 2.384}, "timestamp": "2026-01-19T13:25:46.591513"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1740.492, "latencies_ms": [1740.492], "images_per_second": 0.575, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A white and red bus is parked in front of a building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 80.11, "peak": 114.62, "min": 30.58}, "VIN_SYS_5V0": {"avg": 14.78, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 33.57, "peak": 41.76, "min": 17.74}}, "power_watts_avg": 33.57, "energy_joules_est": 58.44, "sample_count": 17, "duration_seconds": 1.741}, "timestamp": "2026-01-19T13:25:48.369334"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2035.42, "latencies_ms": [2035.42], "images_per_second": 0.491, "prompt_tokens": 1442, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The bus is white with red and blue stripes, and it is parked in front of a building with a glass facade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.5, "ram_available_mb": 99772.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.89, "peak": 118.06, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 33.03, "peak": 42.53, "min": 21.28}}, "power_watts_avg": 33.03, "energy_joules_est": 67.24, "sample_count": 20, "duration_seconds": 2.036}, "timestamp": "2026-01-19T13:25:50.456825"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1868.868, "latencies_ms": [1868.868], "images_per_second": 0.535, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A baseball glove and cap are resting on the ground next to a metal pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.21, "peak": 117.51, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.36, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 32.46, "peak": 42.54, "min": 18.53}}, "power_watts_avg": 32.46, "energy_joules_est": 60.68, "sample_count": 18, "duration_seconds": 1.869}, "timestamp": "2026-01-19T13:25:52.336244"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2565.612, "latencies_ms": [2565.612], "images_per_second": 0.39, "prompt_tokens": 1446, "response_tokens_est": 43, "n_tiles": 1, "output_text": " baseball cap: 1, baseball glove: 1, baseball: 1, baseball bat: 0, baseball bat handle: 0, baseball bat barrel: 0, baseball bat tip: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.72, "peak": 128.47, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 30.6, "peak": 42.53, "min": 19.7}}, "power_watts_avg": 30.6, "energy_joules_est": 78.52, "sample_count": 25, "duration_seconds": 2.566}, "timestamp": "2026-01-19T13:25:54.947370"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2415.004, "latencies_ms": [2415.004], "images_per_second": 0.414, "prompt_tokens": 1450, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The baseball cap is positioned to the left of the glove, which is resting on the ground. The glove is in the foreground of the image, while the cap is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.72, "peak": 126.02, "min": 27.22}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.46, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 30.17, "peak": 42.54, "min": 16.56}}, "power_watts_avg": 30.17, "energy_joules_est": 72.87, "sample_count": 24, "duration_seconds": 2.415}, "timestamp": "2026-01-19T13:25:57.446578"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1843.42, "latencies_ms": [1843.42], "images_per_second": 0.542, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A baseball glove and cap are resting on the ground next to a metal pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.61, "peak": 132.68, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 32.54, "peak": 42.13, "min": 16.16}}, "power_watts_avg": 32.54, "energy_joules_est": 60.0, "sample_count": 18, "duration_seconds": 1.844}, "timestamp": "2026-01-19T13:25:59.329706"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3201.209, "latencies_ms": [3201.209], "images_per_second": 0.312, "prompt_tokens": 1442, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image features a baseball glove and cap resting on the ground, with the glove being brown and the cap being blue. The glove is positioned on the ground, and the cap is placed on top of it. The image was taken during the day, and the lighting appears to be natural, possibly from the sun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.3, "peak": 129.23, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.36, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 28.21, "peak": 42.92, "min": 18.91}}, "power_watts_avg": 28.21, "energy_joules_est": 90.32, "sample_count": 31, "duration_seconds": 3.202}, "timestamp": "2026-01-19T13:26:02.556496"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1452.597, "latencies_ms": [1452.597], "images_per_second": 0.688, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man wearing a red shirt is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.67, "peak": 130.71, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 31.71, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.71, "energy_joules_est": 46.08, "sample_count": 14, "duration_seconds": 1.453}, "timestamp": "2026-01-19T13:26:04.025171"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2577.935, "latencies_ms": [2577.935], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. wave: 1\n4. water: 1\n5. sky: 0\n6. cloud: 0\n7. land: 0\n8. surfboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.58, "peak": 128.02, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.07, "peak": 40.97, "min": 18.91}}, "power_watts_avg": 28.07, "energy_joules_est": 72.37, "sample_count": 25, "duration_seconds": 2.578}, "timestamp": "2026-01-19T13:26:06.632588"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2198.829, "latencies_ms": [2198.829], "images_per_second": 0.455, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, riding a wave that is in the middle ground. The surfer is facing towards the right side of the image, with the wave moving towards the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.57, "peak": 126.55, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.01, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.01, "energy_joules_est": 61.6, "sample_count": 22, "duration_seconds": 2.199}, "timestamp": "2026-01-19T13:26:08.921806"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1365.374, "latencies_ms": [1365.374], "images_per_second": 0.732, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A man is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 81.73, "peak": 125.21, "min": 27.47}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 31.18, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 31.18, "energy_joules_est": 42.59, "sample_count": 14, "duration_seconds": 1.366}, "timestamp": "2026-01-19T13:26:10.386604"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1618.764, "latencies_ms": [1618.764], "images_per_second": 0.618, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The surfer is wearing a red shirt and black shorts, and the water is a bright turquoise color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25999.7, "ram_available_mb": 99772.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.77, "peak": 128.6, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 31.79, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 31.79, "energy_joules_est": 51.48, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T13:26:12.059504"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1528.211, "latencies_ms": [1528.211], "images_per_second": 0.654, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A black and white photo of a bathroom with a toilet, sink, and toilet paper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.03, "peak": 132.62, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.96, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 31.96, "energy_joules_est": 48.86, "sample_count": 15, "duration_seconds": 1.529}, "timestamp": "2026-01-19T13:26:13.629074"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2068.928, "latencies_ms": [2068.928], "images_per_second": 0.483, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " toilet: 1, sink: 1, toilet paper: 1, mirror: 1, faucet: 1, soap dispenser: 1, cabinet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.52, "peak": 122.72, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.59, "peak": 40.18, "min": 19.7}}, "power_watts_avg": 29.59, "energy_joules_est": 61.23, "sample_count": 20, "duration_seconds": 2.069}, "timestamp": "2026-01-19T13:26:15.723263"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2428.909, "latencies_ms": [2428.909], "images_per_second": 0.412, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The toilet is located on the left side of the image, while the sink is on the right side. The toilet is positioned closer to the camera than the sink. The sink is situated in the foreground of the image, while the toilet is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.54, "peak": 115.95, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.43, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 27.43, "energy_joules_est": 66.64, "sample_count": 24, "duration_seconds": 2.43}, "timestamp": "2026-01-19T13:26:18.220530"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1527.509, "latencies_ms": [1527.509], "images_per_second": 0.655, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A black and white photo of a bathroom with a toilet, sink, and toilet paper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.84, "peak": 122.95, "min": 30.04}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.07, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.07, "energy_joules_est": 47.47, "sample_count": 15, "duration_seconds": 1.528}, "timestamp": "2026-01-19T13:26:19.783831"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1530.495, "latencies_ms": [1530.495], "images_per_second": 0.653, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The bathroom is black and white, with a granite countertop and a tiled wall.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26000.0, "ram_available_mb": 99772.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.85, "peak": 119.19, "min": 27.75}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.41, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 32.41, "energy_joules_est": 49.61, "sample_count": 15, "duration_seconds": 1.531}, "timestamp": "2026-01-19T13:26:21.346489"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2147.817, "latencies_ms": [2147.817], "images_per_second": 0.466, "prompt_tokens": 1099, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a picturesque scene of a white clock tower with a blue dome, standing tall against a clear blue sky, surrounded by a vibrant red-tiled roof and adorned with intricate white and gold decorations.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.95, "peak": 111.28, "min": 30.42}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.25, "peak": 40.18, "min": 19.7}}, "power_watts_avg": 29.25, "energy_joules_est": 62.85, "sample_count": 21, "duration_seconds": 2.149}, "timestamp": "2026-01-19T13:26:23.531530"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.281, "latencies_ms": [2590.281], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. clock: 1\n2. roof: 1\n3. tower: 1\n4. gazebo: 1\n5. light: 1\n6. tree: 1\n7. sky: 1\n8. building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.17, "peak": 126.97, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.19, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.19, "energy_joules_est": 70.44, "sample_count": 25, "duration_seconds": 2.591}, "timestamp": "2026-01-19T13:26:26.132989"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2386.849, "latencies_ms": [2386.849], "images_per_second": 0.419, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The clock tower is located in the background, behind the ornate gazebo. The gazebo is situated to the left of the clock tower. The clock tower is near the gazebo, as they are both part of the same architectural structure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.39, "peak": 128.1, "min": 35.15}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.95, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.95, "energy_joules_est": 66.72, "sample_count": 23, "duration_seconds": 2.387}, "timestamp": "2026-01-19T13:26:28.524519"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2622.767, "latencies_ms": [2622.767], "images_per_second": 0.381, "prompt_tokens": 1111, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image captures a picturesque scene of a white clock tower with a blue dome, standing tall against the backdrop of a clear blue sky. The tower is adorned with a white clock face and is surrounded by a vibrant red-tiled roof, adding a warm contrast to the cool blue of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26004.0, "ram_available_mb": 99768.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26005.9, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.91, "peak": 128.3, "min": 30.6}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 26.83, "peak": 39.77, "min": 16.95}}, "power_watts_avg": 26.83, "energy_joules_est": 70.38, "sample_count": 26, "duration_seconds": 2.623}, "timestamp": "2026-01-19T13:26:31.237775"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2808.329, "latencies_ms": [2808.329], "images_per_second": 0.356, "prompt_tokens": 1109, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The image features a white clock tower with a blue dome, set against a clear blue sky. The tower is adorned with intricate white trim and has a white clock face with black numbers and hands. The roof of the building is covered in terracotta tiles, and there is a decorative wrought iron gazebo in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.41, "peak": 135.11, "min": 32.66}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.19, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.19, "energy_joules_est": 73.56, "sample_count": 27, "duration_seconds": 2.809}, "timestamp": "2026-01-19T13:26:34.050762"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2102.291, "latencies_ms": [2102.291], "images_per_second": 0.476, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " In the image, a group of elephants is seen walking on a dirt path in a natural setting, with one elephant in the foreground prominently displayed and the others in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26005.9, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.62, "peak": 118.72, "min": 27.04}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.79, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 27.79, "energy_joules_est": 58.43, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T13:26:36.237237"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2036.563, "latencies_ms": [2036.563], "images_per_second": 0.491, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " elephant: 1, trunk: 1, tusk: 1, ear: 1, eye: 1, trunk: 1, tail: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.23, "peak": 120.44, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.41, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.41, "energy_joules_est": 57.87, "sample_count": 20, "duration_seconds": 2.037}, "timestamp": "2026-01-19T13:26:38.320157"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1940.772, "latencies_ms": [1940.772], "images_per_second": 0.515, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The main elephant is in the foreground, with the other elephants in the background. The elephants are walking on a dirt path, with trees and bushes on either side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.01, "peak": 117.92, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.38, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 29.38, "energy_joules_est": 57.03, "sample_count": 19, "duration_seconds": 1.941}, "timestamp": "2026-01-19T13:26:40.292331"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1425.884, "latencies_ms": [1425.884], "images_per_second": 0.701, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A group of elephants are walking on a dirt path in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.35, "peak": 107.27, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.08, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 32.08, "energy_joules_est": 45.76, "sample_count": 14, "duration_seconds": 1.426}, "timestamp": "2026-01-19T13:26:41.753111"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1481.564, "latencies_ms": [1481.564], "images_per_second": 0.675, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The elephants are gray, the ground is brown, and the trees are green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.72, "peak": 117.54, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.59, "peak": 40.95, "min": 20.89}}, "power_watts_avg": 32.59, "energy_joules_est": 48.3, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T13:26:43.315486"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1807.767, "latencies_ms": [1807.767], "images_per_second": 0.553, "prompt_tokens": 1100, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image depicts an open refrigerator with its door open, revealing the interior with empty shelves and a bottle of yellow liquid placed on the top shelf.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.72, "peak": 116.71, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.21, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.21, "energy_joules_est": 54.64, "sample_count": 18, "duration_seconds": 1.809}, "timestamp": "2026-01-19T13:26:45.199474"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2738.359, "latencies_ms": [2738.359], "images_per_second": 0.365, "prompt_tokens": 1114, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Refrigerator: 1\n2. Shelves: 4\n3. Drawers: 2\n4. Door: 1\n5. Door handle: 1\n6. Door hinge: 1\n7. Door latch: 1\n8. Door lock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.7, "peak": 122.3, "min": 30.53}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.48, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.48, "energy_joules_est": 72.52, "sample_count": 27, "duration_seconds": 2.739}, "timestamp": "2026-01-19T13:26:48.006230"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2350.612, "latencies_ms": [2350.612], "images_per_second": 0.425, "prompt_tokens": 1118, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The refrigerator is located in the kitchen, with the left side of the refrigerator being closer to the camera than the right side. The bottle of mustard is placed on the top shelf of the refrigerator, which is positioned in the middle of the refrigerator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.18, "peak": 122.29, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.44, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.44, "energy_joules_est": 64.51, "sample_count": 23, "duration_seconds": 2.351}, "timestamp": "2026-01-19T13:26:50.399347"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2317.878, "latencies_ms": [2317.878], "images_per_second": 0.431, "prompt_tokens": 1112, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a moment in a kitchen where an open refrigerator stands, its interior empty and inviting. The refrigerator, a symbol of culinary adventures, is situated on a wooden floor, its white exterior contrasting with the warm tones of the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.3, "peak": 125.08, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.12, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.12, "energy_joules_est": 62.87, "sample_count": 23, "duration_seconds": 2.318}, "timestamp": "2026-01-19T13:26:52.796151"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1318.173, "latencies_ms": [1318.173], "images_per_second": 0.759, "prompt_tokens": 1110, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The refrigerator is white and has a brown floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.65, "peak": 131.93, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.67, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 31.67, "energy_joules_est": 41.76, "sample_count": 13, "duration_seconds": 1.319}, "timestamp": "2026-01-19T13:26:54.155701"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1830.1, "latencies_ms": [1830.1], "images_per_second": 0.546, "prompt_tokens": 1432, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A bunch of bananas are lined up on a shelf with a purple background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.17, "peak": 128.15, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 33.22, "peak": 42.53, "min": 21.67}}, "power_watts_avg": 33.22, "energy_joules_est": 60.82, "sample_count": 18, "duration_seconds": 1.831}, "timestamp": "2026-01-19T13:26:56.040367"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1505.728, "latencies_ms": [1505.728], "images_per_second": 0.664, "prompt_tokens": 1446, "response_tokens_est": 4, "n_tiles": 1, "output_text": " banana: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 1.7, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.86, "peak": 124.0, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.36, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 34.71, "peak": 42.53, "min": 19.7}}, "power_watts_avg": 34.71, "energy_joules_est": 52.28, "sample_count": 15, "duration_seconds": 1.506}, "timestamp": "2026-01-19T13:26:57.609435"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2401.548, "latencies_ms": [2401.548], "images_per_second": 0.416, "prompt_tokens": 1450, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The bananas are in the foreground, with the purple object in the background. The bananas are stacked on top of each other, with the top banana being the closest to the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.31, "peak": 114.94, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 16.26, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.6, "peak": 42.92, "min": 18.91}}, "power_watts_avg": 31.6, "energy_joules_est": 75.9, "sample_count": 24, "duration_seconds": 2.402}, "timestamp": "2026-01-19T13:27:00.107583"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1744.638, "latencies_ms": [1744.638], "images_per_second": 0.573, "prompt_tokens": 1444, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A bunch of bananas are on a shelf in a store.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.43, "peak": 120.83, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 16.26, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.81, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 32.81, "energy_joules_est": 57.26, "sample_count": 17, "duration_seconds": 1.745}, "timestamp": "2026-01-19T13:27:01.877998"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1832.62, "latencies_ms": [1832.62], "images_per_second": 0.546, "prompt_tokens": 1442, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The bananas are yellow and ripe, and they are sitting on a black surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.01, "peak": 120.29, "min": 31.03}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 16.36, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 34.16, "peak": 42.53, "min": 22.08}}, "power_watts_avg": 34.16, "energy_joules_est": 62.61, "sample_count": 18, "duration_seconds": 1.833}, "timestamp": "2026-01-19T13:27:03.760209"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2340.983, "latencies_ms": [2340.983], "images_per_second": 0.427, "prompt_tokens": 1432, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, there are four orange cylindrical objects with valves on top, placed on a sidewalk in a city, with a building and trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.36, "peak": 135.85, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.32, "peak": 42.13, "min": 19.31}}, "power_watts_avg": 30.32, "energy_joules_est": 71.0, "sample_count": 23, "duration_seconds": 2.342}, "timestamp": "2026-01-19T13:27:06.161883"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3093.913, "latencies_ms": [3093.913], "images_per_second": 0.323, "prompt_tokens": 1446, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. metal bollards: 4\n2. trees: 2\n3. buildings: 3\n4. windows: 10\n5. street lights: 2\n6. traffic lights: 1\n7. snow: 1\n8. sidewalk: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.65, "peak": 121.37, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.16, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 28.16, "energy_joules_est": 87.14, "sample_count": 30, "duration_seconds": 3.094}, "timestamp": "2026-01-19T13:27:09.288616"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2456.902, "latencies_ms": [2456.902], "images_per_second": 0.407, "prompt_tokens": 1450, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The orange cylindrical objects are positioned in the foreground, with the city buildings in the background. The objects are located on the left side of the image, while the buildings are on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.33, "peak": 117.1, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 30.2, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 30.2, "energy_joules_est": 74.21, "sample_count": 24, "duration_seconds": 2.457}, "timestamp": "2026-01-19T13:27:11.787534"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2321.536, "latencies_ms": [2321.536], "images_per_second": 0.431, "prompt_tokens": 1444, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image captures a city square with a row of orange cylindrical bollards in the foreground, while in the background, a large building with a clock tower stands tall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.99, "peak": 126.72, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 30.54, "peak": 41.76, "min": 16.95}}, "power_watts_avg": 30.54, "energy_joules_est": 70.91, "sample_count": 23, "duration_seconds": 2.322}, "timestamp": "2026-01-19T13:27:14.184606"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2165.304, "latencies_ms": [2165.304], "images_per_second": 0.462, "prompt_tokens": 1442, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image features a city square with a row of orange metal bollards, a snow-covered ground, and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.33, "peak": 135.04, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.26, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.44, "peak": 42.13, "min": 16.56}}, "power_watts_avg": 31.44, "energy_joules_est": 68.09, "sample_count": 21, "duration_seconds": 2.166}, "timestamp": "2026-01-19T13:27:16.368085"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2515.996, "latencies_ms": [2515.996], "images_per_second": 0.397, "prompt_tokens": 1432, "response_tokens_est": 39, "n_tiles": 1, "output_text": " In the image, a person is riding a horse in a race, wearing a helmet and a yellow shirt, with a sign in the background that reads \"WINSTONE AGGREGATES.\"", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.45, "peak": 126.25, "min": 28.28}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.42, "peak": 42.13, "min": 18.52}}, "power_watts_avg": 29.42, "energy_joules_est": 74.04, "sample_count": 25, "duration_seconds": 2.517}, "timestamp": "2026-01-19T13:27:18.970074"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3242.689, "latencies_ms": [3242.689], "images_per_second": 0.308, "prompt_tokens": 1446, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. horse: 1\n2. rider: 1\n3. cart: 1\n4. horse's leg: 2\n5. horse's hoof: 1\n6. horse's mane: 1\n7. horse's tail: 1\n8. horse's ear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.19, "peak": 131.01, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.36, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 27.39, "peak": 42.13, "min": 15.77}}, "power_watts_avg": 27.39, "energy_joules_est": 88.83, "sample_count": 32, "duration_seconds": 3.243}, "timestamp": "2026-01-19T13:27:22.307185"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2488.341, "latencies_ms": [2488.341], "images_per_second": 0.402, "prompt_tokens": 1450, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The horse is positioned in the foreground, with the rider and carriage in the middle ground. The horse is moving towards the right side of the image, while the rider is looking towards the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.38, "peak": 125.29, "min": 30.87}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 16.36, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.68, "peak": 41.76, "min": 14.98}}, "power_watts_avg": 29.68, "energy_joules_est": 73.87, "sample_count": 24, "duration_seconds": 2.489}, "timestamp": "2026-01-19T13:27:24.810857"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1878.9, "latencies_ms": [1878.9], "images_per_second": 0.532, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man wearing a helmet and a yellow shirt is riding a horse in a dirt track.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.43, "peak": 132.45, "min": 27.71}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 32.61, "peak": 42.15, "min": 17.74}}, "power_watts_avg": 32.61, "energy_joules_est": 61.3, "sample_count": 19, "duration_seconds": 1.88}, "timestamp": "2026-01-19T13:27:26.786134"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4203.239, "latencies_ms": [4203.239], "images_per_second": 0.238, "prompt_tokens": 1442, "response_tokens_est": 102, "n_tiles": 1, "output_text": " The image captures a moment of intense competition on a dirt track, where a horse and carriage are in motion. The horse, a majestic creature with a coat of rich brown, is adorned with a harness that gleams under the bright sunlight. The rider, clad in a vibrant yellow shirt and a protective helmet, sits firmly in the carriage, guiding the horse with a calm and focused demeanor. The track itself is a testament to the sport, with its smooth surface and the presence of a fence marking the boundary.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.68, "peak": 128.77, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.26, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 25.97, "peak": 42.13, "min": 18.14}}, "power_watts_avg": 25.97, "energy_joules_est": 109.17, "sample_count": 41, "duration_seconds": 4.204}, "timestamp": "2026-01-19T13:27:31.033486"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1496.676, "latencies_ms": [1496.676], "images_per_second": 0.668, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A brown dog stands on a wooden platform in a backyard with a tree bearing lemons.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.23, "peak": 119.51, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 31.12, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.12, "energy_joules_est": 46.6, "sample_count": 15, "duration_seconds": 1.497}, "timestamp": "2026-01-19T13:27:32.603256"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2537.017, "latencies_ms": [2537.017], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. dog: 1\n2. fence: 1\n3. tree: 1\n4. grass: 1\n5. house: 1\n6. bench: 1\n7. wall: 1\n8. shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.07, "peak": 122.71, "min": 30.63}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.58, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 27.58, "energy_joules_est": 69.99, "sample_count": 25, "duration_seconds": 2.538}, "timestamp": "2026-01-19T13:27:35.205731"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2032.057, "latencies_ms": [2032.057], "images_per_second": 0.492, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The dog is standing on the left side of the image, with the tree and fence in the background. The dog is in the foreground, with the house and grass in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.65, "peak": 131.39, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.78, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.78, "energy_joules_est": 58.5, "sample_count": 20, "duration_seconds": 2.033}, "timestamp": "2026-01-19T13:27:37.283100"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1508.95, "latencies_ms": [1508.95], "images_per_second": 0.663, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A brown dog stands on a wooden platform in a backyard with a tree bearing lemons.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.77, "peak": 120.37, "min": 28.2}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.36, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.36, "energy_joules_est": 47.33, "sample_count": 15, "duration_seconds": 1.509}, "timestamp": "2026-01-19T13:27:38.853021"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1600.037, "latencies_ms": [1600.037], "images_per_second": 0.625, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The dog is brown, the fence is wooden, the tree is green, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.65, "peak": 128.03, "min": 27.38}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.59, "peak": 40.57, "min": 19.31}}, "power_watts_avg": 31.59, "energy_joules_est": 50.56, "sample_count": 16, "duration_seconds": 1.6}, "timestamp": "2026-01-19T13:27:40.524833"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1599.679, "latencies_ms": [1599.679], "images_per_second": 0.625, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A person is standing on a wooden bench with a blue sign on the left side of the bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.86, "peak": 113.67, "min": 30.49}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.98, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 30.98, "energy_joules_est": 49.57, "sample_count": 16, "duration_seconds": 1.6}, "timestamp": "2026-01-19T13:27:42.206025"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2564.408, "latencies_ms": [2564.408], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. bench: 1\n3. paper: 1\n4. wall: 1\n5. bricks: 1\n6. shoes: 1\n7. pants: 1\n8. feet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.62, "peak": 113.62, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.23, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.23, "energy_joules_est": 69.84, "sample_count": 25, "duration_seconds": 2.565}, "timestamp": "2026-01-19T13:27:44.806757"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2380.957, "latencies_ms": [2380.957], "images_per_second": 0.42, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The bench is positioned in the foreground, with the person's feet on it, and the sign is placed on the left side of the bench. The wall is in the background, and the person's legs are positioned in the middle of the bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.83, "peak": 134.51, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.68, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.68, "energy_joules_est": 65.92, "sample_count": 23, "duration_seconds": 2.381}, "timestamp": "2026-01-19T13:27:47.203903"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1478.079, "latencies_ms": [1478.079], "images_per_second": 0.677, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A person is standing on a wooden bench with a sign on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.26, "peak": 113.31, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.25, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.25, "energy_joules_est": 46.2, "sample_count": 15, "duration_seconds": 1.479}, "timestamp": "2026-01-19T13:27:48.770633"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1665.854, "latencies_ms": [1665.854], "images_per_second": 0.6, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The bench is made of wood and has a red brick floor. The person is wearing red pants and blue shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.17, "peak": 118.31, "min": 32.84}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.44, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.52, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.52, "energy_joules_est": 52.52, "sample_count": 16, "duration_seconds": 1.666}, "timestamp": "2026-01-19T13:27:50.442756"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1825.523, "latencies_ms": [1825.523], "images_per_second": 0.548, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a red sofa, a round table with a white tablecloth, and a television mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.12, "peak": 122.93, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.46, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.46, "energy_joules_est": 55.62, "sample_count": 18, "duration_seconds": 1.826}, "timestamp": "2026-01-19T13:27:52.331647"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2607.552, "latencies_ms": [2607.552], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. red sofa: 1\n2. lamp: 2\n3. vase: 1\n4. table: 1\n5. chair: 1\n6. television: 1\n7. curtains: 2\n8. painting: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.11, "peak": 134.05, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.88, "peak": 40.57, "min": 17.35}}, "power_watts_avg": 26.88, "energy_joules_est": 70.1, "sample_count": 26, "duration_seconds": 2.608}, "timestamp": "2026-01-19T13:27:55.032909"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2757.314, "latencies_ms": [2757.314], "images_per_second": 0.363, "prompt_tokens": 1117, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The red sofa is located on the left side of the room, with the white table and chairs positioned in the center. The television is situated on the right side of the room, with the lamp and vase on the left side. The window is located behind the table, and the curtains are drawn to the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.67, "peak": 120.01, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.24, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.24, "energy_joules_est": 72.36, "sample_count": 27, "duration_seconds": 2.758}, "timestamp": "2026-01-19T13:27:57.834216"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1549.93, "latencies_ms": [1549.93], "images_per_second": 0.645, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The room is furnished with a red sofa, a dining table, and a television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.45, "peak": 120.88, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.99, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.99, "energy_joules_est": 48.05, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T13:27:59.401261"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1739.131, "latencies_ms": [1739.131], "images_per_second": 0.575, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the walls are painted in a warm yellow color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.79, "peak": 123.48, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.15, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.15, "energy_joules_est": 54.19, "sample_count": 17, "duration_seconds": 1.74}, "timestamp": "2026-01-19T13:28:01.176373"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1683.846, "latencies_ms": [1683.846], "images_per_second": 0.594, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A black frying pan filled with a mixture of broccoli, carrots, and meat is being stirred with a metal spatula.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.86, "peak": 123.41, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.64, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.64, "energy_joules_est": 51.62, "sample_count": 17, "duration_seconds": 1.685}, "timestamp": "2026-01-19T13:28:02.954924"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2776.539, "latencies_ms": [2776.539], "images_per_second": 0.36, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. broccoli: 12\n2. carrots: 10\n3. meat: 12\n4. broccoli: 12\n5. meat: 12\n6. carrots: 10\n7. broccoli: 12\n8. meat: 12", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.27, "peak": 119.66, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.53, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 26.53, "energy_joules_est": 73.68, "sample_count": 27, "duration_seconds": 2.777}, "timestamp": "2026-01-19T13:28:05.761095"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2268.83, "latencies_ms": [2268.83], "images_per_second": 0.441, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The broccoli is in the middle of the pan, the meat is on the left side, and the vegetables are on the right side. The pan is on the stove, and the broccoli is in the middle of the pan.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.43, "peak": 124.02, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 28.01, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.01, "energy_joules_est": 63.56, "sample_count": 22, "duration_seconds": 2.269}, "timestamp": "2026-01-19T13:28:08.052883"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2106.329, "latencies_ms": [2106.329], "images_per_second": 0.475, "prompt_tokens": 1111, "response_tokens_est": 40, "n_tiles": 1, "output_text": " In a black frying pan on a stove, a colorful mixture of broccoli, carrots, and meat is being cooked. The broccoli is bright green, the carrots are orange, and the meat is pink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.17, "peak": 132.1, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.52, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.52, "energy_joules_est": 60.08, "sample_count": 21, "duration_seconds": 2.107}, "timestamp": "2026-01-19T13:28:10.236231"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2739.458, "latencies_ms": [2739.458], "images_per_second": 0.365, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image features a black frying pan with a variety of vegetables and meat being cooked. The vegetables are in shades of green, orange, and red, while the meat appears to be pink and white. The lighting in the image is bright and even, suggesting that the photo was taken in a well-lit kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.23, "peak": 119.6, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.3, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 26.3, "energy_joules_est": 72.06, "sample_count": 27, "duration_seconds": 2.74}, "timestamp": "2026-01-19T13:28:13.042487"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1453.626, "latencies_ms": [1453.626], "images_per_second": 0.688, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Three hot dogs with mustard on a plate are on a dark countertop.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.61, "peak": 125.46, "min": 31.62}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.12, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 31.12, "energy_joules_est": 45.26, "sample_count": 14, "duration_seconds": 1.454}, "timestamp": "2026-01-19T13:28:14.510881"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1694.466, "latencies_ms": [1694.466], "images_per_second": 0.59, "prompt_tokens": 1114, "response_tokens_est": 25, "n_tiles": 1, "output_text": " hotdog: 4, mustard: 4, bun: 4, plate: 1, book: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.29, "peak": 127.16, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.44, "peak": 40.95, "min": 20.49}}, "power_watts_avg": 31.44, "energy_joules_est": 53.29, "sample_count": 17, "duration_seconds": 1.695}, "timestamp": "2026-01-19T13:28:16.284972"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2068.491, "latencies_ms": [2068.491], "images_per_second": 0.483, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The hot dogs are placed on the left side of the plate, with the mustard being spread on the right side. The plate is positioned in the foreground, with the book in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.1, "peak": 126.23, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.86, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.86, "energy_joules_est": 59.7, "sample_count": 20, "duration_seconds": 2.069}, "timestamp": "2026-01-19T13:28:18.381247"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1452.327, "latencies_ms": [1452.327], "images_per_second": 0.689, "prompt_tokens": 1112, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Three hot dogs with mustard on a plate are on a dark countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.51, "peak": 116.43, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.26, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.26, "energy_joules_est": 45.41, "sample_count": 14, "duration_seconds": 1.453}, "timestamp": "2026-01-19T13:28:19.855563"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2430.268, "latencies_ms": [2430.268], "images_per_second": 0.411, "prompt_tokens": 1110, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a plate with three hot dogs, each topped with mustard, placed on a dark green countertop. The lighting in the image is bright, highlighting the vibrant colors of the food and the contrast between the dark countertop and the lighter hot dogs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.37, "peak": 128.26, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.99, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.99, "energy_joules_est": 68.03, "sample_count": 24, "duration_seconds": 2.431}, "timestamp": "2026-01-19T13:28:22.348093"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2320.316, "latencies_ms": [2320.316], "images_per_second": 0.431, "prompt_tokens": 1432, "response_tokens_est": 33, "n_tiles": 1, "output_text": " In the image, there are four people swimming in the ocean, with a green umbrella and two chairs placed on the beach, and a bird standing on the sand.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.37, "peak": 117.63, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.55, "peak": 41.76, "min": 15.38}}, "power_watts_avg": 29.55, "energy_joules_est": 68.57, "sample_count": 23, "duration_seconds": 2.321}, "timestamp": "2026-01-19T13:28:24.755623"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1797.46, "latencies_ms": [1797.46], "images_per_second": 0.556, "prompt_tokens": 1446, "response_tokens_est": 14, "n_tiles": 1, "output_text": " umbrella: 1\nchairs: 2\npeople: 4", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.07, "peak": 120.84, "min": 27.93}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 16.26, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 32.43, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 32.43, "energy_joules_est": 58.31, "sample_count": 18, "duration_seconds": 1.798}, "timestamp": "2026-01-19T13:28:26.620491"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3053.375, "latencies_ms": [3053.375], "images_per_second": 0.328, "prompt_tokens": 1450, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The green umbrella is positioned to the right of the chairs, which are situated in the foreground of the image. The people are swimming in the water, which is located in the middle ground of the image, while the beach chairs are placed on the sandy shore, which is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.58, "peak": 122.18, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.26, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.51, "peak": 42.54, "min": 18.52}}, "power_watts_avg": 28.51, "energy_joules_est": 87.06, "sample_count": 30, "duration_seconds": 3.054}, "timestamp": "2026-01-19T13:28:29.721344"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2541.494, "latencies_ms": [2541.494], "images_per_second": 0.393, "prompt_tokens": 1444, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a lively beach scene where a group of people are enjoying a swim in the ocean. A green umbrella provides shade for a pair of colorful beach chairs, adding a pop of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.63, "peak": 118.5, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.26, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.69, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 29.69, "energy_joules_est": 75.46, "sample_count": 25, "duration_seconds": 2.542}, "timestamp": "2026-01-19T13:28:32.315528"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2466.489, "latencies_ms": [2466.489], "images_per_second": 0.405, "prompt_tokens": 1442, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image depicts a beach scene with a group of people swimming in the ocean, under a green umbrella with a floral pattern. The sky is clear, and the water is a deep blue color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.05, "peak": 125.23, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.04, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 30.04, "energy_joules_est": 74.11, "sample_count": 24, "duration_seconds": 2.467}, "timestamp": "2026-01-19T13:28:34.837406"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1934.541, "latencies_ms": [1934.541], "images_per_second": 0.517, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image depicts a vintage kitchen with green wallpaper, a white table, a sink, a refrigerator, and a stove, all arranged in a cozy and functional manner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.2, "peak": 126.6, "min": 27.71}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.26, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.26, "energy_joules_est": 56.62, "sample_count": 19, "duration_seconds": 1.935}, "timestamp": "2026-01-19T13:28:36.834236"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2140.027, "latencies_ms": [2140.027], "images_per_second": 0.467, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " table: 1, chair: 1, sink: 1, refrigerator: 1, oven: 1, stove: 1, cabinet: 1, cupboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.8, "peak": 112.95, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.46, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.46, "energy_joules_est": 60.93, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T13:28:39.021093"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2635.156, "latencies_ms": [2635.156], "images_per_second": 0.379, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The white sink is located near the center of the image, with the white refrigerator to its right. The green table is positioned in the foreground, with the green and white cabinet to its left. The green and white stove is located behind the table, with the green and white oven to its right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.38, "peak": 129.5, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.87, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.87, "energy_joules_est": 70.81, "sample_count": 26, "duration_seconds": 2.635}, "timestamp": "2026-01-19T13:28:41.708648"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1906.73, "latencies_ms": [1906.73], "images_per_second": 0.524, "prompt_tokens": 1111, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image depicts a vintage kitchen with green wallpaper and a white table in the center. The kitchen is equipped with a sink, a stove, and a refrigerator.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.66, "peak": 127.16, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.05, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.05, "energy_joules_est": 55.4, "sample_count": 19, "duration_seconds": 1.907}, "timestamp": "2026-01-19T13:28:43.697454"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2266.801, "latencies_ms": [2266.801], "images_per_second": 0.441, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The room is bathed in warm light, with a floral wallpaper and a green and white color scheme. The floor is covered with a floral-patterned carpet, and the walls are adorned with a green and white wallpaper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.02, "peak": 127.87, "min": 30.36}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.94, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.94, "energy_joules_est": 63.35, "sample_count": 22, "duration_seconds": 2.267}, "timestamp": "2026-01-19T13:28:45.989496"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1615.613, "latencies_ms": [1615.613], "images_per_second": 0.619, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A black and white dog is walking on a dirt path next to a tree with green moss on its trunk.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.11, "peak": 118.35, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.95, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.95, "energy_joules_est": 50.02, "sample_count": 16, "duration_seconds": 1.616}, "timestamp": "2026-01-19T13:28:47.664776"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1943.402, "latencies_ms": [1943.402], "images_per_second": 0.515, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " dog: 1, tree: 1, leaves: 1, shadow: 1, ground: 1, bark: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.14, "peak": 113.57, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.92, "energy_joules_est": 58.15, "sample_count": 19, "duration_seconds": 1.944}, "timestamp": "2026-01-19T13:28:49.643087"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2132.231, "latencies_ms": [2132.231], "images_per_second": 0.469, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The dog is in the foreground, walking towards the right side of the image. The tree is in the background, to the left of the dog. The dog is closer to the camera than the tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.02, "peak": 104.96, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.63, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.63, "energy_joules_est": 61.06, "sample_count": 21, "duration_seconds": 2.133}, "timestamp": "2026-01-19T13:28:51.836974"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1632.978, "latencies_ms": [1632.978], "images_per_second": 0.612, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A black and white dog is walking on a dirt path in a wooded area, sniffing around a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.81, "peak": 121.39, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.71, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.71, "energy_joules_est": 50.16, "sample_count": 16, "duration_seconds": 1.633}, "timestamp": "2026-01-19T13:28:53.508281"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2008.329, "latencies_ms": [2008.329], "images_per_second": 0.498, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image features a black and white dog with a blue collar, walking on a dirt path surrounded by fallen leaves. The lighting is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.35, "peak": 129.37, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.41, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.41, "energy_joules_est": 59.08, "sample_count": 20, "duration_seconds": 2.009}, "timestamp": "2026-01-19T13:28:55.585658"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1494.137, "latencies_ms": [1494.137], "images_per_second": 0.669, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is skiing down a snowy hill, wearing a backpack and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.51, "peak": 131.54, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.07, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.07, "energy_joules_est": 46.44, "sample_count": 15, "duration_seconds": 1.495}, "timestamp": "2026-01-19T13:28:57.155770"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.479, "latencies_ms": [2590.479], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. skis: 2\n2. ski poles: 2\n3. backpack: 1\n4. helmet: 1\n5. gloves: 2\n6. jacket: 1\n7. pants: 1\n8. snow: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.66, "peak": 116.96, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.34, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 27.34, "energy_joules_est": 70.84, "sample_count": 25, "duration_seconds": 2.591}, "timestamp": "2026-01-19T13:28:59.771821"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2440.58, "latencies_ms": [2440.58], "images_per_second": 0.41, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The skier is positioned in the foreground of the image, with the snowy mountain landscape and trees in the background. The skier is facing towards the left side of the image, with the ski poles in their hands and the ski poles in the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.1, "peak": 131.66, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.2, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.2, "energy_joules_est": 66.39, "sample_count": 24, "duration_seconds": 2.441}, "timestamp": "2026-01-19T13:29:02.267000"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1415.192, "latencies_ms": [1415.192], "images_per_second": 0.707, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A person is skiing down a snowy hill with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.91, "peak": 103.46, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.01, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.01, "energy_joules_est": 43.89, "sample_count": 14, "duration_seconds": 1.415}, "timestamp": "2026-01-19T13:29:03.724897"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2472.007, "latencies_ms": [2472.007], "images_per_second": 0.405, "prompt_tokens": 1110, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a person skiing down a snowy hill, wearing a white helmet and black pants. The sky is clear and blue, indicating a sunny day. The snow is pristine white, and the person's skis are green and orange, contrasting with the white snow.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.38, "peak": 126.99, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 27.99, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 27.99, "energy_joules_est": 69.2, "sample_count": 24, "duration_seconds": 2.472}, "timestamp": "2026-01-19T13:29:06.219691"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1966.785, "latencies_ms": [1966.785], "images_per_second": 0.508, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " A vibrant orange and black train with the number 639 on its side is moving along a track, with a clear blue sky and leafless trees in the background.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.4, "peak": 132.42, "min": 30.43}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.26, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.26, "energy_joules_est": 57.56, "sample_count": 19, "duration_seconds": 1.967}, "timestamp": "2026-01-19T13:29:08.204752"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2530.51, "latencies_ms": [2530.51], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. train: 1\n2. tracks: 1\n3. trees: 1\n4. sky: 1\n5. bushes: 1\n6. fence: 1\n7. gravel: 1\n8. grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.56, "peak": 114.59, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.26, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 27.26, "energy_joules_est": 68.99, "sample_count": 25, "duration_seconds": 2.531}, "timestamp": "2026-01-19T13:29:10.804784"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2172.617, "latencies_ms": [2172.617], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The orange and black train is positioned on the left side of the image, with the clear blue sky and leafless trees in the background. The train is in the foreground, with the gravel track beneath it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.13, "peak": 115.13, "min": 32.95}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.29, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.29, "energy_joules_est": 61.47, "sample_count": 21, "duration_seconds": 2.173}, "timestamp": "2026-01-19T13:29:12.984821"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1857.659, "latencies_ms": [1857.659], "images_per_second": 0.538, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A vibrant orange and black train with the number 639 on the front is moving along a track, surrounded by trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.26, "peak": 105.57, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 30.1, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 30.1, "energy_joules_est": 55.92, "sample_count": 18, "duration_seconds": 1.858}, "timestamp": "2026-01-19T13:29:14.860227"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1994.636, "latencies_ms": [1994.636], "images_per_second": 0.501, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The locomotive is painted in vibrant shades of orange and yellow, with black and white accents. The sky is a clear blue, and the trees are bare, suggesting a cold season.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.28, "peak": 114.25, "min": 28.04}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.18, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 29.18, "energy_joules_est": 58.22, "sample_count": 20, "duration_seconds": 1.995}, "timestamp": "2026-01-19T13:29:16.940733"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2097.159, "latencies_ms": [2097.159], "images_per_second": 0.477, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a wooden table with a plate containing a serving of avocado spread on a slice of whole grain bread, a bowl of broccoli with a sprinkle of seasoning, and a side of white dip.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.68, "peak": 107.39, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.31, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.31, "energy_joules_est": 59.38, "sample_count": 21, "duration_seconds": 2.097}, "timestamp": "2026-01-19T13:29:19.134271"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2046.406, "latencies_ms": [2046.406], "images_per_second": 0.489, "prompt_tokens": 1113, "response_tokens_est": 37, "n_tiles": 1, "output_text": " plate: 1, bread: 2, broccoli: 1, guacamole: 1, bowl: 1, spoon: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.87, "peak": 122.34, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.66, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.66, "energy_joules_est": 58.66, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T13:29:21.224156"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2879.611, "latencies_ms": [2879.611], "images_per_second": 0.347, "prompt_tokens": 1117, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The plate is located in the foreground, with the bread, bowl of broccoli, and bowl of guacamole arranged in a left-right-left pattern. The bread is positioned on the left side of the plate, while the bowl of broccoli is on the right side, and the bowl of guacamole is in the middle.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.52, "peak": 126.95, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.38, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.38, "energy_joules_est": 75.97, "sample_count": 28, "duration_seconds": 2.88}, "timestamp": "2026-01-19T13:29:24.119207"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2015.034, "latencies_ms": [2015.034], "images_per_second": 0.496, "prompt_tokens": 1111, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image captures a meal set on a wooden table, featuring a plate with a serving of broccoli, a slice of bread topped with guacamole, and a bowl of quinoa.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.98, "peak": 121.58, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.82, "peak": 39.38, "min": 16.55}}, "power_watts_avg": 28.82, "energy_joules_est": 58.09, "sample_count": 20, "duration_seconds": 2.016}, "timestamp": "2026-01-19T13:29:26.204422"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2861.916, "latencies_ms": [2861.916], "images_per_second": 0.349, "prompt_tokens": 1109, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image features a wooden table with a plate of food on it, including a bowl of broccoli and a slice of bread topped with guacamole. The lighting is natural, and the colors are vibrant, with the green of the broccoli contrasting against the brown of the bread and the blue of the bowl.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.56, "peak": 126.29, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 25.62, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 25.62, "energy_joules_est": 73.33, "sample_count": 28, "duration_seconds": 2.862}, "timestamp": "2026-01-19T13:29:29.123085"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1689.466, "latencies_ms": [1689.466], "images_per_second": 0.592, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A person is sleeping on a bench covered with an orange blanket, with a blue bag and a red bag nearby.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.92, "peak": 111.88, "min": 27.26}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.69, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.69, "energy_joules_est": 50.17, "sample_count": 17, "duration_seconds": 1.69}, "timestamp": "2026-01-19T13:29:30.901260"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2323.501, "latencies_ms": [2323.501], "images_per_second": 0.43, "prompt_tokens": 1113, "response_tokens_est": 47, "n_tiles": 1, "output_text": " bench: 1\npark bench: 1\npark bench blanket: 1\npark bench bag: 1\npark bench bag strap: 1\npark bench lamp: 2\npark bench parking meter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.59, "peak": 121.01, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.73, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 27.73, "energy_joules_est": 64.44, "sample_count": 23, "duration_seconds": 2.324}, "timestamp": "2026-01-19T13:29:33.300679"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2059.91, "latencies_ms": [2059.91], "images_per_second": 0.485, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The bench is located in the foreground of the image, with the parking meters and fence in the background. The person is lying on the bench, with the backpack and blanket near them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.03, "peak": 115.73, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.58, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.58, "energy_joules_est": 58.88, "sample_count": 20, "duration_seconds": 2.06}, "timestamp": "2026-01-19T13:29:35.383761"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1348.7, "latencies_ms": [1348.7], "images_per_second": 0.741, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A person is sleeping on a bench in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.5, "peak": 111.77, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.55, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 32.55, "energy_joules_est": 43.91, "sample_count": 13, "duration_seconds": 1.349}, "timestamp": "2026-01-19T13:29:36.745655"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1864.419, "latencies_ms": [1864.419], "images_per_second": 0.536, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The bench is made of wood and is covered with an orange blanket. The scene is bathed in natural light, and the grass is a vibrant green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.87, "peak": 129.73, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 31.54, "peak": 40.97, "min": 21.28}}, "power_watts_avg": 31.54, "energy_joules_est": 58.81, "sample_count": 18, "duration_seconds": 1.865}, "timestamp": "2026-01-19T13:29:38.622209"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1960.794, "latencies_ms": [1960.794], "images_per_second": 0.51, "prompt_tokens": 1100, "response_tokens_est": 35, "n_tiles": 1, "output_text": " A tall, colorful vase with a twisted design is placed on a white pedestal in front of a brown wall, with a red vase and a decorative plate in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.36, "peak": 107.63, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.57, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 29.57, "energy_joules_est": 57.99, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T13:29:40.606192"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1957.149, "latencies_ms": [1957.149], "images_per_second": 0.511, "prompt_tokens": 1114, "response_tokens_est": 35, "n_tiles": 1, "output_text": " vase: 1, pot: 1, wall: 1, wall art: 1, paper: 1, tag: 1, flower: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.24, "peak": 120.33, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.55, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.55, "energy_joules_est": 57.84, "sample_count": 19, "duration_seconds": 1.957}, "timestamp": "2026-01-19T13:29:42.583484"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2046.28, "latencies_ms": [2046.28], "images_per_second": 0.489, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The vase is positioned in the foreground, with the wall and other objects in the background. The vase is placed on a white pedestal, which is situated in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.14, "peak": 121.71, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.02, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.02, "energy_joules_est": 59.4, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T13:29:44.664582"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2873.332, "latencies_ms": [2873.332], "images_per_second": 0.348, "prompt_tokens": 1112, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The image captures a vibrant display of ceramic art in a gallery setting. A tall, twisted vase filled with dried flowers and twigs stands out against the brown wall, its colors ranging from deep purple to bright yellow and green. The vase is placed on a white pedestal, drawing attention to its unique design and the intricate details of the dried flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.68, "peak": 129.18, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 25.93, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 25.93, "energy_joules_est": 74.51, "sample_count": 28, "duration_seconds": 2.874}, "timestamp": "2026-01-19T13:29:47.573368"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2076.174, "latencies_ms": [2076.174], "images_per_second": 0.482, "prompt_tokens": 1110, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The vase is a vibrant mix of colors, with shades of green, yellow, and brown. The lighting is bright and even, highlighting the textures and colors of the vase and the surrounding objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.19, "peak": 108.36, "min": 32.85}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.66, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.66, "energy_joules_est": 59.51, "sample_count": 20, "duration_seconds": 2.076}, "timestamp": "2026-01-19T13:29:49.655566"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1672.405, "latencies_ms": [1672.405], "images_per_second": 0.598, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A skateboarder wearing a helmet and knee pads is performing a trick on a skateboard in a concrete skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.2, "peak": 128.77, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.54, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.54, "energy_joules_est": 51.1, "sample_count": 17, "duration_seconds": 1.673}, "timestamp": "2026-01-19T13:29:51.430826"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2963.652, "latencies_ms": [2963.652], "images_per_second": 0.337, "prompt_tokens": 1113, "response_tokens_est": 71, "n_tiles": 1, "output_text": " 1. skateboard: 1\n2. helmet: 1\n3. knee pads: 1\n4. skateboard wheels: 2\n5. skateboard deck: 1\n6. skateboard trucks: 1\n7. skateboard deck grip tape: 1\n8. skateboard deck trucks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.7, "peak": 115.92, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 25.96, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 25.96, "energy_joules_est": 76.96, "sample_count": 29, "duration_seconds": 2.964}, "timestamp": "2026-01-19T13:29:54.441894"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2019.804, "latencies_ms": [2019.804], "images_per_second": 0.495, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The skateboarder is in the foreground, performing a trick in the center of the image. The skatepark is in the background, with ramps and other skateboarders visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.14, "peak": 118.68, "min": 29.77}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.64, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.64, "energy_joules_est": 57.86, "sample_count": 20, "duration_seconds": 2.02}, "timestamp": "2026-01-19T13:29:56.524598"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1521.185, "latencies_ms": [1521.185], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A skateboarder is performing a trick on a concrete bowl at a skatepark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.67, "peak": 124.19, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.25, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 31.25, "energy_joules_est": 47.55, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T13:29:58.092872"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1760.527, "latencies_ms": [1760.527], "images_per_second": 0.568, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The skateboarder is wearing a helmet and knee pads, and the skate park is lit by natural light during the golden hour.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.81, "peak": 126.85, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.05, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 31.05, "energy_joules_est": 54.67, "sample_count": 17, "duration_seconds": 1.761}, "timestamp": "2026-01-19T13:29:59.865918"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2278.537, "latencies_ms": [2278.537], "images_per_second": 0.439, "prompt_tokens": 1099, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image captures a lively night scene in a bustling city square, where a majestic clock tower stands tall, adorned with a festive Christmas tree and twinkling lights, while the surrounding buildings are illuminated, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.9, "peak": 129.31, "min": 30.2}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.54, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.38, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 28.38, "energy_joules_est": 64.67, "sample_count": 22, "duration_seconds": 2.279}, "timestamp": "2026-01-19T13:30:02.164372"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1960.155, "latencies_ms": [1960.155], "images_per_second": 0.51, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " clock: 1, tower: 1, lights: 1, tree: 1, building: 1, people: 1, street: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.46, "peak": 109.17, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.38, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 29.38, "energy_joules_est": 57.6, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T13:30:04.150503"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2303.815, "latencies_ms": [2303.815], "images_per_second": 0.434, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The clock tower is located in the center of the image, with the Christmas tree to its right. The street is in the foreground, with the buildings in the background. The clock tower is closer to the viewer than the Christmas tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.4, "peak": 134.61, "min": 30.18}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.82, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.82, "energy_joules_est": 64.11, "sample_count": 23, "duration_seconds": 2.304}, "timestamp": "2026-01-19T13:30:06.550390"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2729.888, "latencies_ms": [2729.888], "images_per_second": 0.366, "prompt_tokens": 1111, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image captures a lively night scene in a bustling city square, where a majestic clock tower stands tall, adorned with twinkling lights that add a magical touch to the urban landscape. The square is alive with people, some strolling leisurely, others gathered around a beautifully decorated Christmas tree, creating a festive atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.27, "peak": 120.28, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.11, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.11, "energy_joules_est": 71.3, "sample_count": 27, "duration_seconds": 2.731}, "timestamp": "2026-01-19T13:30:09.359277"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3345.13, "latencies_ms": [3345.13], "images_per_second": 0.299, "prompt_tokens": 1109, "response_tokens_est": 86, "n_tiles": 1, "output_text": " The image captures a vibrant night scene in a bustling city square, where a majestic clock tower stands tall, its white face and black hands glowing under the artificial lights. The square is adorned with a festive Christmas tree, its branches heavy with twinkling lights and ornaments, and a crowd of people can be seen enjoying the festive atmosphere. The sky above is a deep, dark blue, providing a stark contrast to the illuminated cityscape below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.82, "peak": 122.04, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 24.91, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 24.91, "energy_joules_est": 83.33, "sample_count": 33, "duration_seconds": 3.345}, "timestamp": "2026-01-19T13:30:12.789229"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1520.577, "latencies_ms": [1520.577], "images_per_second": 0.658, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A young man is playing tennis on a court with a tennis ball in the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.87, "peak": 131.16, "min": 27.63}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.68, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 30.68, "energy_joules_est": 46.68, "sample_count": 15, "duration_seconds": 1.521}, "timestamp": "2026-01-19T13:30:14.359532"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2909.036, "latencies_ms": [2909.036], "images_per_second": 0.344, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. person: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. chain link fence: 1\n5. tennis court: 1\n6. chain link fence post: 1\n7. tennis court line: 1\n8. logo: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.06, "peak": 126.14, "min": 33.19}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.62, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 26.62, "energy_joules_est": 77.45, "sample_count": 28, "duration_seconds": 2.909}, "timestamp": "2026-01-19T13:30:17.272634"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2608.869, "latencies_ms": [2608.869], "images_per_second": 0.383, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The tennis player is in the foreground of the image, holding a tennis racket and preparing to hit a tennis ball. The tennis ball is in the middle ground, slightly to the right of the player. The tennis court is in the background, with a chain link fence and trees surrounding it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.1, "peak": 115.05, "min": 27.01}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.96}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.64, "peak": 39.38, "min": 16.16}}, "power_watts_avg": 26.64, "energy_joules_est": 69.51, "sample_count": 26, "duration_seconds": 2.609}, "timestamp": "2026-01-19T13:30:19.981188"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1538.307, "latencies_ms": [1538.307], "images_per_second": 0.65, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A young man is playing tennis on a court with a green fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.75, "peak": 127.69, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.52, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 30.52, "energy_joules_est": 46.96, "sample_count": 15, "duration_seconds": 1.539}, "timestamp": "2026-01-19T13:30:21.549030"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1661.768, "latencies_ms": [1661.768], "images_per_second": 0.602, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image is taken during the day with natural lighting, and the tennis court is made of concrete with white lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.85, "peak": 114.79, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.86, "peak": 40.56, "min": 20.1}}, "power_watts_avg": 31.86, "energy_joules_est": 52.96, "sample_count": 16, "duration_seconds": 1.662}, "timestamp": "2026-01-19T13:30:23.223373"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1859.132, "latencies_ms": [1859.132], "images_per_second": 0.538, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a fireplace, a comfortable armchair, and a bookshelf filled with books, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.88, "peak": 129.97, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.62, "peak": 40.57, "min": 19.31}}, "power_watts_avg": 30.62, "energy_joules_est": 56.94, "sample_count": 18, "duration_seconds": 1.86}, "timestamp": "2026-01-19T13:30:25.099616"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2612.038, "latencies_ms": [2612.038], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. sofa: 1\n2. bookshelves: 2\n3. lamp: 2\n4. fireplace: 1\n5. painting: 1\n6. plant: 2\n7. chair: 1\n8. table: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.24, "peak": 106.69, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.86}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.99, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.99, "energy_joules_est": 70.51, "sample_count": 26, "duration_seconds": 2.612}, "timestamp": "2026-01-19T13:30:27.814327"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2990.012, "latencies_ms": [2990.012], "images_per_second": 0.334, "prompt_tokens": 1117, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The beige armchair is positioned in the foreground, to the right of the fireplace, with the green plant to its left. The white bookshelves are located behind the armchair, with the framed picture hanging above the fireplace. The beige sofa is situated in the background, to the left of the fireplace, with the green plant to its right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.64, "peak": 120.3, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 25.73, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 25.73, "energy_joules_est": 76.94, "sample_count": 29, "duration_seconds": 2.99}, "timestamp": "2026-01-19T13:30:30.832601"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1544.488, "latencies_ms": [1544.488], "images_per_second": 0.647, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A living room with a fireplace, two lamps, a chair, and a bookshelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.09, "peak": 129.74, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.12, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.12, "energy_joules_est": 48.08, "sample_count": 15, "duration_seconds": 1.545}, "timestamp": "2026-01-19T13:30:32.399386"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1708.545, "latencies_ms": [1708.545], "images_per_second": 0.585, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming from the windows, and the walls are painted in a light color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.59, "peak": 115.47, "min": 30.25}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.1, "peak": 40.18, "min": 19.71}}, "power_watts_avg": 31.1, "energy_joules_est": 53.15, "sample_count": 17, "duration_seconds": 1.709}, "timestamp": "2026-01-19T13:30:34.173060"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1965.233, "latencies_ms": [1965.233], "images_per_second": 0.509, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " In the image, a group of zebras is seen grazing on the lush green grass in a field, with one zebra standing out due to its unique black and white stripes.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.2, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.17, "peak": 118.38, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.61, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.61, "energy_joules_est": 58.21, "sample_count": 19, "duration_seconds": 1.966}, "timestamp": "2026-01-19T13:30:36.165010"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1564.794, "latencies_ms": [1564.794], "images_per_second": 0.639, "prompt_tokens": 1113, "response_tokens_est": 20, "n_tiles": 1, "output_text": " zebra: 4\ngrass: 1\ndirt: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.2, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.18, "peak": 129.78, "min": 27.1}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 31.17, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.17, "energy_joules_est": 48.78, "sample_count": 16, "duration_seconds": 1.565}, "timestamp": "2026-01-19T13:30:37.835396"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2272.941, "latencies_ms": [2272.941], "images_per_second": 0.44, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the dirt mound and grass in the background. The zebras are facing towards the left side of the image, with the dirt mound located to the right of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 57.23, "peak": 116.27, "min": 30.39}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.53, "min": 12.22}, "VDD_GPU": {"avg": 28.22, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 28.22, "energy_joules_est": 64.16, "sample_count": 22, "duration_seconds": 2.273}, "timestamp": "2026-01-19T13:30:40.135701"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2126.608, "latencies_ms": [2126.608], "images_per_second": 0.47, "prompt_tokens": 1111, "response_tokens_est": 41, "n_tiles": 1, "output_text": " In the image, a group of zebras are grazing on a grassy hillside under a clear blue sky. The zebras are standing close to each other, with one of them bending down to eat grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.71, "peak": 125.91, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.55, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.55, "energy_joules_est": 60.73, "sample_count": 21, "duration_seconds": 2.127}, "timestamp": "2026-01-19T13:30:42.324308"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2176.76, "latencies_ms": [2176.76], "images_per_second": 0.459, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a group of zebras in a grassy field with a clear blue sky in the background. The zebras are grazing on the grass, with their black and white stripes standing out against the green backdrop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.37, "peak": 126.71, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.44, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.44, "energy_joules_est": 61.92, "sample_count": 21, "duration_seconds": 2.177}, "timestamp": "2026-01-19T13:30:44.514109"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1826.681, "latencies_ms": [1826.681], "images_per_second": 0.547, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A group of people are sitting at a table in a restaurant, with a man wearing a hat and a woman wearing a hat sitting at the table.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.93, "peak": 124.39, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.01, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.01, "energy_joules_est": 54.83, "sample_count": 18, "duration_seconds": 1.827}, "timestamp": "2026-01-19T13:30:46.392134"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2981.874, "latencies_ms": [2981.874], "images_per_second": 0.335, "prompt_tokens": 1113, "response_tokens_est": 73, "n_tiles": 1, "output_text": " object: chair, count: 12\nobject: table, count: 1\nobject: person, count: 12\nobject: wall, count: 1\nobject: floor, count: 1\nobject: wall, count: 1\nobject: wall, count: 1\nobject: wall, count: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.25, "peak": 113.29, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.19, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 26.19, "energy_joules_est": 78.1, "sample_count": 29, "duration_seconds": 2.982}, "timestamp": "2026-01-19T13:30:49.407276"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2557.527, "latencies_ms": [2557.527], "images_per_second": 0.391, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The main objects are arranged in a way that the table is in the foreground, with the people sitting around it. The people are positioned in the middle of the image, with the table being the central focus. The background features the restaurant's interior, including the kitchen and other patrons.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.44, "peak": 124.73, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.96, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.96, "energy_joules_est": 68.96, "sample_count": 25, "duration_seconds": 2.558}, "timestamp": "2026-01-19T13:30:52.003424"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1391.91, "latencies_ms": [1391.91], "images_per_second": 0.718, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A group of people are sitting at a table in a restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.8, "peak": 119.88, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 31.71, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.71, "energy_joules_est": 44.15, "sample_count": 14, "duration_seconds": 1.392}, "timestamp": "2026-01-19T13:30:53.456845"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2326.518, "latencies_ms": [2326.518], "images_per_second": 0.43, "prompt_tokens": 1109, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image is taken in a restaurant with a warm and inviting atmosphere, characterized by the rich brown color of the wooden floor and the warm lighting that illuminates the space. The walls are adorned with brick, adding a rustic charm to the setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.44, "peak": 125.06, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.48, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 28.48, "energy_joules_est": 66.27, "sample_count": 23, "duration_seconds": 2.327}, "timestamp": "2026-01-19T13:30:55.853994"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2004.558, "latencies_ms": [2004.558], "images_per_second": 0.499, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " In the image, a group of white swans are swimming in a body of water near a marina, where several boats are docked and covered with purple tarps.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.9, "peak": 113.94, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.54, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.54, "energy_joules_est": 57.22, "sample_count": 20, "duration_seconds": 2.005}, "timestamp": "2026-01-19T13:30:57.942679"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1891.204, "latencies_ms": [1891.204], "images_per_second": 0.529, "prompt_tokens": 1113, "response_tokens_est": 31, "n_tiles": 1, "output_text": " 1. swans: 14\n2. boats: 10\n3. dock: 1\n4. water: 1", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.61, "peak": 125.5, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.03, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.03, "energy_joules_est": 54.92, "sample_count": 19, "duration_seconds": 1.892}, "timestamp": "2026-01-19T13:30:59.922213"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2275.796, "latencies_ms": [2275.796], "images_per_second": 0.439, "prompt_tokens": 1117, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The swans are positioned in the foreground of the image, with the boats in the background. The boats are docked on the left side of the image, while the swans are swimming in the water on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.02, "peak": 113.52, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.72, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 27.72, "energy_joules_est": 63.1, "sample_count": 23, "duration_seconds": 2.276}, "timestamp": "2026-01-19T13:31:02.301290"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1659.084, "latencies_ms": [1659.084], "images_per_second": 0.603, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A group of swans are swimming in the water near a marina with boats docked in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.52, "peak": 123.35, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.23, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.23, "energy_joules_est": 50.16, "sample_count": 16, "duration_seconds": 1.659}, "timestamp": "2026-01-19T13:31:03.974309"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2460.911, "latencies_ms": [2460.911], "images_per_second": 0.406, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a serene harbor scene with a group of swans swimming in the water, their white feathers contrasting with the dark blue-gray of the water. The sky is a clear blue, and the sun is shining brightly, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.24, "peak": 110.75, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.14, "peak": 40.57, "min": 19.31}}, "power_watts_avg": 28.14, "energy_joules_est": 69.26, "sample_count": 24, "duration_seconds": 2.461}, "timestamp": "2026-01-19T13:31:06.456087"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1622.796, "latencies_ms": [1622.796], "images_per_second": 0.616, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man and woman are cutting a wedding cake in a tent with a tablecloth and a guitar in the background.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.8, "peak": 113.91, "min": 30.73}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.82, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.82, "energy_joules_est": 50.03, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T13:31:08.126076"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1925.57, "latencies_ms": [1925.57], "images_per_second": 0.519, "prompt_tokens": 1114, "response_tokens_est": 34, "n_tiles": 1, "output_text": " 1. Tent\n2. Table\n3. Cake\n4. People\n5. Chair\n6. Speaker\n7. Guitar\n8. Flags", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.66, "peak": 107.52, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.86, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.86, "energy_joules_est": 57.51, "sample_count": 19, "duration_seconds": 1.926}, "timestamp": "2026-01-19T13:31:10.092278"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2653.939, "latencies_ms": [2653.939], "images_per_second": 0.377, "prompt_tokens": 1118, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The bride and groom are standing close to each other, with the bride on the left and the groom on the right. The cake is placed in the middle of the table, which is positioned in the foreground. The tent is located in the background, with the guests standing near the edge of the tent.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.2, "peak": 122.4, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.84, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 26.84, "energy_joules_est": 71.24, "sample_count": 26, "duration_seconds": 2.654}, "timestamp": "2026-01-19T13:31:12.792636"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1487.692, "latencies_ms": [1487.692], "images_per_second": 0.672, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A wedding is taking place in a tent with a table with a cake on it.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.51, "peak": 108.07, "min": 27.93}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.57, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.57, "energy_joules_est": 45.49, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T13:31:14.349024"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1865.274, "latencies_ms": [1865.274], "images_per_second": 0.536, "prompt_tokens": 1110, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The wedding is taking place in a tent with white walls and a wooden floor. The lighting is dim, and the tent is decorated with colorful flags.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.29, "peak": 110.4, "min": 33.52}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.18, "peak": 40.16, "min": 18.53}}, "power_watts_avg": 30.18, "energy_joules_est": 56.31, "sample_count": 18, "duration_seconds": 1.866}, "timestamp": "2026-01-19T13:31:16.219437"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2041.51, "latencies_ms": [2041.51], "images_per_second": 0.49, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a blue sofa, a coffee table, and a red patterned rug, all set against a backdrop of red walls adorned with various paintings and framed pictures.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.35, "peak": 113.87, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.41, "peak": 40.16, "min": 18.91}}, "power_watts_avg": 29.41, "energy_joules_est": 60.06, "sample_count": 20, "duration_seconds": 2.042}, "timestamp": "2026-01-19T13:31:18.298251"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1953.781, "latencies_ms": [1953.781], "images_per_second": 0.512, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " sofa: 1, table: 2, lamp: 1, plant: 2, rug: 1, painting: 1, window: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.5, "peak": 117.31, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.63, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.63, "energy_joules_est": 57.9, "sample_count": 19, "duration_seconds": 1.954}, "timestamp": "2026-01-19T13:31:20.276655"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2542.868, "latencies_ms": [2542.868], "images_per_second": 0.393, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The sofa is positioned in the center of the room, with the coffee table in front of it and the side table to its left. The painting is hung on the wall above the sofa, while the lamp is placed on the side table to the left of the sofa.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.31, "peak": 123.12, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.25, "peak": 39.77, "min": 17.73}}, "power_watts_avg": 27.25, "energy_joules_est": 69.3, "sample_count": 25, "duration_seconds": 2.543}, "timestamp": "2026-01-19T13:31:22.879146"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1524.444, "latencies_ms": [1524.444], "images_per_second": 0.656, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A living room with red walls and a blue couch, a coffee table and a rug.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.42, "peak": 119.42, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.07, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.07, "energy_joules_est": 47.38, "sample_count": 15, "duration_seconds": 1.525}, "timestamp": "2026-01-19T13:31:24.442390"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1826.706, "latencies_ms": [1826.706], "images_per_second": 0.547, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The room is painted red with white curtains and a red and yellow rug. The room is well-lit with natural light coming in from the windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.29, "peak": 125.37, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.82, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 30.82, "energy_joules_est": 56.31, "sample_count": 18, "duration_seconds": 1.827}, "timestamp": "2026-01-19T13:31:26.316666"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2709.007, "latencies_ms": [2709.007], "images_per_second": 0.369, "prompt_tokens": 1432, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image depicts a surreal scene where a doll with red hair and a black dress is sitting at a table with a clock on it, and the clock's face is mirrored on the doll's face, creating a sense of symmetry and duality.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.96, "peak": 129.93, "min": 27.32}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.26, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.6, "peak": 41.74, "min": 17.35}}, "power_watts_avg": 28.6, "energy_joules_est": 77.5, "sample_count": 27, "duration_seconds": 2.71}, "timestamp": "2026-01-19T13:31:29.131214"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2051.552, "latencies_ms": [2051.552], "images_per_second": 0.487, "prompt_tokens": 1446, "response_tokens_est": 24, "n_tiles": 1, "output_text": " clock: 1, doll: 1, table: 1, wall: 1, shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.98, "peak": 134.61, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.26, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.6, "peak": 41.76, "min": 15.38}}, "power_watts_avg": 31.6, "energy_joules_est": 64.86, "sample_count": 20, "duration_seconds": 2.052}, "timestamp": "2026-01-19T13:31:31.215821"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2612.001, "latencies_ms": [2612.001], "images_per_second": 0.383, "prompt_tokens": 1450, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The clock is positioned to the left of the doll, and the doll is situated in the foreground of the image. The clock is closer to the viewer than the doll, and the doll is positioned in front of the clock.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.34, "peak": 110.93, "min": 28.1}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.36, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 29.82, "peak": 41.76, "min": 18.52}}, "power_watts_avg": 29.82, "energy_joules_est": 77.9, "sample_count": 26, "duration_seconds": 2.612}, "timestamp": "2026-01-19T13:31:33.915507"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1997.732, "latencies_ms": [1997.732], "images_per_second": 0.501, "prompt_tokens": 1444, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A doll with a clock on its face is sitting at a table with a red object in front of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.28, "peak": 122.97, "min": 28.2}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 16.36, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.59, "peak": 42.13, "min": 15.77}}, "power_watts_avg": 31.59, "energy_joules_est": 63.12, "sample_count": 20, "duration_seconds": 1.998}, "timestamp": "2026-01-19T13:31:35.996822"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2809.754, "latencies_ms": [2809.754], "images_per_second": 0.356, "prompt_tokens": 1442, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a doll with a clock face on its head, set against a white background. The lighting is soft and diffused, creating a warm and inviting atmosphere. The doll's hair is a vibrant red, and it is dressed in a black outfit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.33, "peak": 131.65, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 16.26, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 28.86, "peak": 42.13, "min": 17.73}}, "power_watts_avg": 28.86, "energy_joules_est": 81.1, "sample_count": 28, "duration_seconds": 2.81}, "timestamp": "2026-01-19T13:31:38.909304"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1480.194, "latencies_ms": [1480.194], "images_per_second": 0.676, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man wearing a helmet and a jacket is sitting on a scooter.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.9, "peak": 129.74, "min": 27.26}, "VIN_SYS_5V0": {"avg": 14.01, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.36, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.36, "energy_joules_est": 44.96, "sample_count": 15, "duration_seconds": 1.481}, "timestamp": "2026-01-19T13:31:40.484461"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2663.587, "latencies_ms": [2663.587], "images_per_second": 0.375, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. helmet: 1\n3. scooter: 1\n4. jacket: 1\n5. pants: 1\n6. shoes: 1\n7. seat: 1\n8. seatbelt: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.55, "peak": 132.9, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.34, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.78, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 26.78, "energy_joules_est": 71.34, "sample_count": 26, "duration_seconds": 2.664}, "timestamp": "2026-01-19T13:31:43.181786"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2134.667, "latencies_ms": [2134.667], "images_per_second": 0.468, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The person is sitting on the left side of the scooter, which is positioned in the foreground of the image. The person is wearing a helmet, which is located near the top of the scooter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.54, "peak": 123.05, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.27, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.27, "energy_joules_est": 60.35, "sample_count": 21, "duration_seconds": 2.135}, "timestamp": "2026-01-19T13:31:45.363566"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1375.871, "latencies_ms": [1375.871], "images_per_second": 0.727, "prompt_tokens": 1112, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A man wearing a helmet is sitting on a scooter.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.63, "peak": 121.45, "min": 27.63}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.1, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 31.1, "energy_joules_est": 42.8, "sample_count": 14, "duration_seconds": 1.376}, "timestamp": "2026-01-19T13:31:46.827017"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1771.491, "latencies_ms": [1771.491], "images_per_second": 0.564, "prompt_tokens": 1110, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a red background. The person is wearing a beige jacket and black shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.67, "peak": 124.98, "min": 27.85}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.65, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.32, "peak": 40.97, "min": 19.32}}, "power_watts_avg": 30.32, "energy_joules_est": 53.73, "sample_count": 18, "duration_seconds": 1.772}, "timestamp": "2026-01-19T13:31:48.697124"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1450.034, "latencies_ms": [1450.034], "images_per_second": 0.69, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A large pizza with cheese and herbs is being prepared on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.08, "peak": 126.56, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.46, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 31.46, "energy_joules_est": 45.64, "sample_count": 14, "duration_seconds": 1.451}, "timestamp": "2026-01-19T13:31:50.163509"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2541.114, "latencies_ms": [2541.114], "images_per_second": 0.394, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. pizza: 1\n2. person: 1\n3. spatula: 1\n4. table: 1\n5. counter: 1\n6. oven: 1\n7. pan: 1\n8. cheese: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.18, "peak": 128.78, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 27.86, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 27.86, "energy_joules_est": 70.81, "sample_count": 25, "duration_seconds": 2.542}, "timestamp": "2026-01-19T13:31:52.771604"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2079.364, "latencies_ms": [2079.364], "images_per_second": 0.481, "prompt_tokens": 1118, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The pizza is in the foreground, with the person's hand in the background. The pizza is on the left side of the image, while the person's hand is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.12, "peak": 125.33, "min": 27.91}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.01, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.01, "energy_joules_est": 58.25, "sample_count": 21, "duration_seconds": 2.08}, "timestamp": "2026-01-19T13:31:54.957143"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1551.535, "latencies_ms": [1551.535], "images_per_second": 0.645, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A large pizza with cheese and herbs is being prepared on a wooden table in a kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.92, "peak": 113.47, "min": 31.11}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.21}, "VDD_GPU": {"avg": 30.15, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.15, "energy_joules_est": 46.79, "sample_count": 15, "duration_seconds": 1.552}, "timestamp": "2026-01-19T13:31:56.516856"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1694.169, "latencies_ms": [1694.169], "images_per_second": 0.59, "prompt_tokens": 1110, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The pizza is golden brown and has a creamy white sauce. The lighting is dim and the pizza is on a wooden table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.17, "peak": 121.37, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.1, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.1, "energy_joules_est": 52.7, "sample_count": 17, "duration_seconds": 1.695}, "timestamp": "2026-01-19T13:31:58.289579"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1486.98, "latencies_ms": [1486.98], "images_per_second": 0.673, "prompt_tokens": 1100, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A woman in a white dress and white shoes is playing tennis on a grass court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.09, "peak": 122.55, "min": 27.15}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.31, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.31, "energy_joules_est": 46.58, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T13:31:59.858906"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2780.148, "latencies_ms": [2780.148], "images_per_second": 0.36, "prompt_tokens": 1114, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. tennis racket: 1\n2. woman: 1\n3. grass: 1\n4. white: 1\n5. white skirt: 1\n6. white shoes: 1\n7. white visor: 1\n8. white hat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.66, "peak": 125.27, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.34, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.68, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 26.68, "energy_joules_est": 74.2, "sample_count": 27, "duration_seconds": 2.781}, "timestamp": "2026-01-19T13:32:02.665807"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2195.729, "latencies_ms": [2195.729], "images_per_second": 0.455, "prompt_tokens": 1118, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, with the tennis net and grass court in the background. The player is reaching up with her racket, indicating she is in the process of serving the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.55, "peak": 120.75, "min": 29.9}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.85, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.85, "energy_joules_est": 61.16, "sample_count": 22, "duration_seconds": 2.196}, "timestamp": "2026-01-19T13:32:04.958165"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1427.42, "latencies_ms": [1427.42], "images_per_second": 0.701, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A woman in a white dress is playing tennis on a grass court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.73, "peak": 134.13, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.7, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.7, "energy_joules_est": 43.84, "sample_count": 14, "duration_seconds": 1.428}, "timestamp": "2026-01-19T13:32:06.422099"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1609.769, "latencies_ms": [1609.769], "images_per_second": 0.621, "prompt_tokens": 1110, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The tennis player is wearing a white outfit and white shoes, and the grass on the tennis court is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.34, "peak": 112.2, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.78, "peak": 40.18, "min": 20.49}}, "power_watts_avg": 31.78, "energy_joules_est": 51.19, "sample_count": 16, "duration_seconds": 1.611}, "timestamp": "2026-01-19T13:32:08.090999"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1737.466, "latencies_ms": [1737.466], "images_per_second": 0.576, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image depicts a small bathroom with a white toilet, a bathtub with a shower curtain, and a wire shelf with folded towels.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.88, "peak": 130.08, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.77, "peak": 40.16, "min": 18.53}}, "power_watts_avg": 30.77, "energy_joules_est": 53.49, "sample_count": 17, "duration_seconds": 1.738}, "timestamp": "2026-01-19T13:32:09.872205"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2816.638, "latencies_ms": [2816.638], "images_per_second": 0.355, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. Toilet: 1\n2. Shower curtain: 1\n3. Towel: 1\n4. Bathtub: 1\n5. Sink: 1\n6. Shower rod: 1\n7. Towel rack: 1\n8. Shelf: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.33, "peak": 130.34, "min": 27.72}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.62, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.62, "energy_joules_est": 74.99, "sample_count": 28, "duration_seconds": 2.817}, "timestamp": "2026-01-19T13:32:12.785837"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2301.417, "latencies_ms": [2301.417], "images_per_second": 0.435, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The toilet is located on the left side of the image, while the shower curtain is on the right side. The closet is positioned in the background, and the towel is hanging on the wall above the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.9, "peak": 120.52, "min": 27.59}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.83, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.83, "energy_joules_est": 61.75, "sample_count": 23, "duration_seconds": 2.302}, "timestamp": "2026-01-19T13:32:15.182353"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1456.014, "latencies_ms": [1456.014], "images_per_second": 0.687, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A bathroom with a toilet, bathtub, and shower curtain is shown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.4, "peak": 123.03, "min": 28.15}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.93, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 30.93, "peak": 39.78, "min": 14.19}}, "power_watts_avg": 30.93, "energy_joules_est": 45.06, "sample_count": 14, "duration_seconds": 1.457}, "timestamp": "2026-01-19T13:32:16.649655"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1371.503, "latencies_ms": [1371.503], "images_per_second": 0.729, "prompt_tokens": 1109, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The bathroom is painted yellow, and the toilet is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.23, "peak": 125.1, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 33.32, "peak": 40.97, "min": 21.28}}, "power_watts_avg": 33.32, "energy_joules_est": 45.71, "sample_count": 14, "duration_seconds": 1.372}, "timestamp": "2026-01-19T13:32:18.110221"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1526.597, "latencies_ms": [1526.597], "images_per_second": 0.655, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two men are standing in a room, holding wine glasses, and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.95, "peak": 123.33, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 32.28, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 32.28, "energy_joules_est": 49.3, "sample_count": 15, "duration_seconds": 1.527}, "timestamp": "2026-01-19T13:32:19.684145"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2518.998, "latencies_ms": [2518.998], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. glasses: 2\n2. wine: 1\n3. man: 2\n4. woman: 1\n5. shirt: 2\n6. table: 1\n7. chair: 1\n8. door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.57, "peak": 124.78, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.72, "peak": 40.57, "min": 17.73}}, "power_watts_avg": 27.72, "energy_joules_est": 69.84, "sample_count": 25, "duration_seconds": 2.519}, "timestamp": "2026-01-19T13:32:22.284036"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2725.99, "latencies_ms": [2725.99], "images_per_second": 0.367, "prompt_tokens": 1117, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The two men are standing close to each other, with the man on the left holding a glass of red wine and the man on the right holding a glass of white wine. The man on the left is positioned slightly in front of the man on the right, and the glasses are held in front of their faces.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.85, "peak": 124.4, "min": 28.21}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.28, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.28, "energy_joules_est": 71.65, "sample_count": 27, "duration_seconds": 2.726}, "timestamp": "2026-01-19T13:32:25.105320"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1567.773, "latencies_ms": [1567.773], "images_per_second": 0.638, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two men are standing in a room with wine glasses in their hands, smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.21, "peak": 125.16, "min": 30.15}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.24, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 30.24, "energy_joules_est": 47.42, "sample_count": 16, "duration_seconds": 1.568}, "timestamp": "2026-01-19T13:32:26.775472"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2006.051, "latencies_ms": [2006.051], "images_per_second": 0.498, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming in from the windows. The colors in the image are vibrant and the materials are mostly wooden and glass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.49, "peak": 108.14, "min": 27.47}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.34, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 14.86, "peak": 16.14, "min": 12.21}, "VDD_GPU": {"avg": 29.1, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 29.1, "energy_joules_est": 58.39, "sample_count": 20, "duration_seconds": 2.007}, "timestamp": "2026-01-19T13:32:28.864285"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1617.221, "latencies_ms": [1617.221], "images_per_second": 0.618, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A surfer is riding a wave in the ocean, with a beach and clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.33, "peak": 131.5, "min": 27.48}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.41, "peak": 40.18, "min": 15.38}}, "power_watts_avg": 30.41, "energy_joules_est": 49.21, "sample_count": 16, "duration_seconds": 1.618}, "timestamp": "2026-01-19T13:32:30.540959"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2582.84, "latencies_ms": [2582.84], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. surfer: 1\n2. surfboard: 1\n3. wave: 1\n4. ocean: 1\n5. beach: 1\n6. sky: 1\n7. sand: 1\n8. water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.98, "peak": 127.29, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.5, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 27.5, "energy_joules_est": 71.04, "sample_count": 25, "duration_seconds": 2.583}, "timestamp": "2026-01-19T13:32:33.143633"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2045.334, "latencies_ms": [2045.334], "images_per_second": 0.489, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that dominates the majority of the image. The wave is in the middle ground, with the beach and sky visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.69, "peak": 120.47, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.05, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.05, "energy_joules_est": 59.43, "sample_count": 20, "duration_seconds": 2.046}, "timestamp": "2026-01-19T13:32:35.225043"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1606.54, "latencies_ms": [1606.54], "images_per_second": 0.622, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A surfer is riding a wave in the ocean, with a beach and palm trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.38, "peak": 121.88, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.95, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.95, "energy_joules_est": 49.73, "sample_count": 16, "duration_seconds": 1.607}, "timestamp": "2026-01-19T13:32:36.899004"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2405.411, "latencies_ms": [2405.411], "images_per_second": 0.416, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image captures a surfer riding a wave with a vibrant blue and green color palette, with the sun shining brightly in the background. The surfer is wearing a black wetsuit and is positioned on a white surfboard, skillfully navigating the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.84, "peak": 126.71, "min": 27.21}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.69, "peak": 40.57, "min": 17.73}}, "power_watts_avg": 27.69, "energy_joules_est": 66.62, "sample_count": 24, "duration_seconds": 2.406}, "timestamp": "2026-01-19T13:32:39.391916"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1640.571, "latencies_ms": [1640.571], "images_per_second": 0.61, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A collection of laptops, tablets, and a backpack are scattered on the floor, with cables strewn about.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.56, "peak": 121.93, "min": 30.43}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.43, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.43, "energy_joules_est": 49.95, "sample_count": 16, "duration_seconds": 1.641}, "timestamp": "2026-01-19T13:32:41.064338"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2812.507, "latencies_ms": [2812.507], "images_per_second": 0.356, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. black laptop: 3\n2. red and black backpack: 1\n3. black laptop: 2\n4. black tablet: 1\n5. black monitor: 2\n6. black keyboard: 1\n7. black mouse: 1\n8. black mousepad: 1", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.84, "peak": 115.74, "min": 28.3}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.74, "peak": 40.16, "min": 17.35}}, "power_watts_avg": 26.74, "energy_joules_est": 75.21, "sample_count": 28, "duration_seconds": 2.813}, "timestamp": "2026-01-19T13:32:43.971917"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1764.13, "latencies_ms": [1764.13], "images_per_second": 0.567, "prompt_tokens": 1117, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The backpack is located to the left of the laptop, the cables are in the middle, and the laptop is on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.03, "peak": 123.33, "min": 30.4}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.78, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 29.78, "energy_joules_est": 52.55, "sample_count": 17, "duration_seconds": 1.765}, "timestamp": "2026-01-19T13:32:45.748842"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1462.174, "latencies_ms": [1462.174], "images_per_second": 0.684, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A collection of laptops and a backpack are scattered on the floor in a room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.33, "peak": 104.75, "min": 27.1}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.04, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 32.04, "energy_joules_est": 46.86, "sample_count": 15, "duration_seconds": 1.463}, "timestamp": "2026-01-19T13:32:47.311858"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2254.061, "latencies_ms": [2254.061], "images_per_second": 0.444, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image is taken in a room with a carpeted floor and a white wall. The lighting is natural, coming from a window out of frame. The objects in the image are made of metal, plastic, and fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.88, "peak": 116.08, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 28.44, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 28.44, "energy_joules_est": 64.12, "sample_count": 22, "duration_seconds": 2.255}, "timestamp": "2026-01-19T13:32:49.605349"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1939.219, "latencies_ms": [1939.219], "images_per_second": 0.516, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A skier in a vibrant red and green suit is captured mid-air, performing a jump on a snowy mountain, while a second skier stands in the background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.45, "peak": 103.88, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.34, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.34, "energy_joules_est": 56.92, "sample_count": 19, "duration_seconds": 1.94}, "timestamp": "2026-01-19T13:32:51.590110"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2605.179, "latencies_ms": [2605.179], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. skier: 1\n2. skis: 2\n3. poles: 2\n4. snow: 1\n5. mountain: 1\n6. sky: 1\n7. person: 1\n8. snowboard: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.17, "peak": 122.01, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.91, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.91, "energy_joules_est": 70.11, "sample_count": 26, "duration_seconds": 2.605}, "timestamp": "2026-01-19T13:32:54.295889"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2231.234, "latencies_ms": [2231.234], "images_per_second": 0.448, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The skier is in the foreground, with the snowboarder in the background. The skier is to the left of the snowboarder. The skier is closer to the camera than the snowboarder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.75, "peak": 107.86, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.7, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 27.7, "energy_joules_est": 61.82, "sample_count": 22, "duration_seconds": 2.232}, "timestamp": "2026-01-19T13:32:56.586137"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2066.37, "latencies_ms": [2066.37], "images_per_second": 0.484, "prompt_tokens": 1111, "response_tokens_est": 38, "n_tiles": 1, "output_text": " A skier in a vibrant orange and green suit is captured mid-air, performing a jump on a snowy mountain. In the background, another skier is seen walking on the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.41, "peak": 128.01, "min": 31.16}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.78, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 28.78, "energy_joules_est": 59.49, "sample_count": 20, "duration_seconds": 2.067}, "timestamp": "2026-01-19T13:32:58.669679"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1968.38, "latencies_ms": [1968.38], "images_per_second": 0.508, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The skier is wearing a vibrant red and green outfit, and the snow is a pristine white. The sun is shining brightly, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.54, "peak": 125.32, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.51, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.51, "energy_joules_est": 58.09, "sample_count": 19, "duration_seconds": 1.969}, "timestamp": "2026-01-19T13:33:00.649919"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1491.549, "latencies_ms": [1491.549], "images_per_second": 0.67, "prompt_tokens": 1100, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A bird is standing on the edge of a boat window, looking out at the water.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.94, "peak": 108.48, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.52, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 31.52, "energy_joules_est": 47.04, "sample_count": 15, "duration_seconds": 1.492}, "timestamp": "2026-01-19T13:33:02.219931"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2831.98, "latencies_ms": [2831.98], "images_per_second": 0.353, "prompt_tokens": 1114, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. Bird: 1\n2. Window: 1\n3. Bird's beak: 1\n4. Bird's eye: 1\n5. Bird's legs: 1\n6. Bird's feet: 1\n7. Bird's wing: 1\n8. Bird's tail: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.76, "peak": 112.21, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.54, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 26.54, "energy_joules_est": 75.17, "sample_count": 28, "duration_seconds": 2.832}, "timestamp": "2026-01-19T13:33:05.138426"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1938.819, "latencies_ms": [1938.819], "images_per_second": 0.516, "prompt_tokens": 1118, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The bird is positioned in the foreground, looking out of the window, while the door is located in the background. The bird is closer to the camera than the door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.02, "peak": 115.95, "min": 30.91}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.84, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.84, "energy_joules_est": 55.93, "sample_count": 19, "duration_seconds": 1.939}, "timestamp": "2026-01-19T13:33:07.123161"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1516.864, "latencies_ms": [1516.864], "images_per_second": 0.659, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A bird is standing on the edge of a boat window, looking out at the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.56, "peak": 122.24, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.73, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 30.73, "energy_joules_est": 46.64, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T13:33:08.695305"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2645.052, "latencies_ms": [2645.052], "images_per_second": 0.378, "prompt_tokens": 1110, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image features a bird perched on a window sill, with a view of a body of water in the background. The bird is black, and the window sill is made of wood. The lighting in the image is natural, coming from the outside, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.24, "peak": 123.92, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.03, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 27.03, "energy_joules_est": 71.51, "sample_count": 26, "duration_seconds": 2.646}, "timestamp": "2026-01-19T13:33:11.409178"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1778.658, "latencies_ms": [1778.658], "images_per_second": 0.562, "prompt_tokens": 1100, "response_tokens_est": 28, "n_tiles": 1, "output_text": " Two men are cleaning a bathroom with a bucket and a mop, and there is a black trash can with a white cloth on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.76, "peak": 125.1, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.27, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.27, "energy_joules_est": 52.07, "sample_count": 18, "duration_seconds": 1.779}, "timestamp": "2026-01-19T13:33:13.293571"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2117.97, "latencies_ms": [2117.97], "images_per_second": 0.472, "prompt_tokens": 1114, "response_tokens_est": 41, "n_tiles": 1, "output_text": " toilet: 1, trash can: 1, broom: 1, bucket: 1, shelf: 1, person: 2, bottle: 1, trash bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.09, "peak": 127.29, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.44, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.31, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.31, "energy_joules_est": 59.97, "sample_count": 21, "duration_seconds": 2.118}, "timestamp": "2026-01-19T13:33:15.481698"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2890.425, "latencies_ms": [2890.425], "images_per_second": 0.346, "prompt_tokens": 1118, "response_tokens_est": 70, "n_tiles": 1, "output_text": " The man in the orange hat is standing to the left of the toilet, while the man in the gray shirt is standing to the right of the toilet. The man in the orange hat is closer to the camera than the man in the gray shirt. The man in the orange hat is also closer to the toilet than the man in the gray shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.58, "peak": 113.83, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 25.89, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 25.89, "energy_joules_est": 74.84, "sample_count": 29, "duration_seconds": 2.891}, "timestamp": "2026-01-19T13:33:18.478219"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1392.244, "latencies_ms": [1392.244], "images_per_second": 0.718, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " Two men are cleaning a bathroom with a bucket and mop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.09, "peak": 115.94, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.59, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 30.59, "energy_joules_est": 42.6, "sample_count": 14, "duration_seconds": 1.393}, "timestamp": "2026-01-19T13:33:19.940533"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1570.836, "latencies_ms": [1570.836], "images_per_second": 0.637, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is dimly lit with a blue tint, and the floor is covered in blue paint.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.63, "peak": 127.64, "min": 27.23}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.52, "peak": 40.57, "min": 19.71}}, "power_watts_avg": 31.52, "energy_joules_est": 49.52, "sample_count": 16, "duration_seconds": 1.571}, "timestamp": "2026-01-19T13:33:21.606336"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2044.817, "latencies_ms": [2044.817], "images_per_second": 0.489, "prompt_tokens": 1100, "response_tokens_est": 38, "n_tiles": 1, "output_text": " In the image, a person is seen walking down a hallway, holding an umbrella with the words \"Pour moi\" written on it, as rain pours down around them.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.56, "peak": 118.92, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.04, "peak": 40.16, "min": 17.74}}, "power_watts_avg": 29.04, "energy_joules_est": 59.4, "sample_count": 20, "duration_seconds": 2.045}, "timestamp": "2026-01-19T13:33:23.691925"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2517.23, "latencies_ms": [2517.23], "images_per_second": 0.397, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 1\n2. umbrella: 1\n3. door: 1\n4. wall: 2\n5. window: 1\n6. floor: 1\n7. light: 1\n8. shadow: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.47, "peak": 117.04, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.06, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.06, "energy_joules_est": 68.13, "sample_count": 25, "duration_seconds": 2.518}, "timestamp": "2026-01-19T13:33:26.278290"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1855.29, "latencies_ms": [1855.29], "images_per_second": 0.539, "prompt_tokens": 1118, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The person is standing in the foreground of the image, holding the umbrella in the middle of the image, and the door is located in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.79, "peak": 126.85, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.46, "peak": 40.18, "min": 15.38}}, "power_watts_avg": 29.46, "energy_joules_est": 54.67, "sample_count": 18, "duration_seconds": 1.856}, "timestamp": "2026-01-19T13:33:28.151807"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1699.444, "latencies_ms": [1699.444], "images_per_second": 0.588, "prompt_tokens": 1112, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A person is walking down a hallway with a blue umbrella that has the words \"God's mood\" written on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.32, "peak": 111.81, "min": 29.92}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.15, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 30.15, "energy_joules_est": 51.25, "sample_count": 17, "duration_seconds": 1.7}, "timestamp": "2026-01-19T13:33:29.913540"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2367.192, "latencies_ms": [2367.192], "images_per_second": 0.422, "prompt_tokens": 1110, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a person holding an umbrella with the words \"God's mood\" written on it, walking down a hallway with red walls and a white door. The lighting is dim, and the rain is falling outside, creating a gloomy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.83, "peak": 122.0, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.89, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 27.89, "energy_joules_est": 66.03, "sample_count": 23, "duration_seconds": 2.367}, "timestamp": "2026-01-19T13:33:32.306552"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2033.657, "latencies_ms": [2033.657], "images_per_second": 0.492, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " A man wearing a red shirt and blue jeans is walking on a path in the woods, holding a walking stick, while another man in a yellow jacket stands on a bridge in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.46, "peak": 119.89, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.9, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.9, "energy_joules_est": 58.79, "sample_count": 20, "duration_seconds": 2.034}, "timestamp": "2026-01-19T13:33:34.394361"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2830.633, "latencies_ms": [2830.633], "images_per_second": 0.353, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. man: 1\n2. backpack: 1\n3. walking stick: 1\n4. man's legs: 1\n5. man's feet: 1\n6. man's hands: 1\n7. man's head: 1\n8. man's face: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.43, "peak": 108.16, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.17, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.17, "energy_joules_est": 74.09, "sample_count": 28, "duration_seconds": 2.831}, "timestamp": "2026-01-19T13:33:37.313655"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2488.349, "latencies_ms": [2488.349], "images_per_second": 0.402, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, with the signpost in the middle and the other man on the right side. The man is in the foreground, while the signpost is in the middle ground, and the other man is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.15, "peak": 120.47, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.06, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 27.06, "energy_joules_est": 67.35, "sample_count": 24, "duration_seconds": 2.489}, "timestamp": "2026-01-19T13:33:39.812986"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1748.266, "latencies_ms": [1748.266], "images_per_second": 0.572, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man wearing a red shirt and blue jeans is walking on a path in the woods, carrying a backpack and holding a walking stick.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.08, "peak": 121.47, "min": 28.22}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.5, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 30.5, "energy_joules_est": 53.33, "sample_count": 17, "duration_seconds": 1.749}, "timestamp": "2026-01-19T13:33:41.585449"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2177.26, "latencies_ms": [2177.26], "images_per_second": 0.459, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a man wearing a red shirt and blue jeans, with a backpack on his back, standing on a rocky path in a forest. The lighting is natural, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.94, "peak": 120.82, "min": 36.59}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.95, "peak": 39.78, "min": 18.52}}, "power_watts_avg": 28.95, "energy_joules_est": 63.04, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T13:33:43.769866"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1955.951, "latencies_ms": [1955.951], "images_per_second": 0.511, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Five men are standing together in a room with a red chair and a table with wine bottles.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.64, "peak": 119.91, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 31.62, "peak": 41.76, "min": 16.95}}, "power_watts_avg": 31.62, "energy_joules_est": 61.87, "sample_count": 19, "duration_seconds": 1.957}, "timestamp": "2026-01-19T13:33:45.757056"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2995.34, "latencies_ms": [2995.34], "images_per_second": 0.334, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. chair: 2\n2. man: 5\n3. bottle: 1\n4. wall: 1\n5. table: 1\n6. ring: 1\n7. tie: 1\n8. suit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.76, "peak": 113.72, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 28.61, "peak": 42.15, "min": 18.92}}, "power_watts_avg": 28.61, "energy_joules_est": 85.71, "sample_count": 29, "duration_seconds": 2.996}, "timestamp": "2026-01-19T13:33:48.770188"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2986.918, "latencies_ms": [2986.918], "images_per_second": 0.335, "prompt_tokens": 1450, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The man in the middle is standing between the two men on the left and the two men on the right. The man on the left is standing closer to the camera than the man on the right. The man on the right is standing closer to the camera than the man on the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.93, "peak": 126.88, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.41, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 28.41, "energy_joules_est": 84.87, "sample_count": 29, "duration_seconds": 2.987}, "timestamp": "2026-01-19T13:33:51.793082"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1741.865, "latencies_ms": [1741.865], "images_per_second": 0.574, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " Five men are standing in a room with chairs and a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.06, "peak": 125.04, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 33.23, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 33.23, "energy_joules_est": 57.9, "sample_count": 17, "duration_seconds": 1.742}, "timestamp": "2026-01-19T13:33:53.561463"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2018.602, "latencies_ms": [2018.602], "images_per_second": 0.495, "prompt_tokens": 1442, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The room is well lit with natural light coming from the windows. The chairs are red and the walls are white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.81, "peak": 124.87, "min": 29.42}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.36, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.89, "peak": 42.54, "min": 21.28}}, "power_watts_avg": 32.89, "energy_joules_est": 66.4, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T13:33:55.645984"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4059.79, "latencies_ms": [4059.79], "images_per_second": 0.246, "prompt_tokens": 1100, "response_tokens_est": 113, "n_tiles": 1, "output_text": " The image captures a bustling street corner in a city, where a black lamppost stands as a silent sentinel, its yellow signboard bearing a warning of a pedestrian crossing. The street, a ribbon of asphalt, is flanked by buildings that house a variety of shops and businesses, their facades a testament to the urban landscape. The sky overhead is a canvas of overcast clouds, casting a soft light over the scene. In the distance, a red double-decker bus adds a splash of color to the otherwise muted palette of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.71, "peak": 98.32, "min": 27.63}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 24.08, "peak": 40.57, "min": 16.95}}, "power_watts_avg": 24.08, "energy_joules_est": 97.77, "sample_count": 40, "duration_seconds": 4.06}, "timestamp": "2026-01-19T13:33:59.813514"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2155.491, "latencies_ms": [2155.491], "images_per_second": 0.464, "prompt_tokens": 1114, "response_tokens_est": 40, "n_tiles": 1, "output_text": " 1. black pole\n2. yellow sign\n3. black box\n4. traffic light\n5. street lamp\n6. brick building\n7. blue bus\n8. white car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.38, "peak": 115.59, "min": 30.39}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.34, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.8, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 27.8, "energy_joules_est": 59.93, "sample_count": 21, "duration_seconds": 2.156}, "timestamp": "2026-01-19T13:34:02.010431"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2186.126, "latencies_ms": [2186.126], "images_per_second": 0.457, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The yellow sign is positioned to the right of the black pole, which is situated in the foreground of the image. In the background, there are several cars and buildings, with the traffic light positioned above the sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.58, "peak": 129.69, "min": 33.2}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.38, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.38, "energy_joules_est": 62.05, "sample_count": 21, "duration_seconds": 2.186}, "timestamp": "2026-01-19T13:34:04.201853"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2553.015, "latencies_ms": [2553.015], "images_per_second": 0.392, "prompt_tokens": 1112, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a bustling city street corner, where a yellow traffic signal stands out against the backdrop of a red bus and a blue car. The street is lined with buildings, their facades adorned with various signs and advertisements, and the sky overhead is a canvas of overcast clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.9, "peak": 122.88, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 26.66, "peak": 39.39, "min": 17.35}}, "power_watts_avg": 26.66, "energy_joules_est": 68.07, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T13:34:06.807291"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2003.494, "latencies_ms": [2003.494], "images_per_second": 0.499, "prompt_tokens": 1110, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image depicts a street corner with a yellow traffic signal, a black pole, and a black box. The sky is cloudy, and the street is wet, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.33, "peak": 129.66, "min": 27.64}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.4}, "VDD_GPU": {"avg": 28.47, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.47, "energy_joules_est": 57.05, "sample_count": 20, "duration_seconds": 2.004}, "timestamp": "2026-01-19T13:34:08.890747"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1826.221, "latencies_ms": [1826.221], "images_per_second": 0.548, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A man is playing tennis on a court at night, wearing a white shirt and black shorts, and holding a red and black tennis racket.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.86, "peak": 129.66, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.5, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.5, "energy_joules_est": 53.89, "sample_count": 18, "duration_seconds": 1.827}, "timestamp": "2026-01-19T13:34:10.774891"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2659.608, "latencies_ms": [2659.608], "images_per_second": 0.376, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. person: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. fence: 1\n5. sign: 1\n6. net: 1\n7. court: 1\n8. ball boy: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.45, "peak": 118.97, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.85, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.82, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.82, "energy_joules_est": 71.34, "sample_count": 26, "duration_seconds": 2.66}, "timestamp": "2026-01-19T13:34:13.478592"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2260.171, "latencies_ms": [2260.171], "images_per_second": 0.442, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The main object, the tennis player, is in the foreground of the image, with the tennis court and fence in the background. The player is positioned to the left of the image, while the fence is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.84, "peak": 122.32, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.9, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.9, "energy_joules_est": 63.08, "sample_count": 22, "duration_seconds": 2.261}, "timestamp": "2026-01-19T13:34:15.773505"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1349.115, "latencies_ms": [1349.115], "images_per_second": 0.741, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A man is playing tennis on a court at night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.76, "peak": 123.9, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.27, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 32.27, "energy_joules_est": 43.55, "sample_count": 13, "duration_seconds": 1.35}, "timestamp": "2026-01-19T13:34:17.136088"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2441.698, "latencies_ms": [2441.698], "images_per_second": 0.41, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a dynamic moment on a tennis court bathed in the soft glow of artificial lighting. The court, a vibrant shade of green, contrasts sharply with the red clay surface, while the surrounding fence and advertisements add a touch of urbanity to the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.25, "peak": 133.2, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 28.56, "peak": 40.97, "min": 18.53}}, "power_watts_avg": 28.56, "energy_joules_est": 69.75, "sample_count": 24, "duration_seconds": 2.442}, "timestamp": "2026-01-19T13:34:19.635195"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2274.263, "latencies_ms": [2274.263], "images_per_second": 0.44, "prompt_tokens": 1432, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A group of people, including a man in a black jacket and a woman in a white helmet, are standing near a blue fence on a snowy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.26, "peak": 133.49, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.07, "peak": 41.74, "min": 15.77}}, "power_watts_avg": 30.07, "energy_joules_est": 68.41, "sample_count": 22, "duration_seconds": 2.275}, "timestamp": "2026-01-19T13:34:21.928727"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3004.257, "latencies_ms": [3004.257], "images_per_second": 0.333, "prompt_tokens": 1446, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. skis: 4\n2. skiers: 4\n3. helmet: 4\n4. goggles: 4\n5. gloves: 4\n6. ski poles: 4\n7. net: 1\n8. fence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.3, "peak": 134.14, "min": 31.67}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.69, "peak": 41.76, "min": 18.14}}, "power_watts_avg": 28.69, "energy_joules_est": 86.2, "sample_count": 29, "duration_seconds": 3.005}, "timestamp": "2026-01-19T13:34:24.940972"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2915.738, "latencies_ms": [2915.738], "images_per_second": 0.343, "prompt_tokens": 1450, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The skier in the white helmet and goggles is positioned in the foreground, while the skier in the red and white helmet is in the background. The skier in the white helmet and goggles is standing closer to the camera than the skier in the red and white helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.78, "peak": 128.58, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.36, "min": 12.86}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.46, "peak": 41.74, "min": 16.95}}, "power_watts_avg": 28.46, "energy_joules_est": 82.99, "sample_count": 29, "duration_seconds": 2.916}, "timestamp": "2026-01-19T13:34:27.959823"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2140.123, "latencies_ms": [2140.123], "images_per_second": 0.467, "prompt_tokens": 1444, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A group of people are standing on a snowy slope, some of them wearing helmets and goggles, and they are talking to each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.57, "peak": 116.59, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.36, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.97, "peak": 41.74, "min": 14.98}}, "power_watts_avg": 30.97, "energy_joules_est": 66.29, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T13:34:30.145057"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3246.604, "latencies_ms": [3246.604], "images_per_second": 0.308, "prompt_tokens": 1442, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image captures a lively scene on a snowy mountain where a group of skiers and snowboarders are gathered. The skiers are clad in vibrant winter gear, including helmets and goggles, while the snowboarders sport colorful attire. The sky is a clear blue, and the sun casts a bright glow on the snow-covered landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.87, "peak": 109.6, "min": 29.84}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.36, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 27.86, "peak": 42.53, "min": 18.14}}, "power_watts_avg": 27.86, "energy_joules_est": 90.46, "sample_count": 32, "duration_seconds": 3.247}, "timestamp": "2026-01-19T13:34:33.472507"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1917.607, "latencies_ms": [1917.607], "images_per_second": 0.521, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A blue truck with the text \"HUISMANGROUP.COM\" on the front is driving down a wet road with houses and trees on either side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.42, "peak": 133.12, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.01, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.01, "energy_joules_est": 55.64, "sample_count": 19, "duration_seconds": 1.918}, "timestamp": "2026-01-19T13:34:35.458933"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1957.331, "latencies_ms": [1957.331], "images_per_second": 0.511, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " 1. blue truck\n2. road\n3. street\n4. sidewalk\n5. grass\n6. trees\n7. houses\n8. street lamp", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.75, "peak": 115.63, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.43, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.43, "energy_joules_est": 57.61, "sample_count": 19, "duration_seconds": 1.958}, "timestamp": "2026-01-19T13:34:37.449823"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1906.546, "latencies_ms": [1906.546], "images_per_second": 0.525, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The blue truck is on the left side of the road, while the white car is on the right side. The truck is closer to the camera than the car.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.65, "peak": 119.68, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.61, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.61, "energy_joules_est": 56.46, "sample_count": 19, "duration_seconds": 1.907}, "timestamp": "2026-01-19T13:34:39.434840"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1432.654, "latencies_ms": [1432.654], "images_per_second": 0.698, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A blue truck is driving down a wet road in a residential area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.43, "peak": 127.54, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.8, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.8, "energy_joules_est": 45.57, "sample_count": 14, "duration_seconds": 1.433}, "timestamp": "2026-01-19T13:34:40.899036"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1417.904, "latencies_ms": [1417.904], "images_per_second": 0.705, "prompt_tokens": 1109, "response_tokens_est": 14, "n_tiles": 1, "output_text": " The truck is blue and the road is wet, indicating recent rain.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.41, "peak": 130.98, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 33.37, "peak": 40.57, "min": 21.28}}, "power_watts_avg": 33.37, "energy_joules_est": 47.33, "sample_count": 14, "duration_seconds": 1.418}, "timestamp": "2026-01-19T13:34:42.363266"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2005.384, "latencies_ms": [2005.384], "images_per_second": 0.499, "prompt_tokens": 1099, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a large commercial airplane is taxiing on a wet runway, with a clear view of the runway and surrounding landscape, including distant mountains and a hazy sky.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.17, "peak": 119.92, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.8, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 29.8, "energy_joules_est": 59.77, "sample_count": 20, "duration_seconds": 2.006}, "timestamp": "2026-01-19T13:34:44.451988"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1649.458, "latencies_ms": [1649.458], "images_per_second": 0.606, "prompt_tokens": 1113, "response_tokens_est": 22, "n_tiles": 1, "output_text": " airplane: 1\nrunway: 1\nlight poles: 4\nmountains: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.02, "peak": 129.56, "min": 30.26}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.73, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.73, "energy_joules_est": 50.7, "sample_count": 16, "duration_seconds": 1.65}, "timestamp": "2026-01-19T13:34:46.121998"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2179.17, "latencies_ms": [2179.17], "images_per_second": 0.459, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The airplane is in the foreground, with the runway extending into the background. The runway is surrounded by a variety of objects, including lights, poles, and vegetation, which are positioned at different distances from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.92, "peak": 124.86, "min": 38.67}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 29.17, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 29.17, "energy_joules_est": 63.58, "sample_count": 21, "duration_seconds": 2.18}, "timestamp": "2026-01-19T13:34:48.309155"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1508.991, "latencies_ms": [1508.991], "images_per_second": 0.663, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " An airplane is on the runway, and there are lights on the side of the runway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.83, "peak": 119.98, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.95, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 31.67, "peak": 39.77, "min": 18.13}}, "power_watts_avg": 31.67, "energy_joules_est": 47.8, "sample_count": 15, "duration_seconds": 1.509}, "timestamp": "2026-01-19T13:34:49.880641"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2062.524, "latencies_ms": [2062.524], "images_per_second": 0.485, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image features a large airplane on a runway with a hazy sky in the background. The airplane is white with blue and red stripes, and the runway is wet, reflecting the light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.35, "peak": 128.09, "min": 29.84}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.55, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 29.55, "energy_joules_est": 60.96, "sample_count": 20, "duration_seconds": 2.063}, "timestamp": "2026-01-19T13:34:51.963535"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1761.099, "latencies_ms": [1761.099], "images_per_second": 0.568, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A woman in an orange shirt is holding a tennis racket in the air while a man in a black jacket stands next to her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.54, "peak": 113.03, "min": 31.11}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.54, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.54, "energy_joules_est": 53.8, "sample_count": 17, "duration_seconds": 1.762}, "timestamp": "2026-01-19T13:34:53.743296"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2696.941, "latencies_ms": [2696.941], "images_per_second": 0.371, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. flag: 2\n2. beach: 1\n3. mountain: 1\n4. people: 10\n5. beach umbrella: 1\n6. beach chair: 1\n7. beach ball: 1\n8. beach ball net: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.95, "peak": 108.43, "min": 33.68}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 27.17, "energy_joules_est": 73.29, "sample_count": 26, "duration_seconds": 2.697}, "timestamp": "2026-01-19T13:34:56.443816"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2425.776, "latencies_ms": [2425.776], "images_per_second": 0.412, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The person in the foreground is holding a tennis racket, while the person in the background is walking away from the camera. The person in the foreground is standing on the beach, while the person in the background is walking on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.17, "peak": 123.71, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.85, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 27.2, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.2, "energy_joules_est": 65.99, "sample_count": 24, "duration_seconds": 2.426}, "timestamp": "2026-01-19T13:34:58.936859"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1853.1, "latencies_ms": [1853.1], "images_per_second": 0.54, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A group of people are on a beach with mountains in the background. One person is holding a tennis racket and another is holding a bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.58, "peak": 120.29, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.44, "min": 11.26}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.2, "peak": 39.39, "min": 14.19}}, "power_watts_avg": 29.2, "energy_joules_est": 54.12, "sample_count": 18, "duration_seconds": 1.853}, "timestamp": "2026-01-19T13:35:00.812713"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1871.659, "latencies_ms": [1871.659], "images_per_second": 0.534, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image is taken during the day with clear skies and the sun shining brightly. The sand is light beige and the sky is a light blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.28, "peak": 121.4, "min": 27.47}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.4, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.4, "energy_joules_est": 55.03, "sample_count": 19, "duration_seconds": 1.872}, "timestamp": "2026-01-19T13:35:02.793206"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1652.221, "latencies_ms": [1652.221], "images_per_second": 0.605, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A tennis player is standing on a blue tennis court, holding a tennis racket and looking down at the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.23, "peak": 121.93, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.85, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.85, "energy_joules_est": 49.33, "sample_count": 17, "duration_seconds": 1.653}, "timestamp": "2026-01-19T13:35:04.555484"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2700.515, "latencies_ms": [2700.515], "images_per_second": 0.37, "prompt_tokens": 1114, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. man: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. tennis shoes: 2\n5. shorts: 1\n6. shirt: 1\n7. headband: 1\n8. wristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.12, "peak": 122.38, "min": 29.77}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 12.22}, "VDD_GPU": {"avg": 26.38, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.38, "energy_joules_est": 71.25, "sample_count": 27, "duration_seconds": 2.701}, "timestamp": "2026-01-19T13:35:07.344002"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2577.983, "latencies_ms": [2577.983], "images_per_second": 0.388, "prompt_tokens": 1118, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis court taking up the majority of the space in the background. The player is standing near the center of the court, with the shadow of the player and the court extending towards the bottom right corner of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.9, "peak": 113.72, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 26.84, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.84, "energy_joules_est": 69.2, "sample_count": 25, "duration_seconds": 2.578}, "timestamp": "2026-01-19T13:35:09.937411"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1946.774, "latencies_ms": [1946.774], "images_per_second": 0.514, "prompt_tokens": 1112, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A tennis player is standing on a blue court with a white line on it. He is holding a tennis racket and appears to be wiping his face with his hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.08, "peak": 124.7, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.8, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.8, "energy_joules_est": 56.07, "sample_count": 19, "duration_seconds": 1.947}, "timestamp": "2026-01-19T13:35:11.905839"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1774.214, "latencies_ms": [1774.214], "images_per_second": 0.564, "prompt_tokens": 1110, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The tennis player is wearing a blue shirt and white shorts, and the court is blue. The lighting is bright and the shadows are long.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.37, "peak": 132.47, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.01, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 30.01, "energy_joules_est": 53.25, "sample_count": 18, "duration_seconds": 1.774}, "timestamp": "2026-01-19T13:35:13.773977"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2587.695, "latencies_ms": [2587.695], "images_per_second": 0.386, "prompt_tokens": 1100, "response_tokens_est": 58, "n_tiles": 1, "output_text": " In the image, a woman in a pink and white floral dress and beige shorts is standing in a kitchen, leaning over a black stove with a large black chimney above it. The kitchen is filled with various objects, including a blue kettle, a white bowl, and a wooden spoon.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.04, "peak": 117.57, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.95, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 26.95, "energy_joules_est": 69.75, "sample_count": 25, "duration_seconds": 2.588}, "timestamp": "2026-01-19T13:35:16.384010"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2530.72, "latencies_ms": [2530.72], "images_per_second": 0.395, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. woman: 1\n2. stove: 1\n3. cupboard: 1\n4. shelf: 1\n5. kettle: 1\n6. pan: 1\n7. dish: 1\n8. cup: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.06, "peak": 130.1, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.09, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 27.09, "energy_joules_est": 68.56, "sample_count": 25, "duration_seconds": 2.531}, "timestamp": "2026-01-19T13:35:18.971989"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2172.518, "latencies_ms": [2172.518], "images_per_second": 0.46, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The woman is standing to the left of the stove, which is positioned in the center of the image. The stove is located in the background, with the woman's body and the kitchen counter in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.36, "peak": 114.73, "min": 27.66}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.78, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.78, "energy_joules_est": 60.36, "sample_count": 22, "duration_seconds": 2.173}, "timestamp": "2026-01-19T13:35:21.246571"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1494.147, "latencies_ms": [1494.147], "images_per_second": 0.669, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman is standing in a kitchen with a large stove and a black chimney.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.73, "peak": 123.37, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.02, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.12, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.12, "energy_joules_est": 45.02, "sample_count": 15, "duration_seconds": 1.495}, "timestamp": "2026-01-19T13:35:22.800958"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2818.765, "latencies_ms": [2818.765], "images_per_second": 0.355, "prompt_tokens": 1110, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The image is a black and white photo with a vintage feel, capturing a moment in time in a kitchen with a large black stove and a woman in a floral shirt. The lighting is soft and diffused, creating a warm atmosphere. The materials used in the kitchen are traditional, with wood and metal elements adding to the rustic charm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.84, "peak": 119.16, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.47, "peak": 40.57, "min": 16.95}}, "power_watts_avg": 26.47, "energy_joules_est": 74.62, "sample_count": 28, "duration_seconds": 2.819}, "timestamp": "2026-01-19T13:35:25.716535"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1928.211, "latencies_ms": [1928.211], "images_per_second": 0.519, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two giraffes are standing next to each other in a zoo enclosure with a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.31, "peak": 114.84, "min": 27.62}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.15, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.19, "peak": 40.97, "min": 14.59}}, "power_watts_avg": 31.19, "energy_joules_est": 60.17, "sample_count": 19, "duration_seconds": 1.929}, "timestamp": "2026-01-19T13:35:27.704491"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2922.006, "latencies_ms": [2922.006], "images_per_second": 0.342, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. giraffe: 2\n2. giraffe: 2\n3. giraffe: 2\n4. giraffe: 2\n5. giraffe: 2\n6. giraffe: 2\n7. giraffe: 2\n8. giraffe: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 120.84, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 16.46, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 28.72, "peak": 42.53, "min": 18.52}}, "power_watts_avg": 28.72, "energy_joules_est": 83.93, "sample_count": 29, "duration_seconds": 2.922}, "timestamp": "2026-01-19T13:35:30.698512"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2466.259, "latencies_ms": [2466.259], "images_per_second": 0.405, "prompt_tokens": 1450, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The giraffes are positioned in the foreground of the image, with the building serving as the background. The giraffes are standing on a rocky terrain, which is situated in the middle ground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.45, "peak": 121.28, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.26, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.99, "peak": 41.74, "min": 15.77}}, "power_watts_avg": 29.99, "energy_joules_est": 73.98, "sample_count": 24, "duration_seconds": 2.467}, "timestamp": "2026-01-19T13:35:33.199554"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1984.868, "latencies_ms": [1984.868], "images_per_second": 0.504, "prompt_tokens": 1444, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two giraffes are standing on a rocky ground in a zoo enclosure, with a building and trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.99, "peak": 125.44, "min": 27.91}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 31.87, "peak": 41.74, "min": 16.95}}, "power_watts_avg": 31.87, "energy_joules_est": 63.27, "sample_count": 20, "duration_seconds": 1.985}, "timestamp": "2026-01-19T13:35:35.288144"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1754.496, "latencies_ms": [1754.496], "images_per_second": 0.57, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The giraffes are brown and white, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.44, "peak": 133.02, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.26, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 33.41, "peak": 42.13, "min": 17.35}}, "power_watts_avg": 33.41, "energy_joules_est": 58.63, "sample_count": 17, "duration_seconds": 1.755}, "timestamp": "2026-01-19T13:35:37.064041"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1627.395, "latencies_ms": [1627.395], "images_per_second": 0.614, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A young boy wearing a green and yellow baseball uniform is swinging a blue and white baseball bat at a baseball.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.77, "peak": 122.97, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.28, "peak": 40.95, "min": 21.67}}, "power_watts_avg": 32.28, "energy_joules_est": 52.55, "sample_count": 16, "duration_seconds": 1.628}, "timestamp": "2026-01-19T13:35:38.739645"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2821.437, "latencies_ms": [2821.437], "images_per_second": 0.354, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. boy: 1\n2. helmet: 1\n3. bat: 1\n4. ball: 1\n5. chain link fence: 1\n6. chain link fence post: 1\n7. chain link fence mesh: 1\n8. chain link fence gate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.27, "peak": 106.51, "min": 28.24}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.62, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.62, "energy_joules_est": 75.12, "sample_count": 28, "duration_seconds": 2.822}, "timestamp": "2026-01-19T13:35:41.662532"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2034.495, "latencies_ms": [2034.495], "images_per_second": 0.492, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The baseball player is in the foreground, swinging the bat towards the ball, which is in the middle ground. The fence is in the background, separating the field from the spectators.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.1, "peak": 125.46, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.41, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 28.41, "energy_joules_est": 57.82, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T13:35:43.741566"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1484.888, "latencies_ms": [1484.888], "images_per_second": 0.673, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A young boy is playing baseball in a field with a fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.03, "peak": 111.18, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.31, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.31, "energy_joules_est": 46.5, "sample_count": 15, "duration_seconds": 1.485}, "timestamp": "2026-01-19T13:35:45.308638"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3033.723, "latencies_ms": [3033.723], "images_per_second": 0.33, "prompt_tokens": 1109, "response_tokens_est": 74, "n_tiles": 1, "output_text": " The image captures a young baseball player in action, wearing a green and yellow uniform, swinging a blue bat at a white baseball. The scene is bathed in bright sunlight, casting a warm glow on the player and the field. The fence in the background is made of chain link, and the grass is a vibrant green, suggesting a well-maintained baseball field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.78, "peak": 118.94, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.99, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 25.99, "energy_joules_est": 78.86, "sample_count": 30, "duration_seconds": 3.034}, "timestamp": "2026-01-19T13:35:48.433980"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2278.927, "latencies_ms": [2278.927], "images_per_second": 0.439, "prompt_tokens": 1099, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image captures a bustling scene at a historical reenactment event, where vintage cars and motorcycles are parked in a cobblestone plaza, with a crowd of people gathered around, and a red bus parked in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.24, "peak": 115.94, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.65, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 27.65, "energy_joules_est": 63.04, "sample_count": 22, "duration_seconds": 2.28}, "timestamp": "2026-01-19T13:35:50.732208"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2846.317, "latencies_ms": [2846.317], "images_per_second": 0.351, "prompt_tokens": 1113, "response_tokens_est": 68, "n_tiles": 1, "output_text": " 1. black car: 2\n2. red bus: 1\n3. black motorcycle: 1\n4. red and white bus: 1\n5. black car: 1\n6. black motorcycle: 1\n7. red and white bus: 1\n8. black car: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.06, "peak": 113.86, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.31, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.31, "energy_joules_est": 74.9, "sample_count": 28, "duration_seconds": 2.847}, "timestamp": "2026-01-19T13:35:53.650668"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2314.985, "latencies_ms": [2314.985], "images_per_second": 0.432, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The red bus is positioned in the background, far from the camera, while the black car is in the foreground, close to the camera. The motorcycles are scattered throughout the scene, with some closer to the foreground and others further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.32, "peak": 127.59, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.39, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.39, "energy_joules_est": 63.42, "sample_count": 23, "duration_seconds": 2.315}, "timestamp": "2026-01-19T13:35:56.037854"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2612.312, "latencies_ms": [2612.312], "images_per_second": 0.383, "prompt_tokens": 1111, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image captures a bustling scene at a military parade, with vintage cars and motorcycles parked in the foreground, while a crowd of people gathers in the background. The setting appears to be a public square, possibly a park or a town square, with trees and a building visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.37, "peak": 128.27, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.5, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.5, "energy_joules_est": 69.23, "sample_count": 26, "duration_seconds": 2.613}, "timestamp": "2026-01-19T13:35:58.747449"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2395.002, "latencies_ms": [2395.002], "images_per_second": 0.418, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image features a cobblestone street with a variety of vintage cars and motorcycles parked on the side. The sky is clear and blue, indicating a sunny day. The vehicles are mostly black and silver, with some having red and white accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.71, "peak": 111.06, "min": 27.12}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.94, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.94, "energy_joules_est": 64.53, "sample_count": 24, "duration_seconds": 2.395}, "timestamp": "2026-01-19T13:36:01.234139"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2248.474, "latencies_ms": [2248.474], "images_per_second": 0.445, "prompt_tokens": 1099, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image captures a close-up view of two parking meters, one slightly in front of the other, with the sun setting in the background, casting a warm glow and creating a bokeh effect with the city lights.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.92, "peak": 104.24, "min": 30.21}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.71, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.71, "energy_joules_est": 62.32, "sample_count": 22, "duration_seconds": 2.249}, "timestamp": "2026-01-19T13:36:03.527828"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2667.459, "latencies_ms": [2667.459], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. parking meter: 2\n2. sun: 1\n3. sky: 1\n4. car: 1\n5. window: 1\n6. parking lot: 1\n7. cityscape: 1\n8. parking meter label: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.86, "peak": 109.64, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.82, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.82, "energy_joules_est": 71.55, "sample_count": 26, "duration_seconds": 2.668}, "timestamp": "2026-01-19T13:36:06.215559"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2417.83, "latencies_ms": [2417.83], "images_per_second": 0.414, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The two parking meters are positioned on the left side of the image, with the sun setting in the background, creating a warm and golden glow. The parking meters are in the foreground, with the sun setting in the background, creating a warm and golden glow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.41, "peak": 116.86, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.35, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.35, "energy_joules_est": 66.14, "sample_count": 24, "duration_seconds": 2.418}, "timestamp": "2026-01-19T13:36:08.720570"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2228.193, "latencies_ms": [2228.193], "images_per_second": 0.449, "prompt_tokens": 1111, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image captures a serene moment at a parking lot during sunset, where two parking meters are prominently displayed in the foreground. The warm glow of the setting sun casts a soft light on the scene, creating a tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.19, "peak": 125.83, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.69, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.69, "energy_joules_est": 61.71, "sample_count": 22, "duration_seconds": 2.229}, "timestamp": "2026-01-19T13:36:11.026351"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2437.393, "latencies_ms": [2437.393], "images_per_second": 0.41, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features two parking meters with a warm sunset in the background, casting a golden glow over the scene. The parking meters are black and have a metallic finish, while the sunset is a vibrant orange and yellow, creating a beautiful contrast with the dark sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.6, "peak": 108.27, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.09, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.09, "energy_joules_est": 66.04, "sample_count": 24, "duration_seconds": 2.438}, "timestamp": "2026-01-19T13:36:13.538947"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2544.133, "latencies_ms": [2544.133], "images_per_second": 0.393, "prompt_tokens": 1099, "response_tokens_est": 57, "n_tiles": 1, "output_text": " In the image, there is a large suitcase with various stickers on it, placed on a sidewalk next to a statue. A couple is standing nearby, smiling and posing for a picture. In the background, there is a building with a sign that reads \"Fidelity Investments.\"", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.92, "peak": 113.31, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.86, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.86, "energy_joules_est": 68.35, "sample_count": 25, "duration_seconds": 2.545}, "timestamp": "2026-01-19T13:36:16.148187"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2948.816, "latencies_ms": [2948.816], "images_per_second": 0.339, "prompt_tokens": 1113, "response_tokens_est": 71, "n_tiles": 1, "output_text": " 1. suitcase: 1\n2. man: 1\n3. woman: 1\n4. suitcase sticker: 10\n5. suitcase handle: 1\n6. suitcase buckle: 1\n7. suitcase buckle latch: 1\n8. suitcase buckle lock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.64, "peak": 129.52, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.9, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 25.9, "energy_joules_est": 76.39, "sample_count": 29, "duration_seconds": 2.949}, "timestamp": "2026-01-19T13:36:19.172102"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2034.417, "latencies_ms": [2034.417], "images_per_second": 0.492, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The suitcase is positioned to the left of the couple, who are standing in front of the statue. The suitcase is in the foreground, while the couple is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.3, "peak": 125.25, "min": 30.54}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.54, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.54, "energy_joules_est": 58.07, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T13:36:21.259357"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2442.883, "latencies_ms": [2442.883], "images_per_second": 0.409, "prompt_tokens": 1111, "response_tokens_est": 52, "n_tiles": 1, "output_text": " In the image, there is a large suitcase with stickers on it, and two people standing next to it. The suitcase is placed on a sidewalk, and there is a building with a sign that says \"Fidelity Investments\" in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.51, "peak": 124.75, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.35, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.35, "energy_joules_est": 66.83, "sample_count": 24, "duration_seconds": 2.443}, "timestamp": "2026-01-19T13:36:23.764911"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1634.294, "latencies_ms": [1634.294], "images_per_second": 0.612, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The suitcase is brown with white and red stickers, and the couple is standing in front of a statue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.35, "peak": 127.22, "min": 28.32}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.58, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.58, "energy_joules_est": 49.99, "sample_count": 16, "duration_seconds": 1.635}, "timestamp": "2026-01-19T13:36:25.436902"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2175.261, "latencies_ms": [2175.261], "images_per_second": 0.46, "prompt_tokens": 1099, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image captures a close-up view of a plate filled with a variety of cooked food items, including pieces of chicken, mushrooms, and broccoli, all garnished with fresh parsley and sprinkled with black pepper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.1, "peak": 124.7, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.84, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 28.84, "energy_joules_est": 62.74, "sample_count": 21, "duration_seconds": 2.176}, "timestamp": "2026-01-19T13:36:27.631151"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2536.611, "latencies_ms": [2536.611], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. broccoli: 2\n2. mushrooms: 4\n3. garlic: 1\n4. parsley: 1\n5. chicken: 2\n6. sauce: 1\n7. pepper: 1\n8. plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.39, "peak": 112.5, "min": 30.75}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.08, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 27.08, "energy_joules_est": 68.7, "sample_count": 25, "duration_seconds": 2.537}, "timestamp": "2026-01-19T13:36:30.231483"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1978.607, "latencies_ms": [1978.607], "images_per_second": 0.505, "prompt_tokens": 1117, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The mushrooms are located in the foreground, with the broccoli situated in the background. The parsley is sprinkled on top of the mushrooms and broccoli, creating a visually appealing contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.63, "peak": 130.09, "min": 33.18}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.05, "peak": 39.77, "min": 15.38}}, "power_watts_avg": 29.05, "energy_joules_est": 57.49, "sample_count": 19, "duration_seconds": 1.979}, "timestamp": "2026-01-19T13:36:32.217090"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1978.896, "latencies_ms": [1978.896], "images_per_second": 0.505, "prompt_tokens": 1111, "response_tokens_est": 35, "n_tiles": 1, "output_text": " In the image, there is a plate of food that includes mushrooms, broccoli, and chicken. The food is arranged in a way that makes it look appetizing and delicious.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.56, "peak": 123.37, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.09, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 29.09, "energy_joules_est": 57.57, "sample_count": 20, "duration_seconds": 1.979}, "timestamp": "2026-01-19T13:36:34.300354"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2450.734, "latencies_ms": [2450.734], "images_per_second": 0.408, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a plate of food with a variety of colors, including green broccoli, brown mushrooms, and white chicken. The lighting is bright and natural, highlighting the textures and colors of the food. The plate is made of ceramic and has a glossy finish.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.3, "peak": 119.67, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.12, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.12, "energy_joules_est": 66.48, "sample_count": 24, "duration_seconds": 2.451}, "timestamp": "2026-01-19T13:36:36.800536"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2378.451, "latencies_ms": [2378.451], "images_per_second": 0.42, "prompt_tokens": 1099, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a vibrant display of fresh produce, with a variety of vegetables neatly arranged in baskets and crates, including carrots, leafy greens, and broccoli, all bathed in a soft, natural light that enhances their colors and textures.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.43, "peak": 127.66, "min": 31.84}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.39, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.39, "energy_joules_est": 65.17, "sample_count": 23, "duration_seconds": 2.379}, "timestamp": "2026-01-19T13:36:39.191231"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2577.991, "latencies_ms": [2577.991], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. carrots: 12\n2. broccoli: 1\n3. kale: 1\n4. lettuce: 1\n5. cauliflower: 1\n6. radishes: 1\n7. onions: 1\n8. celery: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.41, "peak": 134.88, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.08, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.08, "energy_joules_est": 69.83, "sample_count": 25, "duration_seconds": 2.579}, "timestamp": "2026-01-19T13:36:41.793956"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1879.433, "latencies_ms": [1879.433], "images_per_second": 0.532, "prompt_tokens": 1117, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The carrots are in the foreground, while the broccoli is in the background. The leafy greens are in the middle ground, with the flowers in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.72, "peak": 121.16, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.2, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 29.2, "energy_joules_est": 54.89, "sample_count": 19, "duration_seconds": 1.88}, "timestamp": "2026-01-19T13:36:43.778016"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2295.12, "latencies_ms": [2295.12], "images_per_second": 0.436, "prompt_tokens": 1111, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image captures a vibrant display of fresh produce at a market stall, with a variety of vegetables and fruits neatly arranged in baskets. The colors and textures of the produce stand out against the dark background, creating a visually appealing scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.65, "peak": 120.43, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.49, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.49, "energy_joules_est": 63.11, "sample_count": 23, "duration_seconds": 2.296}, "timestamp": "2026-01-19T13:36:46.174114"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2554.234, "latencies_ms": [2554.234], "images_per_second": 0.392, "prompt_tokens": 1109, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features a variety of fresh vegetables, including carrots, broccoli, and cauliflower, all displayed in a rustic wooden basket. The lighting is natural and soft, casting a warm glow over the scene, and the vegetables are arranged in a way that highlights their vibrant colors and textures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.98, "peak": 129.17, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.73, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.73, "energy_joules_est": 68.28, "sample_count": 25, "duration_seconds": 2.555}, "timestamp": "2026-01-19T13:36:48.780600"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4114.736, "latencies_ms": [4114.736], "images_per_second": 0.243, "prompt_tokens": 1099, "response_tokens_est": 116, "n_tiles": 1, "output_text": " The image captures a bustling scene inside a donut shop, where a variety of donuts are being meticulously crafted on a conveyor belt. The donuts, in their golden-brown hues, are being expertly handled by a team of workers, who are diligently placing them onto a tray. The shop is well-lit, with a green wall adding a pop of color to the otherwise neutral palette. The workers are dressed in crisp white aprons, adding a professional touch to the scene. The image is taken from a distance, providing a comprehensive view of the shop's operations.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.28, "peak": 114.7, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 23.94, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 23.94, "energy_joules_est": 98.53, "sample_count": 40, "duration_seconds": 4.116}, "timestamp": "2026-01-19T13:36:52.947442"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1484.139, "latencies_ms": [1484.139], "images_per_second": 0.674, "prompt_tokens": 1113, "response_tokens_est": 16, "n_tiles": 1, "output_text": " donut: 10\nmachine: 2\nperson: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.12, "peak": 108.35, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.92, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.92, "energy_joules_est": 45.9, "sample_count": 15, "duration_seconds": 1.484}, "timestamp": "2026-01-19T13:36:54.518171"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1856.918, "latencies_ms": [1856.918], "images_per_second": 0.539, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The donuts are on the left side of the image, the workers are on the right side, and the donut machine is in the middle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.83, "peak": 130.35, "min": 30.25}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.44, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.47, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.47, "energy_joules_est": 56.59, "sample_count": 18, "duration_seconds": 1.857}, "timestamp": "2026-01-19T13:36:56.397134"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2257.649, "latencies_ms": [2257.649], "images_per_second": 0.443, "prompt_tokens": 1111, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image captures a bustling scene inside a donut shop, where a variety of donuts are being prepared on a conveyor belt. The shop is well-lit, with customers visible in the background, adding to the lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.34, "peak": 129.17, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.44, "peak": 39.77, "min": 18.13}}, "power_watts_avg": 28.44, "energy_joules_est": 64.22, "sample_count": 22, "duration_seconds": 2.258}, "timestamp": "2026-01-19T13:36:58.695062"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2089.172, "latencies_ms": [2089.172], "images_per_second": 0.479, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image is taken during the day with natural light illuminating the scene. The colors in the image are vibrant, with the doughnuts being a light brown color and the metal equipment being silver.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.75, "peak": 129.43, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.31, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.31, "energy_joules_est": 59.17, "sample_count": 21, "duration_seconds": 2.09}, "timestamp": "2026-01-19T13:37:00.886189"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1746.806, "latencies_ms": [1746.806], "images_per_second": 0.572, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man in a green jacket and white sneakers is bending down to pick up an orange frisbee in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.97, "peak": 129.8, "min": 28.2}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.87, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 29.87, "energy_joules_est": 52.2, "sample_count": 17, "duration_seconds": 1.748}, "timestamp": "2026-01-19T13:37:02.664719"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2669.098, "latencies_ms": [2669.098], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. tree: 1\n3. frisbee: 2\n4. ground: 1\n5. leaves: 1\n6. sticks: 1\n7. tree trunk: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.26, "peak": 127.18, "min": 30.45}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.54, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.97, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 26.97, "energy_joules_est": 72.0, "sample_count": 26, "duration_seconds": 2.67}, "timestamp": "2026-01-19T13:37:05.369060"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1925.486, "latencies_ms": [1925.486], "images_per_second": 0.519, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The man is standing in the middle of the image, with the trees forming the background. The orange frisbee is in the foreground, close to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.67, "peak": 127.48, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.11, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.11, "energy_joules_est": 56.06, "sample_count": 19, "duration_seconds": 1.926}, "timestamp": "2026-01-19T13:37:07.347833"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1456.692, "latencies_ms": [1456.692], "images_per_second": 0.686, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man in a green jacket is playing frisbee in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.27, "peak": 106.99, "min": 34.34}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.88, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 31.88, "energy_joules_est": 46.45, "sample_count": 14, "duration_seconds": 1.457}, "timestamp": "2026-01-19T13:37:08.811042"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1735.609, "latencies_ms": [1735.609], "images_per_second": 0.576, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image features a person in a green jacket and white shoes standing in a forest with a brown tree trunk and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.81, "peak": 105.56, "min": 27.8}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.7, "peak": 40.97, "min": 21.28}}, "power_watts_avg": 31.7, "energy_joules_est": 55.04, "sample_count": 17, "duration_seconds": 1.736}, "timestamp": "2026-01-19T13:37:10.582869"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1690.423, "latencies_ms": [1690.423], "images_per_second": 0.592, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white sink, a white toilet, and a shower with a blue and white striped curtain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.44, "peak": 119.25, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.45, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 30.45, "energy_joules_est": 51.49, "sample_count": 17, "duration_seconds": 1.691}, "timestamp": "2026-01-19T13:37:12.355084"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2582.548, "latencies_ms": [2582.548], "images_per_second": 0.387, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sink: 1\n2. toilet: 1\n3. shower curtain: 1\n4. mirror: 1\n5. countertop: 1\n6. cabinet: 1\n7. door: 1\n8. floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.69, "peak": 128.72, "min": 28.92}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.17, "energy_joules_est": 70.18, "sample_count": 25, "duration_seconds": 2.583}, "timestamp": "2026-01-19T13:37:14.957425"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2514.155, "latencies_ms": [2514.155], "images_per_second": 0.398, "prompt_tokens": 1118, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The sink is located to the left of the toilet, which is situated in the middle of the bathroom. The shower curtain is hanging to the right of the toilet, and the mirror is above the sink. The blue and green bottles are placed on the countertop near the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.55, "peak": 122.53, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.98, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 26.98, "energy_joules_est": 67.84, "sample_count": 25, "duration_seconds": 2.514}, "timestamp": "2026-01-19T13:37:17.556429"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1750.07, "latencies_ms": [1750.07], "images_per_second": 0.571, "prompt_tokens": 1112, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image depicts a small bathroom with a white sink, a white toilet, and a shower with a blue and white striped curtain.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.69, "peak": 124.84, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.29, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.29, "energy_joules_est": 51.27, "sample_count": 17, "duration_seconds": 1.75}, "timestamp": "2026-01-19T13:37:19.326831"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2061.058, "latencies_ms": [2061.058], "images_per_second": 0.485, "prompt_tokens": 1110, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The bathroom is well-lit with natural light coming from a window, and the walls are painted in a light beige color. The floor is covered with tiles that are a light brown color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.82, "peak": 118.69, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.23, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 29.23, "energy_joules_est": 60.26, "sample_count": 20, "duration_seconds": 2.062}, "timestamp": "2026-01-19T13:37:21.406929"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1951.005, "latencies_ms": [1951.005], "images_per_second": 0.513, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image depicts a kitchen with a large island made of wood and a black countertop, featuring a sink and a faucet, and a dining area with a table and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.99, "peak": 121.15, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.61, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 29.61, "energy_joules_est": 57.79, "sample_count": 19, "duration_seconds": 1.952}, "timestamp": "2026-01-19T13:37:23.384991"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1953.184, "latencies_ms": [1953.184], "images_per_second": 0.512, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " sink: 1, countertop: 1, cabinet: 1, window: 1, chair: 1, table: 1, door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.14, "peak": 125.59, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.77, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 29.77, "energy_joules_est": 58.16, "sample_count": 19, "duration_seconds": 1.953}, "timestamp": "2026-01-19T13:37:25.363436"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2565.41, "latencies_ms": [2565.41], "images_per_second": 0.39, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The sink is located to the left of the stove, which is situated in the middle of the kitchen. The countertop extends from the sink to the stove, creating a continuous workspace. The dining table is positioned in the background, with chairs placed around it, suggesting a communal dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.48, "peak": 122.14, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.4, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.4, "energy_joules_est": 70.31, "sample_count": 25, "duration_seconds": 2.566}, "timestamp": "2026-01-19T13:37:27.954930"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1415.217, "latencies_ms": [1415.217], "images_per_second": 0.707, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A kitchen with a large island and a dining table in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.14, "peak": 125.5, "min": 29.51}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.83, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.83, "energy_joules_est": 45.07, "sample_count": 14, "duration_seconds": 1.416}, "timestamp": "2026-01-19T13:37:29.426468"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1348.158, "latencies_ms": [1348.158], "images_per_second": 0.742, "prompt_tokens": 1109, "response_tokens_est": 11, "n_tiles": 1, "output_text": " The kitchen has a black countertop and white walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.37, "peak": 120.73, "min": 28.26}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 33.55, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 33.55, "energy_joules_est": 45.26, "sample_count": 13, "duration_seconds": 1.349}, "timestamp": "2026-01-19T13:37:30.789117"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2047.995, "latencies_ms": [2047.995], "images_per_second": 0.488, "prompt_tokens": 1100, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image depicts a dimly lit bedroom with a bed covered in a dark blue comforter adorned with a pattern of white daisies, and a person lying down under the covers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.46, "peak": 115.13, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 30.06, "peak": 41.36, "min": 19.7}}, "power_watts_avg": 30.06, "energy_joules_est": 61.58, "sample_count": 20, "duration_seconds": 2.049}, "timestamp": "2026-01-19T13:37:32.880682"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2644.268, "latencies_ms": [2644.268], "images_per_second": 0.378, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. bed: 1\n2. blanket: 1\n3. pillow: 1\n4. sheet: 1\n5. daisy pattern: 1\n6. wall: 1\n7. bedside table: 1\n8. lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.05, "peak": 111.07, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.67, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.67, "energy_joules_est": 70.53, "sample_count": 26, "duration_seconds": 2.645}, "timestamp": "2026-01-19T13:37:35.589770"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2712.202, "latencies_ms": [2712.202], "images_per_second": 0.369, "prompt_tokens": 1118, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The bed is positioned in the center of the image, with the person lying down on the left side. The blanket is draped over the person, covering them partially. The bed is situated in the foreground of the image, with the wall and other objects in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.33, "peak": 129.0, "min": 27.86}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.75, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.52, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 25.52, "energy_joules_est": 69.23, "sample_count": 27, "duration_seconds": 2.713}, "timestamp": "2026-01-19T13:37:38.398233"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2456.238, "latencies_ms": [2456.238], "images_per_second": 0.407, "prompt_tokens": 1112, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image depicts a dimly lit bedroom with a bed covered in a dark blue comforter adorned with white daisy patterns. The bed is unmade, with the comforter slightly crumpled and the sheets peeking out from underneath.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.8, "peak": 109.93, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.17, "peak": 39.0, "min": 14.19}}, "power_watts_avg": 26.17, "energy_joules_est": 64.29, "sample_count": 24, "duration_seconds": 2.457}, "timestamp": "2026-01-19T13:37:40.892893"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2206.993, "latencies_ms": [2206.993], "images_per_second": 0.453, "prompt_tokens": 1110, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image depicts a bedroom with a bed covered in a dark blue comforter adorned with white daisy patterns. The room is dimly lit, with a single light source illuminating the bed and creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.42, "peak": 97.71, "min": 34.24}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.16, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.16, "energy_joules_est": 62.16, "sample_count": 21, "duration_seconds": 2.207}, "timestamp": "2026-01-19T13:37:43.103853"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1667.134, "latencies_ms": [1667.134], "images_per_second": 0.6, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A person is riding a skateboard on a ramp, wearing shorts and sneakers, with a shadow on the ground.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.07, "peak": 115.0, "min": 37.82}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 30.81, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.81, "energy_joules_est": 51.37, "sample_count": 16, "duration_seconds": 1.667}, "timestamp": "2026-01-19T13:37:44.777483"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2644.288, "latencies_ms": [2644.288], "images_per_second": 0.378, "prompt_tokens": 1114, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. skateboard: 1\n2. person: 1\n3. shorts: 1\n4. socks: 1\n5. shoes: 1\n6. skateboard wheels: 2\n7. skateboard deck: 1\n8. shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.73, "peak": 124.93, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.15, "peak": 40.16, "min": 17.73}}, "power_watts_avg": 27.15, "energy_joules_est": 71.8, "sample_count": 26, "duration_seconds": 2.645}, "timestamp": "2026-01-19T13:37:47.485730"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2215.04, "latencies_ms": [2215.04], "images_per_second": 0.451, "prompt_tokens": 1118, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The skateboarder's feet are in the foreground, with the skateboard and the ramp in the middle ground. The skateboarder is positioned to the left of the ramp, and the buildings are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.44, "peak": 116.2, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.69, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.69, "energy_joules_est": 61.34, "sample_count": 22, "duration_seconds": 2.215}, "timestamp": "2026-01-19T13:37:49.781218"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1693.072, "latencies_ms": [1693.072], "images_per_second": 0.591, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A person is riding a skateboard on a concrete surface, with another person sitting on a ledge in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.83, "peak": 120.77, "min": 27.71}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.23, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.23, "energy_joules_est": 49.5, "sample_count": 17, "duration_seconds": 1.693}, "timestamp": "2026-01-19T13:37:51.556837"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2697.741, "latencies_ms": [2697.741], "images_per_second": 0.371, "prompt_tokens": 1110, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image is in black and white, with the skateboarder's legs and shoes being the main focus. The skateboarder is wearing a white shirt and plaid shorts, and the skateboard has a visible logo on it. The background features a concrete ramp and some buildings, suggesting an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.67, "peak": 122.62, "min": 30.24}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.6, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.6, "energy_joules_est": 71.77, "sample_count": 26, "duration_seconds": 2.698}, "timestamp": "2026-01-19T13:37:54.269591"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1649.026, "latencies_ms": [1649.026], "images_per_second": 0.606, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man is lying on the floor with a laptop, a tennis racket, and a book on the floor.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.09, "peak": 128.21, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.73, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.73, "energy_joules_est": 50.69, "sample_count": 16, "duration_seconds": 1.649}, "timestamp": "2026-01-19T13:37:55.951764"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2143.568, "latencies_ms": [2143.568], "images_per_second": 0.467, "prompt_tokens": 1113, "response_tokens_est": 41, "n_tiles": 1, "output_text": " laptop: 1\ncamera: 1\nphone: 1\nbottle: 1\nbook: 1\nracket: 1\nkey: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.82, "peak": 122.31, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 29.0, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 29.0, "energy_joules_est": 62.18, "sample_count": 21, "duration_seconds": 2.144}, "timestamp": "2026-01-19T13:37:58.139089"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2364.332, "latencies_ms": [2364.332], "images_per_second": 0.423, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The laptop is on the left side of the image, the tennis racket is in the middle, and the book is on the right side. The laptop is closest to the camera, followed by the tennis racket, and then the book.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.47, "peak": 122.7, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.8, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.8, "energy_joules_est": 65.74, "sample_count": 23, "duration_seconds": 2.365}, "timestamp": "2026-01-19T13:38:00.531329"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1579.232, "latencies_ms": [1579.232], "images_per_second": 0.633, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man is laying on the floor with a laptop, a tennis racket, and a book.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.88, "peak": 115.91, "min": 27.65}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.65, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.65, "energy_joules_est": 48.42, "sample_count": 16, "duration_seconds": 1.58}, "timestamp": "2026-01-19T13:38:02.202357"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1758.183, "latencies_ms": [1758.183], "images_per_second": 0.569, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image is taken in a room with a carpeted floor, the lighting is natural and bright, and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.75, "peak": 121.94, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.59, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 30.59, "energy_joules_est": 53.81, "sample_count": 17, "duration_seconds": 1.759}, "timestamp": "2026-01-19T13:38:03.972207"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1916.656, "latencies_ms": [1916.656], "images_per_second": 0.522, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image depicts a kitchen with a black stove top, a black range hood above it, and a backsplash made of tiles in shades of brown and beige.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.49, "peak": 120.57, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.02, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.02, "energy_joules_est": 57.56, "sample_count": 19, "duration_seconds": 1.917}, "timestamp": "2026-01-19T13:38:05.952318"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2341.898, "latencies_ms": [2341.898], "images_per_second": 0.427, "prompt_tokens": 1113, "response_tokens_est": 49, "n_tiles": 1, "output_text": " 1. black stove top\n2. black range hood\n3. black knife block\n4. black pot\n5. black oven\n6. black stove top burner\n7. black stove top burner\n8. black stove top burner", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.72, "peak": 123.88, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.75, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.75, "energy_joules_est": 65.0, "sample_count": 23, "duration_seconds": 2.342}, "timestamp": "2026-01-19T13:38:08.355268"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2022.039, "latencies_ms": [2022.039], "images_per_second": 0.495, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The stove is located in the foreground, with the backsplash behind it. The knives are on the left side of the stove, while the hanging towels are on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.93, "peak": 114.89, "min": 31.48}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.82, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.82, "energy_joules_est": 58.29, "sample_count": 20, "duration_seconds": 2.023}, "timestamp": "2026-01-19T13:38:10.432393"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1812.568, "latencies_ms": [1812.568], "images_per_second": 0.552, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image captures a modern kitchen with a black induction cooktop and a backsplash made of large tiles in shades of brown and beige.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.9, "peak": 116.1, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.9, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.9, "energy_joules_est": 54.21, "sample_count": 18, "duration_seconds": 1.813}, "timestamp": "2026-01-19T13:38:12.307596"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1549.948, "latencies_ms": [1549.948], "images_per_second": 0.645, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The kitchen has a black stove top with a marble backsplash and a black range hood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.37, "peak": 123.61, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.7, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 31.7, "energy_joules_est": 49.14, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T13:38:13.869574"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1955.411, "latencies_ms": [1955.411], "images_per_second": 0.511, "prompt_tokens": 1100, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a person is seated at a table in a restaurant, holding a cup of coffee and a donut, with a plate of donuts in front of them.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.06, "peak": 121.27, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.15, "peak": 40.18, "min": 20.5}}, "power_watts_avg": 30.15, "energy_joules_est": 58.97, "sample_count": 19, "duration_seconds": 1.956}, "timestamp": "2026-01-19T13:38:15.858765"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2162.024, "latencies_ms": [2162.024], "images_per_second": 0.463, "prompt_tokens": 1114, "response_tokens_est": 43, "n_tiles": 1, "output_text": " coffee cup: 1, donut: 1, plate: 1, person: 1, table: 1, donut: 1, person: 1, donut: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.14, "peak": 130.71, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.69, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.69, "energy_joules_est": 62.04, "sample_count": 21, "duration_seconds": 2.163}, "timestamp": "2026-01-19T13:38:18.035635"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2424.736, "latencies_ms": [2424.736], "images_per_second": 0.412, "prompt_tokens": 1118, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground, with the person in the background. The person is holding a cup of coffee, which is placed on the table. The donuts are placed on the table, with the chocolate donut being the closest to the camera.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.8, "peak": 116.63, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.46, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 27.46, "energy_joules_est": 66.6, "sample_count": 24, "duration_seconds": 2.425}, "timestamp": "2026-01-19T13:38:20.537433"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1960.138, "latencies_ms": [1960.138], "images_per_second": 0.51, "prompt_tokens": 1112, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In a bustling coffee shop, a person is enjoying a delicious chocolate donut with a side of coffee. The shop is filled with other patrons, creating a lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.16, "peak": 123.09, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.47, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.47, "energy_joules_est": 55.81, "sample_count": 19, "duration_seconds": 1.96}, "timestamp": "2026-01-19T13:38:22.513597"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1800.073, "latencies_ms": [1800.073], "images_per_second": 0.556, "prompt_tokens": 1110, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken in a dimly lit cafe with warm lighting, and the food is displayed on a tray with a white paper underneath.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.32, "peak": 117.89, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 30.03, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 30.03, "energy_joules_est": 54.07, "sample_count": 18, "duration_seconds": 1.801}, "timestamp": "2026-01-19T13:38:24.377570"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1981.986, "latencies_ms": [1981.986], "images_per_second": 0.505, "prompt_tokens": 1100, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image depicts a bathroom with a black and white checkered floor, a white sink with a glass top, and a white toilet with a black and white cow print cover.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.9, "peak": 106.07, "min": 27.3}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.82, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.82, "energy_joules_est": 57.15, "sample_count": 20, "duration_seconds": 1.983}, "timestamp": "2026-01-19T13:38:26.467956"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2655.781, "latencies_ms": [2655.781], "images_per_second": 0.377, "prompt_tokens": 1114, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Toilet: 1\n2. Sink: 1\n3. Tile: 1\n4. Bathtub: 1\n5. Glass: 1\n6. Plate: 1\n7. Tile: 1\n8. Floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.22, "peak": 128.64, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.62, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.62, "energy_joules_est": 70.7, "sample_count": 26, "duration_seconds": 2.656}, "timestamp": "2026-01-19T13:38:29.168288"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2072.331, "latencies_ms": [2072.331], "images_per_second": 0.483, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The toilet is located in the foreground of the image, with the sink and mirror in the background. The sink is situated to the left of the toilet, while the mirror is above the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.08, "peak": 107.66, "min": 28.64}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.62, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.62, "energy_joules_est": 59.32, "sample_count": 20, "duration_seconds": 2.073}, "timestamp": "2026-01-19T13:38:31.258146"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1743.881, "latencies_ms": [1743.881], "images_per_second": 0.573, "prompt_tokens": 1112, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A bathroom with a black and white checkered floor, a white sink, and a black and white checkered toilet seat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.24, "peak": 118.23, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.03, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 30.03, "energy_joules_est": 52.38, "sample_count": 17, "duration_seconds": 1.744}, "timestamp": "2026-01-19T13:38:33.032218"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1770.378, "latencies_ms": [1770.378], "images_per_second": 0.565, "prompt_tokens": 1110, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The bathroom has a black and white checkered floor, a white sink, and a white toilet with a black and white cow pattern.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.17, "peak": 111.97, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.05, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 30.05, "energy_joules_est": 53.21, "sample_count": 18, "duration_seconds": 1.771}, "timestamp": "2026-01-19T13:38:34.905674"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2880.84, "latencies_ms": [2880.84], "images_per_second": 0.347, "prompt_tokens": 1100, "response_tokens_est": 67, "n_tiles": 1, "output_text": " In the image, there is a statue of a teddy bear with a sign that reads \"Joseph Pais, 1952-2012, Died 1-1-53, 41-8-88, Died 4-1-98\" placed in a garden.", "error": null, "sys_before": {"cpu_percent": 6.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.7, "peak": 118.86, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 12.21}, "VDD_GPU": {"avg": 25.89, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 25.89, "energy_joules_est": 74.61, "sample_count": 28, "duration_seconds": 2.882}, "timestamp": "2026-01-19T13:38:37.825976"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2538.887, "latencies_ms": [2538.887], "images_per_second": 0.394, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. teddy bear: 3\n2. sign: 1\n3. grass: 1\n4. brick: 1\n5. tree: 1\n6. house: 1\n7. fence: 1\n8. path: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.87, "peak": 127.66, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.87, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.87, "energy_joules_est": 68.24, "sample_count": 25, "duration_seconds": 2.54}, "timestamp": "2026-01-19T13:38:40.433203"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1996.531, "latencies_ms": [1996.531], "images_per_second": 0.501, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The teddy bear is positioned in the foreground, with the sign behind it. The teddy bear is located to the left of the sign, and the sign is situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.09, "peak": 126.28, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.49, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.49, "energy_joules_est": 56.89, "sample_count": 20, "duration_seconds": 1.997}, "timestamp": "2026-01-19T13:38:42.520648"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3185.194, "latencies_ms": [3185.194], "images_per_second": 0.314, "prompt_tokens": 1112, "response_tokens_est": 81, "n_tiles": 1, "output_text": " In a serene garden, a poignant memorial stands, adorned with a teddy bear and a sign that reads \"Joseph Pais, 1952-2012, Died 1-3-13, 41-8-8 Died 4-1-198\". The garden is a tranquil setting, with a house and lush greenery in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.35, "peak": 116.87, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.44, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 25.04, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 25.04, "energy_joules_est": 79.76, "sample_count": 31, "duration_seconds": 3.185}, "timestamp": "2026-01-19T13:38:45.737856"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2832.812, "latencies_ms": [2832.812], "images_per_second": 0.353, "prompt_tokens": 1110, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image depicts a statue of a teddy bear with a sign that reads \"Joseph Pais\" and \"Misty Malahan\" on it. The statue is made of a beige material and is surrounded by a small garden with green grass and plants. The lighting in the image is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.85, "peak": 123.24, "min": 27.86}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.02, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.02, "energy_joules_est": 73.72, "sample_count": 28, "duration_seconds": 2.833}, "timestamp": "2026-01-19T13:38:48.650774"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1968.812, "latencies_ms": [1968.812], "images_per_second": 0.508, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image captures a bustling restaurant with a large clock face on the wall, where patrons are seated at tables, enjoying their meals and drinks, with a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.88, "peak": 117.69, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.01, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.01, "energy_joules_est": 57.13, "sample_count": 19, "duration_seconds": 1.969}, "timestamp": "2026-01-19T13:38:50.636843"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1910.461, "latencies_ms": [1910.461], "images_per_second": 0.523, "prompt_tokens": 1113, "response_tokens_est": 33, "n_tiles": 1, "output_text": " table: 10, chair: 20, clock: 1, person: 20, cup: 10, glass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.34, "peak": 121.88, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.69, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.69, "energy_joules_est": 56.73, "sample_count": 19, "duration_seconds": 1.911}, "timestamp": "2026-01-19T13:38:52.609848"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2607.699, "latencies_ms": [2607.699], "images_per_second": 0.383, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The clock is positioned in the background, far from the camera, and occupies a significant portion of the image. The bar is located in the foreground, close to the camera, and is surrounded by patrons. The tables and chairs are situated in the middle ground, with the patrons seated at them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.44, "peak": 116.27, "min": 27.04}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.78, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.78, "energy_joules_est": 69.84, "sample_count": 26, "duration_seconds": 2.608}, "timestamp": "2026-01-19T13:38:55.310763"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2210.867, "latencies_ms": [2210.867], "images_per_second": 0.452, "prompt_tokens": 1111, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image captures a bustling restaurant with a large clock face on the wall, where people are enjoying their meals and drinks. The restaurant is filled with tables and chairs, and there are waiters attending to the customers.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.76, "peak": 117.4, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.67, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.67, "energy_joules_est": 61.18, "sample_count": 22, "duration_seconds": 2.211}, "timestamp": "2026-01-19T13:38:57.612772"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2449.749, "latencies_ms": [2449.749], "images_per_second": 0.408, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image is taken during the day, with natural light streaming in through the large window. The interior is bathed in warm tones, with the wooden beams and metal beams adding a rustic charm. The lighting is soft and diffused, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.91, "peak": 124.54, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.09, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.09, "energy_joules_est": 66.37, "sample_count": 24, "duration_seconds": 2.45}, "timestamp": "2026-01-19T13:39:00.116450"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1839.938, "latencies_ms": [1839.938], "images_per_second": 0.543, "prompt_tokens": 1100, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A child in a black jacket and helmet is standing on a snowy slope, while another child in a pink jacket and helmet is skiing down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.55, "peak": 123.11, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.31, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.31, "energy_joules_est": 53.94, "sample_count": 18, "duration_seconds": 1.84}, "timestamp": "2026-01-19T13:39:02.007764"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2764.754, "latencies_ms": [2764.754], "images_per_second": 0.362, "prompt_tokens": 1114, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. skis: 2\n2. snowboard: 1\n3. helmet: 1\n4. goggles: 1\n5. backpack: 1\n6. ski poles: 2\n7. snowboard bindings: 1\n8. skis bindings: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.6, "peak": 127.08, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 26.51, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.51, "energy_joules_est": 73.3, "sample_count": 27, "duration_seconds": 2.765}, "timestamp": "2026-01-19T13:39:04.818733"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2378.064, "latencies_ms": [2378.064], "images_per_second": 0.421, "prompt_tokens": 1118, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The skier is positioned in the foreground, with the child in the background. The skier is standing on the left side of the image, while the child is on the right side. The skier is closer to the camera than the child.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.34, "peak": 106.01, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.44, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.44, "energy_joules_est": 65.27, "sample_count": 23, "duration_seconds": 2.379}, "timestamp": "2026-01-19T13:39:07.219038"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1420.218, "latencies_ms": [1420.218], "images_per_second": 0.704, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A child is skiing down a snowy hill with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.37, "peak": 112.29, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.21, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.21, "energy_joules_est": 44.34, "sample_count": 14, "duration_seconds": 1.421}, "timestamp": "2026-01-19T13:39:08.684163"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2217.919, "latencies_ms": [2217.919], "images_per_second": 0.451, "prompt_tokens": 1110, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features a snowy mountain with a child wearing a black jacket and yellow skis standing on the slope. The sky is clear with a few clouds, and the sun is shining brightly, casting shadows on the snow.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.24, "peak": 131.91, "min": 31.51}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.69, "peak": 40.97, "min": 18.13}}, "power_watts_avg": 28.69, "energy_joules_est": 63.64, "sample_count": 22, "duration_seconds": 2.218}, "timestamp": "2026-01-19T13:39:10.976186"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1772.364, "latencies_ms": [1772.364], "images_per_second": 0.564, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A pair of feet wearing flip-flops is standing on a wooden floor with a broken mobile phone and its components scattered around them.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.23, "peak": 114.59, "min": 34.78}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.04, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.04, "energy_joules_est": 53.26, "sample_count": 17, "duration_seconds": 1.773}, "timestamp": "2026-01-19T13:39:12.757545"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2625.118, "latencies_ms": [2625.118], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. flip phone: 2\n2. person: 1\n3. flip phone: 2\n4. person: 1\n5. flip phone: 2\n6. person: 1\n7. flip phone: 2\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.61, "peak": 126.56, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.17, "energy_joules_est": 71.33, "sample_count": 26, "duration_seconds": 2.625}, "timestamp": "2026-01-19T13:39:15.463979"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2947.885, "latencies_ms": [2947.885], "images_per_second": 0.339, "prompt_tokens": 1117, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The main objects are arranged in a diagonal line from the top left to the bottom right of the image. The person's feet are positioned at the bottom of the image, with the cell phones placed above them. The cell phones are positioned in the middle of the image, with the circuit board of the first cell phone placed closest to the person's feet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.38, "peak": 119.96, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.69, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 25.69, "energy_joules_est": 75.74, "sample_count": 29, "duration_seconds": 2.948}, "timestamp": "2026-01-19T13:39:18.483036"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1742.327, "latencies_ms": [1742.327], "images_per_second": 0.574, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A pair of feet wearing flip flops are standing on a wooden floor with a broken cell phone and its components scattered around them.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.23, "peak": 121.65, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.06, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.06, "energy_joules_est": 52.39, "sample_count": 17, "duration_seconds": 1.743}, "timestamp": "2026-01-19T13:39:20.243703"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1646.996, "latencies_ms": [1646.996], "images_per_second": 0.607, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The wooden floor is brown and the person's feet are black. The lighting is natural and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.1, "ram_available_mb": 99765.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.78, "peak": 130.9, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.49, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 31.49, "energy_joules_est": 51.88, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T13:39:21.912016"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2567.725, "latencies_ms": [2567.725], "images_per_second": 0.389, "prompt_tokens": 1099, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image captures the majestic Palace of Westminster, also known as the Houses of Parliament, bathed in the warm glow of the setting sun, with the iconic Big Ben clock tower standing tall in the background, while a boat with a red and white flag floats on the river in the foreground.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26007.1, "ram_available_mb": 99765.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.76, "peak": 106.55, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.48, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 27.48, "energy_joules_est": 70.58, "sample_count": 25, "duration_seconds": 2.568}, "timestamp": "2026-01-19T13:39:24.520250"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2525.697, "latencies_ms": [2525.697], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Boat: 2\n2. Boat: 1\n3. Boat: 1\n4. Boat: 1\n5. Boat: 1\n6. Boat: 1\n7. Boat: 1\n8. Boat: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.78, "peak": 130.6, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.96, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.96, "energy_joules_est": 68.1, "sample_count": 25, "duration_seconds": 2.526}, "timestamp": "2026-01-19T13:39:27.119523"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2330.697, "latencies_ms": [2330.697], "images_per_second": 0.429, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The clock tower is located in the background, far away from the camera, while the river is in the foreground, close to the camera. The buildings are in the middle ground, with the clock tower being the tallest and most prominent structure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.93, "peak": 118.71, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.43, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.43, "energy_joules_est": 63.94, "sample_count": 23, "duration_seconds": 2.331}, "timestamp": "2026-01-19T13:39:29.516637"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3226.006, "latencies_ms": [3226.006], "images_per_second": 0.31, "prompt_tokens": 1111, "response_tokens_est": 82, "n_tiles": 1, "output_text": " The image captures the majestic Palace of Westminster, also known as the Houses of Parliament, in London, England. The scene is set against a backdrop of a cloudy sky, with the river Thames flowing in the foreground. The palace, illuminated by warm lights, stands majestically on the north bank of the river, while boats are seen on the river, adding a touch of life to the otherwise serene setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.48, "peak": 126.15, "min": 27.21}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.19, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 25.19, "energy_joules_est": 81.27, "sample_count": 32, "duration_seconds": 3.226}, "timestamp": "2026-01-19T13:39:32.848276"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2090.918, "latencies_ms": [2090.918], "images_per_second": 0.478, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The sky is overcast with a grayish hue, and the water is a murky brown. The buildings are bathed in a warm yellow glow, contrasting with the cool tones of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.75, "peak": 129.38, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.96, "peak": 39.39, "min": 14.19}}, "power_watts_avg": 27.96, "energy_joules_est": 58.47, "sample_count": 21, "duration_seconds": 2.091}, "timestamp": "2026-01-19T13:39:35.043650"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2067.556, "latencies_ms": [2067.556], "images_per_second": 0.484, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image depicts a spacious living room with a red carpet, a green couch, a black sofa, and a wooden floor, with a ceiling fan, a television, and a bookshelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.8, "peak": 126.89, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.14, "peak": 39.0, "min": 14.98}}, "power_watts_avg": 28.14, "energy_joules_est": 58.19, "sample_count": 21, "duration_seconds": 2.068}, "timestamp": "2026-01-19T13:39:37.222165"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2239.57, "latencies_ms": [2239.57], "images_per_second": 0.447, "prompt_tokens": 1113, "response_tokens_est": 45, "n_tiles": 1, "output_text": " chair: 2, sofa: 1, television: 1, potted plant: 2, mirror: 1, television stand: 1, bookshelf: 1, ceiling fan: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.37, "peak": 116.97, "min": 29.56}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.88, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.88, "energy_joules_est": 62.45, "sample_count": 22, "duration_seconds": 2.24}, "timestamp": "2026-01-19T13:39:39.508112"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2818.402, "latencies_ms": [2818.402], "images_per_second": 0.355, "prompt_tokens": 1117, "response_tokens_est": 67, "n_tiles": 1, "output_text": " The living room is situated in the center of the image, with the furniture arranged around it. The foreground features a red rug and a coffee table, while the background includes a television and a bookshelf. The ceiling fan is positioned in the upper right corner of the room, and the windows are located on the left and right sides.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.13, "peak": 124.6, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.14, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.14, "energy_joules_est": 73.68, "sample_count": 28, "duration_seconds": 2.819}, "timestamp": "2026-01-19T13:39:42.429747"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1444.873, "latencies_ms": [1444.873], "images_per_second": 0.692, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A living room with a red carpet, a couch, and a chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.73, "peak": 116.55, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.27, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 31.27, "energy_joules_est": 45.2, "sample_count": 14, "duration_seconds": 1.445}, "timestamp": "2026-01-19T13:39:43.897990"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1561.767, "latencies_ms": [1561.767], "images_per_second": 0.64, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is filled with natural light from the windows, and the wooden floor is polished and shiny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.83, "peak": 121.43, "min": 35.03}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.86, "peak": 40.57, "min": 21.28}}, "power_watts_avg": 32.86, "energy_joules_est": 51.33, "sample_count": 15, "duration_seconds": 1.562}, "timestamp": "2026-01-19T13:39:45.464104"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2474.819, "latencies_ms": [2474.819], "images_per_second": 0.404, "prompt_tokens": 1100, "response_tokens_est": 53, "n_tiles": 1, "output_text": " A red pole with a circular metal ring stands on a sidewalk, with a parking meter attached to it, in front of a building with a large window that has the words \"100 YEARS OF SAVING LIVES\" printed on it.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.77, "peak": 113.38, "min": 30.76}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 27.96, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 27.96, "energy_joules_est": 69.21, "sample_count": 24, "duration_seconds": 2.475}, "timestamp": "2026-01-19T13:39:47.962602"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2175.412, "latencies_ms": [2175.412], "images_per_second": 0.46, "prompt_tokens": 1114, "response_tokens_est": 43, "n_tiles": 1, "output_text": " 1. red post\n2. red circular holder\n3. red post with holder\n4. parking meter\n5. parking meter\n6. parking meter\n7. parking meter\n8. parking meter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.49, "peak": 126.69, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.39, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.39, "energy_joules_est": 61.77, "sample_count": 21, "duration_seconds": 2.176}, "timestamp": "2026-01-19T13:39:50.151411"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2463.23, "latencies_ms": [2463.23], "images_per_second": 0.406, "prompt_tokens": 1118, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The red parking meter is located in the foreground on the left side of the image, while the building with the \"100 Years of Saving Lives\" sign is in the background on the right side. The parking meter is positioned closer to the viewer than the building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.9, "peak": 121.86, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.45, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.45, "energy_joules_est": 67.62, "sample_count": 24, "duration_seconds": 2.463}, "timestamp": "2026-01-19T13:39:52.644366"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2636.072, "latencies_ms": [2636.072], "images_per_second": 0.379, "prompt_tokens": 1112, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image captures a vibrant urban scene, where a red parking meter stands proudly on a sidewalk, its circular top gleaming under the sunlight. In the background, a building proudly displays a sign that reads \"100 Years of Saving Lives,\" a testament to the building's long-standing legacy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.89, "peak": 128.75, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.08, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.08, "energy_joules_est": 68.76, "sample_count": 26, "duration_seconds": 2.637}, "timestamp": "2026-01-19T13:39:55.366523"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2695.777, "latencies_ms": [2695.777], "images_per_second": 0.371, "prompt_tokens": 1110, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image features a red metal pole with a circular metal ring on top, standing on a sidewalk. The pole is located in front of a building with a large window that has the words \"100 Years of Saving Lives\" written on it. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.27, "peak": 127.09, "min": 31.1}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.34, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.34, "energy_joules_est": 71.02, "sample_count": 26, "duration_seconds": 2.696}, "timestamp": "2026-01-19T13:39:58.080114"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1610.019, "latencies_ms": [1610.019], "images_per_second": 0.621, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man and a woman are sitting on a couch in a living room, watching TV and eating snacks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.58, "peak": 113.73, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.58, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.58, "energy_joules_est": 49.25, "sample_count": 16, "duration_seconds": 1.611}, "timestamp": "2026-01-19T13:39:59.754299"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2844.59, "latencies_ms": [2844.59], "images_per_second": 0.352, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. sofa: 2\n2. television: 1\n3. coffee table: 1\n4. television stand: 1\n5. bookshelf: 1\n6. potted plant: 1\n7. couch: 1\n8. bag of chips: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.85, "peak": 127.84, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.44, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.19, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.19, "energy_joules_est": 74.51, "sample_count": 28, "duration_seconds": 2.845}, "timestamp": "2026-01-19T13:40:02.679437"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2257.276, "latencies_ms": [2257.276], "images_per_second": 0.443, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The television is on the left side of the room, the couch is in the middle, and the coffee table is in the foreground. The person on the right is closer to the camera than the person on the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.54, "peak": 129.87, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.56, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 27.56, "energy_joules_est": 62.22, "sample_count": 22, "duration_seconds": 2.258}, "timestamp": "2026-01-19T13:40:04.973550"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1431.797, "latencies_ms": [1431.797], "images_per_second": 0.698, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A couple is sitting on a couch in a living room watching TV.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.2, "peak": 119.33, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.69, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 31.69, "energy_joules_est": 45.39, "sample_count": 14, "duration_seconds": 1.432}, "timestamp": "2026-01-19T13:40:06.447740"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2001.315, "latencies_ms": [2001.315], "images_per_second": 0.5, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The room is lit by a warm light, and the walls are painted in a light color. The furniture is made of wood, and the floor is covered with a carpet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.59, "peak": 121.97, "min": 30.41}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.74, "peak": 40.97, "min": 18.91}}, "power_watts_avg": 29.74, "energy_joules_est": 59.53, "sample_count": 20, "duration_seconds": 2.002}, "timestamp": "2026-01-19T13:40:08.531529"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1520.092, "latencies_ms": [1520.092], "images_per_second": 0.658, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A person is holding a white power strip with multiple outlets in front of a white appliance.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.28, "peak": 117.76, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.17, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.17, "energy_joules_est": 47.4, "sample_count": 15, "duration_seconds": 1.521}, "timestamp": "2026-01-19T13:40:10.105229"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2381.005, "latencies_ms": [2381.005], "images_per_second": 0.42, "prompt_tokens": 1113, "response_tokens_est": 44, "n_tiles": 1, "output_text": " 1. person's hand\n2. white box\n3. white floor\n4. white wall\n5. white door\n6. white door handle\n7. white door lock\n8. white door knob", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.21, "peak": 116.8, "min": 28.02}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.68, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 27.68, "energy_joules_est": 65.92, "sample_count": 23, "duration_seconds": 2.382}, "timestamp": "2026-01-19T13:40:12.504427"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2261.497, "latencies_ms": [2261.497], "images_per_second": 0.442, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The hand is holding the power strip in front of the washing machine, which is located to the left of the power strip. The power strip is positioned in the foreground, while the washing machine is situated in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.63, "peak": 117.42, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.88, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.88, "energy_joules_est": 63.06, "sample_count": 22, "duration_seconds": 2.262}, "timestamp": "2026-01-19T13:40:14.802826"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1634.774, "latencies_ms": [1634.774], "images_per_second": 0.612, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A person is holding a white power strip with multiple outlets. The power strip is plugged into a wall outlet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.64, "peak": 128.28, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.63, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.63, "energy_joules_est": 50.09, "sample_count": 16, "duration_seconds": 1.635}, "timestamp": "2026-01-19T13:40:16.476054"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1533.05, "latencies_ms": [1533.05], "images_per_second": 0.652, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The room is well-lit with natural light, and the floor is made of wood.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.25, "peak": 126.02, "min": 30.3}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.91, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 31.91, "energy_joules_est": 48.93, "sample_count": 15, "duration_seconds": 1.533}, "timestamp": "2026-01-19T13:40:18.045385"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2166.512, "latencies_ms": [2166.512], "images_per_second": 0.462, "prompt_tokens": 1099, "response_tokens_est": 42, "n_tiles": 1, "output_text": " In the image, a snowboarder is captured mid-air performing a trick on a snowy ramp, with a crowd of spectators watching from the side, and a red flag marking the end of the ramp.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.51, "peak": 127.07, "min": 30.11}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.17, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 29.17, "energy_joules_est": 63.22, "sample_count": 21, "duration_seconds": 2.167}, "timestamp": "2026-01-19T13:40:20.240291"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2681.32, "latencies_ms": [2681.32], "images_per_second": 0.373, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. snowboarder: 2\n2. snowboard: 1\n3. crowd: 1\n4. flag: 1\n5. snow: 1\n6. mountain: 1\n7. sky: 1\n8. snowboarder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.12, "peak": 118.1, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.84, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.84, "energy_joules_est": 71.97, "sample_count": 26, "duration_seconds": 2.682}, "timestamp": "2026-01-19T13:40:22.950312"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2620.015, "latencies_ms": [2620.015], "images_per_second": 0.382, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The main objects are the snowboarder and the crowd. The snowboarder is in the foreground, performing a trick in the air, while the crowd is in the background, watching the event. The snowboarder is also near the crowd, as they are both in the same location.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.7, "peak": 126.92, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.69, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.69, "energy_joules_est": 69.95, "sample_count": 26, "duration_seconds": 2.621}, "timestamp": "2026-01-19T13:40:25.659338"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1767.346, "latencies_ms": [1767.346], "images_per_second": 0.566, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A snowboarder is performing a trick in the air above a snow ramp. The ramp is surrounded by a crowd of spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.25, "peak": 91.27, "min": 32.09}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.83, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.83, "energy_joules_est": 52.73, "sample_count": 17, "duration_seconds": 1.768}, "timestamp": "2026-01-19T13:40:27.436822"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2189.836, "latencies_ms": [2189.836], "images_per_second": 0.457, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a snowboarder performing a trick in the air, with a clear blue sky in the background. The snowboarder is wearing a red and white outfit, and the snow is white and pristine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.82, "peak": 122.46, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.49, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 28.49, "energy_joules_est": 62.4, "sample_count": 22, "duration_seconds": 2.19}, "timestamp": "2026-01-19T13:40:29.731922"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2093.921, "latencies_ms": [2093.921], "images_per_second": 0.478, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image depicts a well-organized home office with a wooden desk, a black office chair, and a computer setup, including a monitor, keyboard, and mouse, all placed on the desk.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.37, "peak": 104.51, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.94, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.94, "energy_joules_est": 58.52, "sample_count": 21, "duration_seconds": 2.095}, "timestamp": "2026-01-19T13:40:31.928496"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2083.929, "latencies_ms": [2083.929], "images_per_second": 0.48, "prompt_tokens": 1113, "response_tokens_est": 38, "n_tiles": 1, "output_text": " 1. white sofa\n2. black chair\n3. desk\n4. computer monitor\n5. keyboard\n6. mouse\n7. lamp\n8. potted plant", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.88, "peak": 119.18, "min": 32.08}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.54, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.54, "energy_joules_est": 59.48, "sample_count": 20, "duration_seconds": 2.084}, "timestamp": "2026-01-19T13:40:34.017846"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2146.781, "latencies_ms": [2146.781], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The desk is positioned to the left of the white couch, with the chair in front of it. The potted plant is located near the desk, while the bookshelf is situated further back in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.26, "peak": 126.27, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.78, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 28.78, "energy_joules_est": 61.79, "sample_count": 21, "duration_seconds": 2.147}, "timestamp": "2026-01-19T13:40:36.210038"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1397.842, "latencies_ms": [1397.842], "images_per_second": 0.715, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A room with a desk, chair, and computer on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.7, "peak": 129.46, "min": 30.15}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.85, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.85, "energy_joules_est": 44.54, "sample_count": 14, "duration_seconds": 1.398}, "timestamp": "2026-01-19T13:40:37.673675"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1651.566, "latencies_ms": [1651.566], "images_per_second": 0.605, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming from a window, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.06, "peak": 122.62, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.81, "peak": 40.97, "min": 20.11}}, "power_watts_avg": 31.81, "energy_joules_est": 52.55, "sample_count": 16, "duration_seconds": 1.652}, "timestamp": "2026-01-19T13:40:39.344512"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1487.163, "latencies_ms": [1487.163], "images_per_second": 0.672, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is sitting on a motorcycle on a dirt road with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.52, "peak": 129.34, "min": 28.17}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.05, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 32.05, "energy_joules_est": 47.68, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T13:40:40.929769"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2770.799, "latencies_ms": [2770.799], "images_per_second": 0.361, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Motorcycle: 1\n2. Motorcycle: 1\n3. Motorcycle: 1\n4. Motorcycle: 1\n5. Motorcycle: 1\n6. Motorcycle: 1\n7. Motorcycle: 1\n8. Motorcycle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.19, "peak": 119.94, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.72, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 26.72, "energy_joules_est": 74.05, "sample_count": 27, "duration_seconds": 2.771}, "timestamp": "2026-01-19T13:40:43.745488"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2698.069, "latencies_ms": [2698.069], "images_per_second": 0.371, "prompt_tokens": 1117, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The motorcycle is positioned on the left side of the image, with the rider sitting on it. The motorcycle is in the foreground of the image, while the rider is in the background. The rider is positioned slightly to the right of the motorcycle, and the motorcycle is positioned slightly to the left of the rider.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.56, "peak": 108.22, "min": 31.31}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.7, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.7, "energy_joules_est": 72.05, "sample_count": 26, "duration_seconds": 2.698}, "timestamp": "2026-01-19T13:40:46.452991"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1674.031, "latencies_ms": [1674.031], "images_per_second": 0.597, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A person is standing on a dirt road with a motorcycle parked beside them. The road is surrounded by mountains and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.92, "peak": 112.57, "min": 28.39}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.95, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 30.34, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.34, "energy_joules_est": 50.8, "sample_count": 17, "duration_seconds": 1.674}, "timestamp": "2026-01-19T13:40:48.226858"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1495.042, "latencies_ms": [1495.042], "images_per_second": 0.669, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The sky is blue with white clouds, and the road is gray with rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.71, "peak": 126.34, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.26, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 31.26, "energy_joules_est": 46.75, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T13:40:49.795296"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1831.401, "latencies_ms": [1831.401], "images_per_second": 0.546, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image shows a kitchen with wooden cabinets, a white stove, and a white refrigerator, with a dining table in the center of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.83, "peak": 125.05, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.31, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 30.31, "energy_joules_est": 55.53, "sample_count": 18, "duration_seconds": 1.832}, "timestamp": "2026-01-19T13:40:51.673458"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2801.413, "latencies_ms": [2801.413], "images_per_second": 0.357, "prompt_tokens": 1113, "response_tokens_est": 66, "n_tiles": 1, "output_text": " 1. white refrigerator: 1\n2. white stove: 1\n3. white oven: 1\n4. white sink: 1\n5. white door: 2\n6. white countertop: 1\n7. wooden cabinets: 10\n8. wooden table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.73, "peak": 122.25, "min": 32.27}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 26.72, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 26.72, "energy_joules_est": 74.87, "sample_count": 27, "duration_seconds": 2.802}, "timestamp": "2026-01-19T13:40:54.482145"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2136.576, "latencies_ms": [2136.576], "images_per_second": 0.468, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The kitchen is located in the background of the image, with the dining table and chairs in the foreground. The stove is on the left side of the kitchen, while the refrigerator is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.35, "peak": 127.29, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 28.48, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 28.48, "energy_joules_est": 60.86, "sample_count": 21, "duration_seconds": 2.137}, "timestamp": "2026-01-19T13:40:56.672143"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2038.967, "latencies_ms": [2038.967], "images_per_second": 0.49, "prompt_tokens": 1111, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image captures a cozy kitchen with wooden cabinets, a white stove, and a white refrigerator. A dining table with a fruit bowl on it is placed in the center of the room.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.33, "peak": 125.17, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.86, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.86, "energy_joules_est": 58.86, "sample_count": 20, "duration_seconds": 2.039}, "timestamp": "2026-01-19T13:40:58.760539"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2005.757, "latencies_ms": [2005.757], "images_per_second": 0.499, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The kitchen has white walls and a white ceiling, with wooden cabinets and a white stove. The floor is tiled, and there is a bowl of oranges on the table.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.16, "peak": 118.71, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.88, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.88, "energy_joules_est": 57.93, "sample_count": 20, "duration_seconds": 2.006}, "timestamp": "2026-01-19T13:41:00.843573"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1482.253, "latencies_ms": [1482.253], "images_per_second": 0.675, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A female tennis player is preparing to serve the ball on a blue tennis court.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.46, "peak": 123.1, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.97, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.97, "energy_joules_est": 45.93, "sample_count": 15, "duration_seconds": 1.483}, "timestamp": "2026-01-19T13:41:02.413105"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3013.107, "latencies_ms": [3013.107], "images_per_second": 0.332, "prompt_tokens": 1113, "response_tokens_est": 71, "n_tiles": 1, "output_text": " 1. woman: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. woman's shoes: 2\n5. woman's shorts: 1\n6. woman's shirt: 1\n7. woman's headband: 1\n8. woman's wristband: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.72, "peak": 130.88, "min": 32.32}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.34, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.23, "peak": 40.56, "min": 18.53}}, "power_watts_avg": 26.23, "energy_joules_est": 79.05, "sample_count": 29, "duration_seconds": 3.014}, "timestamp": "2026-01-19T13:41:05.433755"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2405.135, "latencies_ms": [2405.135], "images_per_second": 0.416, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, with the tennis ball in the middle ground and the blue wall in the background. The player is facing the tennis ball, which is near the center of the image, and the wall is behind her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.5, "peak": 100.53, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.95, "min": 12.86}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 27.3, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 27.3, "energy_joules_est": 65.67, "sample_count": 24, "duration_seconds": 2.405}, "timestamp": "2026-01-19T13:41:07.940004"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1464.617, "latencies_ms": [1464.617], "images_per_second": 0.683, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A female tennis player is preparing to serve the ball on a blue court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.29, "peak": 122.32, "min": 27.99}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.65, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.65, "energy_joules_est": 44.9, "sample_count": 15, "duration_seconds": 1.465}, "timestamp": "2026-01-19T13:41:09.507116"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1544.784, "latencies_ms": [1544.784], "images_per_second": 0.647, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The tennis player is wearing a white shirt and black shorts, and the court is blue.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.27, "peak": 130.09, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.44, "min": 11.36}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 31.83, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 31.83, "energy_joules_est": 49.19, "sample_count": 15, "duration_seconds": 1.545}, "timestamp": "2026-01-19T13:41:11.080678"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1611.193, "latencies_ms": [1611.193], "images_per_second": 0.621, "prompt_tokens": 1100, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A red fire hydrant is on the sidewalk, with a yellow sign above it showing a pedestrian crossing symbol.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.85, "peak": 110.66, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.54, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 31.54, "energy_joules_est": 50.84, "sample_count": 16, "duration_seconds": 1.612}, "timestamp": "2026-01-19T13:41:12.754578"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2215.197, "latencies_ms": [2215.197], "images_per_second": 0.451, "prompt_tokens": 1114, "response_tokens_est": 44, "n_tiles": 1, "output_text": " 1. red fire hydrant\n2. yellow pedestrian sign\n3. white helmet\n4. person in white shirt\n5. person in blue shirt\n6. tree\n7. building\n8. sidewalk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.0, "peak": 125.9, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.4, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 28.4, "energy_joules_est": 62.92, "sample_count": 22, "duration_seconds": 2.215}, "timestamp": "2026-01-19T13:41:15.042249"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2406.522, "latencies_ms": [2406.522], "images_per_second": 0.416, "prompt_tokens": 1118, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The red fire hydrant is located on the left side of the image, in the foreground, and is near a sidewalk. In the background, there is a person standing on the sidewalk, and a yellow pedestrian crossing sign is visible above the hydrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.61, "peak": 135.3, "min": 28.19}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.07, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.07, "energy_joules_est": 65.17, "sample_count": 24, "duration_seconds": 2.407}, "timestamp": "2026-01-19T13:41:17.542206"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1527.519, "latencies_ms": [1527.519], "images_per_second": 0.655, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A red fire hydrant is on the sidewalk, and a boy is standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.79, "peak": 97.29, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.04, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.02, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 30.02, "energy_joules_est": 45.87, "sample_count": 15, "duration_seconds": 1.528}, "timestamp": "2026-01-19T13:41:19.118034"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1446.388, "latencies_ms": [1446.388], "images_per_second": 0.691, "prompt_tokens": 1110, "response_tokens_est": 15, "n_tiles": 1, "output_text": " The fire hydrant is red and white, and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.83, "peak": 128.35, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.5, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 32.5, "energy_joules_est": 47.02, "sample_count": 14, "duration_seconds": 1.447}, "timestamp": "2026-01-19T13:41:20.577537"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1660.662, "latencies_ms": [1660.662], "images_per_second": 0.602, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image depicts a small bathroom with a white toilet, a green trash can, and a white paper towel dispenser.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.8, "peak": 114.0, "min": 34.02}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.35, "peak": 40.97, "min": 22.07}}, "power_watts_avg": 32.35, "energy_joules_est": 53.74, "sample_count": 16, "duration_seconds": 1.661}, "timestamp": "2026-01-19T13:41:22.247956"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2269.378, "latencies_ms": [2269.378], "images_per_second": 0.441, "prompt_tokens": 1113, "response_tokens_est": 47, "n_tiles": 1, "output_text": " toilet: 1, toilet paper roll: 1, trash can: 1, person: 1, toilet paper holder: 1, toilet tank: 1, toilet seat: 1, toilet lid: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.0, "peak": 115.3, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.87, "peak": 40.18, "min": 19.7}}, "power_watts_avg": 28.87, "energy_joules_est": 65.53, "sample_count": 22, "duration_seconds": 2.27}, "timestamp": "2026-01-19T13:41:24.536766"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2282.768, "latencies_ms": [2282.768], "images_per_second": 0.438, "prompt_tokens": 1117, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The toilet is located in the center of the image, with the tank on the right and the seat on the left. The person's feet are visible in the foreground, while the trash can is positioned to the left of the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.44, "peak": 126.81, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.22, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 28.22, "energy_joules_est": 64.43, "sample_count": 22, "duration_seconds": 2.283}, "timestamp": "2026-01-19T13:41:26.828467"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1452.682, "latencies_ms": [1452.682], "images_per_second": 0.688, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A small bathroom with a toilet, a trash can, and a water filter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.52, "peak": 120.95, "min": 38.31}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 32.27, "peak": 39.77, "min": 17.73}}, "power_watts_avg": 32.27, "energy_joules_est": 46.89, "sample_count": 14, "duration_seconds": 1.453}, "timestamp": "2026-01-19T13:41:28.290004"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1562.317, "latencies_ms": [1562.317], "images_per_second": 0.64, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The bathroom is small and has a white toilet, a green toilet seat, and a brown carpet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.39, "peak": 123.75, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.15, "peak": 40.95, "min": 20.89}}, "power_watts_avg": 32.15, "energy_joules_est": 50.25, "sample_count": 16, "duration_seconds": 1.563}, "timestamp": "2026-01-19T13:41:29.959570"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1703.166, "latencies_ms": [1703.166], "images_per_second": 0.587, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A person wearing a red jacket and black pants is skiing down a snowy mountain with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.38, "peak": 116.73, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 16.53, "min": 12.22}, "VDD_GPU": {"avg": 30.5, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.5, "energy_joules_est": 51.97, "sample_count": 17, "duration_seconds": 1.704}, "timestamp": "2026-01-19T13:41:31.731770"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2632.646, "latencies_ms": [2632.646], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. skis: 2\n3. poles: 2\n4. backpack: 1\n5. helmet: 1\n6. jacket: 1\n7. pants: 1\n8. snow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.21, "peak": 122.28, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.85, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.85, "energy_joules_est": 70.69, "sample_count": 26, "duration_seconds": 2.633}, "timestamp": "2026-01-19T13:41:34.431124"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2083.682, "latencies_ms": [2083.682], "images_per_second": 0.48, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The skier is positioned on the left side of the image, with the mountain peak in the background. The skier is in the foreground, with the mountain ridge extending into the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.31, "peak": 113.99, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.08, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.08, "energy_joules_est": 58.53, "sample_count": 21, "duration_seconds": 2.084}, "timestamp": "2026-01-19T13:41:36.621545"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1536.964, "latencies_ms": [1536.964], "images_per_second": 0.651, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is skiing on a snowy mountain with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.92, "peak": 127.35, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.03, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.68, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.68, "energy_joules_est": 47.16, "sample_count": 15, "duration_seconds": 1.537}, "timestamp": "2026-01-19T13:41:38.193812"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2202.626, "latencies_ms": [2202.626], "images_per_second": 0.454, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a person wearing a red jacket and black pants, skiing down a snow-covered mountain under a clear blue sky. The snow is pristine white, and the person's skis are a vibrant green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.3, "peak": 117.27, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.67, "peak": 40.57, "min": 18.14}}, "power_watts_avg": 28.67, "energy_joules_est": 63.16, "sample_count": 22, "duration_seconds": 2.203}, "timestamp": "2026-01-19T13:41:40.485333"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1856.072, "latencies_ms": [1856.072], "images_per_second": 0.539, "prompt_tokens": 1100, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A man wearing a red and white jacket and a bib with the number 30 is skiing down a snowy mountain with trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.86, "peak": 107.29, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.26, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.26, "energy_joules_est": 54.33, "sample_count": 18, "duration_seconds": 1.857}, "timestamp": "2026-01-19T13:41:42.373690"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2717.739, "latencies_ms": [2717.739], "images_per_second": 0.368, "prompt_tokens": 1114, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. skier: 1\n2. bib: 1\n3. bib number: 30\n4. skis: 2\n5. ski poles: 2\n6. backpack: 1\n7. hat: 1\n8. gloves: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.62, "peak": 108.67, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.54, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 26.54, "energy_joules_est": 72.14, "sample_count": 27, "duration_seconds": 2.718}, "timestamp": "2026-01-19T13:41:45.183338"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2220.224, "latencies_ms": [2220.224], "images_per_second": 0.45, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The main object, a person skiing, is in the foreground, with the background consisting of trees and a mountain. The person is positioned to the left of the frame, while the mountain is located to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.83, "peak": 124.1, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.51, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.51, "energy_joules_est": 61.1, "sample_count": 22, "duration_seconds": 2.221}, "timestamp": "2026-01-19T13:41:47.484338"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1448.901, "latencies_ms": [1448.901], "images_per_second": 0.69, "prompt_tokens": 1112, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A man is skiing down a snowy mountain with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.81, "peak": 129.44, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.73, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 30.73, "energy_joules_est": 44.54, "sample_count": 14, "duration_seconds": 1.449}, "timestamp": "2026-01-19T13:41:48.947364"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2018.842, "latencies_ms": [2018.842], "images_per_second": 0.495, "prompt_tokens": 1110, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image features a man skiing in a snowy forest with a white background and a gray sky. The man is wearing a red and white jacket, black pants, and a gray hat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.08, "peak": 129.01, "min": 30.47}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.65, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 29.65, "energy_joules_est": 59.87, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T13:41:51.038622"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1536.756, "latencies_ms": [1536.756], "images_per_second": 0.651, "prompt_tokens": 1100, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A black and white photo of a computer desk with a monitor, keyboard, and mouse.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.45, "peak": 130.2, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.89, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 30.89, "energy_joules_est": 47.5, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T13:41:52.611257"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1821.019, "latencies_ms": [1821.019], "images_per_second": 0.549, "prompt_tokens": 1114, "response_tokens_est": 29, "n_tiles": 1, "output_text": " monitor: 1, keyboard: 1, mouse: 1, desk: 1, wall: 1, text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.13, "peak": 106.43, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.42, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 30.42, "energy_joules_est": 55.41, "sample_count": 18, "duration_seconds": 1.822}, "timestamp": "2026-01-19T13:41:54.491389"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2216.101, "latencies_ms": [2216.101], "images_per_second": 0.451, "prompt_tokens": 1118, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The computer monitor is on the left side of the desk, the keyboard is in the middle, and the mouse is on the right side. The desk is in the foreground, and the wall is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.27, "peak": 122.56, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.89, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.89, "energy_joules_est": 61.81, "sample_count": 22, "duration_seconds": 2.216}, "timestamp": "2026-01-19T13:41:56.797017"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1536.316, "latencies_ms": [1536.316], "images_per_second": 0.651, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A black and white photo of a computer desk with a monitor, keyboard, and mouse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.5, "peak": 133.06, "min": 28.18}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.26, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.26, "energy_joules_est": 46.5, "sample_count": 15, "duration_seconds": 1.537}, "timestamp": "2026-01-19T13:41:58.359425"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1801.551, "latencies_ms": [1801.551], "images_per_second": 0.555, "prompt_tokens": 1110, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image is in black and white, with a computer monitor displaying a webpage. The desk is made of wood and has a white surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.63, "peak": 122.6, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.25, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.25, "energy_joules_est": 54.51, "sample_count": 18, "duration_seconds": 1.802}, "timestamp": "2026-01-19T13:42:00.238655"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1583.973, "latencies_ms": [1583.973], "images_per_second": 0.631, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A woman wearing a striped shirt is sitting at a table on a train and eating a bagel.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.06, "peak": 124.25, "min": 27.48}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.41, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.41, "energy_joules_est": 48.19, "sample_count": 16, "duration_seconds": 1.585}, "timestamp": "2026-01-19T13:42:01.919070"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2598.046, "latencies_ms": [2598.046], "images_per_second": 0.385, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. bagel: 1\n3. coffee cup: 1\n4. box: 1\n5. food: 1\n6. person: 1\n7. window: 1\n8. table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.6, "peak": 104.45, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.19, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.19, "energy_joules_est": 70.65, "sample_count": 25, "duration_seconds": 2.599}, "timestamp": "2026-01-19T13:42:04.533041"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1937.706, "latencies_ms": [1937.706], "images_per_second": 0.516, "prompt_tokens": 1118, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The woman is in the foreground, holding a bagel, while the coffee cup is in the background. The bagel is closer to the camera than the coffee cup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.8, "peak": 124.62, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.17, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.17, "energy_joules_est": 56.54, "sample_count": 19, "duration_seconds": 1.938}, "timestamp": "2026-01-19T13:42:06.526660"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1537.157, "latencies_ms": [1537.157], "images_per_second": 0.651, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman is sitting at a table on a train, eating a bagel and smiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.0, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.65, "peak": 123.15, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.7, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.7, "energy_joules_est": 47.22, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T13:42:08.095822"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1941.653, "latencies_ms": [1941.653], "images_per_second": 0.515, "prompt_tokens": 1110, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image has a warm color tone, with natural light coming from the window. The woman is wearing a striped shirt, and the food is in a white paper bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.9, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.13, "peak": 126.24, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.84, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 29.84, "energy_joules_est": 57.95, "sample_count": 19, "duration_seconds": 1.942}, "timestamp": "2026-01-19T13:42:10.082434"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1541.283, "latencies_ms": [1541.283], "images_per_second": 0.649, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two zebras with black and white stripes are grazing on green grass in a grassy field.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.9, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.9, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.11, "peak": 128.1, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.44, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.44, "energy_joules_est": 48.48, "sample_count": 15, "duration_seconds": 1.542}, "timestamp": "2026-01-19T13:42:11.654763"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1145.14, "latencies_ms": [1145.14], "images_per_second": 0.873, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.86, "peak": 130.16, "min": 30.3}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 34.63, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 34.63, "energy_joules_est": 39.67, "sample_count": 11, "duration_seconds": 1.146}, "timestamp": "2026-01-19T13:42:12.809263"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2499.028, "latencies_ms": [2499.028], "images_per_second": 0.4, "prompt_tokens": 1117, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the zebra on the left slightly closer to the camera than the one on the right. The zebras are grazing on the grass, with the one on the left eating more than the one on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.19, "peak": 114.49, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 28.99, "peak": 42.13, "min": 17.34}}, "power_watts_avg": 28.99, "energy_joules_est": 72.46, "sample_count": 25, "duration_seconds": 2.499}, "timestamp": "2026-01-19T13:42:15.407190"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1456.989, "latencies_ms": [1456.989], "images_per_second": 0.686, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " Two zebras are grazing on green grass in a fenced enclosure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.04, "peak": 119.22, "min": 28.04}, "VIN_SYS_5V0": {"avg": 13.97, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.55, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.55, "energy_joules_est": 44.52, "sample_count": 15, "duration_seconds": 1.457}, "timestamp": "2026-01-19T13:42:16.967805"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2490.557, "latencies_ms": [2490.557], "images_per_second": 0.402, "prompt_tokens": 1109, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image features two zebras with their black and white stripes grazing on a lush green field. The lighting is natural and bright, suggesting it is daytime. The zebras appear to be in a peaceful environment, with no other animals or objects in the immediate vicinity.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.77, "peak": 123.13, "min": 32.0}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.36}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 27.79, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 27.79, "energy_joules_est": 69.23, "sample_count": 24, "duration_seconds": 2.491}, "timestamp": "2026-01-19T13:42:19.464653"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1565.825, "latencies_ms": [1565.825], "images_per_second": 0.639, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " Two young men are riding a green bicycle on a busy street with a storefront in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.44, "peak": 108.32, "min": 27.58}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.88, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.88, "energy_joules_est": 48.36, "sample_count": 16, "duration_seconds": 1.566}, "timestamp": "2026-01-19T13:42:21.135455"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.712, "latencies_ms": [2590.712], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bicycle: 2\n2. person: 2\n3. motorcycle: 2\n4. scooter: 1\n5. store: 1\n6. sign: 1\n7. plant: 1\n8. building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.54, "peak": 116.87, "min": 30.29}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.25, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.25, "energy_joules_est": 70.61, "sample_count": 25, "duration_seconds": 2.591}, "timestamp": "2026-01-19T13:42:23.737164"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2025.662, "latencies_ms": [2025.662], "images_per_second": 0.494, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The two boys are positioned in the foreground of the image, with the bicycle in front of them. The motorcycle is parked behind the bicycle, and the storefront is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.5, "peak": 124.99, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.99, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.99, "energy_joules_est": 58.73, "sample_count": 20, "duration_seconds": 2.026}, "timestamp": "2026-01-19T13:42:25.823226"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1467.949, "latencies_ms": [1467.949], "images_per_second": 0.681, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Two young men are riding a green bicycle down a busy street in Vietnam.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.73, "peak": 122.62, "min": 27.66}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.15, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 31.15, "energy_joules_est": 45.74, "sample_count": 15, "duration_seconds": 1.468}, "timestamp": "2026-01-19T13:42:27.389926"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1964.078, "latencies_ms": [1964.078], "images_per_second": 0.509, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken during the day with natural light illuminating the scene. The colors are vibrant, with the green of the bicycle and the blue of the sky standing out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.72, "peak": 119.94, "min": 30.73}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.44, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.78, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.78, "energy_joules_est": 58.5, "sample_count": 19, "duration_seconds": 1.965}, "timestamp": "2026-01-19T13:42:29.373533"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3623.92, "latencies_ms": [3623.92], "images_per_second": 0.276, "prompt_tokens": 1099, "response_tokens_est": 98, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a tennis match, with a player in white attire poised to strike the ball, while another player in a contrasting blue outfit is seen preparing to return the shot. The court, marked with crisp white lines, is set against a backdrop of a large, attentive audience, all engrossed in the unfolding action. The scene is further accentuated by the presence of a cameraman and a ball boy, who are strategically positioned to capture the essence of the match.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.04, "peak": 130.11, "min": 27.84}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 24.77, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 24.77, "energy_joules_est": 89.78, "sample_count": 36, "duration_seconds": 3.624}, "timestamp": "2026-01-19T13:42:33.100268"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2021.085, "latencies_ms": [2021.085], "images_per_second": 0.495, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " 1. tennis court\n2. tennis players\n3. chairs\n4. bags\n5. spectators\n6. ball\n7. net\n8. umpire", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.32, "peak": 125.97, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.43, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 28.43, "energy_joules_est": 57.47, "sample_count": 20, "duration_seconds": 2.021}, "timestamp": "2026-01-19T13:42:35.177507"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2168.816, "latencies_ms": [2168.816], "images_per_second": 0.461, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground, with the ball in the middle ground, and the audience in the background. The player is closer to the camera than the ball, and the audience is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.41, "peak": 119.41, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.5, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.5, "energy_joules_est": 61.82, "sample_count": 21, "duration_seconds": 2.169}, "timestamp": "2026-01-19T13:42:37.366303"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2308.862, "latencies_ms": [2308.862], "images_per_second": 0.433, "prompt_tokens": 1111, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image captures a moment from a professional tennis match, with two players in white attire actively engaged in the game on a grass court. The court is surrounded by a crowd of spectators, all seated in the stands, watching the match intently.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.26, "peak": 123.53, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.82, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.82, "energy_joules_est": 64.24, "sample_count": 23, "duration_seconds": 2.309}, "timestamp": "2026-01-19T13:42:39.760192"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2018.211, "latencies_ms": [2018.211], "images_per_second": 0.495, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image is taken during a daytime tennis match, with the players dressed in white and the court's grass green. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.34, "peak": 126.75, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.53, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.53, "energy_joules_est": 57.59, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T13:42:41.848489"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2177.164, "latencies_ms": [2177.164], "images_per_second": 0.459, "prompt_tokens": 1099, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a brown sofa, a television on a wooden stand, a coffee table with a few items on it, and a variety of potted plants placed around the room.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.78, "peak": 126.7, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.16, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.16, "energy_joules_est": 61.33, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T13:42:44.051250"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2303.934, "latencies_ms": [2303.934], "images_per_second": 0.434, "prompt_tokens": 1113, "response_tokens_est": 47, "n_tiles": 1, "output_text": " television: 1, sofa: 1, television stand: 1, television stand: 1, sofa cushion: 1, television stand: 1, television stand: 1, television stand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.51, "peak": 113.81, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.68, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.68, "energy_joules_est": 63.8, "sample_count": 23, "duration_seconds": 2.305}, "timestamp": "2026-01-19T13:42:46.460146"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2624.436, "latencies_ms": [2624.436], "images_per_second": 0.381, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The sofa is located in the foreground of the image, with the television set and other decorative items placed on the left side of the sofa. The plants are positioned in the background, with the largest one on the wall above the sofa and the smaller ones on the windowsill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.68, "peak": 100.29, "min": 30.15}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.35, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.35, "energy_joules_est": 69.17, "sample_count": 26, "duration_seconds": 2.625}, "timestamp": "2026-01-19T13:42:49.170879"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1513.353, "latencies_ms": [1513.353], "images_per_second": 0.661, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A living room with a brown couch, a TV, and a coffee table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.2, "peak": 128.64, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.65, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.65, "energy_joules_est": 46.4, "sample_count": 15, "duration_seconds": 1.514}, "timestamp": "2026-01-19T13:42:50.743653"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2013.469, "latencies_ms": [2013.469], "images_per_second": 0.497, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the windows, and the walls are painted white. The furniture is made of wood and the floor is covered with a carpet.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.74, "peak": 123.54, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.43, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 29.43, "energy_joules_est": 59.27, "sample_count": 20, "duration_seconds": 2.014}, "timestamp": "2026-01-19T13:42:52.832288"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1729.669, "latencies_ms": [1729.669], "images_per_second": 0.578, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A woman in a red dress and white visor is holding a tennis racket and smiling on a clay tennis court.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.96, "peak": 114.57, "min": 27.51}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.99, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.99, "energy_joules_est": 51.89, "sample_count": 17, "duration_seconds": 1.73}, "timestamp": "2026-01-19T13:42:54.605032"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2758.726, "latencies_ms": [2758.726], "images_per_second": 0.362, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. woman: 1\n2. tennis racket: 1\n3. visor: 1\n4. tennis ball: 0\n5. tennis court: 1\n6. green fence: 1\n7. white line: 1\n8. white cloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.77, "peak": 112.61, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.66, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.66, "energy_joules_est": 73.56, "sample_count": 27, "duration_seconds": 2.759}, "timestamp": "2026-01-19T13:42:57.420619"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2218.428, "latencies_ms": [2218.428], "images_per_second": 0.451, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The tennis player is positioned in the foreground of the image, holding a tennis racket and smiling. The tennis court is in the background, with the green fence and white lines marking the boundaries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.52, "peak": 123.2, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.26, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.26, "energy_joules_est": 60.49, "sample_count": 22, "duration_seconds": 2.219}, "timestamp": "2026-01-19T13:42:59.708881"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1578.035, "latencies_ms": [1578.035], "images_per_second": 0.634, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A female tennis player is on a clay court, holding a tennis racket and smiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.29, "peak": 128.78, "min": 28.16}, "VIN_SYS_5V0": {"avg": 14.01, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 14.84, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.04, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.04, "energy_joules_est": 47.42, "sample_count": 16, "duration_seconds": 1.579}, "timestamp": "2026-01-19T13:43:01.383008"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1794.101, "latencies_ms": [1794.101], "images_per_second": 0.557, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The tennis player is wearing a red dress and a white visor. The court is made of red clay and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.53, "peak": 109.92, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.06, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.94, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 29.94, "energy_joules_est": 53.73, "sample_count": 18, "duration_seconds": 1.795}, "timestamp": "2026-01-19T13:43:03.254934"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1960.215, "latencies_ms": [1960.215], "images_per_second": 0.51, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image captures a bustling city street lined with a variety of shops and restaurants, including a prominent \"Omni-Restaurant\" sign, under a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.24, "peak": 127.23, "min": 30.38}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.16, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.16, "energy_joules_est": 57.17, "sample_count": 19, "duration_seconds": 1.961}, "timestamp": "2026-01-19T13:43:05.265067"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2611.561, "latencies_ms": [2611.561], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. car: 4\n2. street: 1\n3. building: 10\n4. sign: 10\n5. person: 2\n6. table: 1\n7. streetlight: 1\n8. airplane: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.97, "peak": 121.67, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.81, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.81, "energy_joules_est": 70.03, "sample_count": 26, "duration_seconds": 2.612}, "timestamp": "2026-01-19T13:43:07.976554"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2603.929, "latencies_ms": [2603.929], "images_per_second": 0.384, "prompt_tokens": 1117, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The main objects in the image are the cars on the street, the buildings, and the people sitting at the outdoor tables. The cars are located in the foreground, while the buildings are in the background. The people are sitting at the outdoor tables, which are located near the buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.87, "peak": 128.3, "min": 28.02}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.26, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.26, "energy_joules_est": 68.39, "sample_count": 26, "duration_seconds": 2.604}, "timestamp": "2026-01-19T13:43:10.684041"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1978.224, "latencies_ms": [1978.224], "images_per_second": 0.506, "prompt_tokens": 1111, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image captures a bustling city street lined with a variety of shops and restaurants. The street is filled with cars and people going about their day, creating a lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.74, "peak": 110.49, "min": 30.27}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.8, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 28.8, "energy_joules_est": 56.99, "sample_count": 19, "duration_seconds": 1.979}, "timestamp": "2026-01-19T13:43:12.672233"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2918.141, "latencies_ms": [2918.141], "images_per_second": 0.343, "prompt_tokens": 1109, "response_tokens_est": 71, "n_tiles": 1, "output_text": " The image depicts a bustling city street with a variety of colors, including the red brick buildings, the green awnings of the restaurants, and the white cars parked along the street. The lighting is natural, with the sun casting shadows on the buildings and the street. The weather appears to be overcast, with a gray sky and no visible sunshine.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.41, "peak": 124.81, "min": 28.1}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.13, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.13, "energy_joules_est": 76.26, "sample_count": 29, "duration_seconds": 2.918}, "timestamp": "2026-01-19T13:43:15.690019"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1764.261, "latencies_ms": [1764.261], "images_per_second": 0.567, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A female tennis player in a red outfit is playing a tennis match on a blue court, with a tennis ball in the air.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.16, "peak": 128.61, "min": 34.8}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.59, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 29.59, "energy_joules_est": 52.23, "sample_count": 17, "duration_seconds": 1.765}, "timestamp": "2026-01-19T13:43:17.465582"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2759.644, "latencies_ms": [2759.644], "images_per_second": 0.362, "prompt_tokens": 1113, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. tennis racket: 1\n2. tennis ball: 1\n3. tennis player: 1\n4. tennis court: 1\n5. white line: 1\n6. blue surface: 1\n7. green surface: 1\n8. white shoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.03, "peak": 124.22, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 26.89, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 26.89, "energy_joules_est": 74.21, "sample_count": 27, "duration_seconds": 2.76}, "timestamp": "2026-01-19T13:43:20.276109"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2075.404, "latencies_ms": [2075.404], "images_per_second": 0.482, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis ball located in the center. The player is in the foreground, while the tennis court extends into the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.21, "peak": 108.12, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 28.22, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.22, "energy_joules_est": 58.58, "sample_count": 21, "duration_seconds": 2.076}, "timestamp": "2026-01-19T13:43:22.457423"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1772.306, "latencies_ms": [1772.306], "images_per_second": 0.564, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A female tennis player is playing on a green court with a blue boundary line. She is wearing a red outfit and white shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.34, "peak": 110.59, "min": 31.75}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.87, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.87, "energy_joules_est": 52.95, "sample_count": 17, "duration_seconds": 1.773}, "timestamp": "2026-01-19T13:43:24.233538"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1614.069, "latencies_ms": [1614.069], "images_per_second": 0.62, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The tennis player is wearing a red outfit and white shoes, and the court is blue with a green surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.78, "peak": 129.19, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.61, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 31.61, "energy_joules_est": 51.03, "sample_count": 16, "duration_seconds": 1.615}, "timestamp": "2026-01-19T13:43:25.902552"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1817.483, "latencies_ms": [1817.483], "images_per_second": 0.55, "prompt_tokens": 1432, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A blue and orange train is traveling on a track through a wooded area.", "error": null, "sys_before": {"cpu_percent": 6.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.67, "peak": 115.44, "min": 30.98}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.48, "peak": 41.76, "min": 18.14}}, "power_watts_avg": 32.48, "energy_joules_est": 59.06, "sample_count": 18, "duration_seconds": 1.818}, "timestamp": "2026-01-19T13:43:27.796170"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3017.753, "latencies_ms": [3017.753], "images_per_second": 0.331, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. train: 1\n2. tracks: 1\n3. bushes: 1\n4. trees: 1\n5. grass: 1\n6. train car: 1\n7. windows: 1\n8. doors: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.45, "peak": 126.87, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.26, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.31, "peak": 42.13, "min": 17.34}}, "power_watts_avg": 28.31, "energy_joules_est": 85.45, "sample_count": 30, "duration_seconds": 3.018}, "timestamp": "2026-01-19T13:43:30.915630"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2424.394, "latencies_ms": [2424.394], "images_per_second": 0.412, "prompt_tokens": 1450, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The train is positioned on the left side of the image, moving towards the right. The train is in the foreground, with the tracks curving to the left in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.53, "peak": 116.91, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 16.36, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 17.32, "min": 12.21}, "VDD_GPU": {"avg": 29.68, "peak": 42.13, "min": 14.98}}, "power_watts_avg": 29.68, "energy_joules_est": 71.96, "sample_count": 24, "duration_seconds": 2.425}, "timestamp": "2026-01-19T13:43:33.412101"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1800.999, "latencies_ms": [1800.999], "images_per_second": 0.555, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A blue and orange train is traveling down a track through a wooded area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.82, "peak": 120.81, "min": 27.28}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.15, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.48, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 32.48, "energy_joules_est": 58.51, "sample_count": 18, "duration_seconds": 1.801}, "timestamp": "2026-01-19T13:43:35.300490"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1738.846, "latencies_ms": [1738.846], "images_per_second": 0.575, "prompt_tokens": 1442, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The train is blue and orange, and the sky is clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.49, "peak": 134.9, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.15, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 34.06, "peak": 42.13, "min": 19.32}}, "power_watts_avg": 34.06, "energy_joules_est": 59.24, "sample_count": 17, "duration_seconds": 1.739}, "timestamp": "2026-01-19T13:43:37.076305"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1619.826, "latencies_ms": [1619.826], "images_per_second": 0.617, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two tabby cats are sleeping on a pink blanket, with a remote control on the left cat's back.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.27, "peak": 129.8, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.23, "peak": 40.97, "min": 21.28}}, "power_watts_avg": 32.23, "energy_joules_est": 52.23, "sample_count": 16, "duration_seconds": 1.621}, "timestamp": "2026-01-19T13:43:38.754110"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.478, "latencies_ms": [2610.478], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. cat: 2\n2. remote control: 2\n3. blanket: 1\n4. couch: 1\n5. tail: 2\n6. paw: 2\n7. head: 1\n8. body: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.69, "peak": 124.1, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.05, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 27.05, "energy_joules_est": 70.62, "sample_count": 26, "duration_seconds": 2.611}, "timestamp": "2026-01-19T13:43:41.457287"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2780.397, "latencies_ms": [2780.397], "images_per_second": 0.36, "prompt_tokens": 1117, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The two cats are lying on a pink blanket, with the larger cat on the left and the smaller cat on the right. The larger cat is positioned closer to the camera than the smaller cat. The remote controls are placed on the blanket, with one on the left side and the other on the right side of the cats.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.92, "peak": 128.82, "min": 30.31}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.27, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 26.27, "energy_joules_est": 73.05, "sample_count": 27, "duration_seconds": 2.781}, "timestamp": "2026-01-19T13:43:44.266205"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1404.53, "latencies_ms": [1404.53], "images_per_second": 0.712, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " Two tabby cats are sleeping on a pink blanket on a couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.38, "peak": 121.69, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.94, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.94, "energy_joules_est": 44.87, "sample_count": 14, "duration_seconds": 1.405}, "timestamp": "2026-01-19T13:43:45.728271"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1938.337, "latencies_ms": [1938.337], "images_per_second": 0.516, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image features two tabby cats sleeping on a pink blanket, with one cat having a green collar. The lighting is natural, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.46, "peak": 125.5, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.42, "peak": 40.57, "min": 20.49}}, "power_watts_avg": 30.42, "energy_joules_est": 58.97, "sample_count": 19, "duration_seconds": 1.939}, "timestamp": "2026-01-19T13:43:47.695273"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2104.126, "latencies_ms": [2104.126], "images_per_second": 0.475, "prompt_tokens": 1099, "response_tokens_est": 41, "n_tiles": 1, "output_text": " In the image, a person clad in a black wetsuit is skillfully riding a wave in a river, while another individual stands on the riverbank, holding a blue surfboard, observing the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.6, "peak": 114.45, "min": 28.19}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.7, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 28.7, "energy_joules_est": 60.4, "sample_count": 21, "duration_seconds": 2.104}, "timestamp": "2026-01-19T13:43:49.886618"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2564.71, "latencies_ms": [2564.71], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. river: 1\n4. tree: 1\n5. bridge: 1\n6. bench: 1\n7. sign: 1\n8. water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.36, "peak": 115.42, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.97, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.97, "energy_joules_est": 69.18, "sample_count": 25, "duration_seconds": 2.565}, "timestamp": "2026-01-19T13:43:52.481290"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2621.409, "latencies_ms": [2621.409], "images_per_second": 0.381, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding the waves towards the right side of the image. The river is located in the middle ground, with the surfer's location being closer to the viewer. The background features a bridge and trees, providing a sense of depth and scale to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.66, "peak": 116.16, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.75, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.75, "energy_joules_est": 70.13, "sample_count": 26, "duration_seconds": 2.622}, "timestamp": "2026-01-19T13:43:55.186837"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1780.938, "latencies_ms": [1780.938], "images_per_second": 0.562, "prompt_tokens": 1111, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man in a wetsuit is surfing on a river with a blue surfboard. The river is surrounded by trees and a bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.68, "peak": 123.5, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.46, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.46, "energy_joules_est": 52.48, "sample_count": 18, "duration_seconds": 1.781}, "timestamp": "2026-01-19T13:43:57.052154"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2266.413, "latencies_ms": [2266.413], "images_per_second": 0.441, "prompt_tokens": 1109, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a surfer in a black wetsuit riding a wave in a river, with the water appearing white and foamy. The lighting is natural, suggesting daytime, and the sky is not visible in the frame.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.1, "peak": 128.41, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.22, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.22, "energy_joules_est": 63.97, "sample_count": 22, "duration_seconds": 2.267}, "timestamp": "2026-01-19T13:43:59.342917"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1913.945, "latencies_ms": [1913.945], "images_per_second": 0.522, "prompt_tokens": 1432, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman and a child are flying a colorful kite in a park on a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.57, "peak": 122.14, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.26, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 31.72, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 31.72, "energy_joules_est": 60.72, "sample_count": 19, "duration_seconds": 1.914}, "timestamp": "2026-01-19T13:44:01.314828"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2994.823, "latencies_ms": [2994.823], "images_per_second": 0.334, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. child: 1\n3. kite: 1\n4. grass: 1\n5. trees: 1\n6. sky: 1\n7. person's hand: 1\n8. backpack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.02, "peak": 121.36, "min": 30.22}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.64, "peak": 42.13, "min": 18.53}}, "power_watts_avg": 28.64, "energy_joules_est": 85.79, "sample_count": 29, "duration_seconds": 2.995}, "timestamp": "2026-01-19T13:44:04.351399"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2710.62, "latencies_ms": [2710.62], "images_per_second": 0.369, "prompt_tokens": 1450, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the person is in the background, standing on the grass. The person is holding the kite string, which is in the foreground, and the kite is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.96, "peak": 131.88, "min": 27.51}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 16.36, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 28.89, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 28.89, "energy_joules_est": 78.32, "sample_count": 27, "duration_seconds": 2.711}, "timestamp": "2026-01-19T13:44:07.168045"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1780.369, "latencies_ms": [1780.369], "images_per_second": 0.562, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A woman and a child are flying a kite in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.26, "peak": 118.36, "min": 27.69}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 16.26, "min": 11.26}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 17.32, "min": 12.22}, "VDD_GPU": {"avg": 31.98, "peak": 41.76, "min": 14.98}}, "power_watts_avg": 31.98, "energy_joules_est": 56.95, "sample_count": 18, "duration_seconds": 1.781}, "timestamp": "2026-01-19T13:44:09.045058"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2204.91, "latencies_ms": [2204.91], "images_per_second": 0.454, "prompt_tokens": 1442, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image features a woman and a child flying a kite in a park on a sunny day. The kite is colorful and has a long tail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.17, "peak": 127.88, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 16.26, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.71, "min": 12.21}, "VDD_GPU": {"avg": 31.27, "peak": 42.15, "min": 19.32}}, "power_watts_avg": 31.27, "energy_joules_est": 68.96, "sample_count": 22, "duration_seconds": 2.205}, "timestamp": "2026-01-19T13:44:11.345941"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2028.325, "latencies_ms": [2028.325], "images_per_second": 0.493, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " A young boy is playing tennis on a court, wearing a red cap and a white shirt, and is about to hit a yellow tennis ball with a yellow and black tennis racket.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.51, "peak": 135.07, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.9, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.9, "energy_joules_est": 58.63, "sample_count": 20, "duration_seconds": 2.029}, "timestamp": "2026-01-19T13:44:13.433201"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2668.409, "latencies_ms": [2668.409], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. person: 1\n2. tennis racket: 1\n3. ball: 1\n4. tennis court: 1\n5. net: 1\n6. green tarp: 1\n7. sign: 1\n8. text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.18, "peak": 117.42, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.82, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.82, "energy_joules_est": 71.57, "sample_count": 26, "duration_seconds": 2.669}, "timestamp": "2026-01-19T13:44:16.134009"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2146.955, "latencies_ms": [2146.955], "images_per_second": 0.466, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The main objects are positioned in the foreground of the image, with the tennis player in the center of the frame. The tennis ball is located near the player, and the green wall is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.91, "peak": 125.45, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.35, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.35, "energy_joules_est": 60.88, "sample_count": 21, "duration_seconds": 2.147}, "timestamp": "2026-01-19T13:44:18.321789"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1616.173, "latencies_ms": [1616.173], "images_per_second": 0.619, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A young boy is playing tennis on a court. He is wearing a red hat and a white shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.55, "peak": 109.45, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.83, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.83, "energy_joules_est": 49.84, "sample_count": 16, "duration_seconds": 1.617}, "timestamp": "2026-01-19T13:44:19.989050"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2162.855, "latencies_ms": [2162.855], "images_per_second": 0.462, "prompt_tokens": 1109, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The image features a young boy playing tennis on a green court with a blue net. The boy is wearing a red cap and a white shirt. The lighting is bright and sunny, and the colors are vibrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.49, "peak": 127.92, "min": 30.09}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.12, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 29.12, "energy_joules_est": 62.99, "sample_count": 21, "duration_seconds": 2.163}, "timestamp": "2026-01-19T13:44:22.172414"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1965.432, "latencies_ms": [1965.432], "images_per_second": 0.509, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image depicts a room with a bed, a chair, and a desk, all of which are covered in various items such as clothes, boxes, and a trash can.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.84, "peak": 119.05, "min": 37.78}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.57, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.57, "energy_joules_est": 58.13, "sample_count": 19, "duration_seconds": 1.966}, "timestamp": "2026-01-19T13:44:24.150036"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2532.657, "latencies_ms": [2532.657], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 1\n2. chair: 1\n3. desk: 1\n4. box: 1\n5. blanket: 1\n6. pillow: 1\n7. door: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.85, "peak": 116.93, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 27.25, "peak": 39.77, "min": 17.74}}, "power_watts_avg": 27.25, "energy_joules_est": 69.02, "sample_count": 25, "duration_seconds": 2.533}, "timestamp": "2026-01-19T13:44:26.762309"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2550.064, "latencies_ms": [2550.064], "images_per_second": 0.392, "prompt_tokens": 1117, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The bed is located on the left side of the room, with the chair and desk situated on the right side. The desk is positioned closer to the camera than the chair, which is near the door. The bed is in the foreground, while the door is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.7, "peak": 131.12, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.82, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.82, "energy_joules_est": 68.41, "sample_count": 25, "duration_seconds": 2.551}, "timestamp": "2026-01-19T13:44:29.364229"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1488.323, "latencies_ms": [1488.323], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The room is a bedroom with a bed, a chair, and a desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.09, "peak": 127.15, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.75, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.99, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.99, "energy_joules_est": 46.14, "sample_count": 15, "duration_seconds": 1.489}, "timestamp": "2026-01-19T13:44:30.927409"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1600.164, "latencies_ms": [1600.164], "images_per_second": 0.625, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is lit by natural light coming from a window, and the walls are made of brick.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.77, "peak": 121.43, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.75, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.49, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 31.49, "energy_joules_est": 50.4, "sample_count": 16, "duration_seconds": 1.601}, "timestamp": "2026-01-19T13:44:32.594485"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1621.957, "latencies_ms": [1621.957], "images_per_second": 0.617, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A woman in a red and green outfit is riding a brown horse over a jump obstacle in a field.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.99, "peak": 113.15, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.22, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 31.22, "energy_joules_est": 50.66, "sample_count": 16, "duration_seconds": 1.623}, "timestamp": "2026-01-19T13:44:34.271478"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2149.291, "latencies_ms": [2149.291], "images_per_second": 0.465, "prompt_tokens": 1113, "response_tokens_est": 41, "n_tiles": 1, "output_text": " horse: 1, rider: 1, saddle: 1, bridle: 1, leg: 2, leg protection: 2, helmet: 1, number: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.61, "peak": 126.69, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.0, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 29.0, "energy_joules_est": 62.34, "sample_count": 21, "duration_seconds": 2.15}, "timestamp": "2026-01-19T13:44:36.458423"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2325.383, "latencies_ms": [2325.383], "images_per_second": 0.43, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The horse is in the foreground, jumping over the obstacle, while the rider is in the background, wearing a helmet and a red and green jacket. The horse's legs are near the obstacle, while the rider's legs are further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.85, "peak": 110.9, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.8, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 27.8, "energy_joules_est": 64.66, "sample_count": 23, "duration_seconds": 2.326}, "timestamp": "2026-01-19T13:44:38.847112"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1627.733, "latencies_ms": [1627.733], "images_per_second": 0.614, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A woman in a red and green outfit is riding a brown horse over a jump obstacle in a field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.0, "peak": 122.46, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.44, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.44, "energy_joules_est": 49.57, "sample_count": 16, "duration_seconds": 1.629}, "timestamp": "2026-01-19T13:44:40.518429"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2495.34, "latencies_ms": [2495.34], "images_per_second": 0.401, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a vibrant scene with a brown horse and rider jumping over a wooden fence, set against a backdrop of lush green trees and a clear blue sky. The horse's coat gleams under the sunlight, while the rider's attire stands out with its bright colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.61, "peak": 122.59, "min": 30.36}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.37, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.37, "energy_joules_est": 68.32, "sample_count": 25, "duration_seconds": 2.496}, "timestamp": "2026-01-19T13:44:43.111011"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1747.843, "latencies_ms": [1747.843], "images_per_second": 0.572, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " Two men are sitting under a large umbrella on a sidewalk, with a car parked behind them and a bicycle leaning against a wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.84, "peak": 125.87, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.83, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.83, "energy_joules_est": 52.16, "sample_count": 17, "duration_seconds": 1.749}, "timestamp": "2026-01-19T13:44:44.885994"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2523.4, "latencies_ms": [2523.4], "images_per_second": 0.396, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. umbrella: 1\n2. man: 2\n3. chair: 1\n4. table: 1\n5. bicycle: 1\n6. car: 1\n7. sign: 1\n8. street: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.19, "peak": 119.88, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.53, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.53, "energy_joules_est": 69.48, "sample_count": 25, "duration_seconds": 2.524}, "timestamp": "2026-01-19T13:44:47.480214"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2123.868, "latencies_ms": [2123.868], "images_per_second": 0.471, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The man sitting on the chair is to the left of the man standing, and the man standing is in the foreground. The man sitting on the chair is closer to the camera than the man standing.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.52, "peak": 112.11, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.24, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.24, "energy_joules_est": 59.99, "sample_count": 21, "duration_seconds": 2.124}, "timestamp": "2026-01-19T13:44:49.666406"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1568.655, "latencies_ms": [1568.655], "images_per_second": 0.637, "prompt_tokens": 1111, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two men are sitting under a large umbrella on a sidewalk, with a car parked behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.51, "peak": 134.73, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.53, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.53, "energy_joules_est": 47.9, "sample_count": 16, "duration_seconds": 1.569}, "timestamp": "2026-01-19T13:44:51.340421"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1895.011, "latencies_ms": [1895.011], "images_per_second": 0.528, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image is in black and white, with the exception of the man's white shirt. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.73, "peak": 127.55, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.44, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 29.44, "energy_joules_est": 55.8, "sample_count": 19, "duration_seconds": 1.895}, "timestamp": "2026-01-19T13:44:53.321617"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1894.15, "latencies_ms": [1894.15], "images_per_second": 0.528, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image depicts a kitchen with white cabinets, a white stove, and a white refrigerator, all of which are situated in a room with a tiled floor.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.17, "peak": 127.5, "min": 27.88}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.03, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 29.03, "energy_joules_est": 55.01, "sample_count": 19, "duration_seconds": 1.895}, "timestamp": "2026-01-19T13:44:55.307473"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2151.428, "latencies_ms": [2151.428], "images_per_second": 0.465, "prompt_tokens": 1114, "response_tokens_est": 41, "n_tiles": 1, "output_text": " 1. white cabinets\n2. white refrigerator\n3. white stove\n4. white dishwasher\n5. white oven\n6. white sink\n7. white microwave\n8. white toaster", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.45, "peak": 125.16, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.24, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.24, "energy_joules_est": 60.77, "sample_count": 21, "duration_seconds": 2.152}, "timestamp": "2026-01-19T13:44:57.500518"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2263.742, "latencies_ms": [2263.742], "images_per_second": 0.442, "prompt_tokens": 1118, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The stove is located to the left of the sink, and the refrigerator is positioned to the right of the sink. The sink is situated in the middle of the kitchen, with the stove and refrigerator flanking it on either side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.05, "peak": 129.87, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.95, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.95, "energy_joules_est": 63.28, "sample_count": 22, "duration_seconds": 2.264}, "timestamp": "2026-01-19T13:44:59.797183"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2068.997, "latencies_ms": [2068.997], "images_per_second": 0.483, "prompt_tokens": 1112, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a kitchen with white cabinets, a white stove, and a white refrigerator. There is a ceiling fan hanging from the ceiling, and a clock is hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.13, "peak": 108.27, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.19, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.19, "energy_joules_est": 58.33, "sample_count": 20, "duration_seconds": 2.069}, "timestamp": "2026-01-19T13:45:01.883024"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1597.722, "latencies_ms": [1597.722], "images_per_second": 0.626, "prompt_tokens": 1110, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The kitchen is well-lit with natural light coming through the windows, and the cabinets are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.89, "peak": 114.11, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.73, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.73, "energy_joules_est": 49.12, "sample_count": 16, "duration_seconds": 1.598}, "timestamp": "2026-01-19T13:45:03.556002"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2141.159, "latencies_ms": [2141.159], "images_per_second": 0.467, "prompt_tokens": 1100, "response_tokens_est": 41, "n_tiles": 1, "output_text": " A young child is sleeping in a bed with a blue and white checkered blanket, wearing a white tank top and blue pajama pants with a pattern of white daisies and other shapes.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.12, "peak": 120.1, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.55, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 28.55, "energy_joules_est": 61.14, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T13:45:05.752711"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1907.466, "latencies_ms": [1907.466], "images_per_second": 0.524, "prompt_tokens": 1114, "response_tokens_est": 32, "n_tiles": 1, "output_text": " 1. child\n2. bed\n3. pillow\n4. blanket\n5. wall\n6. television\n7. cable\n8. light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.64, "peak": 120.45, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.03, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.03, "energy_joules_est": 55.38, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T13:45:07.735835"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2181.24, "latencies_ms": [2181.24], "images_per_second": 0.458, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The child is lying on the bed, which is positioned in the foreground of the image. The bed is situated in the middle of the room, with the wall and a television mounted on it in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.49, "peak": 122.52, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.24, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.24, "energy_joules_est": 61.61, "sample_count": 21, "duration_seconds": 2.181}, "timestamp": "2026-01-19T13:45:09.927234"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1549.205, "latencies_ms": [1549.205], "images_per_second": 0.645, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A young child is sleeping in a small room with a blue and white checkered blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.3, "peak": 123.07, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 30.89, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 30.89, "energy_joules_est": 47.87, "sample_count": 15, "duration_seconds": 1.55}, "timestamp": "2026-01-19T13:45:11.499327"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2308.916, "latencies_ms": [2308.916], "images_per_second": 0.433, "prompt_tokens": 1110, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a child sleeping on a bed. The child is wearing a white tank top and blue patterned pajama pants. The bed is covered with a blue and white checkered blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.82, "peak": 119.95, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.11, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 28.11, "energy_joules_est": 64.92, "sample_count": 23, "duration_seconds": 2.309}, "timestamp": "2026-01-19T13:45:13.896674"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2212.386, "latencies_ms": [2212.386], "images_per_second": 0.452, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image shows a green highway sign with the words \"NO TRUCKS\" at the top, and \"EAT\" and \"Queens\" written on it, with \"Bronx\" below it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.54, "peak": 115.6, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.63, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.63, "energy_joules_est": 61.15, "sample_count": 22, "duration_seconds": 2.213}, "timestamp": "2026-01-19T13:45:16.196480"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2594.336, "latencies_ms": [2594.336], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. sign: 1\n2. graffiti: 1\n3. bridge: 1\n4. road: 1\n5. highway: 1\n6. highway sign: 1\n7. metal: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.62, "peak": 116.11, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.79, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.79, "energy_joules_est": 69.51, "sample_count": 25, "duration_seconds": 2.595}, "timestamp": "2026-01-19T13:45:18.805251"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2109.284, "latencies_ms": [2109.284], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The green highway sign is positioned in the foreground, with the metal bridge structure in the background. The sign is to the left of the bridge, and the bridge is to the right of the sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.84, "peak": 125.38, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.52, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.52, "energy_joules_est": 60.17, "sample_count": 21, "duration_seconds": 2.11}, "timestamp": "2026-01-19T13:45:20.992305"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1750.902, "latencies_ms": [1750.902], "images_per_second": 0.571, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A green highway sign with the words \"No Trucks\" and \"Eat\" on it is hanging from a metal structure.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.34, "peak": 112.0, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.97, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.97, "energy_joules_est": 52.49, "sample_count": 17, "duration_seconds": 1.751}, "timestamp": "2026-01-19T13:45:22.771069"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1802.924, "latencies_ms": [1802.924], "images_per_second": 0.555, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The sign is green with white lettering and has a black border. The sky is overcast and the sign is lit by natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.25, "peak": 122.21, "min": 27.65}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.27, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 30.27, "energy_joules_est": 54.6, "sample_count": 18, "duration_seconds": 1.804}, "timestamp": "2026-01-19T13:45:24.643880"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1676.592, "latencies_ms": [1676.592], "images_per_second": 0.596, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A red Chevrolet pickup truck is parked in a parking lot, with a blue flag flying in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.54, "peak": 118.48, "min": 27.39}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.13, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 30.13, "energy_joules_est": 50.54, "sample_count": 17, "duration_seconds": 1.677}, "timestamp": "2026-01-19T13:45:26.428749"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2169.687, "latencies_ms": [2169.687], "images_per_second": 0.461, "prompt_tokens": 1113, "response_tokens_est": 41, "n_tiles": 1, "output_text": " 1. red truck\n2. white wheels\n3. white license plate\n4. green canopy\n5. blue flag\n6. white light\n7. black tires\n8. silver exhaust", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.82, "peak": 114.06, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.5, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 28.5, "energy_joules_est": 61.85, "sample_count": 21, "duration_seconds": 2.17}, "timestamp": "2026-01-19T13:45:28.614732"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2239.76, "latencies_ms": [2239.76], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The red pickup truck is positioned in the foreground of the image, with the blue pickup truck in the background. The truck is parked on the left side of the image, while the street lamp is located on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.59, "peak": 127.83, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.12, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.12, "energy_joules_est": 63.0, "sample_count": 22, "duration_seconds": 2.24}, "timestamp": "2026-01-19T13:45:30.912643"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1684.863, "latencies_ms": [1684.863], "images_per_second": 0.594, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A red Chevrolet pickup truck is parked in a parking lot, with a blue flag flying in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.9, "peak": 128.33, "min": 27.19}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.92, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.92, "energy_joules_est": 50.43, "sample_count": 17, "duration_seconds": 1.685}, "timestamp": "2026-01-19T13:45:32.695563"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1635.813, "latencies_ms": [1635.813], "images_per_second": 0.611, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The truck is red with chrome wheels and a shiny finish. The sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.64, "peak": 111.94, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.21}, "VDD_GPU": {"avg": 30.78, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.78, "energy_joules_est": 50.36, "sample_count": 16, "duration_seconds": 1.636}, "timestamp": "2026-01-19T13:45:34.365755"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1701.433, "latencies_ms": [1701.433], "images_per_second": 0.588, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In this black and white photo, a group of cows are standing behind a barbed wire fence, looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.49, "peak": 106.76, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.91, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 30.91, "energy_joules_est": 52.62, "sample_count": 17, "duration_seconds": 1.702}, "timestamp": "2026-01-19T13:45:36.144564"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2595.227, "latencies_ms": [2595.227], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. barbed wire: 1\n2. cows: 5\n3. grass: 1\n4. fence: 1\n5. trees: 2\n6. clouds: 1\n7. sky: 1\n8. fence post: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.0, "peak": 133.23, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.25, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.25, "energy_joules_est": 70.73, "sample_count": 25, "duration_seconds": 2.596}, "timestamp": "2026-01-19T13:45:38.752643"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1805.712, "latencies_ms": [1805.712], "images_per_second": 0.554, "prompt_tokens": 1117, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The cows are positioned in the foreground of the image, with the barbed wire fence in the middle ground, and the distant landscape in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.5, "peak": 120.77, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.98, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.98, "energy_joules_est": 54.14, "sample_count": 18, "duration_seconds": 1.806}, "timestamp": "2026-01-19T13:45:40.627037"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1592.277, "latencies_ms": [1592.277], "images_per_second": 0.628, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of cows are standing in a field behind a barbed wire fence, looking at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.43, "peak": 116.69, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.8, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.8, "energy_joules_est": 49.07, "sample_count": 16, "duration_seconds": 1.593}, "timestamp": "2026-01-19T13:45:42.302394"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1697.598, "latencies_ms": [1697.598], "images_per_second": 0.589, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image is in black and white, with a barbed wire fence in the foreground and a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.51, "peak": 129.92, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.66, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 30.66, "energy_joules_est": 52.07, "sample_count": 17, "duration_seconds": 1.698}, "timestamp": "2026-01-19T13:45:44.075273"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1942.041, "latencies_ms": [1942.041], "images_per_second": 0.515, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image depicts a cozy bedroom with a large bed, a comfortable chair, and a fireplace, all set against a warm and inviting backdrop of wooden walls and ceiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.38, "peak": 117.53, "min": 29.77}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.75, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.44, "peak": 40.16, "min": 17.35}}, "power_watts_avg": 29.44, "energy_joules_est": 57.2, "sample_count": 19, "duration_seconds": 1.943}, "timestamp": "2026-01-19T13:45:46.070452"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2817.761, "latencies_ms": [2817.761], "images_per_second": 0.355, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. bed: 1\n2. clock: 1\n3. television: 1\n4. chair: 2\n5. ottoman: 1\n6. fireplace: 1\n7. window blinds: 1\n8. lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.12, "peak": 128.16, "min": 27.46}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.68, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 25.68, "energy_joules_est": 72.37, "sample_count": 28, "duration_seconds": 2.818}, "timestamp": "2026-01-19T13:45:48.986181"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2425.763, "latencies_ms": [2425.763], "images_per_second": 0.412, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The bed is located on the left side of the room, with the clock above it. The fireplace is on the right side of the room, with the television above it. The chair is in the middle of the room, with the window behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.61, "peak": 115.17, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 26.83, "peak": 39.39, "min": 14.19}}, "power_watts_avg": 26.83, "energy_joules_est": 65.11, "sample_count": 24, "duration_seconds": 2.427}, "timestamp": "2026-01-19T13:45:51.487450"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2345.576, "latencies_ms": [2345.576], "images_per_second": 0.426, "prompt_tokens": 1111, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The image depicts a cozy bedroom with a large bed, a fireplace, and a television. The room is well-lit with warm lighting, and there are several pieces of furniture, including a chair, a bench, and a dresser.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.02, "peak": 127.38, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 27.41, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.41, "energy_joules_est": 64.3, "sample_count": 23, "duration_seconds": 2.346}, "timestamp": "2026-01-19T13:45:53.885096"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1738.818, "latencies_ms": [1738.818], "images_per_second": 0.575, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is bathed in warm light, with a wooden ceiling and walls, and a fireplace with a stone surround.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.75, "peak": 117.93, "min": 27.71}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.85, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.08, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.08, "energy_joules_est": 52.31, "sample_count": 17, "duration_seconds": 1.739}, "timestamp": "2026-01-19T13:45:55.653701"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2279.33, "latencies_ms": [2279.33], "images_per_second": 0.439, "prompt_tokens": 1099, "response_tokens_est": 45, "n_tiles": 1, "output_text": " In the image, there are three birds with black feathers and white spots, standing on a dry, grassy field, with one bird having a blue head and red neck, and another bird having a red head and blue neck.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.24, "peak": 110.77, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.44, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.12, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 28.12, "energy_joules_est": 64.11, "sample_count": 22, "duration_seconds": 2.28}, "timestamp": "2026-01-19T13:45:57.957301"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2535.869, "latencies_ms": [2535.869], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bird: 4\n2. grass: 1\n3. bush: 1\n4. tree: 1\n5. sky: 1\n6. ground: 1\n7. dirt: 1\n8. dirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.74, "peak": 127.59, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.06, "peak": 39.39, "min": 16.55}}, "power_watts_avg": 27.06, "energy_joules_est": 68.64, "sample_count": 25, "duration_seconds": 2.537}, "timestamp": "2026-01-19T13:46:00.554863"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2344.963, "latencies_ms": [2344.963], "images_per_second": 0.426, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The main objects are located in the foreground of the image, with the two larger birds in the center and the smaller bird to the right. The larger birds are positioned closer to the camera than the smaller bird, which is situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.76, "peak": 121.38, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 27.56, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.56, "energy_joules_est": 64.65, "sample_count": 23, "duration_seconds": 2.346}, "timestamp": "2026-01-19T13:46:02.948093"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1575.588, "latencies_ms": [1575.588], "images_per_second": 0.635, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A group of birds, possibly guineafowl, are walking on a dry, grassy field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.88, "peak": 118.96, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 30.73, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.73, "energy_joules_est": 48.43, "sample_count": 16, "duration_seconds": 1.576}, "timestamp": "2026-01-19T13:46:04.605244"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1769.249, "latencies_ms": [1769.249], "images_per_second": 0.565, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image features a group of birds with black feathers and white speckles, standing on a dry, grassy field under a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.43, "peak": 122.18, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.14, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 30.14, "energy_joules_est": 53.34, "sample_count": 18, "duration_seconds": 1.77}, "timestamp": "2026-01-19T13:46:06.479682"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1602.079, "latencies_ms": [1602.079], "images_per_second": 0.624, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " Three people are standing in the snow, wearing winter clothing and holding ski poles, with a forested background.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.43, "peak": 110.81, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.78, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.78, "energy_joules_est": 49.34, "sample_count": 16, "duration_seconds": 1.603}, "timestamp": "2026-01-19T13:46:08.146323"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2556.773, "latencies_ms": [2556.773], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. skis: 3\n2. snow: 1\n3. trees: 1\n4. people: 3\n5. poles: 3\n6. jacket: 1\n7. hat: 1\n8. goggles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.49, "peak": 110.71, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.47, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 27.47, "energy_joules_est": 70.25, "sample_count": 25, "duration_seconds": 2.558}, "timestamp": "2026-01-19T13:46:10.739003"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4168.857, "latencies_ms": [4168.857], "images_per_second": 0.24, "prompt_tokens": 1117, "response_tokens_est": 118, "n_tiles": 1, "output_text": " The skier on the left is positioned closer to the camera than the skier in the middle, who is standing slightly behind and to the right of the skier on the left. The skier on the right is positioned furthest from the camera, standing in the background. The skier in the middle is standing on a snow-covered slope, while the skier on the left is standing on a snow-covered hill. The skier on the right is standing on a snow-covered slope, while the skier on the left is standing on a snow-covered hill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.55, "peak": 132.53, "min": 30.48}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 23.91, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 23.91, "energy_joules_est": 99.69, "sample_count": 41, "duration_seconds": 4.17}, "timestamp": "2026-01-19T13:46:14.994344"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1445.714, "latencies_ms": [1445.714], "images_per_second": 0.692, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " Three people are standing in the snow wearing winter clothing and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.31, "peak": 122.98, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.43, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.43, "energy_joules_est": 45.45, "sample_count": 14, "duration_seconds": 1.446}, "timestamp": "2026-01-19T13:46:16.453878"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2116.496, "latencies_ms": [2116.496], "images_per_second": 0.472, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image features three people standing in a snowy landscape, wearing winter clothing and holding ski poles. The sky is overcast, and the snow is pristine white, with a few trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.19, "peak": 116.14, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 29.59, "peak": 40.97, "min": 18.52}}, "power_watts_avg": 29.59, "energy_joules_est": 62.63, "sample_count": 21, "duration_seconds": 2.117}, "timestamp": "2026-01-19T13:46:18.642639"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2118.848, "latencies_ms": [2118.848], "images_per_second": 0.472, "prompt_tokens": 1432, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A white city bus with a blue stripe and the number 61 on it is parked on the side of the road.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.39, "peak": 120.84, "min": 30.37}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 16.36, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 30.39, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 30.39, "energy_joules_est": 64.41, "sample_count": 21, "duration_seconds": 2.119}, "timestamp": "2026-01-19T13:46:20.846908"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3168.237, "latencies_ms": [3168.237], "images_per_second": 0.316, "prompt_tokens": 1446, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Bus: 1\n2. License plate: 1\n3. Bike rack: 1\n4. Side mirror: 1\n5. Headlights: 2\n6. Windshield: 1\n7. Roof rack: 1\n8. Side mirror: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.4, "peak": 133.06, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 16.36, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 27.82, "peak": 42.54, "min": 16.95}}, "power_watts_avg": 27.82, "energy_joules_est": 88.15, "sample_count": 31, "duration_seconds": 3.169}, "timestamp": "2026-01-19T13:46:24.079236"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2524.405, "latencies_ms": [2524.405], "images_per_second": 0.396, "prompt_tokens": 1450, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The bus is positioned on the left side of the image, with the street and sidewalk on the right. The license plate is located on the front of the bus, near the bottom of the windshield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.03, "peak": 134.6, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.42, "peak": 41.76, "min": 15.77}}, "power_watts_avg": 29.42, "energy_joules_est": 74.28, "sample_count": 25, "duration_seconds": 2.525}, "timestamp": "2026-01-19T13:46:26.689505"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1948.449, "latencies_ms": [1948.449], "images_per_second": 0.513, "prompt_tokens": 1444, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A white city bus with a bike rack on the front is parked on the side of the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.41, "peak": 131.35, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.36, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 32.1, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 32.1, "energy_joules_est": 62.55, "sample_count": 19, "duration_seconds": 1.949}, "timestamp": "2026-01-19T13:46:28.672500"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1906.585, "latencies_ms": [1906.585], "images_per_second": 0.524, "prompt_tokens": 1442, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The bus is white with blue and green graphics, and the sky is blue with white clouds.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.42, "peak": 137.95, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.26, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.97, "peak": 42.15, "min": 19.71}}, "power_watts_avg": 32.97, "energy_joules_est": 62.87, "sample_count": 19, "duration_seconds": 1.907}, "timestamp": "2026-01-19T13:46:30.659157"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1792.418, "latencies_ms": [1792.418], "images_per_second": 0.558, "prompt_tokens": 768, "response_tokens_est": 46, "n_tiles": 1, "output_text": " A man is standing in front of a wall with a red stripe on the bottom, wearing a blue blazer, a white shirt, a red and blue striped tie, a gray skirt, black tights, and black shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.18, "peak": 100.74, "min": 27.3}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 14.91, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.76, "min": 13.0}, "VDD_GPU": {"avg": 25.93, "peak": 38.21, "min": 17.34}}, "power_watts_avg": 25.93, "energy_joules_est": 46.48, "sample_count": 18, "duration_seconds": 1.793}, "timestamp": "2026-01-19T13:46:32.540791"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2051.568, "latencies_ms": [2051.568], "images_per_second": 0.487, "prompt_tokens": 782, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. suit: 1\n2. tie: 1\n3. shirt: 1\n4. tie: 1\n5. skirt: 1\n6. bag: 1\n7. shoes: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.95, "peak": 98.28, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 14.83, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 24.69, "peak": 37.82, "min": 15.77}}, "power_watts_avg": 24.69, "energy_joules_est": 50.66, "sample_count": 20, "duration_seconds": 2.052}, "timestamp": "2026-01-19T13:46:34.623438"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1701.011, "latencies_ms": [1701.011], "images_per_second": 0.588, "prompt_tokens": 786, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The man is standing in the foreground of the image, with the red carpet and white wall in the background. The black bag is held in his left hand, while his right hand is resting on his hip.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.62, "peak": 118.79, "min": 30.41}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.14, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 13.79}, "VDD_GPU": {"avg": 25.87, "peak": 37.42, "min": 16.95}}, "power_watts_avg": 25.87, "energy_joules_est": 44.02, "sample_count": 17, "duration_seconds": 1.701}, "timestamp": "2026-01-19T13:46:36.409216"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1460.205, "latencies_ms": [1460.205], "images_per_second": 0.685, "prompt_tokens": 780, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man is standing in front of a wall with a red stripe on it. He is wearing a blue blazer, a white shirt, and a tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.21, "peak": 106.8, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.04, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 27.04, "peak": 38.6, "min": 15.38}}, "power_watts_avg": 27.04, "energy_joules_est": 39.49, "sample_count": 14, "duration_seconds": 1.46}, "timestamp": "2026-01-19T13:46:37.883189"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1509.683, "latencies_ms": [1509.683], "images_per_second": 0.662, "prompt_tokens": 778, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with a red carpet on the floor. The man is wearing a blue blazer, a white shirt, and a tie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.15, "peak": 101.98, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.24, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.57}, "VDD_GPU": {"avg": 27.47, "peak": 38.21, "min": 18.91}}, "power_watts_avg": 27.47, "energy_joules_est": 41.48, "sample_count": 15, "duration_seconds": 1.51}, "timestamp": "2026-01-19T13:46:39.455878"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2568.042, "latencies_ms": [2568.042], "images_per_second": 0.389, "prompt_tokens": 1099, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image captures a serene scene of a railway track shrouded in a hazy mist, with numerous train cars lined up in a row, their metallic bodies gleaming under the soft light, and the sky above them painted in a muted palette of grays and browns.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.92, "peak": 106.26, "min": 30.09}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.77, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.77, "energy_joules_est": 68.77, "sample_count": 25, "duration_seconds": 2.569}, "timestamp": "2026-01-19T13:46:42.071987"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2655.299, "latencies_ms": [2655.299], "images_per_second": 0.377, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. train: 3\n2. train tracks: 2\n3. poles: 10\n4. wires: 10\n5. metal pipes: 2\n6. fence: 1\n7. trees: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.74, "peak": 118.46, "min": 30.62}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.56, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.56, "energy_joules_est": 70.54, "sample_count": 26, "duration_seconds": 2.656}, "timestamp": "2026-01-19T13:46:44.783388"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2120.196, "latencies_ms": [2120.196], "images_per_second": 0.472, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The trains are positioned on the tracks, with the tracks running from the foreground to the background. The poles are located on the left side of the image, while the trees are situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.39, "peak": 112.07, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 28.24, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.24, "energy_joules_est": 59.89, "sample_count": 21, "duration_seconds": 2.121}, "timestamp": "2026-01-19T13:46:46.972594"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2369.177, "latencies_ms": [2369.177], "images_per_second": 0.422, "prompt_tokens": 1111, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image captures a serene scene of a railway track shrouded in a hazy mist, with numerous train tracks stretching out into the distance. The tracks are lined with a series of metal poles and wires, creating a sense of order amidst the fog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.64, "peak": 123.11, "min": 29.94}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.54, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.54, "energy_joules_est": 65.26, "sample_count": 23, "duration_seconds": 2.369}, "timestamp": "2026-01-19T13:46:49.377395"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2240.708, "latencies_ms": [2240.708], "images_per_second": 0.446, "prompt_tokens": 1109, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The image features a sepia-toned photograph with a hazy atmosphere, showcasing a series of train tracks and overhead power lines. The trains are stationary, and the sky is overcast, creating a somber mood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.66, "peak": 117.16, "min": 28.3}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.99, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.99, "energy_joules_est": 62.73, "sample_count": 22, "duration_seconds": 2.241}, "timestamp": "2026-01-19T13:46:51.668494"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1882.905, "latencies_ms": [1882.905], "images_per_second": 0.531, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white toilet, a blue stool with a green plant on it, and a pile of clothes and shoes scattered on the floor.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.89, "peak": 122.62, "min": 27.54}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.97, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.97, "energy_joules_est": 54.57, "sample_count": 19, "duration_seconds": 1.884}, "timestamp": "2026-01-19T13:46:53.651618"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2520.416, "latencies_ms": [2520.416], "images_per_second": 0.397, "prompt_tokens": 1114, "response_tokens_est": 55, "n_tiles": 1, "output_text": " toilet: 1\ntoilet paper: 0\ntoilet brush: 0\ntoilet seat: 1\ntoilet lid: 1\ntoilet tank: 1\ntoilet paper holder: 0\ntoilet paper roll: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.43, "peak": 126.84, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.73, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.73, "energy_joules_est": 67.38, "sample_count": 25, "duration_seconds": 2.521}, "timestamp": "2026-01-19T13:46:56.261183"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2165.722, "latencies_ms": [2165.722], "images_per_second": 0.462, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The toilet is located on the left side of the image, while the bookshelf is on the right side. The clothes are scattered throughout the room, with some near the toilet and others near the bookshelf.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.65, "peak": 107.55, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.11, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.11, "energy_joules_est": 60.89, "sample_count": 21, "duration_seconds": 2.166}, "timestamp": "2026-01-19T13:46:58.443164"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1539.279, "latencies_ms": [1539.279], "images_per_second": 0.65, "prompt_tokens": 1112, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A messy bathroom with a toilet, a plant, and a bunch of clothes on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.47, "peak": 127.51, "min": 32.76}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.94, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.94, "energy_joules_est": 47.64, "sample_count": 15, "duration_seconds": 1.54}, "timestamp": "2026-01-19T13:47:00.006374"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1535.23, "latencies_ms": [1535.23], "images_per_second": 0.651, "prompt_tokens": 1110, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The bathroom is lit by natural light coming from a window, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.61, "peak": 126.49, "min": 30.47}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.17, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 32.17, "energy_joules_est": 49.41, "sample_count": 15, "duration_seconds": 1.536}, "timestamp": "2026-01-19T13:47:01.571653"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1945.368, "latencies_ms": [1945.368], "images_per_second": 0.514, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, a polar bear is seen playfully interacting with two green and yellow balls in a pool of water, with a rocky background and a sandy shore visible.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.49, "peak": 107.04, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.15, "peak": 40.56, "min": 19.7}}, "power_watts_avg": 30.15, "energy_joules_est": 58.67, "sample_count": 19, "duration_seconds": 1.946}, "timestamp": "2026-01-19T13:47:03.555266"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1840.369, "latencies_ms": [1840.369], "images_per_second": 0.543, "prompt_tokens": 1113, "response_tokens_est": 30, "n_tiles": 1, "output_text": " polar bear: 1, ball: 3, water: 1, rock: 1, sand: 1, wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.25, "peak": 124.41, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.07, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.07, "energy_joules_est": 55.35, "sample_count": 18, "duration_seconds": 1.841}, "timestamp": "2026-01-19T13:47:05.434538"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2116.137, "latencies_ms": [2116.137], "images_per_second": 0.473, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The green and yellow balls are in the foreground, with the polar bear in the background. The polar bear is in the water, which is in the foreground, and the rocks are in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.72, "peak": 125.89, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 28.72, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.72, "energy_joules_est": 60.79, "sample_count": 21, "duration_seconds": 2.117}, "timestamp": "2026-01-19T13:47:07.626003"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1719.022, "latencies_ms": [1719.022], "images_per_second": 0.582, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " In a zoo enclosure, a polar bear is playfully interacting with two green and yellow balls in a pool of water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.21, "peak": 108.4, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.8, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.8, "energy_joules_est": 51.25, "sample_count": 17, "duration_seconds": 1.72}, "timestamp": "2026-01-19T13:47:09.404496"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2021.262, "latencies_ms": [2021.262], "images_per_second": 0.495, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The polar bear is in a pool of water, with a green and yellow ball in its mouth. The lighting is natural, and the bear's fur is wet and matted.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.0, "peak": 123.57, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.13, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 29.13, "energy_joules_est": 58.89, "sample_count": 20, "duration_seconds": 2.022}, "timestamp": "2026-01-19T13:47:11.492662"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2469.708, "latencies_ms": [2469.708], "images_per_second": 0.405, "prompt_tokens": 1432, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The image is a split-screen photo showing a person sitting on a chair with their legs crossed, wearing blue socks and black pants, and holding a silver cell phone with a black screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.79, "peak": 111.35, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 16.36, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 29.43, "peak": 42.15, "min": 16.56}}, "power_watts_avg": 29.43, "energy_joules_est": 72.7, "sample_count": 24, "duration_seconds": 2.47}, "timestamp": "2026-01-19T13:47:13.997612"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2363.709, "latencies_ms": [2363.709], "images_per_second": 0.423, "prompt_tokens": 1446, "response_tokens_est": 35, "n_tiles": 1, "output_text": " chair: 1, person: 1, cell phone: 1, window: 1, floor: 1, legs: 1, socks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.77, "peak": 131.68, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.46, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 30.56, "peak": 42.53, "min": 16.95}}, "power_watts_avg": 30.56, "energy_joules_est": 72.24, "sample_count": 23, "duration_seconds": 2.364}, "timestamp": "2026-01-19T13:47:16.400391"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2963.84, "latencies_ms": [2963.84], "images_per_second": 0.337, "prompt_tokens": 1450, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The left side of the image shows a person sitting on a chair with their legs crossed, with the chair positioned on the left side of the image. The right side of the image shows a person holding a cell phone, with the cell phone positioned on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.09, "peak": 125.97, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 16.36, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 28.45, "peak": 42.13, "min": 17.35}}, "power_watts_avg": 28.45, "energy_joules_est": 84.33, "sample_count": 29, "duration_seconds": 2.964}, "timestamp": "2026-01-19T13:47:19.418265"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1981.948, "latencies_ms": [1981.948], "images_per_second": 0.505, "prompt_tokens": 1444, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person is sitting on a chair with their feet up on the floor, while holding a cell phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.88, "peak": 112.25, "min": 28.17}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 31.44, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 31.44, "energy_joules_est": 62.33, "sample_count": 20, "duration_seconds": 1.983}, "timestamp": "2026-01-19T13:47:21.506386"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2263.818, "latencies_ms": [2263.818], "images_per_second": 0.442, "prompt_tokens": 1442, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image features a wooden floor with a chair and a person wearing blue jeans. The sky outside the window is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.2, "peak": 121.34, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 16.15, "min": 11.26}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 12.21}, "VDD_GPU": {"avg": 30.98, "peak": 42.13, "min": 17.35}}, "power_watts_avg": 30.98, "energy_joules_est": 70.14, "sample_count": 22, "duration_seconds": 2.264}, "timestamp": "2026-01-19T13:47:23.808604"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1866.368, "latencies_ms": [1866.368], "images_per_second": 0.536, "prompt_tokens": 1432, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A yellow train is traveling down a snowy track, surrounded by trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.94, "peak": 128.75, "min": 32.04}, "VIN_SYS_5V0": {"avg": 14.71, "peak": 16.46, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 32.21, "peak": 42.53, "min": 17.74}}, "power_watts_avg": 32.21, "energy_joules_est": 60.14, "sample_count": 18, "duration_seconds": 1.867}, "timestamp": "2026-01-19T13:47:25.692446"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3041.106, "latencies_ms": [3041.106], "images_per_second": 0.329, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. train: 1\n2. trees: 4\n3. snow: 1\n4. sky: 1\n5. wires: 1\n6. poles: 1\n7. tracks: 1\n8. snow on train: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.89, "peak": 123.41, "min": 27.37}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.48, "peak": 42.13, "min": 17.34}}, "power_watts_avg": 28.48, "energy_joules_est": 86.62, "sample_count": 30, "duration_seconds": 3.042}, "timestamp": "2026-01-19T13:47:28.820561"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2355.203, "latencies_ms": [2355.203], "images_per_second": 0.425, "prompt_tokens": 1450, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The train is moving from the left to the right of the image, while the trees are located in the background. The train is closer to the viewer than the trees.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.0, "peak": 126.54, "min": 29.83}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 16.26, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.15, "peak": 41.74, "min": 14.98}}, "power_watts_avg": 30.15, "energy_joules_est": 71.02, "sample_count": 23, "duration_seconds": 2.356}, "timestamp": "2026-01-19T13:47:31.204158"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1835.767, "latencies_ms": [1835.767], "images_per_second": 0.545, "prompt_tokens": 1444, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A yellow train is traveling down a snowy track, surrounded by trees and bushes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.36, "peak": 117.76, "min": 29.15}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 16.46, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 32.94, "peak": 42.53, "min": 17.74}}, "power_watts_avg": 32.94, "energy_joules_est": 60.49, "sample_count": 18, "duration_seconds": 1.836}, "timestamp": "2026-01-19T13:47:33.087387"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2082.837, "latencies_ms": [2082.837], "images_per_second": 0.48, "prompt_tokens": 1442, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image features a yellow train traveling through a snowy landscape, with the sky appearing overcast and the trees covered in snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.92, "peak": 109.65, "min": 33.48}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.36, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 32.68, "peak": 42.92, "min": 20.5}}, "power_watts_avg": 32.68, "energy_joules_est": 68.08, "sample_count": 20, "duration_seconds": 2.083}, "timestamp": "2026-01-19T13:47:35.174813"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1880.867, "latencies_ms": [1880.867], "images_per_second": 0.532, "prompt_tokens": 1099, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, a pile of snow has been piled up on a street, with a black pipe protruding from the snow, and people walking in the background.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.12, "peak": 127.25, "min": 27.45}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.98, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 29.98, "energy_joules_est": 56.41, "sample_count": 19, "duration_seconds": 1.882}, "timestamp": "2026-01-19T13:47:37.160093"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2767.722, "latencies_ms": [2767.722], "images_per_second": 0.361, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. snow: 1000\n2. people: 6\n3. pipe: 1\n4. snowbank: 1\n5. building: 1\n6. road: 1\n7. snowplow: 1\n8. snowdrift: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.62, "peak": 126.56, "min": 28.67}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.37, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.37, "energy_joules_est": 73.0, "sample_count": 27, "duration_seconds": 2.768}, "timestamp": "2026-01-19T13:47:39.971557"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2679.887, "latencies_ms": [2679.887], "images_per_second": 0.373, "prompt_tokens": 1117, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The main objects are located in the foreground of the image, with the snow-covered pipe being the closest object to the viewer. The people are in the background, walking away from the camera. The snow-covered area is to the left of the pipe, and the brick building is to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.13, "peak": 118.67, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.85, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.66, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 26.66, "energy_joules_est": 71.46, "sample_count": 26, "duration_seconds": 2.68}, "timestamp": "2026-01-19T13:47:42.671940"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1717.852, "latencies_ms": [1717.852], "images_per_second": 0.582, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " In a snowy urban setting, a group of people are walking on a street, with a pile of snow in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.73, "peak": 115.04, "min": 28.4}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.1, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.1, "energy_joules_est": 51.72, "sample_count": 17, "duration_seconds": 1.718}, "timestamp": "2026-01-19T13:47:44.451278"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2203.446, "latencies_ms": [2203.446], "images_per_second": 0.454, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image depicts a snowy scene with a pile of snow in the foreground, and a group of people walking in the background. The snow is white and fluffy, and the lighting is natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.42, "peak": 121.52, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.19, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 28.19, "energy_joules_est": 62.13, "sample_count": 22, "duration_seconds": 2.204}, "timestamp": "2026-01-19T13:47:46.748494"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2286.958, "latencies_ms": [2286.958], "images_per_second": 0.437, "prompt_tokens": 1099, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The image features a street sign with a red and white \"TOW ZONE\" sign, indicating a restricted area for towing, and a yellow sign with a black and white cartoon face, possibly a warning or caution sign.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.4, "peak": 122.45, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.7, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.7, "energy_joules_est": 63.37, "sample_count": 22, "duration_seconds": 2.288}, "timestamp": "2026-01-19T13:47:49.052682"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2575.232, "latencies_ms": [2575.232], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. sign: 2\n2. pole: 2\n3. tree: 2\n4. building: 1\n5. street light: 1\n6. fence: 1\n7. trash can: 1\n8. car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.22, "peak": 123.15, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.17, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.17, "energy_joules_est": 69.98, "sample_count": 25, "duration_seconds": 2.576}, "timestamp": "2026-01-19T13:47:51.663529"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1908.924, "latencies_ms": [1908.924], "images_per_second": 0.524, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The sign is located in the foreground, to the right of the tree, and is near a building. The tree is in the background, behind the sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.44, "peak": 123.89, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.24, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 29.24, "energy_joules_est": 55.83, "sample_count": 19, "duration_seconds": 1.909}, "timestamp": "2026-01-19T13:47:53.632117"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1649.864, "latencies_ms": [1649.864], "images_per_second": 0.606, "prompt_tokens": 1111, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image captures a street scene with a sign indicating a tow zone, surrounded by lush green trees and buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.98, "peak": 128.61, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.95, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 30.95, "energy_joules_est": 51.08, "sample_count": 16, "duration_seconds": 1.65}, "timestamp": "2026-01-19T13:47:55.300481"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2082.194, "latencies_ms": [2082.194], "images_per_second": 0.48, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image features a street scene with a sign indicating a tow zone, surrounded by lush green trees and a brick building. The sunlight filters through the leaves, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.25, "peak": 120.82, "min": 27.81}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.1, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 29.1, "energy_joules_est": 60.6, "sample_count": 21, "duration_seconds": 2.083}, "timestamp": "2026-01-19T13:47:57.480867"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1657.689, "latencies_ms": [1657.689], "images_per_second": 0.603, "prompt_tokens": 1100, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A teddy bear wearing glasses and headphones sits on a desk with a keyboard, a microphone, and a tape recorder.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.02, "peak": 131.97, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.29, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.29, "energy_joules_est": 50.24, "sample_count": 16, "duration_seconds": 1.659}, "timestamp": "2026-01-19T13:47:59.155251"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2202.23, "latencies_ms": [2202.23], "images_per_second": 0.454, "prompt_tokens": 1114, "response_tokens_est": 43, "n_tiles": 1, "output_text": " teddy bear: 1\nkeyboard: 1\nmic: 1\ncamera: 1\nipod: 1\nglasses: 1\nremote: 1\nmouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.55, "peak": 109.74, "min": 28.95}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.53, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 28.53, "energy_joules_est": 62.85, "sample_count": 22, "duration_seconds": 2.203}, "timestamp": "2026-01-19T13:48:01.443145"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2718.526, "latencies_ms": [2718.526], "images_per_second": 0.368, "prompt_tokens": 1118, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The teddy bear is positioned in the center of the image, with the keyboard to its left and the microphone to its right. The objects are arranged in a way that the teddy bear is the focal point, with the keyboard and microphone placed in front of it, and the iPod and tape recorder positioned to its right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.47, "peak": 126.91, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.24, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.24, "energy_joules_est": 71.34, "sample_count": 27, "duration_seconds": 2.719}, "timestamp": "2026-01-19T13:48:04.229106"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1688.339, "latencies_ms": [1688.339], "images_per_second": 0.592, "prompt_tokens": 1112, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A teddy bear wearing glasses and headphones is sitting on a desk with a keyboard, a microphone, and a tape recorder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.28, "peak": 118.56, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.3, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.3, "energy_joules_est": 49.48, "sample_count": 17, "duration_seconds": 1.689}, "timestamp": "2026-01-19T13:48:06.000149"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1500.206, "latencies_ms": [1500.206], "images_per_second": 0.667, "prompt_tokens": 1110, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The teddy bear is brown, the keyboard is black, and the background is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.13, "peak": 119.88, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.23, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 31.23, "energy_joules_est": 46.86, "sample_count": 15, "duration_seconds": 1.501}, "timestamp": "2026-01-19T13:48:07.569868"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3838.372, "latencies_ms": [3838.372], "images_per_second": 0.261, "prompt_tokens": 1432, "response_tokens_est": 84, "n_tiles": 1, "output_text": " The image captures a dynamic moment of a skier in mid-air, with their skis and poles prominently displayed against the backdrop of a snowy mountain landscape. The skier is dressed in a white and orange suit, and the image is marked with various logos and text, including \"TATA,\" \"Tour de Sas,\" and \"Trentino,\" suggesting a possible association with a skiing event or brand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 127.93, "min": 32.59}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 26.16, "peak": 42.53, "min": 18.92}}, "power_watts_avg": 26.16, "energy_joules_est": 100.43, "sample_count": 37, "duration_seconds": 3.839}, "timestamp": "2026-01-19T13:48:11.419732"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2952.242, "latencies_ms": [2952.242], "images_per_second": 0.339, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. skis: 2\n2. person: 1\n3. snow: 1\n4. mountain: 1\n5. trees: 1\n6. sky: 1\n7. logo: 4\n8. text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.17, "peak": 104.76, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 28.38, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 28.38, "energy_joules_est": 83.8, "sample_count": 29, "duration_seconds": 2.953}, "timestamp": "2026-01-19T13:48:14.435220"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2478.203, "latencies_ms": [2478.203], "images_per_second": 0.404, "prompt_tokens": 1450, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The skier is in the foreground, with the mountain and trees in the background. The skier is to the left of the mountain, and the skier is in front of the mountain.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.15, "peak": 133.03, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 16.36, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 29.88, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 29.88, "energy_joules_est": 74.06, "sample_count": 24, "duration_seconds": 2.479}, "timestamp": "2026-01-19T13:48:16.943405"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1868.192, "latencies_ms": [1868.192], "images_per_second": 0.535, "prompt_tokens": 1444, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A skier is performing a jump in the snow, wearing a helmet and goggles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.35, "peak": 108.05, "min": 35.12}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.46, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 32.94, "peak": 42.53, "min": 17.35}}, "power_watts_avg": 32.94, "energy_joules_est": 61.55, "sample_count": 18, "duration_seconds": 1.869}, "timestamp": "2026-01-19T13:48:18.816739"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3122.804, "latencies_ms": [3122.804], "images_per_second": 0.32, "prompt_tokens": 1442, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image features a skier in a white and orange suit, with the skis in the air, against a backdrop of snow-covered mountains and a clear blue sky. The lighting is bright and natural, suggesting daytime, and the snow appears to be freshly fallen, giving it a pristine white appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.67, "peak": 112.0, "min": 31.95}, "VIN_SYS_5V0": {"avg": 14.74, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 28.68, "peak": 42.13, "min": 19.7}}, "power_watts_avg": 28.68, "energy_joules_est": 89.57, "sample_count": 30, "duration_seconds": 3.123}, "timestamp": "2026-01-19T13:48:21.943187"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2508.622, "latencies_ms": [2508.622], "images_per_second": 0.399, "prompt_tokens": 1432, "response_tokens_est": 40, "n_tiles": 1, "output_text": " In the image, there are two surfers riding the waves in the ocean, with one surfer on a green surfboard and the other on a black surfboard, both skillfully navigating the waves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.48, "peak": 115.17, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.84, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.4, "peak": 41.34, "min": 16.56}}, "power_watts_avg": 29.4, "energy_joules_est": 73.77, "sample_count": 24, "duration_seconds": 2.509}, "timestamp": "2026-01-19T13:48:24.487288"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2985.873, "latencies_ms": [2985.873], "images_per_second": 0.335, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. surfboard: 2\n2. wave: 2\n3. surfer: 2\n4. ocean: 1\n5. sky: 1\n6. sun: 1\n7. water: 1\n8. sand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.84, "peak": 124.4, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.26, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 28.39, "peak": 41.74, "min": 16.95}}, "power_watts_avg": 28.39, "energy_joules_est": 84.79, "sample_count": 29, "duration_seconds": 2.987}, "timestamp": "2026-01-19T13:48:27.516030"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3091.475, "latencies_ms": [3091.475], "images_per_second": 0.323, "prompt_tokens": 1450, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The surfer on the left is closer to the camera than the surfer on the right. The surfer on the left is in the foreground, while the surfer on the right is in the background. The surfer on the left is closer to the camera than the surfer on the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.17, "peak": 109.51, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 16.36, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 28.07, "peak": 42.15, "min": 16.16}}, "power_watts_avg": 28.07, "energy_joules_est": 86.79, "sample_count": 30, "duration_seconds": 3.092}, "timestamp": "2026-01-19T13:48:30.639665"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2037.449, "latencies_ms": [2037.449], "images_per_second": 0.491, "prompt_tokens": 1444, "response_tokens_est": 19, "n_tiles": 1, "output_text": " Two surfers are riding the waves in the ocean, with the sun setting in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.28, "peak": 118.93, "min": 29.65}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.36, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 31.57, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 31.57, "energy_joules_est": 64.33, "sample_count": 20, "duration_seconds": 2.038}, "timestamp": "2026-01-19T13:48:32.724391"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2927.018, "latencies_ms": [2927.018], "images_per_second": 0.342, "prompt_tokens": 1442, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image captures the dynamic interaction between the surfer and the ocean, with the surfer riding a wave and the ocean's surface reflecting the sunlight. The colors are predominantly blue and white, with the surfer's dark attire contrasting against the lighter hues of the water and sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.61, "peak": 123.06, "min": 28.16}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 16.15, "min": 11.26}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 17.32, "min": 12.21}, "VDD_GPU": {"avg": 28.35, "peak": 41.74, "min": 16.56}}, "power_watts_avg": 28.35, "energy_joules_est": 82.99, "sample_count": 29, "duration_seconds": 2.927}, "timestamp": "2026-01-19T13:48:35.749072"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1823.776, "latencies_ms": [1823.776], "images_per_second": 0.548, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A pizza with a variety of toppings is placed on a metal tray, with a glass jar of pepperoni sauce and a napkin nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.98, "peak": 125.24, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.44, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.44, "energy_joules_est": 53.7, "sample_count": 18, "duration_seconds": 1.824}, "timestamp": "2026-01-19T13:48:37.631070"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1998.711, "latencies_ms": [1998.711], "images_per_second": 0.5, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " pizza: 1, plate: 1, pepperoni: 1, cheese: 1, sauce: 1, crust: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.05, "peak": 125.01, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.96, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 28.96, "energy_joules_est": 57.89, "sample_count": 20, "duration_seconds": 1.999}, "timestamp": "2026-01-19T13:48:39.722152"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2098.559, "latencies_ms": [2098.559], "images_per_second": 0.477, "prompt_tokens": 1117, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The pizza is located in the foreground, with the pepperoni pizza in the background. The pepperoni pizza is placed on the table, while the salt shaker is positioned closer to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.15, "peak": 115.47, "min": 27.11}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.24, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.24, "energy_joules_est": 59.28, "sample_count": 21, "duration_seconds": 2.099}, "timestamp": "2026-01-19T13:48:41.916893"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2205.288, "latencies_ms": [2205.288], "images_per_second": 0.453, "prompt_tokens": 1111, "response_tokens_est": 43, "n_tiles": 1, "output_text": " In a dimly lit room, a pizza sits on a table, its golden crust and vibrant toppings inviting. A glass jar of pepperoni sauce stands nearby, ready to add a spicy kick to the meal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.02, "peak": 129.12, "min": 27.05}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.78, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.78, "energy_joules_est": 61.27, "sample_count": 22, "duration_seconds": 2.206}, "timestamp": "2026-01-19T13:48:44.209083"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1501.587, "latencies_ms": [1501.587], "images_per_second": 0.666, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The pizza is on a metal tray with a red tablecloth in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.51, "peak": 110.55, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 30.76, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.76, "energy_joules_est": 46.2, "sample_count": 15, "duration_seconds": 1.502}, "timestamp": "2026-01-19T13:48:45.779900"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2758.626, "latencies_ms": [2758.626], "images_per_second": 0.362, "prompt_tokens": 1100, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image captures a quaint town square bathed in the soft glow of dusk, with a prominent clock tower standing tall on the left, its face adorned with Roman numerals, and a row of buildings lining the street, their facades painted in a palette of red and brown, their windows reflecting the fading light of the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.41, "peak": 113.61, "min": 30.59}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.75, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 26.75, "energy_joules_est": 73.81, "sample_count": 27, "duration_seconds": 2.759}, "timestamp": "2026-01-19T13:48:48.597923"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2531.519, "latencies_ms": [2531.519], "images_per_second": 0.395, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. clock: 1\n2. pole: 1\n3. snow: 1\n4. building: 2\n5. street: 1\n6. car: 1\n7. tree: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.62, "peak": 128.07, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.65, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.65, "energy_joules_est": 67.48, "sample_count": 25, "duration_seconds": 2.532}, "timestamp": "2026-01-19T13:48:51.207388"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2021.31, "latencies_ms": [2021.31], "images_per_second": 0.495, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The clock is positioned on the left side of the image, with the street extending into the background. The clock is in the foreground, with the buildings and street extending into the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.52, "peak": 108.62, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.39, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.39, "energy_joules_est": 57.39, "sample_count": 20, "duration_seconds": 2.022}, "timestamp": "2026-01-19T13:48:53.297083"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2699.762, "latencies_ms": [2699.762], "images_per_second": 0.37, "prompt_tokens": 1112, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image captures a quaint town square bathed in the soft glow of dusk. A clock, standing tall on a black pole, marks the passage of time amidst the serene beauty of the snow-covered streets. The buildings, adorned with festive lights, stand as silent spectators to the quiet beauty of the evening.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.79, "peak": 128.66, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.23, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.23, "energy_joules_est": 70.83, "sample_count": 26, "duration_seconds": 2.7}, "timestamp": "2026-01-19T13:48:56.010133"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1504.127, "latencies_ms": [1504.127], "images_per_second": 0.665, "prompt_tokens": 1110, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The clock is black with white numbers and hands, and the street is covered in snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.42, "peak": 109.02, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.17, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 31.17, "energy_joules_est": 46.91, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T13:48:57.583322"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1822.047, "latencies_ms": [1822.047], "images_per_second": 0.549, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A baseball player in a white uniform with blue stripes and a blue helmet is swinging a wooden bat at a baseball that is in mid-air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.52, "peak": 126.92, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.18, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 30.18, "energy_joules_est": 55.01, "sample_count": 18, "duration_seconds": 1.823}, "timestamp": "2026-01-19T13:48:59.472390"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3119.75, "latencies_ms": [3119.75], "images_per_second": 0.321, "prompt_tokens": 1113, "response_tokens_est": 75, "n_tiles": 1, "output_text": " 1. baseball bat: 1\n2. baseball: 1\n3. catcher's mitt: 1\n4. baseball player: 1\n5. baseball player's uniform: 1\n6. baseball player's helmet: 1\n7. baseball player's cleats: 1\n8. baseball player's glove: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.45, "peak": 131.86, "min": 28.13}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 25.45, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 25.45, "energy_joules_est": 79.41, "sample_count": 31, "duration_seconds": 3.12}, "timestamp": "2026-01-19T13:49:02.696288"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1920.185, "latencies_ms": [1920.185], "images_per_second": 0.521, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher in the background. The ball is in the middle ground, between the batter and the catcher.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.92, "peak": 123.33, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.53, "min": 12.22}, "VDD_GPU": {"avg": 28.72, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 28.72, "energy_joules_est": 55.16, "sample_count": 19, "duration_seconds": 1.921}, "timestamp": "2026-01-19T13:49:04.672165"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1877.715, "latencies_ms": [1877.715], "images_per_second": 0.533, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A baseball player in a white uniform is swinging a bat at a baseball. The catcher is wearing a red uniform and is ready to catch the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.24, "peak": 122.61, "min": 27.18}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.38, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 29.38, "energy_joules_est": 55.18, "sample_count": 19, "duration_seconds": 1.878}, "timestamp": "2026-01-19T13:49:06.652544"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3468.011, "latencies_ms": [3468.011], "images_per_second": 0.288, "prompt_tokens": 1109, "response_tokens_est": 91, "n_tiles": 1, "output_text": " The image captures a dynamic moment in a baseball game, with the batter in full swing, the ball suspended in mid-air, and the catcher poised to catch it. The colors are vibrant, with the blue of the batter's uniform standing out against the green of the field and the red of the catcher's uniform. The lighting is natural, suggesting it's a sunny day, and the shadows cast by the players add depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.88, "peak": 114.25, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 24.85, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 24.85, "energy_joules_est": 86.19, "sample_count": 34, "duration_seconds": 3.469}, "timestamp": "2026-01-19T13:49:10.185860"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1538.463, "latencies_ms": [1538.463], "images_per_second": 0.65, "prompt_tokens": 1100, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A brown teddy bear with a red bow tie is sitting on a chair with a red cushion.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.22, "peak": 128.19, "min": 29.77}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.68, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.68, "energy_joules_est": 47.23, "sample_count": 15, "duration_seconds": 1.539}, "timestamp": "2026-01-19T13:49:11.758242"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2556.613, "latencies_ms": [2556.613], "images_per_second": 0.391, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. chair: 1\n2. teddy bear: 1\n3. cushion: 1\n4. wall: 1\n5. curtain: 1\n6. wicker: 1\n7. wood: 1\n8. fabric: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.05, "peak": 131.9, "min": 29.52}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.66, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 27.66, "energy_joules_est": 70.74, "sample_count": 25, "duration_seconds": 2.557}, "timestamp": "2026-01-19T13:49:14.362140"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2246.913, "latencies_ms": [2246.913], "images_per_second": 0.445, "prompt_tokens": 1118, "response_tokens_est": 46, "n_tiles": 1, "output_text": " The teddy bear is positioned on the left side of the chair, which is located in the foreground of the image. The teddy bear is sitting on the right side of the chair, which is positioned in the foreground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.4, "peak": 123.65, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.87, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.87, "energy_joules_est": 62.63, "sample_count": 22, "duration_seconds": 2.247}, "timestamp": "2026-01-19T13:49:16.661588"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1391.535, "latencies_ms": [1391.535], "images_per_second": 0.719, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A teddy bear is sitting on a chair with a red cushion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.89, "peak": 121.61, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 30.99, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.99, "energy_joules_est": 43.13, "sample_count": 14, "duration_seconds": 1.392}, "timestamp": "2026-01-19T13:49:18.122507"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1491.747, "latencies_ms": [1491.747], "images_per_second": 0.67, "prompt_tokens": 1110, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The teddy bear is brown, the chair is red, and the background is yellow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.07, "peak": 125.03, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 32.2, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 32.2, "energy_joules_est": 48.06, "sample_count": 15, "duration_seconds": 1.493}, "timestamp": "2026-01-19T13:49:19.691471"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1522.054, "latencies_ms": [1522.054], "images_per_second": 0.657, "prompt_tokens": 1100, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two people are standing on a snowy mountain, one of them is holding a snowboard.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.22, "peak": 134.56, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 31.73, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.73, "energy_joules_est": 48.31, "sample_count": 15, "duration_seconds": 1.522}, "timestamp": "2026-01-19T13:49:21.264280"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2543.911, "latencies_ms": [2543.911], "images_per_second": 0.393, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. snowboard: 1\n2. helmet: 1\n3. goggles: 1\n4. jacket: 2\n5. pants: 2\n6. boots: 2\n7. sun: 1\n8. rock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.13, "peak": 132.01, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.55, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.55, "energy_joules_est": 70.1, "sample_count": 25, "duration_seconds": 2.544}, "timestamp": "2026-01-19T13:49:23.863064"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2851.694, "latencies_ms": [2851.694], "images_per_second": 0.351, "prompt_tokens": 1118, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The person on the left is standing closer to the camera than the person on the right. The person on the right is standing in the foreground of the image, while the person on the left is standing in the background. The person on the right is holding a snowboard, while the person on the left is wearing a helmet and goggles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.23, "peak": 107.22, "min": 29.77}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.02, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.02, "energy_joules_est": 74.21, "sample_count": 28, "duration_seconds": 2.852}, "timestamp": "2026-01-19T13:49:26.766676"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1647.038, "latencies_ms": [1647.038], "images_per_second": 0.607, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " Two snowboarders stand on a snowy mountain, one holding a snowboard and the other wearing a helmet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.4, "peak": 120.54, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.79, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.79, "energy_joules_est": 49.09, "sample_count": 16, "duration_seconds": 1.648}, "timestamp": "2026-01-19T13:49:28.440297"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2498.03, "latencies_ms": [2498.03], "images_per_second": 0.4, "prompt_tokens": 1110, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image features two snowboarders standing on a snowy mountain with a bright sun shining in the background. The snowboarders are wearing red jackets and black pants, and they are holding snowboards. The sun is shining brightly, creating a lens flare effect in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.3, "peak": 121.87, "min": 27.55}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.27, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.27, "energy_joules_est": 68.13, "sample_count": 25, "duration_seconds": 2.498}, "timestamp": "2026-01-19T13:49:31.035406"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2102.438, "latencies_ms": [2102.438], "images_per_second": 0.476, "prompt_tokens": 1099, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The image captures a tree with a large, prominent knot in its trunk, surrounded by a variety of apples hanging from its branches, set against a backdrop of a warm, autumnal forest.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.37, "peak": 121.62, "min": 28.23}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.94, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 27.94, "energy_joules_est": 58.77, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T13:49:33.233895"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2007.58, "latencies_ms": [2007.58], "images_per_second": 0.498, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " apple: 5, tree: 1, hole: 1, branch: 1, leaf: 1, tree trunk: 1, background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.75, "peak": 109.67, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.75, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.6, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.6, "energy_joules_est": 57.42, "sample_count": 20, "duration_seconds": 2.008}, "timestamp": "2026-01-19T13:49:35.317770"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2263.451, "latencies_ms": [2263.451], "images_per_second": 0.442, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The tree trunk is located in the foreground on the right side of the image, while the apples are hanging from the branches in the background on the left side. The apples are positioned closer to the camera than the tree trunk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.45, "peak": 120.22, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.01, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 28.01, "energy_joules_est": 63.41, "sample_count": 22, "duration_seconds": 2.264}, "timestamp": "2026-01-19T13:49:37.605008"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2463.349, "latencies_ms": [2463.349], "images_per_second": 0.406, "prompt_tokens": 1111, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a serene autumn scene where a tree laden with ripe red apples stands tall, surrounded by a field of golden leaves. The warm hues of the setting sun cast a soft glow on the scene, enhancing the natural beauty of the apple tree and its surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.79, "peak": 128.72, "min": 30.08}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.33, "peak": 39.38, "min": 16.55}}, "power_watts_avg": 27.33, "energy_joules_est": 67.34, "sample_count": 24, "duration_seconds": 2.464}, "timestamp": "2026-01-19T13:49:40.107820"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2511.403, "latencies_ms": [2511.403], "images_per_second": 0.398, "prompt_tokens": 1109, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image features a tree with a hole in its trunk, surrounded by red apples and autumn leaves. The lighting is warm and soft, creating a serene atmosphere. The colors are vibrant, with the red apples contrasting against the brown tree bark and the orange hues of the leaves.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.6, "peak": 123.15, "min": 29.14}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.95, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.95, "energy_joules_est": 67.69, "sample_count": 25, "duration_seconds": 2.512}, "timestamp": "2026-01-19T13:49:42.702122"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4269.749, "latencies_ms": [4269.749], "images_per_second": 0.234, "prompt_tokens": 1099, "response_tokens_est": 121, "n_tiles": 1, "output_text": " In the image, a bustling kitchen scene unfolds with two chefs diligently working. The chef in the foreground, donned in a crisp white shirt and a blue baseball cap, is engrossed in the task of preparing food, his hands skillfully maneuvering a metal container. His counterpart in the background, also clad in a white shirt and a blue baseball cap, is engrossed in the task of cooking, his hands moving with practiced ease as he stirs a pot on the stove. The kitchen itself is a symphony of stainless steel appliances and white countertops, a testament to the efficiency and cleanliness of the culinary space.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.85, "peak": 115.74, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 23.52, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 23.52, "energy_joules_est": 100.44, "sample_count": 42, "duration_seconds": 4.271}, "timestamp": "2026-01-19T13:49:47.083850"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2811.04, "latencies_ms": [2811.04], "images_per_second": 0.356, "prompt_tokens": 1113, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. person: 2\n2. white t-shirt: 2\n3. blue cap: 1\n4. metal tray: 1\n5. food container: 1\n6. metal container: 1\n7. metal tray: 1\n8. food container: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.6, "peak": 124.37, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 25.69, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 25.69, "energy_joules_est": 72.23, "sample_count": 28, "duration_seconds": 2.812}, "timestamp": "2026-01-19T13:49:50.000391"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2169.348, "latencies_ms": [2169.348], "images_per_second": 0.461, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The man in the foreground is working on a fryer, while the man in the background is working on a grill. The man in the foreground is closer to the camera than the man in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.01, "peak": 101.9, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.77, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 27.77, "energy_joules_est": 60.26, "sample_count": 21, "duration_seconds": 2.17}, "timestamp": "2026-01-19T13:49:52.212842"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1540.898, "latencies_ms": [1540.898], "images_per_second": 0.649, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two men are working in a kitchen, one is preparing food and the other is cooking.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.23, "peak": 104.91, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.31, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.31, "energy_joules_est": 48.26, "sample_count": 15, "duration_seconds": 1.541}, "timestamp": "2026-01-19T13:49:53.784864"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1633.8, "latencies_ms": [1633.8], "images_per_second": 0.612, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The kitchen is well-lit with fluorescent lights, and the stainless steel appliances gleam under the bright lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.72, "peak": 117.16, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.64, "peak": 40.56, "min": 19.7}}, "power_watts_avg": 31.64, "energy_joules_est": 51.7, "sample_count": 16, "duration_seconds": 1.634}, "timestamp": "2026-01-19T13:49:55.460406"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1310.49, "latencies_ms": [1310.49], "images_per_second": 0.763, "prompt_tokens": 766, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A group of motorcyclists are gathered on a roadside, with several parked motorcycles and a few standing, and a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5033.1, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.3, "peak": 120.34, "min": 28.33}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.04, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 28.37, "peak": 38.21, "min": 19.31}}, "power_watts_avg": 28.37, "energy_joules_est": 37.2, "sample_count": 13, "duration_seconds": 1.311}, "timestamp": "2026-01-19T13:49:56.829228"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2457.566, "latencies_ms": [2457.566], "images_per_second": 0.407, "prompt_tokens": 780, "response_tokens_est": 72, "n_tiles": 1, "output_text": " 1. Motorcycle: 10\n2. Motorcycle: 10\n3. Motorcycle: 10\n4. Motorcycle: 10\n5. Motorcycle: 10\n6. Motorcycle: 10\n7. Motorcycle: 10\n8. Motorcycle: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.59, "peak": 125.31, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.04, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 24.15, "peak": 38.6, "min": 17.74}}, "power_watts_avg": 24.15, "energy_joules_est": 59.36, "sample_count": 24, "duration_seconds": 2.458}, "timestamp": "2026-01-19T13:49:59.317657"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1614.891, "latencies_ms": [1614.891], "images_per_second": 0.619, "prompt_tokens": 784, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The motorcycles are positioned on the left side of the road, with the group of people standing on the right side. The motorcycles are in the foreground, while the people are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.9, "peak": 122.45, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.14, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.54, "min": 14.18}, "VDD_GPU": {"avg": 26.13, "peak": 37.03, "min": 17.35}}, "power_watts_avg": 26.13, "energy_joules_est": 42.2, "sample_count": 16, "duration_seconds": 1.615}, "timestamp": "2026-01-19T13:50:00.995691"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1227.371, "latencies_ms": [1227.371], "images_per_second": 0.815, "prompt_tokens": 778, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A group of motorcyclists are gathered on a roadside near a body of water, with a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.03, "peak": 129.77, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.04, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 28.3, "peak": 38.6, "min": 16.16}}, "power_watts_avg": 28.3, "energy_joules_est": 34.75, "sample_count": 12, "duration_seconds": 1.228}, "timestamp": "2026-01-19T13:50:02.247211"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2329.254, "latencies_ms": [2329.254], "images_per_second": 0.429, "prompt_tokens": 776, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image features a group of motorcyclists gathered on a roadside, with their motorcycles parked in a line. The sky is filled with clouds, and the lighting suggests it is either early morning or late afternoon. The colors in the image are predominantly black, white, and blue, with the motorcycles adding a splash of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.1, "peak": 124.4, "min": 30.23}, "VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.54, "min": 14.18}, "VDD_GPU": {"avg": 24.62, "peak": 38.21, "min": 16.95}}, "power_watts_avg": 24.62, "energy_joules_est": 57.36, "sample_count": 23, "duration_seconds": 2.33}, "timestamp": "2026-01-19T13:50:04.648610"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1776.387, "latencies_ms": [1776.387], "images_per_second": 0.563, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A small airplane with the letters G-RMWZ on the tail flies through the sky, leaving a trail of smoke behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.39, "peak": 118.54, "min": 30.1}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.15, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 29.15, "energy_joules_est": 51.8, "sample_count": 18, "duration_seconds": 1.777}, "timestamp": "2026-01-19T13:50:06.531696"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2582.582, "latencies_ms": [2582.582], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. airplane: 1\n2. smoke: 1\n3. clouds: 2\n4. sky: 1\n5. tail number: 1\n6. propeller: 1\n7. wing: 1\n8. fuselage: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.44, "peak": 124.65, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 26.93, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 26.93, "energy_joules_est": 69.56, "sample_count": 25, "duration_seconds": 2.583}, "timestamp": "2026-01-19T13:50:09.147223"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1854.37, "latencies_ms": [1854.37], "images_per_second": 0.539, "prompt_tokens": 1117, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The airplane is in the foreground, flying from left to right, while the clouds are in the background. The airplane is flying higher than the clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.37, "peak": 119.84, "min": 31.5}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.7, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.7, "energy_joules_est": 55.09, "sample_count": 18, "duration_seconds": 1.855}, "timestamp": "2026-01-19T13:50:11.031645"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1516.13, "latencies_ms": [1516.13], "images_per_second": 0.66, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A small airplane is flying in the sky, leaving a trail of smoke behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.68, "peak": 118.21, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.62, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.62, "energy_joules_est": 47.95, "sample_count": 15, "duration_seconds": 1.517}, "timestamp": "2026-01-19T13:50:12.599814"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1644.211, "latencies_ms": [1644.211], "images_per_second": 0.608, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The image is in black and white, with a sepia tone, and the sky is filled with clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.38, "peak": 127.85, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 31.47, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 31.47, "energy_joules_est": 51.76, "sample_count": 16, "duration_seconds": 1.645}, "timestamp": "2026-01-19T13:50:14.276884"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1736.915, "latencies_ms": [1736.915], "images_per_second": 0.576, "prompt_tokens": 1099, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In the image, a group of sheep are grazing on a grassy hillside, with a serene lake and majestic mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.18, "peak": 115.81, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.82, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.82, "energy_joules_est": 53.54, "sample_count": 17, "duration_seconds": 1.737}, "timestamp": "2026-01-19T13:50:16.060855"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2555.942, "latencies_ms": [2555.942], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. sheep: 4\n2. mountains: 3\n3. lake: 1\n4. grass: 1\n5. rocks: 1\n6. sky: 1\n7. clouds: 1\n8. water: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.71, "peak": 129.32, "min": 28.61}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.29, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.29, "energy_joules_est": 69.76, "sample_count": 25, "duration_seconds": 2.556}, "timestamp": "2026-01-19T13:50:18.663865"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2077.454, "latencies_ms": [2077.454], "images_per_second": 0.481, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The sheep are positioned in the foreground of the image, with the mountains in the background. The sheep are standing on a grassy hill, with the lake visible to the right of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.68, "peak": 117.94, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.64, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.64, "energy_joules_est": 59.51, "sample_count": 20, "duration_seconds": 2.078}, "timestamp": "2026-01-19T13:50:20.753170"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1745.829, "latencies_ms": [1745.829], "images_per_second": 0.573, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " In this picturesque landscape, a group of sheep graze on a grassy hillside, with a serene lake and majestic mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.49, "peak": 124.67, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.5, "peak": 39.77, "min": 17.35}}, "power_watts_avg": 30.5, "energy_joules_est": 53.26, "sample_count": 17, "duration_seconds": 1.746}, "timestamp": "2026-01-19T13:50:22.532871"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2665.865, "latencies_ms": [2665.865], "images_per_second": 0.375, "prompt_tokens": 1109, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image features a group of sheep standing on a grassy hill, with the mountains in the background. The sky is clear and blue, and the sun is shining brightly, casting a warm glow on the scene. The sheep are white, with some having black faces, and they are grazing on the grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.3, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.5, "peak": 118.14, "min": 29.93}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.98, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 26.98, "energy_joules_est": 71.94, "sample_count": 26, "duration_seconds": 2.666}, "timestamp": "2026-01-19T13:50:25.249761"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1493.543, "latencies_ms": [1493.543], "images_per_second": 0.67, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman in a wheelchair is holding a tennis racket and looking at something.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.3, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.3, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.47, "peak": 131.2, "min": 30.9}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 30.86, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.86, "energy_joules_est": 46.11, "sample_count": 15, "duration_seconds": 1.494}, "timestamp": "2026-01-19T13:50:26.819475"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2641.167, "latencies_ms": [2641.167], "images_per_second": 0.379, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. woman: 1\n2. wheelchair: 1\n3. racket: 1\n4. person: 2\n5. wall: 1\n6. floor: 1\n7. ball: 1\n8. racket strings: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.79, "peak": 124.83, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 27.12, "energy_joules_est": 71.64, "sample_count": 26, "duration_seconds": 2.642}, "timestamp": "2026-01-19T13:50:29.534346"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1851.902, "latencies_ms": [1851.902], "images_per_second": 0.54, "prompt_tokens": 1117, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The woman in the wheelchair is in the foreground, holding a tennis racket. The other person is in the background, standing near the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.67, "peak": 121.79, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.33, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.33, "energy_joules_est": 54.34, "sample_count": 18, "duration_seconds": 1.853}, "timestamp": "2026-01-19T13:50:31.424570"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1707.732, "latencies_ms": [1707.732], "images_per_second": 0.586, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A woman in a wheelchair is holding a tennis racket and looking at something. There are other people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.64, "peak": 129.14, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.5, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.5, "energy_joules_est": 52.1, "sample_count": 17, "duration_seconds": 1.708}, "timestamp": "2026-01-19T13:50:33.206659"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2420.35, "latencies_ms": [2420.35], "images_per_second": 0.413, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image is a photograph taken in an indoor sports facility with a white wall in the background. The lighting is natural, coming from the windows, and the colors are vibrant, with the woman's gray shirt and the red and blue racket standing out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.11, "peak": 122.84, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.41, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.41, "energy_joules_est": 66.36, "sample_count": 24, "duration_seconds": 2.421}, "timestamp": "2026-01-19T13:50:35.705205"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1898.787, "latencies_ms": [1898.787], "images_per_second": 0.527, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A young girl is sitting on a brown saddle with a black helmet on her head.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.47, "peak": 133.6, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 31.02, "peak": 42.13, "min": 15.38}}, "power_watts_avg": 31.02, "energy_joules_est": 58.92, "sample_count": 19, "duration_seconds": 1.9}, "timestamp": "2026-01-19T13:50:37.705584"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3092.831, "latencies_ms": [3092.831], "images_per_second": 0.323, "prompt_tokens": 1446, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. girl: 1\n2. helmet: 1\n3. pink shirt: 1\n4. blue jeans: 1\n5. brown saddle: 1\n6. purple blanket: 1\n7. white fence: 1\n8. trees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.17, "peak": 131.53, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 16.26, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 28.16, "peak": 42.13, "min": 17.74}}, "power_watts_avg": 28.16, "energy_joules_est": 87.1, "sample_count": 30, "duration_seconds": 3.093}, "timestamp": "2026-01-19T13:50:40.843606"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3276.556, "latencies_ms": [3276.556], "images_per_second": 0.305, "prompt_tokens": 1450, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The child is sitting on the saddle of the horse, which is positioned in the foreground of the image. The child is wearing a pink and white plaid shirt, blue jeans, and a black helmet, which are all in the foreground. The background of the image features a green field with trees, which is far away from the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.58, "peak": 121.25, "min": 30.18}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 27.39, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 27.39, "energy_joules_est": 89.76, "sample_count": 32, "duration_seconds": 3.277}, "timestamp": "2026-01-19T13:50:44.176560"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1762.195, "latencies_ms": [1762.195], "images_per_second": 0.567, "prompt_tokens": 1444, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A little girl is sitting on a brown saddle in a field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.23, "peak": 121.05, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 32.84, "peak": 42.15, "min": 16.16}}, "power_watts_avg": 32.84, "energy_joules_est": 57.89, "sample_count": 17, "duration_seconds": 1.763}, "timestamp": "2026-01-19T13:50:45.963879"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2211.228, "latencies_ms": [2211.228], "images_per_second": 0.452, "prompt_tokens": 1442, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The child is wearing a pink and white plaid shirt, blue jeans, and a black helmet. The child is sitting on a brown leather saddle.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.56, "peak": 128.93, "min": 27.73}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.36, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 31.95, "peak": 42.53, "min": 19.71}}, "power_watts_avg": 31.95, "energy_joules_est": 70.66, "sample_count": 22, "duration_seconds": 2.212}, "timestamp": "2026-01-19T13:50:48.256423"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2125.123, "latencies_ms": [2125.123], "images_per_second": 0.471, "prompt_tokens": 1099, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a dynamic scene of three surfers riding the waves in the ocean, with the text \"Ranglin - New Zealand\" at the bottom, indicating the location of the surfing activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.9, "peak": 126.93, "min": 30.32}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.39, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.39, "energy_joules_est": 60.35, "sample_count": 21, "duration_seconds": 2.126}, "timestamp": "2026-01-19T13:50:50.457745"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2690.075, "latencies_ms": [2690.075], "images_per_second": 0.372, "prompt_tokens": 1113, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Surfboard: 1\n2. Surfer: 2\n3. Ocean: 1\n4. Wave: 1\n5. Photographer: 1\n6. Photograph: 1\n7. Frame: 1\n8. Text: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.85, "peak": 126.22, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.55, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.55, "energy_joules_est": 71.43, "sample_count": 26, "duration_seconds": 2.69}, "timestamp": "2026-01-19T13:50:53.176837"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1930.843, "latencies_ms": [1930.843], "images_per_second": 0.518, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The surfer is positioned on the left side of the image, with the wave on the right side. The surfer is closer to the camera than the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.32, "peak": 108.76, "min": 29.64}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.13, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.13, "energy_joules_est": 56.25, "sample_count": 19, "duration_seconds": 1.931}, "timestamp": "2026-01-19T13:50:55.173040"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1711.852, "latencies_ms": [1711.852], "images_per_second": 0.584, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " Two surfers are riding the waves in the ocean, one is closer to the camera and the other is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.38, "peak": 128.02, "min": 30.49}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.22, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 30.22, "energy_joules_est": 51.74, "sample_count": 17, "duration_seconds": 1.712}, "timestamp": "2026-01-19T13:50:56.948454"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2455.386, "latencies_ms": [2455.386], "images_per_second": 0.407, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image captures a dynamic scene of two surfers riding the waves in the ocean, with the water appearing deep blue and the sky clear. The surfers are dressed in wetsuits, and the sunlight reflects off the water, creating a shimmering effect.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.09, "peak": 124.67, "min": 30.39}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.55, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 27.55, "energy_joules_est": 67.66, "sample_count": 24, "duration_seconds": 2.456}, "timestamp": "2026-01-19T13:50:59.451414"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1873.262, "latencies_ms": [1873.262], "images_per_second": 0.534, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The image depicts a cozy kitchen with a large window that has frosted glass, a wooden cabinet, and a stove with pots and pans on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.75, "peak": 115.3, "min": 29.67}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 28.97, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.97, "energy_joules_est": 54.29, "sample_count": 19, "duration_seconds": 1.874}, "timestamp": "2026-01-19T13:51:01.435452"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2592.668, "latencies_ms": [2592.668], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. window: 1\n2. shelf: 2\n3. cabinet: 1\n4. stove: 1\n5. pot: 1\n6. plant: 1\n7. cupboard: 1\n8. shelf: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.48, "peak": 122.14, "min": 30.5}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.21}, "VDD_GPU": {"avg": 26.88, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 26.88, "energy_joules_est": 69.7, "sample_count": 25, "duration_seconds": 2.593}, "timestamp": "2026-01-19T13:51:04.041097"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2314.076, "latencies_ms": [2314.076], "images_per_second": 0.432, "prompt_tokens": 1117, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The window is located in the center of the image, with the wooden cabinet to the right and the stove to the left. The plants are placed on the windowsill, which is in the foreground, while the stove is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.37, "peak": 112.94, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.77, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.77, "energy_joules_est": 64.27, "sample_count": 23, "duration_seconds": 2.314}, "timestamp": "2026-01-19T13:51:06.444825"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2941.762, "latencies_ms": [2941.762], "images_per_second": 0.34, "prompt_tokens": 1111, "response_tokens_est": 65, "n_tiles": 1, "output_text": " The image captures a cozy kitchen scene with a large window that allows natural light to filter in, illuminating the room. The window is adorned with a colorful poster and a small plant, adding a touch of vibrancy to the space. The kitchen is equipped with various appliances and utensils, suggesting a well-used and functional space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.77, "peak": 126.14, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 25.16, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 25.16, "energy_joules_est": 74.02, "sample_count": 29, "duration_seconds": 2.942}, "timestamp": "2026-01-19T13:51:09.466017"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2102.136, "latencies_ms": [2102.136], "images_per_second": 0.476, "prompt_tokens": 1109, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The room is bathed in warm light, with a window that lets in a soft glow. The walls are painted a warm yellow, and the wooden cabinet is a rich, dark brown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.0, "peak": 123.49, "min": 27.18}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.88, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 27.88, "energy_joules_est": 58.62, "sample_count": 21, "duration_seconds": 2.103}, "timestamp": "2026-01-19T13:51:11.659893"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3416.227, "latencies_ms": [3416.227], "images_per_second": 0.293, "prompt_tokens": 1099, "response_tokens_est": 89, "n_tiles": 1, "output_text": " In the image, a vibrant red background serves as a backdrop for a traditional Chinese altar, adorned with a pineapple, a vase filled with red incense sticks, and a plate of oranges. The altar is meticulously arranged with a red plate holding the oranges, a white pedestal supporting the vase of incense sticks, and a red cup placed in front of the plate. The arrangement exudes a sense of reverence and tranquility, characteristic of Chinese cultural practices.", "error": null, "sys_before": {"cpu_percent": 7.1, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.39, "peak": 120.77, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 24.82, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 24.82, "energy_joules_est": 84.81, "sample_count": 33, "duration_seconds": 3.417}, "timestamp": "2026-01-19T13:51:15.133463"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1907.413, "latencies_ms": [1907.413], "images_per_second": 0.524, "prompt_tokens": 1113, "response_tokens_est": 32, "n_tiles": 1, "output_text": " pineapple: 1, red cups: 6, oranges: 4, candle: 1, incense sticks: 1, red plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.41, "peak": 122.38, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.09, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.09, "energy_joules_est": 55.5, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T13:51:17.109710"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2452.201, "latencies_ms": [2452.201], "images_per_second": 0.408, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The pineapple is positioned to the left of the incense holder, which is situated in the center of the image. The oranges are placed on a white pedestal to the right of the incense holder, while the red cups are arranged in a row in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.89, "peak": 131.35, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.33, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.33, "energy_joules_est": 67.03, "sample_count": 24, "duration_seconds": 2.453}, "timestamp": "2026-01-19T13:51:19.616065"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2099.103, "latencies_ms": [2099.103], "images_per_second": 0.476, "prompt_tokens": 1111, "response_tokens_est": 39, "n_tiles": 1, "output_text": " In this image, we can see a pineapple, a pot with some objects, a plate with some oranges, and some cups. We can also see a wall with some text written on it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.91, "peak": 129.14, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.25, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.25, "energy_joules_est": 59.31, "sample_count": 21, "duration_seconds": 2.099}, "timestamp": "2026-01-19T13:51:21.787924"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2410.863, "latencies_ms": [2410.863], "images_per_second": 0.415, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a vibrant red background with a pineapple and a plate of oranges. The lighting is bright and even, highlighting the colors and textures of the objects. The materials appear to be wood and ceramic, and the weather seems to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.04, "peak": 136.78, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.17, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 27.17, "energy_joules_est": 65.52, "sample_count": 24, "duration_seconds": 2.411}, "timestamp": "2026-01-19T13:51:24.296086"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1793.802, "latencies_ms": [1793.802], "images_per_second": 0.557, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A man wearing glasses and a beard is smiling while eating a large slice of pizza with a side of fries and a small cup of sauce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.56, "peak": 126.53, "min": 28.29}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.13, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.13, "energy_joules_est": 52.27, "sample_count": 18, "duration_seconds": 1.794}, "timestamp": "2026-01-19T13:51:26.192436"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2542.225, "latencies_ms": [2542.225], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. plate: 1\n2. fries: 1\n3. sandwich: 1\n4. sauce: 1\n5. man: 1\n6. clock: 1\n7. wall: 1\n8. table: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.2, "peak": 121.11, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.97, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 26.97, "energy_joules_est": 68.58, "sample_count": 25, "duration_seconds": 2.543}, "timestamp": "2026-01-19T13:51:28.803807"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1906.399, "latencies_ms": [1906.399], "images_per_second": 0.525, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The man is in the foreground, holding the plate of food. The fries are in the middle ground, and the background shows a restaurant setting with other patrons.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.8, "peak": 125.96, "min": 27.49}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.95, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.95, "energy_joules_est": 55.2, "sample_count": 19, "duration_seconds": 1.907}, "timestamp": "2026-01-19T13:51:30.799720"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1779.049, "latencies_ms": [1779.049], "images_per_second": 0.562, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man is eating a large slice of pizza with fries and dipping sauce. He is in a restaurant with other people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.12, "peak": 124.48, "min": 27.11}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.51, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 29.51, "energy_joules_est": 52.51, "sample_count": 18, "duration_seconds": 1.78}, "timestamp": "2026-01-19T13:51:32.680192"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1594.77, "latencies_ms": [1594.77], "images_per_second": 0.627, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The image is taken in a restaurant with warm lighting and the food is served on a white plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.31, "peak": 106.65, "min": 30.67}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.58, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 30.58, "energy_joules_est": 48.78, "sample_count": 16, "duration_seconds": 1.595}, "timestamp": "2026-01-19T13:51:34.351980"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2020.616, "latencies_ms": [2020.616], "images_per_second": 0.495, "prompt_tokens": 1100, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image captures a rainy day scene from inside a building, where the view outside is of a courtyard with a tall metal structure, bicycles parked, and people walking with umbrellas.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.85, "peak": 129.03, "min": 30.31}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.94, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 28.94, "energy_joules_est": 58.49, "sample_count": 20, "duration_seconds": 2.021}, "timestamp": "2026-01-19T13:51:36.450364"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2752.495, "latencies_ms": [2752.495], "images_per_second": 0.363, "prompt_tokens": 1114, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. Bike: 4\n2. Bike: 2\n3. Bike: 1\n4. Bike: 1\n5. Bike: 1\n6. Bike: 1\n7. Bike: 1\n8. Bike: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.44, "peak": 124.81, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.3, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.3, "energy_joules_est": 72.41, "sample_count": 27, "duration_seconds": 2.753}, "timestamp": "2026-01-19T13:51:39.258252"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1664.772, "latencies_ms": [1664.772], "images_per_second": 0.601, "prompt_tokens": 1118, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The bicycle is in the foreground, the building is in the background, and the person is in the middle ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.3, "peak": 124.97, "min": 32.93}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.39, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.39, "energy_joules_est": 50.61, "sample_count": 16, "duration_seconds": 1.665}, "timestamp": "2026-01-19T13:51:40.928208"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2018.543, "latencies_ms": [2018.543], "images_per_second": 0.495, "prompt_tokens": 1112, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image captures a rainy day at a university campus, with a large courtyard in the center. The courtyard is surrounded by modern buildings, and there are several bicycles parked in the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.13, "peak": 127.33, "min": 30.34}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.02, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 29.02, "energy_joules_est": 58.59, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T13:51:43.011766"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2861.391, "latencies_ms": [2861.391], "images_per_second": 0.349, "prompt_tokens": 1110, "response_tokens_est": 68, "n_tiles": 1, "output_text": " The image is taken from inside a building, looking out through a window. The weather outside is rainy, as evidenced by the wet ground and the people holding umbrellas. The colors in the image are muted due to the overcast sky, with the gray of the buildings and the wet ground providing a neutral backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.09, "peak": 113.08, "min": 30.06}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 25.95, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 25.95, "energy_joules_est": 74.27, "sample_count": 28, "duration_seconds": 2.862}, "timestamp": "2026-01-19T13:51:45.930580"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2477.25, "latencies_ms": [2477.25], "images_per_second": 0.404, "prompt_tokens": 1099, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image shows a close-up view of a plate with a creamy, yellow-colored pasta dish, possibly macaroni and cheese, topped with a generous amount of melted cheese that has a slightly glossy appearance, suggesting it has been recently cooked and is still warm.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.2, "peak": 128.94, "min": 29.53}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 27.15, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.15, "energy_joules_est": 67.28, "sample_count": 24, "duration_seconds": 2.478}, "timestamp": "2026-01-19T13:51:48.424030"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1671.093, "latencies_ms": [1671.093], "images_per_second": 0.598, "prompt_tokens": 1113, "response_tokens_est": 24, "n_tiles": 1, "output_text": " fork: 1, knife: 1, plate: 1, noodles: 1, cheese: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.07, "peak": 120.61, "min": 28.31}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.95, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.41, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 30.41, "energy_joules_est": 50.83, "sample_count": 17, "duration_seconds": 1.672}, "timestamp": "2026-01-19T13:51:50.197271"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2048.357, "latencies_ms": [2048.357], "images_per_second": 0.488, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The fork is located to the right of the plate, which is in the foreground of the image. The plate is placed on a table, which is in the background of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.64, "peak": 107.52, "min": 29.04}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.94, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.94, "energy_joules_est": 59.29, "sample_count": 20, "duration_seconds": 2.049}, "timestamp": "2026-01-19T13:51:52.290801"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3065.75, "latencies_ms": [3065.75], "images_per_second": 0.326, "prompt_tokens": 1111, "response_tokens_est": 76, "n_tiles": 1, "output_text": " In the image, there is a close-up view of a plate of food that appears to be a type of pasta dish. The pasta is covered in a creamy sauce and is topped with a generous amount of grated cheese. The plate is placed on a table, and in the background, there is a fork and knife, suggesting that the dish is ready to be served.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.07, "peak": 130.21, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.74, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 25.74, "energy_joules_est": 78.93, "sample_count": 30, "duration_seconds": 3.066}, "timestamp": "2026-01-19T13:51:55.411525"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2403.642, "latencies_ms": [2403.642], "images_per_second": 0.416, "prompt_tokens": 1109, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image features a close-up of a plate of macaroni and cheese, with a fork and knife placed on the plate. The macaroni and cheese is topped with a creamy yellow sauce, and the plate is placed on a dark surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.65, "peak": 116.7, "min": 28.16}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 27.14, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.14, "energy_joules_est": 65.25, "sample_count": 24, "duration_seconds": 2.404}, "timestamp": "2026-01-19T13:51:57.915971"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1667.22, "latencies_ms": [1667.22], "images_per_second": 0.6, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A silver laptop sits open on a desk with a green leaf wallpaper, a black mouse, and a black keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.79, "peak": 127.49, "min": 31.81}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.31, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.31, "energy_joules_est": 50.55, "sample_count": 16, "duration_seconds": 1.668}, "timestamp": "2026-01-19T13:51:59.592637"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2611.004, "latencies_ms": [2611.004], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. laptop: 1\n2. mouse: 1\n3. keyboard: 1\n4. monitor: 2\n5. speaker: 1\n6. mouse pad: 1\n7. computer: 1\n8. screen: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.28, "peak": 107.4, "min": 31.43}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 27.39, "peak": 40.16, "min": 18.91}}, "power_watts_avg": 27.39, "energy_joules_est": 71.53, "sample_count": 25, "duration_seconds": 2.611}, "timestamp": "2026-01-19T13:52:02.209165"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2229.197, "latencies_ms": [2229.197], "images_per_second": 0.449, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The laptop is in the foreground, with the mouse to its right. The keyboard is in front of the laptop, and the mousepad is in front of the keyboard. The computer monitor is to the left of the laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.41, "peak": 123.74, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.1, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.1, "energy_joules_est": 62.65, "sample_count": 22, "duration_seconds": 2.23}, "timestamp": "2026-01-19T13:52:04.506060"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1463.789, "latencies_ms": [1463.789], "images_per_second": 0.683, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A silver laptop computer is open on a desk with a green leaf wallpaper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.61, "peak": 127.28, "min": 28.35}, "VIN_SYS_5V0": {"avg": 14.15, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.94, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 30.94, "energy_joules_est": 45.3, "sample_count": 15, "duration_seconds": 1.464}, "timestamp": "2026-01-19T13:52:06.073276"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1746.362, "latencies_ms": [1746.362], "images_per_second": 0.573, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The laptop is silver and the mouse is black. The desk is white and the laptop is on the left side of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.52, "peak": 126.55, "min": 29.6}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.44, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.82, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 30.82, "energy_joules_est": 53.84, "sample_count": 17, "duration_seconds": 1.747}, "timestamp": "2026-01-19T13:52:07.852862"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1648.412, "latencies_ms": [1648.412], "images_per_second": 0.607, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A young girl is sitting on a bed in a bedroom with orange walls, playing with toys on the floor.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.4, "peak": 134.18, "min": 27.56}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.17, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 31.17, "energy_joules_est": 51.39, "sample_count": 16, "duration_seconds": 1.649}, "timestamp": "2026-01-19T13:52:09.534181"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.719, "latencies_ms": [2552.719], "images_per_second": 0.392, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bed: 1\n2. girl: 1\n3. lamp: 1\n4. nightstand: 1\n5. blanket: 1\n6. box: 1\n7. floor: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.73, "peak": 107.44, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.42, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.42, "energy_joules_est": 70.01, "sample_count": 25, "duration_seconds": 2.553}, "timestamp": "2026-01-19T13:52:12.140301"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2624.058, "latencies_ms": [2624.058], "images_per_second": 0.381, "prompt_tokens": 1117, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The bed is positioned to the left of the girl, who is sitting on it. The bed is in the foreground of the image, while the lamp and the clothes on the chair are in the background. The girl is sitting near the bed, while the clothes on the chair are farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.79, "peak": 110.37, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 26.55, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.55, "energy_joules_est": 69.68, "sample_count": 26, "duration_seconds": 2.625}, "timestamp": "2026-01-19T13:52:14.854025"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1702.837, "latencies_ms": [1702.837], "images_per_second": 0.587, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A young girl is sitting on a bed in a bedroom with orange walls. She is playing with a toy on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.78, "peak": 122.08, "min": 27.29}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.89, "peak": 39.0, "min": 14.98}}, "power_watts_avg": 29.89, "energy_joules_est": 50.91, "sample_count": 17, "duration_seconds": 1.703}, "timestamp": "2026-01-19T13:52:16.616017"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1995.001, "latencies_ms": [1995.001], "images_per_second": 0.501, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The room is painted in a warm orange color, and the floor is made of stone tiles. The lighting is natural, coming from a window on the left side of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.5, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.75, "peak": 107.7, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.06, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 29.06, "energy_joules_est": 57.99, "sample_count": 20, "duration_seconds": 1.995}, "timestamp": "2026-01-19T13:52:18.711209"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2152.981, "latencies_ms": [2152.981], "images_per_second": 0.464, "prompt_tokens": 1099, "response_tokens_est": 42, "n_tiles": 1, "output_text": " In the image, a baseball game is in progress with a batter in a red uniform and a catcher in a gray uniform, both in the midst of a swing, while the umpire stands nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.78, "peak": 127.73, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.25, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.25, "energy_joules_est": 60.83, "sample_count": 21, "duration_seconds": 2.153}, "timestamp": "2026-01-19T13:52:20.902172"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2669.14, "latencies_ms": [2669.14], "images_per_second": 0.375, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. baseball bat: 1\n2. baseball player: 1\n3. catcher: 1\n4. umpire: 1\n5. pitcher: 1\n6. baseball field: 1\n7. grass: 1\n8. dirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.84, "peak": 133.96, "min": 29.99}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.93, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.93, "energy_joules_est": 71.89, "sample_count": 26, "duration_seconds": 2.67}, "timestamp": "2026-01-19T13:52:23.588496"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1986.13, "latencies_ms": [1986.13], "images_per_second": 0.503, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire in the background. The batter is closer to the camera than the catcher and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.6, "peak": 119.13, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.86}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.92, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.92, "energy_joules_est": 57.45, "sample_count": 20, "duration_seconds": 1.987}, "timestamp": "2026-01-19T13:52:25.680463"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1718.555, "latencies_ms": [1718.555], "images_per_second": 0.582, "prompt_tokens": 1111, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A baseball game is taking place on a sunny day in a stadium with a batter, catcher, and umpire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.7, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.03, "peak": 128.87, "min": 28.49}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.04, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.04, "energy_joules_est": 51.64, "sample_count": 17, "duration_seconds": 1.719}, "timestamp": "2026-01-19T13:52:27.450976"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3367.021, "latencies_ms": [3367.021], "images_per_second": 0.297, "prompt_tokens": 1109, "response_tokens_est": 88, "n_tiles": 1, "output_text": " The image captures a moment of intense action in a baseball game, with the batter in the midst of a powerful swing, the catcher poised to react, and the umpire closely observing the play. The colors are vibrant, with the green of the field contrasting against the red of the batter's uniform and the black of the catcher's gear. The lighting is natural, casting sharp shadows and highlighting the dynamic movement of the players.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.41, "peak": 122.52, "min": 28.58}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 25.33, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 25.33, "energy_joules_est": 85.29, "sample_count": 33, "duration_seconds": 3.367}, "timestamp": "2026-01-19T13:52:30.876121"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1356.812, "latencies_ms": [1356.812], "images_per_second": 0.737, "prompt_tokens": 1099, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A cat is eating a bird on a concrete surface.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.42, "peak": 127.19, "min": 34.15}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.82, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 31.82, "energy_joules_est": 43.19, "sample_count": 13, "duration_seconds": 1.357}, "timestamp": "2026-01-19T13:52:32.241721"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2604.019, "latencies_ms": [2604.019], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. cat: 1\n2. bird: 1\n3. feathers: 1\n4. fur: 1\n5. concrete: 1\n6. leaves: 1\n7. grass: 1\n8. shadow: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.77, "peak": 119.84, "min": 33.39}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.0, "peak": 40.97, "min": 18.91}}, "power_watts_avg": 28.0, "energy_joules_est": 72.92, "sample_count": 25, "duration_seconds": 2.604}, "timestamp": "2026-01-19T13:52:34.851831"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2117.844, "latencies_ms": [2117.844], "images_per_second": 0.472, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The bird is in the foreground, with the cat's head in the middle ground. The cat's body is partially obscured by the bird, and the bird is positioned to the left of the cat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.43, "peak": 117.56, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.38, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 28.38, "energy_joules_est": 60.12, "sample_count": 21, "duration_seconds": 2.118}, "timestamp": "2026-01-19T13:52:37.041832"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1368.994, "latencies_ms": [1368.994], "images_per_second": 0.73, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A cat is eating a bird on a concrete surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.76, "peak": 124.38, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.38, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 31.38, "energy_joules_est": 42.97, "sample_count": 14, "duration_seconds": 1.369}, "timestamp": "2026-01-19T13:52:38.504107"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2399.649, "latencies_ms": [2399.649], "images_per_second": 0.417, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a cat with a white and gray coat, a black nose, and a pink tongue, standing on a concrete surface. The lighting is natural, and the cat is in the foreground, with a shadow cast on the ground behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.33, "peak": 121.04, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.21}, "VDD_GPU": {"avg": 27.96, "peak": 40.97, "min": 17.35}}, "power_watts_avg": 27.96, "energy_joules_est": 67.11, "sample_count": 24, "duration_seconds": 2.4}, "timestamp": "2026-01-19T13:52:41.000222"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1901.817, "latencies_ms": [1901.817], "images_per_second": 0.526, "prompt_tokens": 1432, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is holding a sandwich with lettuce, tomato, and cheese in their hand.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.55, "peak": 122.7, "min": 28.28}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 16.36, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 31.0, "peak": 41.74, "min": 14.98}}, "power_watts_avg": 31.0, "energy_joules_est": 58.97, "sample_count": 19, "duration_seconds": 1.902}, "timestamp": "2026-01-19T13:52:42.992934"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2956.558, "latencies_ms": [2956.558], "images_per_second": 0.338, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. hand: 1\n2. sandwich: 1\n3. bread: 1\n4. tomato: 1\n5. lettuce: 1\n6. cheese: 1\n7. stove: 1\n8. bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.2, "peak": 112.94, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 16.26, "min": 11.36}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 28.48, "peak": 42.13, "min": 18.14}}, "power_watts_avg": 28.48, "energy_joules_est": 84.21, "sample_count": 29, "duration_seconds": 2.957}, "timestamp": "2026-01-19T13:52:46.012092"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2280.123, "latencies_ms": [2280.123], "images_per_second": 0.439, "prompt_tokens": 1450, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The sandwich is held in the left hand, with the right hand holding the plate. The sandwich is in the foreground, while the stove is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.05, "peak": 132.44, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.36, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 30.68, "peak": 42.13, "min": 16.16}}, "power_watts_avg": 30.68, "energy_joules_est": 69.97, "sample_count": 22, "duration_seconds": 2.281}, "timestamp": "2026-01-19T13:52:48.312820"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1904.88, "latencies_ms": [1904.88], "images_per_second": 0.525, "prompt_tokens": 1444, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A person is holding a sandwich with lettuce, tomato, and cheese on a white cutting board.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.83, "peak": 122.05, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 32.65, "peak": 41.74, "min": 18.14}}, "power_watts_avg": 32.65, "energy_joules_est": 62.2, "sample_count": 19, "duration_seconds": 1.905}, "timestamp": "2026-01-19T13:52:50.295190"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2116.011, "latencies_ms": [2116.011], "images_per_second": 0.473, "prompt_tokens": 1442, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The sandwich is white, the bread is white, the tomato is red, the lettuce is green, and the cheese is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.18, "peak": 125.82, "min": 28.31}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 16.26, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.63, "peak": 42.53, "min": 18.53}}, "power_watts_avg": 31.63, "energy_joules_est": 66.95, "sample_count": 21, "duration_seconds": 2.117}, "timestamp": "2026-01-19T13:52:52.492472"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1523.273, "latencies_ms": [1523.273], "images_per_second": 0.656, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Three girls are sitting on the back of a boat, looking out at the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.28, "peak": 122.87, "min": 28.65}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.33, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 31.33, "energy_joules_est": 47.75, "sample_count": 15, "duration_seconds": 1.524}, "timestamp": "2026-01-19T13:52:54.061379"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2599.787, "latencies_ms": [2599.787], "images_per_second": 0.385, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. hat: 2\n2. lifebuoy: 1\n3. rope: 2\n4. sail: 1\n5. boat: 1\n6. person: 3\n7. ocean: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.77, "peak": 124.99, "min": 31.99}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 12.99}, "VDD_GPU": {"avg": 27.7, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 27.7, "energy_joules_est": 72.02, "sample_count": 25, "duration_seconds": 2.6}, "timestamp": "2026-01-19T13:52:56.668159"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2421.009, "latencies_ms": [2421.009], "images_per_second": 0.413, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The girl on the left is closer to the camera than the girl on the right. The girl on the right is farther away from the camera than the girl on the left. The girl on the right is closer to the water than the girl on the left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.6, "peak": 126.96, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 27.5, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.5, "energy_joules_est": 66.59, "sample_count": 24, "duration_seconds": 2.421}, "timestamp": "2026-01-19T13:52:59.170311"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1520.967, "latencies_ms": [1520.967], "images_per_second": 0.657, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " Three people are on a boat in the ocean, looking out at the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.62, "peak": 128.03, "min": 29.38}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.81, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.81, "energy_joules_est": 46.87, "sample_count": 15, "duration_seconds": 1.521}, "timestamp": "2026-01-19T13:53:00.732231"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2971.857, "latencies_ms": [2971.857], "images_per_second": 0.336, "prompt_tokens": 1109, "response_tokens_est": 72, "n_tiles": 1, "output_text": " The image captures a serene moment on a boat with three individuals, two of whom are seated on a lifebuoy, while the third stands on the deck. The sky is clear, and the water is a deep blue, reflecting the sunlight. The individuals are dressed in casual summer attire, and the lifebuoy is a stark contrast to the surrounding water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.21, "peak": 117.26, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.51, "peak": 40.97, "min": 18.13}}, "power_watts_avg": 26.51, "energy_joules_est": 78.8, "sample_count": 29, "duration_seconds": 2.972}, "timestamp": "2026-01-19T13:53:03.747734"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1792.928, "latencies_ms": [1792.928], "images_per_second": 0.558, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " In the image, a white sheep is seen standing on a grassy field, with a stone wall and yellow-green moss in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.95, "peak": 123.46, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 39.38, "min": 15.77}}, "power_watts_avg": 29.53, "energy_joules_est": 52.96, "sample_count": 18, "duration_seconds": 1.793}, "timestamp": "2026-01-19T13:53:05.639802"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2554.727, "latencies_ms": [2554.727], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. sheep: 1\n2. grass: 1\n3. wall: 1\n4. stone: 1\n5. ear: 1\n6. eye: 1\n7. nose: 1\n8. hoof: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.29, "peak": 128.17, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.04, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 27.04, "energy_joules_est": 69.09, "sample_count": 25, "duration_seconds": 2.555}, "timestamp": "2026-01-19T13:53:08.245196"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2067.497, "latencies_ms": [2067.497], "images_per_second": 0.484, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The sheep is located in the foreground of the image, standing on a grassy field. The stone wall is in the background, and the green bushes are situated to the left of the sheep.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.43, "peak": 115.85, "min": 29.34}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 28.76, "peak": 39.39, "min": 15.76}}, "power_watts_avg": 28.76, "energy_joules_est": 59.47, "sample_count": 20, "duration_seconds": 2.068}, "timestamp": "2026-01-19T13:53:10.332705"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1468.032, "latencies_ms": [1468.032], "images_per_second": 0.681, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A sheep is standing in a grassy field with a stone wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.65, "peak": 122.77, "min": 35.78}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.25, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 32.25, "energy_joules_est": 47.36, "sample_count": 14, "duration_seconds": 1.468}, "timestamp": "2026-01-19T13:53:11.806610"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1456.764, "latencies_ms": [1456.764], "images_per_second": 0.686, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The sheep is white, the grass is green, and the wall is gray.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.52, "peak": 115.04, "min": 32.04}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 33.51, "peak": 40.57, "min": 21.68}}, "power_watts_avg": 33.51, "energy_joules_est": 48.84, "sample_count": 14, "duration_seconds": 1.457}, "timestamp": "2026-01-19T13:53:13.273324"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1680.163, "latencies_ms": [1680.163], "images_per_second": 0.595, "prompt_tokens": 1099, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man in a white shirt and plaid shorts is standing next to a truck with a large black pipe on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.42, "peak": 114.99, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.37, "peak": 40.56, "min": 20.1}}, "power_watts_avg": 31.37, "energy_joules_est": 52.73, "sample_count": 17, "duration_seconds": 1.681}, "timestamp": "2026-01-19T13:53:15.053588"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2134.339, "latencies_ms": [2134.339], "images_per_second": 0.469, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " truck: 1, man: 2, man on truck: 1, man on ground: 1, container: 1, cable: 1, manhole cover: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.58, "peak": 120.64, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.48, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 28.48, "energy_joules_est": 60.8, "sample_count": 21, "duration_seconds": 2.135}, "timestamp": "2026-01-19T13:53:17.245481"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2232.419, "latencies_ms": [2232.419], "images_per_second": 0.448, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The man in the foreground is standing near the truck, while the man in the background is standing on top of the truck. The man on top of the truck is positioned to the left of the man in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.5, "ram_available_mb": 99764.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.8, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.38, "peak": 125.75, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.97, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 27.97, "energy_joules_est": 62.45, "sample_count": 22, "duration_seconds": 2.233}, "timestamp": "2026-01-19T13:53:19.537330"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1700.147, "latencies_ms": [1700.147], "images_per_second": 0.588, "prompt_tokens": 1111, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man in a white shirt and plaid shorts is standing next to a truck with a large black pipe on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.8, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.44, "peak": 122.37, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.22, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.22, "energy_joules_est": 51.39, "sample_count": 17, "duration_seconds": 1.7}, "timestamp": "2026-01-19T13:53:21.309256"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2034.565, "latencies_ms": [2034.565], "images_per_second": 0.492, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a man standing on the side of a truck, with a large black pipe on the back of the truck. The sky is overcast, and the lighting is natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.8, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.36, "peak": 113.99, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.06, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.06, "energy_joules_est": 59.14, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T13:53:23.401040"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1837.593, "latencies_ms": [1837.593], "images_per_second": 0.544, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " In the image, there are four giraffes walking in a line on a dirt path, with a pond nearby, and a deer sitting on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.6, "peak": 126.79, "min": 28.55}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.74, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.74, "energy_joules_est": 54.66, "sample_count": 18, "duration_seconds": 1.838}, "timestamp": "2026-01-19T13:53:25.288274"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2535.078, "latencies_ms": [2535.078], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. giraffe: 4\n2. giraffe: 1\n3. giraffe: 1\n4. giraffe: 1\n5. giraffe: 1\n6. giraffe: 1\n7. giraffe: 1\n8. giraffe: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.16, "peak": 123.69, "min": 31.02}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.26, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.26, "energy_joules_est": 69.12, "sample_count": 25, "duration_seconds": 2.536}, "timestamp": "2026-01-19T13:53:27.886392"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2025.385, "latencies_ms": [2025.385], "images_per_second": 0.494, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The giraffes are positioned in the foreground of the image, with the deer in the background. The pond is located to the left of the giraffes, and the trees are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26009.1, "ram_available_mb": 99763.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.28, "peak": 122.95, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.56, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 28.56, "energy_joules_est": 57.86, "sample_count": 20, "duration_seconds": 2.026}, "timestamp": "2026-01-19T13:53:29.984798"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2157.49, "latencies_ms": [2157.49], "images_per_second": 0.464, "prompt_tokens": 1111, "response_tokens_est": 42, "n_tiles": 1, "output_text": " In a serene setting, four giraffes are seen traversing a dirt path, their long necks reaching towards the sky. The backdrop is a lush expanse of trees and bushes, providing a natural habitat for these majestic creatures.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26009.1, "ram_available_mb": 99763.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.44, "peak": 126.49, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.27, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.27, "energy_joules_est": 61.0, "sample_count": 21, "duration_seconds": 2.158}, "timestamp": "2026-01-19T13:53:32.183956"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1539.189, "latencies_ms": [1539.189], "images_per_second": 0.65, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The giraffes are brown and white, the trees are green, and the sky is cloudy.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.66, "peak": 122.84, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.41, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.41, "energy_joules_est": 48.36, "sample_count": 15, "duration_seconds": 1.54}, "timestamp": "2026-01-19T13:53:33.739051"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1688.531, "latencies_ms": [1688.531], "images_per_second": 0.592, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A pizza with various toppings is on a white plate, with a glass of beer and a wine glass in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.19, "peak": 120.89, "min": 27.53}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.35, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 31.35, "energy_joules_est": 52.95, "sample_count": 17, "duration_seconds": 1.689}, "timestamp": "2026-01-19T13:53:35.519283"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2609.123, "latencies_ms": [2609.123], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. plate: 1\n2. pizza: 1\n3. wine glass: 2\n4. beer glass: 1\n5. fork: 1\n6. knife: 1\n7. tablecloth: 1\n8. chair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.63, "peak": 126.48, "min": 37.39}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.44, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.11, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.11, "energy_joules_est": 70.75, "sample_count": 25, "duration_seconds": 2.61}, "timestamp": "2026-01-19T13:53:38.137634"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2172.783, "latencies_ms": [2172.783], "images_per_second": 0.46, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The pizza is in the foreground, with the glasses of beer and the wine bottle in the background. The glasses of beer are on the right side of the pizza, while the wine bottle is on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.42, "peak": 126.39, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 28.57, "peak": 39.39, "min": 17.35}}, "power_watts_avg": 28.57, "energy_joules_est": 62.08, "sample_count": 21, "duration_seconds": 2.173}, "timestamp": "2026-01-19T13:53:40.335251"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1756.088, "latencies_ms": [1756.088], "images_per_second": 0.569, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " In a restaurant with tables and chairs, a pizza is served on a white plate with a glass of beer and a glass of water.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.79, "peak": 122.25, "min": 32.87}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.59, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.59, "energy_joules_est": 53.74, "sample_count": 17, "duration_seconds": 1.757}, "timestamp": "2026-01-19T13:53:42.096831"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2621.023, "latencies_ms": [2621.023], "images_per_second": 0.382, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image features a pizza with a variety of toppings, including ham, pineapple, and mushrooms, placed on a white plate. The pizza is served on a tablecloth with a blue and white pattern. The lighting in the room is warm, and the colors are vibrant, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.28, "peak": 123.27, "min": 27.37}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.95, "min": 13.06}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 27.12, "peak": 40.18, "min": 17.34}}, "power_watts_avg": 27.12, "energy_joules_est": 71.09, "sample_count": 26, "duration_seconds": 2.621}, "timestamp": "2026-01-19T13:53:44.811907"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1410.06, "latencies_ms": [1410.06], "images_per_second": 0.709, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black cat is drinking water from a faucet in a bathroom sink.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.76, "peak": 126.09, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.35, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.35, "energy_joules_est": 44.21, "sample_count": 14, "duration_seconds": 1.41}, "timestamp": "2026-01-19T13:53:46.268967"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1979.82, "latencies_ms": [1979.82], "images_per_second": 0.505, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " 1. black cat\n2. sink\n3. faucet\n4. water\n5. soap bottle\n6. soap dish\n7. wall\n8. window", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.1, "peak": 125.82, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.86, "peak": 40.57, "min": 18.52}}, "power_watts_avg": 29.86, "energy_joules_est": 59.13, "sample_count": 20, "duration_seconds": 1.98}, "timestamp": "2026-01-19T13:53:48.338872"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.165, "latencies_ms": [2395.165], "images_per_second": 0.418, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The cat is positioned to the right of the faucet, with the sink basin directly in front of it. The soap bottle is located to the left of the faucet, and the dish is situated to the right of the faucet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.36, "peak": 115.64, "min": 32.48}, "VIN_SYS_5V0": {"avg": 14.11, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 14.95, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.08, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.08, "energy_joules_est": 64.87, "sample_count": 23, "duration_seconds": 2.396}, "timestamp": "2026-01-19T13:53:50.739299"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1400.921, "latencies_ms": [1400.921], "images_per_second": 0.714, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black cat is drinking water from a faucet in a bathroom sink.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.19, "peak": 122.5, "min": 30.93}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.94, "peak": 39.39, "min": 17.35}}, "power_watts_avg": 31.94, "energy_joules_est": 44.76, "sample_count": 14, "duration_seconds": 1.401}, "timestamp": "2026-01-19T13:53:52.206142"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2662.899, "latencies_ms": [2662.899], "images_per_second": 0.376, "prompt_tokens": 1109, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image depicts a black cat drinking water from a faucet in a bathroom. The cat is positioned in front of a white sink, with a bottle of soap and a soap dish nearby. The lighting in the bathroom is bright, and the overall color scheme consists of white, black, and beige tones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.49, "peak": 130.9, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.47, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 27.47, "energy_joules_est": 73.16, "sample_count": 26, "duration_seconds": 2.663}, "timestamp": "2026-01-19T13:53:54.892729"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2438.752, "latencies_ms": [2438.752], "images_per_second": 0.41, "prompt_tokens": 1432, "response_tokens_est": 37, "n_tiles": 1, "output_text": " In the image, a man wearing a cowboy hat and a woman wearing a hat are riding in a horse-drawn carriage, with the horse pulling the carriage through a muddy field.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.08, "peak": 128.72, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.46, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.35, "peak": 42.13, "min": 16.16}}, "power_watts_avg": 29.35, "energy_joules_est": 71.6, "sample_count": 24, "duration_seconds": 2.439}, "timestamp": "2026-01-19T13:53:57.393205"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3216.046, "latencies_ms": [3216.046], "images_per_second": 0.311, "prompt_tokens": 1446, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. horse: 1\n2. carriage: 1\n3. man: 2\n4. woman: 1\n5. horse's tail: 1\n6. horse's mane: 1\n7. horse's hoof: 1\n8. horse's leg: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.7, "ram_available_mb": 99763.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.48, "peak": 117.75, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 27.71, "peak": 41.76, "min": 16.56}}, "power_watts_avg": 27.71, "energy_joules_est": 89.12, "sample_count": 31, "duration_seconds": 3.216}, "timestamp": "2026-01-19T13:54:00.626386"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2967.98, "latencies_ms": [2967.98], "images_per_second": 0.337, "prompt_tokens": 1450, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The horse is in the foreground, pulling the carriage, while the two people are in the background. The carriage is in the middle of the image, with the horse in front of it. The reflection of the carriage and horse can be seen in the puddle of water below them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.99, "peak": 109.47, "min": 29.18}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.46, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 28.31, "peak": 42.15, "min": 16.56}}, "power_watts_avg": 28.31, "energy_joules_est": 84.04, "sample_count": 29, "duration_seconds": 2.969}, "timestamp": "2026-01-19T13:54:03.650109"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2615.359, "latencies_ms": [2615.359], "images_per_second": 0.382, "prompt_tokens": 1444, "response_tokens_est": 44, "n_tiles": 1, "output_text": " A man and woman are riding in a horse-drawn carriage, with a horse pulling it. The carriage is in a muddy field, and there is a puddle of water reflecting the carriage and the people in it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.63, "peak": 109.72, "min": 27.89}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 29.03, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 29.03, "energy_joules_est": 75.95, "sample_count": 26, "duration_seconds": 2.616}, "timestamp": "2026-01-19T13:54:06.360378"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3011.896, "latencies_ms": [3011.896], "images_per_second": 0.332, "prompt_tokens": 1442, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The image features a muddy field with a horse-drawn carriage, reflecting the sky and surrounding trees. The sky is clear and blue, indicating a sunny day. The carriage is made of wood and has large spoked wheels, while the horse is brown with a black mane and tail.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.8, "ram_available_mb": 99764.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.56, "peak": 125.24, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 16.26, "min": 11.36}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 27.76, "peak": 41.74, "min": 15.38}}, "power_watts_avg": 27.76, "energy_joules_est": 83.62, "sample_count": 30, "duration_seconds": 3.012}, "timestamp": "2026-01-19T13:54:09.476918"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1691.88, "latencies_ms": [1691.88], "images_per_second": 0.591, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man and woman are standing under a white and black umbrella, with a stone building and other people in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.61, "peak": 121.32, "min": 29.06}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 29.55, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.55, "energy_joules_est": 50.02, "sample_count": 17, "duration_seconds": 1.693}, "timestamp": "2026-01-19T13:54:11.255299"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1955.49, "latencies_ms": [1955.49], "images_per_second": 0.511, "prompt_tokens": 1114, "response_tokens_est": 34, "n_tiles": 1, "output_text": " umbrella: 1, man: 1, woman: 1, flower: 1, grass: 1, stone: 1, house: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.65, "peak": 133.91, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.4, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.4, "energy_joules_est": 57.51, "sample_count": 19, "duration_seconds": 1.956}, "timestamp": "2026-01-19T13:54:13.236766"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2342.729, "latencies_ms": [2342.729], "images_per_second": 0.427, "prompt_tokens": 1118, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The bride is standing to the left of the groom, with the umbrella held above them. The bride is positioned closer to the camera than the groom. The umbrella is held by the groom, and the bride is holding a bouquet of flowers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.32, "peak": 129.8, "min": 29.89}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.9, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.9, "energy_joules_est": 65.38, "sample_count": 23, "duration_seconds": 2.343}, "timestamp": "2026-01-19T13:54:15.627324"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1511.915, "latencies_ms": [1511.915], "images_per_second": 0.661, "prompt_tokens": 1112, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A newlywed couple is standing under a black and white umbrella in a grassy area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.29, "peak": 119.09, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.46, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.46, "energy_joules_est": 46.07, "sample_count": 15, "duration_seconds": 1.512}, "timestamp": "2026-01-19T13:54:17.197433"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2708.032, "latencies_ms": [2708.032], "images_per_second": 0.369, "prompt_tokens": 1110, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image features a couple dressed in formal attire, with the bride wearing a white wedding dress and the groom in a black suit. The weather appears to be rainy, as evidenced by the presence of an umbrella held by the groom. The couple is standing on a grassy lawn, with a stone building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.8, "peak": 124.87, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.78, "peak": 40.57, "min": 16.95}}, "power_watts_avg": 26.78, "energy_joules_est": 72.53, "sample_count": 27, "duration_seconds": 2.709}, "timestamp": "2026-01-19T13:54:20.008244"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1466.083, "latencies_ms": [1466.083], "images_per_second": 0.682, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A man is lying on the beach with a kite flying in the sky.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26006.7, "ram_available_mb": 99765.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.02, "peak": 126.47, "min": 27.23}, "VIN_SYS_5V0": {"avg": 14.07, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.52, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 30.52, "energy_joules_est": 44.76, "sample_count": 15, "duration_seconds": 1.466}, "timestamp": "2026-01-19T13:54:21.578432"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2556.805, "latencies_ms": [2556.805], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 2\n2. legs: 2\n3. shorts: 2\n4. feet: 2\n5. sand: 1\n6. kite: 1\n7. kite string: 1\n8. ocean: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.64, "peak": 119.79, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.44, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.5, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 27.5, "energy_joules_est": 70.33, "sample_count": 25, "duration_seconds": 2.557}, "timestamp": "2026-01-19T13:54:24.180097"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2253.973, "latencies_ms": [2253.973], "images_per_second": 0.444, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The kite is in the foreground, flying high in the sky, while the person is in the background, lying on the beach. The person is near the kite, as they are both in the same location on the beach.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.74, "peak": 129.19, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.92, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.92, "energy_joules_est": 62.94, "sample_count": 22, "duration_seconds": 2.254}, "timestamp": "2026-01-19T13:54:26.477302"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1665.225, "latencies_ms": [1665.225], "images_per_second": 0.601, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man is laying on the beach with his legs up in the air, while another man is flying a kite.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.2, "peak": 127.21, "min": 31.9}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.8, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.8, "energy_joules_est": 51.3, "sample_count": 16, "duration_seconds": 1.666}, "timestamp": "2026-01-19T13:54:28.148302"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2651.035, "latencies_ms": [2651.035], "images_per_second": 0.377, "prompt_tokens": 1109, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image captures a serene beach scene with a man lying on the sand, wearing a white shirt and brown shorts, while another person is sitting on the sand, holding a colorful kite. The sky is clear with a few clouds, and the sun is shining brightly, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.92, "peak": 127.24, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.69, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.38, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.38, "energy_joules_est": 72.59, "sample_count": 26, "duration_seconds": 2.651}, "timestamp": "2026-01-19T13:54:30.849918"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2065.801, "latencies_ms": [2065.801], "images_per_second": 0.484, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image depicts a cozy living room with a brown sofa, a red armchair, a small table, and a television set, all arranged around a window with wooden shutters.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.01, "peak": 127.32, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.62, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.62, "energy_joules_est": 59.13, "sample_count": 20, "duration_seconds": 2.066}, "timestamp": "2026-01-19T13:54:32.934397"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1976.516, "latencies_ms": [1976.516], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 36, "n_tiles": 1, "output_text": " sofa: 1, chair: 1, television: 1, lamp: 1, table: 1, window: 2, floor lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.88, "peak": 113.69, "min": 28.28}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.86}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.13, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 29.13, "energy_joules_est": 57.59, "sample_count": 20, "duration_seconds": 1.977}, "timestamp": "2026-01-19T13:54:35.014718"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2880.634, "latencies_ms": [2880.634], "images_per_second": 0.347, "prompt_tokens": 1117, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The sofa is located to the left of the coffee table, which is in front of the television. The coffee table is situated in the middle of the room, with the sofa to its left and the television to its right. The lamp is positioned to the left of the television, and the chair is to the right of the television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.45, "peak": 119.11, "min": 29.13}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 26.1, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.1, "energy_joules_est": 75.19, "sample_count": 28, "duration_seconds": 2.881}, "timestamp": "2026-01-19T13:54:37.930231"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1484.783, "latencies_ms": [1484.783], "images_per_second": 0.673, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A living room with a brown couch, red chair, and a TV is shown.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.99, "peak": 124.78, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.23, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.23, "energy_joules_est": 46.38, "sample_count": 15, "duration_seconds": 1.485}, "timestamp": "2026-01-19T13:54:39.494409"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1570.675, "latencies_ms": [1570.675], "images_per_second": 0.637, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The room is well lit with natural light coming through the windows, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.46, "peak": 128.14, "min": 27.14}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.54, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.54, "energy_joules_est": 49.55, "sample_count": 16, "duration_seconds": 1.571}, "timestamp": "2026-01-19T13:54:41.153608"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2011.095, "latencies_ms": [2011.095], "images_per_second": 0.497, "prompt_tokens": 1432, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A bald man wearing glasses and a blue shirt is eating a slice of cake with a fork in a park.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 26008.8, "ram_available_mb": 99763.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.94, "peak": 112.91, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 16.15, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.42, "peak": 41.76, "min": 17.74}}, "power_watts_avg": 31.42, "energy_joules_est": 63.2, "sample_count": 20, "duration_seconds": 2.012}, "timestamp": "2026-01-19T13:54:43.252583"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2940.124, "latencies_ms": [2940.124], "images_per_second": 0.34, "prompt_tokens": 1446, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. plate: 1\n3. fork: 1\n4. spoon: 1\n5. cake: 1\n6. grass: 1\n7. tree: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.11, "peak": 125.08, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.26, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 28.42, "peak": 42.13, "min": 17.35}}, "power_watts_avg": 28.42, "energy_joules_est": 83.57, "sample_count": 29, "duration_seconds": 2.94}, "timestamp": "2026-01-19T13:54:46.286175"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2513.111, "latencies_ms": [2513.111], "images_per_second": 0.398, "prompt_tokens": 1450, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The man is in the foreground of the image, holding a plate of cake in his hands. The cake is in the middle of the image, with the man's face and the trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.45, "peak": 122.41, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.26, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.55, "peak": 41.76, "min": 15.38}}, "power_watts_avg": 29.55, "energy_joules_est": 74.27, "sample_count": 25, "duration_seconds": 2.514}, "timestamp": "2026-01-19T13:54:48.888551"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1879.273, "latencies_ms": [1879.273], "images_per_second": 0.532, "prompt_tokens": 1444, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A man wearing glasses and a blue shirt is eating a slice of cake in a park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.5, "peak": 125.0, "min": 32.08}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.26, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.59, "peak": 41.74, "min": 15.77}}, "power_watts_avg": 32.59, "energy_joules_est": 61.26, "sample_count": 18, "duration_seconds": 1.88}, "timestamp": "2026-01-19T13:54:50.770889"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2073.692, "latencies_ms": [2073.692], "images_per_second": 0.482, "prompt_tokens": 1442, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The man is wearing a blue shirt and is eating a slice of cake in a park with green grass and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.9, "ram_available_mb": 99765.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.54, "peak": 135.77, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.9, "peak": 16.46, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 33.01, "peak": 42.15, "min": 20.89}}, "power_watts_avg": 33.01, "energy_joules_est": 68.46, "sample_count": 20, "duration_seconds": 2.074}, "timestamp": "2026-01-19T13:54:52.856077"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1769.84, "latencies_ms": [1769.84], "images_per_second": 0.565, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man wearing a purple vest and gray pants is walking alongside a brown horse that is carrying a large load of luggage on its back.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.77, "peak": 119.79, "min": 37.53}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 30.94, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.94, "energy_joules_est": 54.77, "sample_count": 17, "duration_seconds": 1.77}, "timestamp": "2026-01-19T13:54:54.636056"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2510.942, "latencies_ms": [2510.942], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. horse: 1\n3. luggage: 1\n4. backpack: 1\n5. blanket: 1\n6. rope: 1\n7. tree: 1\n8. ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.0, "peak": 123.06, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.4, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.4, "energy_joules_est": 68.81, "sample_count": 25, "duration_seconds": 2.511}, "timestamp": "2026-01-19T13:54:57.243554"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2354.148, "latencies_ms": [2354.148], "images_per_second": 0.425, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The man is standing to the left of the donkey, which is positioned in the foreground of the image. The donkey is facing the camera, while the man is facing away from it. The man is standing closer to the camera than the donkey.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.19, "peak": 129.08, "min": 30.94}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.46, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.46, "energy_joules_est": 64.66, "sample_count": 23, "duration_seconds": 2.355}, "timestamp": "2026-01-19T13:54:59.635861"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1789.853, "latencies_ms": [1789.853], "images_per_second": 0.559, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A man is walking with a donkey that is carrying a large load of luggage. The man is wearing a purple vest and gray pants.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.61, "peak": 126.82, "min": 27.87}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 29.55, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 29.55, "energy_joules_est": 52.9, "sample_count": 18, "duration_seconds": 1.79}, "timestamp": "2026-01-19T13:55:01.514829"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1801.75, "latencies_ms": [1801.75], "images_per_second": 0.555, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image features a man walking alongside a brown horse with a colorful blanket on its back, and the scene is bathed in natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.0, "ram_available_mb": 99765.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.75, "peak": 124.7, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.77, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.77, "energy_joules_est": 53.66, "sample_count": 18, "duration_seconds": 1.803}, "timestamp": "2026-01-19T13:55:03.391902"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2121.291, "latencies_ms": [2121.291], "images_per_second": 0.471, "prompt_tokens": 766, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The image captures a vibrant night scene of a bridge adorned with blue lights, standing majestically over a river, with a boat gently floating on the water, and a crowd of people gathered on the riverbank, all bathed in the soft glow of the illuminated bridge and the surrounding lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5033.1, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.87, "peak": 91.51, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.01, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.15, "min": 13.0}, "VDD_GPU": {"avg": 24.71, "peak": 38.19, "min": 16.95}}, "power_watts_avg": 24.71, "energy_joules_est": 52.44, "sample_count": 21, "duration_seconds": 2.122}, "timestamp": "2026-01-19T13:55:05.583665"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2123.878, "latencies_ms": [2123.878], "images_per_second": 0.471, "prompt_tokens": 780, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. bridge: 3\n2. lights: 100\n3. people: 10\n4. boat: 1\n5. water: 1\n6. street: 1\n7. sky: 1\n8. clouds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.61, "peak": 122.4, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.04, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 24.43, "peak": 38.21, "min": 14.98}}, "power_watts_avg": 24.43, "energy_joules_est": 51.9, "sample_count": 21, "duration_seconds": 2.124}, "timestamp": "2026-01-19T13:55:07.774948"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1778.99, "latencies_ms": [1778.99], "images_per_second": 0.562, "prompt_tokens": 784, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The main objects in the image are the bridge and the river. The bridge is located in the background, while the river is in the foreground. The bridge is near the river, and the river is far from the bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 61.81, "peak": 97.88, "min": 27.34}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.04, "min": 12.25}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 25.24, "peak": 37.82, "min": 14.97}}, "power_watts_avg": 25.24, "energy_joules_est": 44.92, "sample_count": 18, "duration_seconds": 1.78}, "timestamp": "2026-01-19T13:55:09.647071"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2303.983, "latencies_ms": [2303.983], "images_per_second": 0.434, "prompt_tokens": 778, "response_tokens_est": 66, "n_tiles": 1, "output_text": " The image captures a vibrant night scene of a bridge adorned with blue lights, spanning across a river. The bridge, a marvel of engineering, is a spectacle in the dark sky, its lights reflecting off the water's surface. People can be seen strolling along the riverbank, adding a touch of life to the serene setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.8, "ram_available_mb": 99765.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.38, "peak": 120.97, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 23.97, "peak": 38.21, "min": 15.38}}, "power_watts_avg": 23.97, "energy_joules_est": 55.23, "sample_count": 23, "duration_seconds": 2.304}, "timestamp": "2026-01-19T13:55:12.044056"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1954.977, "latencies_ms": [1954.977], "images_per_second": 0.512, "prompt_tokens": 776, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image captures a vibrant night scene of a bridge adorned with blue lights, reflecting beautifully on the calm river below. The sky is a dark canvas, punctuated by a few clouds, while the city lights in the distance add a warm glow to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.38, "peak": 116.9, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.01, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 24.78, "peak": 37.82, "min": 14.59}}, "power_watts_avg": 24.78, "energy_joules_est": 48.45, "sample_count": 19, "duration_seconds": 1.955}, "timestamp": "2026-01-19T13:55:14.034225"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1506.023, "latencies_ms": [1506.023], "images_per_second": 0.664, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A person's foot is resting on a wooden surface with a pink shoe and blue jeans.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.68, "peak": 132.19, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.07, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.07, "energy_joules_est": 46.82, "sample_count": 15, "duration_seconds": 1.507}, "timestamp": "2026-01-19T13:55:15.613235"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2149.425, "latencies_ms": [2149.425], "images_per_second": 0.465, "prompt_tokens": 1113, "response_tokens_est": 40, "n_tiles": 1, "output_text": " 1. pink shoe\n2. blue jeans\n3. wooden surface\n4. pink bow\n5. blue paint\n6. green paint\n7. yellow paint\n8. black paint", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.37, "peak": 123.27, "min": 29.32}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.85, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 28.85, "energy_joules_est": 62.02, "sample_count": 21, "duration_seconds": 2.15}, "timestamp": "2026-01-19T13:55:17.808222"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2108.93, "latencies_ms": [2108.93], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The pink shoe is positioned on the left side of the image, while the blue jeans are on the right side. The pink shoe is in the foreground, while the blue jeans are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.04, "peak": 116.68, "min": 29.88}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.41, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.41, "energy_joules_est": 59.93, "sample_count": 21, "duration_seconds": 2.109}, "timestamp": "2026-01-19T13:55:19.994298"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1350.229, "latencies_ms": [1350.229], "images_per_second": 0.741, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A person is wearing a pink shoe and blue jeans.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 80.04, "peak": 122.83, "min": 33.48}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 32.03, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 32.03, "energy_joules_est": 43.26, "sample_count": 13, "duration_seconds": 1.351}, "timestamp": "2026-01-19T13:55:21.349261"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2456.555, "latencies_ms": [2456.555], "images_per_second": 0.407, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features a person wearing a pink high-heeled shoe with a bow on the toe, standing on a wooden floor with a blue and green paint job. The lighting is bright and the colors are vivid, giving the image a vibrant and lively appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.5, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.88, "peak": 132.76, "min": 30.67}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 28.71, "peak": 41.36, "min": 18.91}}, "power_watts_avg": 28.71, "energy_joules_est": 70.53, "sample_count": 24, "duration_seconds": 2.457}, "timestamp": "2026-01-19T13:55:23.832043"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1652.079, "latencies_ms": [1652.079], "images_per_second": 0.605, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A woman and a boy are standing in a room with a red wall, with a table in front of them.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.01, "peak": 117.62, "min": 27.58}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.68, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.68, "energy_joules_est": 50.71, "sample_count": 16, "duration_seconds": 1.653}, "timestamp": "2026-01-19T13:55:25.509229"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1964.988, "latencies_ms": [1964.988], "images_per_second": 0.509, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " woman: 1, boy: 1, mug: 1, plate: 1, cake: 1, chair: 1, door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.73, "peak": 120.81, "min": 32.47}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.11, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 30.11, "energy_joules_est": 59.17, "sample_count": 19, "duration_seconds": 1.965}, "timestamp": "2026-01-19T13:55:27.482399"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1881.732, "latencies_ms": [1881.732], "images_per_second": 0.531, "prompt_tokens": 1117, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The woman is standing to the left of the boy, and the table is in front of them. The boy is standing closer to the table than the woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.21, "peak": 128.75, "min": 27.15}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.67, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 29.67, "energy_joules_est": 55.85, "sample_count": 19, "duration_seconds": 1.882}, "timestamp": "2026-01-19T13:55:29.465288"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2025.116, "latencies_ms": [2025.116], "images_per_second": 0.494, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " A woman and a boy are standing in a room with a red wall. The woman is holding a knife and the boy is standing next to a table with a cake on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.98, "peak": 116.63, "min": 29.85}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.64, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.64, "energy_joules_est": 58.01, "sample_count": 20, "duration_seconds": 2.026}, "timestamp": "2026-01-19T13:55:31.551973"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1840.361, "latencies_ms": [1840.361], "images_per_second": 0.543, "prompt_tokens": 1109, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The image is taken in a room with a red wall and a white door. The lighting is natural, coming from a window out of frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.49, "peak": 127.53, "min": 30.17}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.75, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.75, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 29.75, "energy_joules_est": 54.76, "sample_count": 18, "duration_seconds": 1.841}, "timestamp": "2026-01-19T13:55:33.431763"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4187.643, "latencies_ms": [4187.643], "images_per_second": 0.239, "prompt_tokens": 1099, "response_tokens_est": 119, "n_tiles": 1, "output_text": " In the center of an indoor arena, three elephants are engaged in a performance, their trunks extended towards a red and yellow circular platform. The platform, adorned with a vibrant border, serves as the stage for this spectacle. The elephants, each distinct in color - one a rich brown, another a lighter shade of brown, and the third a gray - are harnessed with blue ropes, indicating their role in the performance. In the background, a photographer's tripod stands ready to capture the moment, while a disco ball hangs from the ceiling, adding a touch of whimsy to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.04, "peak": 116.39, "min": 28.93}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 24.02, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 24.02, "energy_joules_est": 100.6, "sample_count": 41, "duration_seconds": 4.188}, "timestamp": "2026-01-19T13:55:37.699812"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1980.086, "latencies_ms": [1980.086], "images_per_second": 0.505, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " elephant: 3, elephant: 3, elephant: 3, elephant: 3, elephant: 3, elephant: 3, elephant: 3", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.94, "peak": 106.58, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.33, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.33, "energy_joules_est": 56.11, "sample_count": 20, "duration_seconds": 1.981}, "timestamp": "2026-01-19T13:55:39.783154"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2203.592, "latencies_ms": [2203.592], "images_per_second": 0.454, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The elephants are positioned in the foreground of the image, with the camera and the audience in the background. The elephants are standing on a red and yellow striped circus ring, which is surrounded by a white fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.16, "peak": 109.26, "min": 27.92}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.74, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.74, "energy_joules_est": 61.15, "sample_count": 22, "duration_seconds": 2.204}, "timestamp": "2026-01-19T13:55:42.079855"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2401.482, "latencies_ms": [2401.482], "images_per_second": 0.416, "prompt_tokens": 1111, "response_tokens_est": 50, "n_tiles": 1, "output_text": " In an indoor circus arena, three elephants are performing a trick. The largest elephant is wearing a blue harness, while the other two are wearing red harnesses. The elephants are standing on a dirt floor, and there are bleachers in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.89, "peak": 128.13, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.97, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.97, "energy_joules_est": 64.78, "sample_count": 24, "duration_seconds": 2.402}, "timestamp": "2026-01-19T13:55:44.587481"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1962.24, "latencies_ms": [1962.24], "images_per_second": 0.51, "prompt_tokens": 1109, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The elephants are brown and gray, and the circus ring is red and yellow. The lighting is bright and artificial, and the elephants are standing on a dirt floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.31, "peak": 124.07, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.86, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.86, "energy_joules_est": 56.64, "sample_count": 19, "duration_seconds": 1.963}, "timestamp": "2026-01-19T13:55:46.565740"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2256.394, "latencies_ms": [2256.394], "images_per_second": 0.443, "prompt_tokens": 1099, "response_tokens_est": 46, "n_tiles": 1, "output_text": " In the sepia-toned photograph, a group of jockeys and their horses are captured mid-gallop on a beach, their bodies blurred due to the motion, with the sky and water serving as a backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.18, "peak": 116.12, "min": 28.48}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.28, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 28.28, "energy_joules_est": 63.83, "sample_count": 22, "duration_seconds": 2.257}, "timestamp": "2026-01-19T13:55:48.859860"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2196.774, "latencies_ms": [2196.774], "images_per_second": 0.455, "prompt_tokens": 1113, "response_tokens_est": 43, "n_tiles": 1, "output_text": " horse: 1\nrider: 1\nhorse: 1\nrider: 1\nhorse: 1\nrider: 1\nhorse: 1\nrider: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.5, "peak": 118.48, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 27.97, "peak": 39.77, "min": 16.55}}, "power_watts_avg": 27.97, "energy_joules_est": 61.46, "sample_count": 22, "duration_seconds": 2.197}, "timestamp": "2026-01-19T13:55:51.156847"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2274.548, "latencies_ms": [2274.548], "images_per_second": 0.44, "prompt_tokens": 1117, "response_tokens_est": 45, "n_tiles": 1, "output_text": " The horses are positioned in the foreground, with the riders slightly behind them. The riders are positioned in the middle ground, with the horses in the foreground. The background is the sky, which is visible above the horizon line.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.88, "peak": 109.31, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.72, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.72, "energy_joules_est": 63.06, "sample_count": 22, "duration_seconds": 2.275}, "timestamp": "2026-01-19T13:55:53.456898"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1537.256, "latencies_ms": [1537.256], "images_per_second": 0.651, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A group of people are riding horses on a beach, with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.97, "peak": 115.03, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.3, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.3, "energy_joules_est": 48.12, "sample_count": 15, "duration_seconds": 1.538}, "timestamp": "2026-01-19T13:55:55.023775"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1702.063, "latencies_ms": [1702.063], "images_per_second": 0.588, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The image is in black and white, with a sepia tone, and the horses are running on a wet beach.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.77, "peak": 130.4, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.08, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 31.08, "energy_joules_est": 52.91, "sample_count": 17, "duration_seconds": 1.703}, "timestamp": "2026-01-19T13:55:56.799399"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1692.06, "latencies_ms": [1692.06], "images_per_second": 0.591, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A person wearing a black jacket and a black helmet with blue goggles is talking on a cell phone in a snowy park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 59.7, "peak": 113.47, "min": 27.41}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.15, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 30.15, "energy_joules_est": 51.04, "sample_count": 17, "duration_seconds": 1.693}, "timestamp": "2026-01-19T13:55:58.579618"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2640.05, "latencies_ms": [2640.05], "images_per_second": 0.379, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. person: 1\n2. goggles: 1\n3. jacket: 1\n4. tree: 1\n5. snow: 1\n6. sun: 1\n7. tree trunk: 1\n8. ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.23, "peak": 130.79, "min": 28.62}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.49, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.49, "energy_joules_est": 69.95, "sample_count": 26, "duration_seconds": 2.641}, "timestamp": "2026-01-19T13:56:01.288523"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2050.533, "latencies_ms": [2050.533], "images_per_second": 0.488, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The person is standing in the foreground of the image, wearing a black jacket and a black helmet with goggles. The trees are in the background, and the snow is covering the ground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.7, "peak": 113.81, "min": 30.42}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.37, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.37, "energy_joules_est": 58.19, "sample_count": 20, "duration_seconds": 2.051}, "timestamp": "2026-01-19T13:56:03.381614"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1597.945, "latencies_ms": [1597.945], "images_per_second": 0.626, "prompt_tokens": 1112, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A man wearing a black jacket and a black helmet is talking on a cell phone in the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.13, "peak": 115.78, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.07, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.07, "energy_joules_est": 48.07, "sample_count": 16, "duration_seconds": 1.599}, "timestamp": "2026-01-19T13:56:05.051020"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1840.851, "latencies_ms": [1840.851], "images_per_second": 0.543, "prompt_tokens": 1110, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The person is wearing a black jacket and a black helmet with blue goggles. The sun is shining brightly, and there is snow on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.2, "ram_available_mb": 99765.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.8, "peak": 128.2, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 29.92, "energy_joules_est": 55.1, "sample_count": 18, "duration_seconds": 1.842}, "timestamp": "2026-01-19T13:56:06.927088"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1563.198, "latencies_ms": [1563.198], "images_per_second": 0.64, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A motorcycle is parked in a field with a green tent and a blue flag in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.03, "peak": 124.09, "min": 27.7}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.0, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.0, "energy_joules_est": 48.48, "sample_count": 16, "duration_seconds": 1.564}, "timestamp": "2026-01-19T13:56:08.601625"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2683.924, "latencies_ms": [2683.924], "images_per_second": 0.373, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. green tent: 1\n2. motorcycle: 1\n3. backpack: 1\n4. saddlebags: 1\n5. saddle: 1\n6. handlebars: 1\n7. seat: 1\n8. front wheel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.04, "peak": 117.21, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 26.84, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.84, "energy_joules_est": 72.05, "sample_count": 26, "duration_seconds": 2.684}, "timestamp": "2026-01-19T13:56:11.316402"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2437.548, "latencies_ms": [2437.548], "images_per_second": 0.41, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The motorcycle is positioned to the right of the tent, with the motorcycle being closer to the foreground and the tent being further back in the scene. The motorcycle is also positioned in front of the tent, with the motorcycle being closer to the viewer than the tent.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.4, "ram_available_mb": 99764.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.05, "peak": 118.71, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.25, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.25, "energy_joules_est": 66.44, "sample_count": 24, "duration_seconds": 2.438}, "timestamp": "2026-01-19T13:56:13.820372"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1435.621, "latencies_ms": [1435.621], "images_per_second": 0.697, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A motorcycle and a tent are parked in a field at sunset.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.19, "peak": 130.86, "min": 28.11}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.32, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.32, "energy_joules_est": 44.97, "sample_count": 14, "duration_seconds": 1.436}, "timestamp": "2026-01-19T13:56:15.287865"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1750.61, "latencies_ms": [1750.61], "images_per_second": 0.571, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The motorcycle is white and black, and the tent is green. The sun is setting, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.55, "peak": 124.18, "min": 29.28}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.38, "peak": 40.57, "min": 20.5}}, "power_watts_avg": 31.38, "energy_joules_est": 54.95, "sample_count": 17, "duration_seconds": 1.751}, "timestamp": "2026-01-19T13:56:17.068230"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1774.825, "latencies_ms": [1774.825], "images_per_second": 0.563, "prompt_tokens": 1099, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A black and white photo shows a steam locomotive with the number 67371 on it, and people standing near it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26006.5, "ram_available_mb": 99765.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.84, "peak": 126.79, "min": 32.86}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.7, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 30.7, "energy_joules_est": 54.51, "sample_count": 17, "duration_seconds": 1.776}, "timestamp": "2026-01-19T13:56:18.852203"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2577.292, "latencies_ms": [2577.292], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. steam locomotive: 1\n2. people: 4\n3. train cars: 3\n4. platform: 1\n5. buildings: 2\n6. smoke: 1\n7. tracks: 1\n8. sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.42, "peak": 126.91, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.56, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 27.56, "energy_joules_est": 71.04, "sample_count": 25, "duration_seconds": 2.578}, "timestamp": "2026-01-19T13:56:21.452305"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2190.894, "latencies_ms": [2190.894], "images_per_second": 0.456, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The train is on the left side of the image, with the platform on the right. The passengers are standing on the platform, near the train. The buildings are in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.21, "peak": 117.53, "min": 28.07}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.97, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.97, "energy_joules_est": 61.29, "sample_count": 22, "duration_seconds": 2.191}, "timestamp": "2026-01-19T13:56:23.749653"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1482.023, "latencies_ms": [1482.023], "images_per_second": 0.675, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A black and white photo of a train with people standing next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.49, "peak": 120.93, "min": 27.69}, "VIN_SYS_5V0": {"avg": 14.03, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.6, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.6, "energy_joules_est": 45.36, "sample_count": 15, "duration_seconds": 1.482}, "timestamp": "2026-01-19T13:56:25.320360"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1426.189, "latencies_ms": [1426.189], "images_per_second": 0.701, "prompt_tokens": 1109, "response_tokens_est": 13, "n_tiles": 1, "output_text": " The train is black and white, and the smoke is white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.45, "peak": 120.75, "min": 27.61}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 32.25, "peak": 40.57, "min": 18.53}}, "power_watts_avg": 32.25, "energy_joules_est": 46.01, "sample_count": 14, "duration_seconds": 1.427}, "timestamp": "2026-01-19T13:56:26.781324"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1986.105, "latencies_ms": [1986.105], "images_per_second": 0.503, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image captures a bustling street scene in a densely populated urban area, where numerous signs and banners in Chinese characters are suspended from the buildings, creating a vibrant and chaotic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.9, "peak": 128.64, "min": 27.09}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.79, "peak": 40.97, "min": 18.14}}, "power_watts_avg": 29.79, "energy_joules_est": 59.18, "sample_count": 20, "duration_seconds": 1.986}, "timestamp": "2026-01-19T13:56:28.870153"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2822.634, "latencies_ms": [2822.634], "images_per_second": 0.354, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. sign: 10\n2. building: 1\n3. window: 1\n4. air conditioner: 1\n5. power line: 1\n6. street sign: 1\n7. pole: 1\n8. wire: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.1, "peak": 127.34, "min": 27.69}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.37, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 25.37, "energy_joules_est": 71.62, "sample_count": 28, "duration_seconds": 2.823}, "timestamp": "2026-01-19T13:56:31.796026"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2017.697, "latencies_ms": [2017.697], "images_per_second": 0.496, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The main objects are the signs and the buildings. The signs are in the foreground and the buildings are in the background. The signs are closer to the viewer than the buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.3, "peak": 125.76, "min": 28.01}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.54, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 14.98, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 28.09, "peak": 39.39, "min": 13.8}}, "power_watts_avg": 28.09, "energy_joules_est": 56.69, "sample_count": 20, "duration_seconds": 2.018}, "timestamp": "2026-01-19T13:56:33.889370"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1895.476, "latencies_ms": [1895.476], "images_per_second": 0.528, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image captures a bustling street in a densely populated urban area, where numerous signs and banners are suspended from the buildings, creating a vibrant and chaotic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.9, "peak": 105.28, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.99, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.99, "energy_joules_est": 54.96, "sample_count": 19, "duration_seconds": 1.896}, "timestamp": "2026-01-19T13:56:35.876092"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1829.225, "latencies_ms": [1829.225], "images_per_second": 0.547, "prompt_tokens": 1109, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The image is in black and white, with a high contrast of light and dark. The buildings are made of concrete and have many windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.44, "peak": 104.6, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.42, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.42, "energy_joules_est": 53.83, "sample_count": 18, "duration_seconds": 1.83}, "timestamp": "2026-01-19T13:56:37.751093"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1905.849, "latencies_ms": [1905.849], "images_per_second": 0.525, "prompt_tokens": 1100, "response_tokens_est": 32, "n_tiles": 1, "output_text": " A man is lying on the grass by the water, while a sign with the word \"CLOSED\" is lying on the ground in front of him.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.18, "peak": 115.59, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 29.14, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 29.14, "energy_joules_est": 55.55, "sample_count": 19, "duration_seconds": 1.906}, "timestamp": "2026-01-19T13:56:39.744068"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2587.097, "latencies_ms": [2587.097], "images_per_second": 0.387, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. grass: 1\n3. lake: 1\n4. wooden board: 1\n5. sign: 1\n6. trash: 1\n7. water: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26005.9, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.71, "peak": 120.21, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.89, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.89, "energy_joules_est": 69.58, "sample_count": 25, "duration_seconds": 2.588}, "timestamp": "2026-01-19T13:56:42.347141"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2450.007, "latencies_ms": [2450.007], "images_per_second": 0.408, "prompt_tokens": 1118, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The sign is in the foreground, close to the camera, and the man is in the background, far away from the camera. The wooden planks are in the foreground, close to the camera, and the lake is in the background, far away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.06, "peak": 124.14, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.25, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.25, "energy_joules_est": 66.79, "sample_count": 24, "duration_seconds": 2.451}, "timestamp": "2026-01-19T13:56:44.851711"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2078.061, "latencies_ms": [2078.061], "images_per_second": 0.481, "prompt_tokens": 1112, "response_tokens_est": 38, "n_tiles": 1, "output_text": " A man is sunbathing on a concrete ledge overlooking a lake. In the foreground, there is a sign that reads \"SENSORI\" and \"CROSSING\".", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.56, "peak": 128.55, "min": 31.87}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.4}, "VDD_GPU": {"avg": 28.07, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.07, "energy_joules_est": 58.34, "sample_count": 20, "duration_seconds": 2.079}, "timestamp": "2026-01-19T13:56:46.940278"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2679.841, "latencies_ms": [2679.841], "images_per_second": 0.373, "prompt_tokens": 1110, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image features a man lounging on a concrete ledge by a body of water, with a sign in the foreground that reads \"SENSORI\" and a wooden plank with a red and white sign that reads \"CROSS\". The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.16, "peak": 115.88, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 26.78, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 26.78, "energy_joules_est": 71.78, "sample_count": 26, "duration_seconds": 2.68}, "timestamp": "2026-01-19T13:56:49.649113"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2022.541, "latencies_ms": [2022.541], "images_per_second": 0.494, "prompt_tokens": 1100, "response_tokens_est": 37, "n_tiles": 1, "output_text": " In the image, a brown horse and a golden retriever are standing in a field of tall grass, with the horse looking down at the dog, who is panting happily.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.01, "peak": 120.78, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.56, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.56, "energy_joules_est": 57.79, "sample_count": 20, "duration_seconds": 2.023}, "timestamp": "2026-01-19T13:56:51.744153"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2211.939, "latencies_ms": [2211.939], "images_per_second": 0.452, "prompt_tokens": 1114, "response_tokens_est": 43, "n_tiles": 1, "output_text": " horse: 1, dog: 1, grass: 1, horse's mane: 1, horse's tail: 1, horse's ear: 1, horse's eye: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.27, "peak": 122.09, "min": 28.9}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.76, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.76, "energy_joules_est": 61.42, "sample_count": 22, "duration_seconds": 2.212}, "timestamp": "2026-01-19T13:56:54.041572"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2873.931, "latencies_ms": [2873.931], "images_per_second": 0.348, "prompt_tokens": 1118, "response_tokens_est": 69, "n_tiles": 1, "output_text": " The brown horse is positioned to the left of the golden retriever, with the horse's head slightly above the dog's. The golden retriever is in the foreground, with the horse's body extending into the background. The horse is closer to the camera than the dog, and the dog is closer to the viewer than the horse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.06, "peak": 116.29, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 25.93, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 25.93, "energy_joules_est": 74.53, "sample_count": 28, "duration_seconds": 2.874}, "timestamp": "2026-01-19T13:56:56.944595"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2268.239, "latencies_ms": [2268.239], "images_per_second": 0.441, "prompt_tokens": 1112, "response_tokens_est": 46, "n_tiles": 1, "output_text": " In a serene field, a brown horse and a golden retriever are enjoying a sunny day. The horse, with its mane and tail flowing in the wind, stands next to the dog, who is panting happily.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.42, "peak": 108.93, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 27.42, "peak": 39.0, "min": 16.16}}, "power_watts_avg": 27.42, "energy_joules_est": 62.21, "sample_count": 22, "duration_seconds": 2.269}, "timestamp": "2026-01-19T13:56:59.241934"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2425.177, "latencies_ms": [2425.177], "images_per_second": 0.412, "prompt_tokens": 1110, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image features a brown horse and a golden retriever in a grassy field, with the horse's mane and tail being a darker shade of brown. The lighting in the image is natural and bright, suggesting it was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.3, "ram_available_mb": 99764.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.79, "peak": 110.44, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.92, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 26.92, "energy_joules_est": 65.3, "sample_count": 24, "duration_seconds": 2.426}, "timestamp": "2026-01-19T13:57:01.745305"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2944.89, "latencies_ms": [2944.89], "images_per_second": 0.34, "prompt_tokens": 1099, "response_tokens_est": 71, "n_tiles": 1, "output_text": " In the image, a group of people are gathered in a gymnasium, playing volleyball. The gymnasium has a blue floor with white lines marking the boundaries of the court. The players are dressed in athletic clothing, with one person in the foreground wearing a blue shirt and shorts, and another person in the background wearing a green shirt and shorts.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26007.5, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.66, "peak": 129.3, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 25.64, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 25.64, "energy_joules_est": 75.52, "sample_count": 29, "duration_seconds": 2.945}, "timestamp": "2026-01-19T13:57:04.776024"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2184.673, "latencies_ms": [2184.673], "images_per_second": 0.458, "prompt_tokens": 1113, "response_tokens_est": 42, "n_tiles": 1, "output_text": " volleyball: 1\nvolleyball net: 1\nperson: 1\nperson: 1\nperson: 1\nperson: 1\nperson: 1\nperson: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.3, "peak": 122.15, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.09, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.09, "energy_joules_est": 61.38, "sample_count": 21, "duration_seconds": 2.185}, "timestamp": "2026-01-19T13:57:06.971931"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2135.136, "latencies_ms": [2135.136], "images_per_second": 0.468, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The volleyball court is located in the foreground of the image, with the players positioned in the middle ground. The walls of the gymnasium are in the background, and the ceiling is above the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.29, "peak": 115.73, "min": 29.31}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.66, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 28.66, "energy_joules_est": 61.21, "sample_count": 21, "duration_seconds": 2.136}, "timestamp": "2026-01-19T13:57:09.150774"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1416.58, "latencies_ms": [1416.58], "images_per_second": 0.706, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A group of people are playing volleyball in a gymnasium.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 82.46, "peak": 118.47, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.85, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.69, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 31.69, "energy_joules_est": 44.91, "sample_count": 14, "duration_seconds": 1.417}, "timestamp": "2026-01-19T13:57:10.611275"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2098.342, "latencies_ms": [2098.342], "images_per_second": 0.477, "prompt_tokens": 1109, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The indoor volleyball court is blue with white lines, and the ceiling is white with brown acoustic panels. The lighting is bright and natural, coming from the windows on the right side of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.32, "peak": 126.44, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.45, "peak": 41.36, "min": 18.14}}, "power_watts_avg": 29.45, "energy_joules_est": 61.82, "sample_count": 21, "duration_seconds": 2.099}, "timestamp": "2026-01-19T13:57:12.804379"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2390.016, "latencies_ms": [2390.016], "images_per_second": 0.418, "prompt_tokens": 1099, "response_tokens_est": 51, "n_tiles": 1, "output_text": " In the image, a herd of zebras and wildebeests graze on the lush green grass in a vast field, while a flock of pink flamingos wades in the water in the background, creating a picturesque scene of wildlife cohabitation.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.58, "peak": 119.51, "min": 27.72}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.09, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 27.09, "energy_joules_est": 64.76, "sample_count": 24, "duration_seconds": 2.39}, "timestamp": "2026-01-19T13:57:15.306666"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1939.83, "latencies_ms": [1939.83], "images_per_second": 0.516, "prompt_tokens": 1113, "response_tokens_est": 33, "n_tiles": 1, "output_text": " zebra: 4\nflamingo: 100\ngrass: 100\nwater: 100\nmountain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.23, "peak": 116.32, "min": 29.33}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.89, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.89, "energy_joules_est": 56.05, "sample_count": 19, "duration_seconds": 1.94}, "timestamp": "2026-01-19T13:57:17.296437"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2226.623, "latencies_ms": [2226.623], "images_per_second": 0.449, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the flamingos in the background. The zebras are closer to the camera than the flamingos, which are situated in the middle ground of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.01, "peak": 119.98, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.22, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.22, "energy_joules_est": 62.85, "sample_count": 22, "duration_seconds": 2.227}, "timestamp": "2026-01-19T13:57:19.588666"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2497.705, "latencies_ms": [2497.705], "images_per_second": 0.4, "prompt_tokens": 1111, "response_tokens_est": 54, "n_tiles": 1, "output_text": " In the image, a group of zebras and wildebeests are grazing in a grassy field, while a flock of pink flamingos wades in the water behind them. The scene is set in a savanna-like environment with a mountain range in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.43, "peak": 127.54, "min": 27.69}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.75, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 26.81, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 26.81, "energy_joules_est": 66.97, "sample_count": 25, "duration_seconds": 2.498}, "timestamp": "2026-01-19T13:57:22.189272"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2238.137, "latencies_ms": [2238.137], "images_per_second": 0.447, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features a vibrant scene with a mix of green grass, black and white zebras, and pink flamingos. The lighting is natural and bright, suggesting a sunny day, and the colors are vivid and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.88, "peak": 128.55, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.65, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.65, "energy_joules_est": 61.9, "sample_count": 22, "duration_seconds": 2.239}, "timestamp": "2026-01-19T13:57:24.473544"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1583.894, "latencies_ms": [1583.894], "images_per_second": 0.631, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A ginger and white cat is sitting on a wooden deck and looking at its reflection in a window.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.47, "peak": 119.39, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.49, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.49, "energy_joules_est": 48.3, "sample_count": 16, "duration_seconds": 1.584}, "timestamp": "2026-01-19T13:57:26.157272"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2843.538, "latencies_ms": [2843.538], "images_per_second": 0.352, "prompt_tokens": 1113, "response_tokens_est": 67, "n_tiles": 1, "output_text": " 1. cat: 1\n2. glass: 1\n3. wooden surface: 1\n4. cat's reflection: 1\n5. cat's fur: 1\n6. cat's eyes: 1\n7. cat's nose: 1\n8. cat's mouth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.79, "peak": 120.52, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.38, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.38, "energy_joules_est": 75.02, "sample_count": 28, "duration_seconds": 2.844}, "timestamp": "2026-01-19T13:57:29.064669"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1941.378, "latencies_ms": [1941.378], "images_per_second": 0.515, "prompt_tokens": 1117, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The cat is in the foreground, looking at its reflection in the window. The window is in the middle ground, and the cat is positioned on a wooden deck.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.84, "peak": 123.09, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.97, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.97, "energy_joules_est": 56.26, "sample_count": 19, "duration_seconds": 1.942}, "timestamp": "2026-01-19T13:57:31.050075"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1599.775, "latencies_ms": [1599.775], "images_per_second": 0.625, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A ginger and white cat is sitting on a wooden deck, looking at its reflection in a window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.36, "peak": 104.52, "min": 28.59}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.85, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.93, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 30.93, "energy_joules_est": 49.49, "sample_count": 16, "duration_seconds": 1.6}, "timestamp": "2026-01-19T13:57:32.714566"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1525.614, "latencies_ms": [1525.614], "images_per_second": 0.655, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The cat is orange and white, and the photo is taken in a cold environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.4, "peak": 124.04, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.7, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 31.7, "energy_joules_est": 48.38, "sample_count": 15, "duration_seconds": 1.526}, "timestamp": "2026-01-19T13:57:34.280403"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1911.456, "latencies_ms": [1911.456], "images_per_second": 0.523, "prompt_tokens": 1099, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image captures a serene lakeside scene with boats docked at a pier, a bustling marina, and a picturesque backdrop of mountains under a hazy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.08, "peak": 131.33, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.19, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 30.19, "energy_joules_est": 57.73, "sample_count": 19, "duration_seconds": 1.912}, "timestamp": "2026-01-19T13:57:36.259928"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2545.338, "latencies_ms": [2545.338], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. Boat: 3\n2. Boat: 2\n3. Boat: 1\n4. Boat: 1\n5. Boat: 1\n6. Boat: 1\n7. Boat: 1\n8. Boat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.17, "peak": 125.65, "min": 28.73}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.08, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.08, "energy_joules_est": 68.94, "sample_count": 25, "duration_seconds": 2.546}, "timestamp": "2026-01-19T13:57:38.868270"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1833.315, "latencies_ms": [1833.315], "images_per_second": 0.545, "prompt_tokens": 1117, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The boats are in the foreground, with the city in the background. The boats are near the shore, while the city is far away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.59, "peak": 124.3, "min": 27.27}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.4, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.4, "energy_joules_est": 53.91, "sample_count": 18, "duration_seconds": 1.834}, "timestamp": "2026-01-19T13:57:40.747130"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2048.085, "latencies_ms": [2048.085], "images_per_second": 0.488, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image captures a serene lakeside scene where boats are docked at a pier, with a backdrop of a town nestled amidst lush greenery and distant mountains under a hazy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.96, "peak": 122.77, "min": 31.14}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.44, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.74, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 28.74, "energy_joules_est": 58.88, "sample_count": 20, "duration_seconds": 2.049}, "timestamp": "2026-01-19T13:57:42.842545"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1617.162, "latencies_ms": [1617.162], "images_per_second": 0.618, "prompt_tokens": 1109, "response_tokens_est": 21, "n_tiles": 1, "output_text": " The sky is overcast, the water is a deep blue, and the boats are white and black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.89, "peak": 130.75, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 30.76, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.76, "energy_joules_est": 49.77, "sample_count": 16, "duration_seconds": 1.618}, "timestamp": "2026-01-19T13:57:44.507584"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2140.291, "latencies_ms": [2140.291], "images_per_second": 0.467, "prompt_tokens": 1099, "response_tokens_est": 41, "n_tiles": 1, "output_text": " In the black and white photo, a man is standing next to his bicycle, holding the handlebars, while a woman is walking in the background, and a man is riding a bicycle in the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.66, "peak": 126.05, "min": 28.75}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.78, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 28.78, "energy_joules_est": 61.61, "sample_count": 21, "duration_seconds": 2.141}, "timestamp": "2026-01-19T13:57:46.702228"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2616.346, "latencies_ms": [2616.346], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. man: 1\n2. bicycle: 1\n3. umbrella: 1\n4. building: 1\n5. sign: 2\n6. street light: 1\n7. power lines: 1\n8. air conditioner: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.96, "peak": 113.7, "min": 28.37}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 26.73, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.73, "energy_joules_est": 69.95, "sample_count": 26, "duration_seconds": 2.617}, "timestamp": "2026-01-19T13:57:49.408157"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2418.136, "latencies_ms": [2418.136], "images_per_second": 0.414, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The man is standing on the sidewalk, with the bicycle in front of him. The bicycle is parked on the sidewalk, and the man is standing next to it. The man is standing close to the bicycle, and the bicycle is close to the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.9, "peak": 120.21, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.02, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.02, "energy_joules_est": 65.35, "sample_count": 24, "duration_seconds": 2.419}, "timestamp": "2026-01-19T13:57:51.913103"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1510.698, "latencies_ms": [1510.698], "images_per_second": 0.662, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A man is standing next to his bicycle in a street with shops and signs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.04, "peak": 120.46, "min": 28.05}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.78, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.78, "energy_joules_est": 46.51, "sample_count": 15, "duration_seconds": 1.511}, "timestamp": "2026-01-19T13:57:53.478005"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2026.756, "latencies_ms": [2026.756], "images_per_second": 0.493, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image is in black and white, with the exception of the man's shirt, which is white. The lighting is natural, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.15, "peak": 135.72, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.37, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 29.37, "energy_joules_est": 59.53, "sample_count": 20, "duration_seconds": 2.027}, "timestamp": "2026-01-19T13:57:55.565493"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2520.596, "latencies_ms": [2520.596], "images_per_second": 0.397, "prompt_tokens": 1432, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a fruit stand, where bunches of ripe bananas are hanging from the ceiling, their yellow hue contrasting with the blue and white tarp that serves as the backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.05, "peak": 134.58, "min": 29.79}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.36, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 29.0, "peak": 42.15, "min": 16.16}}, "power_watts_avg": 29.0, "energy_joules_est": 73.12, "sample_count": 25, "duration_seconds": 2.521}, "timestamp": "2026-01-19T13:57:58.169025"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2482.011, "latencies_ms": [2482.011], "images_per_second": 0.403, "prompt_tokens": 1446, "response_tokens_est": 39, "n_tiles": 1, "output_text": " banana: 100, bunches: 100, cloth: 1, metal: 1, wall: 1, door: 1, roof: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.05, "peak": 125.68, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 16.26, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.97, "peak": 41.76, "min": 16.16}}, "power_watts_avg": 29.97, "energy_joules_est": 74.4, "sample_count": 24, "duration_seconds": 2.482}, "timestamp": "2026-01-19T13:58:00.672261"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2791.498, "latencies_ms": [2791.498], "images_per_second": 0.358, "prompt_tokens": 1450, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The bananas are hanging from the ceiling, with the ones in the front being closer to the camera than those in the back. The bananas are arranged in rows, with the ones in the front row being closer to the viewer than those in the back row.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.15, "peak": 132.35, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.17, "peak": 42.13, "min": 17.35}}, "power_watts_avg": 29.17, "energy_joules_est": 81.44, "sample_count": 27, "duration_seconds": 2.792}, "timestamp": "2026-01-19T13:58:03.478672"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2826.347, "latencies_ms": [2826.347], "images_per_second": 0.354, "prompt_tokens": 1444, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a fruit stand, where bunches of ripe bananas are hanging from the ceiling, ready to be purchased. The stand is situated in a bustling market, with other stalls and people visible in the background, adding to the lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.18, "peak": 133.12, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.76, "peak": 41.76, "min": 16.95}}, "power_watts_avg": 28.76, "energy_joules_est": 81.31, "sample_count": 28, "duration_seconds": 2.827}, "timestamp": "2026-01-19T13:58:06.402056"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2036.876, "latencies_ms": [2036.876], "images_per_second": 0.491, "prompt_tokens": 1442, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The bananas are yellow and hanging from the ceiling, the lighting is bright and natural, and the weather is sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.6, "peak": 128.67, "min": 28.38}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 16.26, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 31.44, "peak": 41.76, "min": 15.38}}, "power_watts_avg": 31.44, "energy_joules_est": 64.06, "sample_count": 20, "duration_seconds": 2.037}, "timestamp": "2026-01-19T13:58:08.473273"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1655.11, "latencies_ms": [1655.11], "images_per_second": 0.604, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A train with a white and green body is traveling on a track through a green field with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.77, "peak": 125.0, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.45, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 31.45, "energy_joules_est": 52.07, "sample_count": 16, "duration_seconds": 1.656}, "timestamp": "2026-01-19T13:58:10.151249"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2521.747, "latencies_ms": [2521.747], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. train: 1\n2. tracks: 1\n3. grass: 1\n4. mountains: 1\n5. trees: 1\n6. houses: 1\n7. poles: 1\n8. wires: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.3, "peak": 122.48, "min": 31.14}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.67, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.67, "energy_joules_est": 69.8, "sample_count": 25, "duration_seconds": 2.522}, "timestamp": "2026-01-19T13:58:12.737465"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.717, "latencies_ms": [2395.717], "images_per_second": 0.417, "prompt_tokens": 1117, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The train is positioned on the left side of the image, moving from left to right. The grassy field is in the foreground, while the mountains are in the background. The train is relatively close to the camera, while the mountains are farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.05, "peak": 118.04, "min": 34.28}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.56, "peak": 40.18, "min": 15.38}}, "power_watts_avg": 27.56, "energy_joules_est": 66.04, "sample_count": 23, "duration_seconds": 2.396}, "timestamp": "2026-01-19T13:58:15.137944"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1581.757, "latencies_ms": [1581.757], "images_per_second": 0.632, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A train with a white and green body is traveling through a green field with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.11, "peak": 122.96, "min": 27.35}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.73, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 30.73, "energy_joules_est": 48.62, "sample_count": 16, "duration_seconds": 1.582}, "timestamp": "2026-01-19T13:58:16.821637"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1905.429, "latencies_ms": [1905.429], "images_per_second": 0.525, "prompt_tokens": 1109, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The train is white with green accents and red cargo containers. The sky is blue with white clouds, and the landscape is green with mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.69, "peak": 132.41, "min": 27.66}, "VIN_SYS_5V0": {"avg": 14.12, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.34, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.34, "energy_joules_est": 55.92, "sample_count": 19, "duration_seconds": 1.906}, "timestamp": "2026-01-19T13:58:18.796825"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1848.703, "latencies_ms": [1848.703], "images_per_second": 0.541, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A man wearing a hat and shorts is standing on the beach, holding a cell phone and a kite, with the ocean and waves in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.09, "peak": 118.16, "min": 29.1}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 29.81, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 29.81, "energy_joules_est": 55.12, "sample_count": 18, "duration_seconds": 1.849}, "timestamp": "2026-01-19T13:58:20.667497"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2516.863, "latencies_ms": [2516.863], "images_per_second": 0.397, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. man: 1\n2. hat: 1\n3. shorts: 1\n4. sand: 1\n5. chair: 1\n6. kite: 1\n7. ocean: 1\n8. beach: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.41, "peak": 119.34, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.33, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 27.33, "energy_joules_est": 68.8, "sample_count": 25, "duration_seconds": 2.517}, "timestamp": "2026-01-19T13:58:23.272298"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2046.413, "latencies_ms": [2046.413], "images_per_second": 0.489, "prompt_tokens": 1117, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The man is standing on the left side of the image, with the ocean waves on the right side. The green chair is in the foreground, while the kite is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.06, "peak": 127.53, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.58, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.58, "energy_joules_est": 58.5, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T13:58:25.350610"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1814.462, "latencies_ms": [1814.462], "images_per_second": 0.551, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A man wearing a hat and shorts is standing on a beach, holding a phone and a kite, with the ocean and waves in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.78, "peak": 131.86, "min": 29.62}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.94, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.94, "energy_joules_est": 54.33, "sample_count": 18, "duration_seconds": 1.815}, "timestamp": "2026-01-19T13:58:27.230658"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1757.832, "latencies_ms": [1757.832], "images_per_second": 0.569, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The sky is blue and cloudy, the sand is light brown, and the man is wearing a black shirt and khaki shorts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.9, "peak": 113.16, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.47, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.47, "energy_joules_est": 53.58, "sample_count": 17, "duration_seconds": 1.758}, "timestamp": "2026-01-19T13:58:29.002575"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1528.444, "latencies_ms": [1528.444], "images_per_second": 0.654, "prompt_tokens": 1099, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The image shows a garden with several potted plants, including broccoli, placed on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.0, "peak": 119.02, "min": 29.86}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 32.09, "peak": 40.18, "min": 19.7}}, "power_watts_avg": 32.09, "energy_joules_est": 49.06, "sample_count": 15, "duration_seconds": 1.529}, "timestamp": "2026-01-19T13:58:30.573575"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2745.97, "latencies_ms": [2745.97], "images_per_second": 0.364, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. broccoli plant: 3\n2. broccoli plant: 2\n3. broccoli plant: 1\n4. broccoli plant: 1\n5. broccoli plant: 1\n6. broccoli plant: 1\n7. broccoli plant: 1\n8. broccoli plant: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.25, "peak": 114.17, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.01, "peak": 40.57, "min": 17.73}}, "power_watts_avg": 27.01, "energy_joules_est": 74.18, "sample_count": 27, "duration_seconds": 2.746}, "timestamp": "2026-01-19T13:58:33.385572"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2218.859, "latencies_ms": [2218.859], "images_per_second": 0.451, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The broccoli plants are positioned in the foreground, with the orange plastic pots placed in the middle ground. The dirt ground is visible in the background, and there is a pink object in the top right corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.44, "peak": 120.24, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 27.83, "peak": 39.38, "min": 15.37}}, "power_watts_avg": 27.83, "energy_joules_est": 61.76, "sample_count": 22, "duration_seconds": 2.219}, "timestamp": "2026-01-19T13:58:35.682854"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3386.865, "latencies_ms": [3386.865], "images_per_second": 0.295, "prompt_tokens": 1111, "response_tokens_est": 88, "n_tiles": 1, "output_text": " The image captures a serene garden scene where two large potted plants, each with a vibrant green hue, are nestled in terracotta pots. The plants, exhibiting a mix of broccoli and kale, are the main subjects of this image, their leaves forming a lush canopy over the soil. The background, though blurred, reveals a glimpse of a dirt path, suggesting that this garden is located in a rural or semi-rural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.29, "peak": 116.7, "min": 29.73}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.54, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 25.0, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 25.0, "energy_joules_est": 84.69, "sample_count": 33, "duration_seconds": 3.387}, "timestamp": "2026-01-19T13:58:39.121402"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2631.462, "latencies_ms": [2631.462], "images_per_second": 0.38, "prompt_tokens": 1109, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The image shows a garden with a variety of plants, including broccoli and kale, growing in pots. The plants are green and leafy, and the pots are made of terracotta. The lighting in the garden appears to be natural, coming from the sky, and the weather seems to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.02, "peak": 119.19, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.53, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.53, "energy_joules_est": 69.82, "sample_count": 26, "duration_seconds": 2.632}, "timestamp": "2026-01-19T13:58:41.832242"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2620.762, "latencies_ms": [2620.762], "images_per_second": 0.382, "prompt_tokens": 1099, "response_tokens_est": 59, "n_tiles": 1, "output_text": " In the image, an elderly man is seen walking a small brown and white pony on a leash, accompanied by a young boy who is sitting on the pony. They are walking past a red building with white windows and doors, and there is a woman sitting at a table in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.19, "peak": 117.69, "min": 27.43}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.4, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 26.4, "energy_joules_est": 69.2, "sample_count": 26, "duration_seconds": 2.621}, "timestamp": "2026-01-19T13:58:44.551536"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2560.477, "latencies_ms": [2560.477], "images_per_second": 0.391, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. person: 2\n2. pony: 1\n3. child: 1\n4. man: 1\n5. building: 1\n6. chair: 1\n7. lantern: 1\n8. rope: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.1, "peak": 125.39, "min": 29.94}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.65, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.65, "energy_joules_est": 68.25, "sample_count": 25, "duration_seconds": 2.561}, "timestamp": "2026-01-19T13:58:47.161924"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2690.417, "latencies_ms": [2690.417], "images_per_second": 0.372, "prompt_tokens": 1117, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The man is holding the pony's reins, which are in front of him, and the pony is in front of the man. The pony is in the foreground, while the man and the boy are in the background. The woman is standing to the left of the man, and the building is behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.94, "peak": 114.4, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.73, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.73, "energy_joules_est": 71.92, "sample_count": 26, "duration_seconds": 2.691}, "timestamp": "2026-01-19T13:58:49.870063"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1863.816, "latencies_ms": [1863.816], "images_per_second": 0.537, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A man is walking a pony with a rope while a little boy rides on its back. They are walking on a sidewalk in front of a red building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.14, "peak": 121.71, "min": 28.99}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.81, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.81, "energy_joules_est": 55.57, "sample_count": 18, "duration_seconds": 1.864}, "timestamp": "2026-01-19T13:58:51.746512"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1913.887, "latencies_ms": [1913.887], "images_per_second": 0.522, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image is taken during the day with natural light illuminating the scene. The colors are vibrant, with the red of the building standing out against the blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.43, "peak": 127.53, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.65, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 29.65, "energy_joules_est": 56.76, "sample_count": 19, "duration_seconds": 1.914}, "timestamp": "2026-01-19T13:58:53.727109"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2183.489, "latencies_ms": [2183.489], "images_per_second": 0.458, "prompt_tokens": 1099, "response_tokens_est": 43, "n_tiles": 1, "output_text": " A young boy with blonde hair, wearing a blue and white striped shirt and blue jeans, is walking down a dirt path in a lush garden filled with blue flowers, carrying a stuffed animal in his left arm.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.03, "peak": 121.37, "min": 32.96}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.39, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.39, "energy_joules_est": 62.01, "sample_count": 21, "duration_seconds": 2.184}, "timestamp": "2026-01-19T13:58:55.925566"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2512.366, "latencies_ms": [2512.366], "images_per_second": 0.398, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. child: 1\n2. blue: 1\n3. striped: 1\n4. blue: 1\n5. blue: 1\n6. blue: 1\n7. blue: 1\n8. blue: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.9, "ram_available_mb": 99763.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.92, "peak": 122.46, "min": 27.42}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.2, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.2, "energy_joules_est": 68.34, "sample_count": 25, "duration_seconds": 2.513}, "timestamp": "2026-01-19T13:58:58.530007"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2015.311, "latencies_ms": [2015.311], "images_per_second": 0.496, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The child is standing on the left side of the image, with the path leading to the right. The child is positioned in the foreground, with the flowers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.56, "peak": 127.32, "min": 30.04}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.43, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.43, "energy_joules_est": 57.3, "sample_count": 20, "duration_seconds": 2.016}, "timestamp": "2026-01-19T13:59:00.619378"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1753.551, "latencies_ms": [1753.551], "images_per_second": 0.57, "prompt_tokens": 1111, "response_tokens_est": 26, "n_tiles": 1, "output_text": " A young boy with blonde hair is walking down a dirt path in a forest, carrying a stuffed animal in his left hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.96, "peak": 124.88, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.13, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 30.13, "energy_joules_est": 52.85, "sample_count": 17, "duration_seconds": 1.754}, "timestamp": "2026-01-19T13:59:02.392666"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2970.787, "latencies_ms": [2970.787], "images_per_second": 0.337, "prompt_tokens": 1109, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The image features a young boy with blonde hair, wearing a blue and white striped shirt and blue jeans, walking on a dirt path surrounded by a field of blue flowers. The lighting is natural and soft, suggesting it is either early morning or late afternoon. The boy is carrying a stuffed animal in his left arm, which appears to be a brown teddy bear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.16, "peak": 126.07, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.37, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 26.37, "energy_joules_est": 78.35, "sample_count": 29, "duration_seconds": 2.971}, "timestamp": "2026-01-19T13:59:05.418207"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1845.638, "latencies_ms": [1845.638], "images_per_second": 0.542, "prompt_tokens": 1099, "response_tokens_est": 30, "n_tiles": 1, "output_text": " An orange is placed on a black asphalt surface with a white line running parallel to it, and in the background, there are parked cars and trees.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.31, "peak": 124.8, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.48, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.48, "energy_joules_est": 54.42, "sample_count": 18, "duration_seconds": 1.846}, "timestamp": "2026-01-19T13:59:07.299500"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2761.309, "latencies_ms": [2761.309], "images_per_second": 0.362, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. Orange: 1\n2. Pavement: 1\n3. White line: 1\n4. Cars: 1\n5. Trees: 1\n6. Sky: 1\n7. Clouds: 1\n8. Orange peel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.27, "peak": 121.88, "min": 28.24}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.19, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.19, "energy_joules_est": 72.33, "sample_count": 27, "duration_seconds": 2.762}, "timestamp": "2026-01-19T13:59:10.121467"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2134.089, "latencies_ms": [2134.089], "images_per_second": 0.469, "prompt_tokens": 1117, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The orange is positioned to the left of the white line, which is in the foreground of the image. The orange is also in the foreground, while the background features a parking lot with several cars parked.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.64, "peak": 124.96, "min": 28.98}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.18, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.18, "energy_joules_est": 60.15, "sample_count": 21, "duration_seconds": 2.134}, "timestamp": "2026-01-19T13:59:12.312198"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1583.947, "latencies_ms": [1583.947], "images_per_second": 0.631, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A single orange is placed on the edge of a parking lot, with cars parked in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.59, "peak": 124.31, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.71, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.71, "energy_joules_est": 48.66, "sample_count": 16, "duration_seconds": 1.584}, "timestamp": "2026-01-19T13:59:13.982370"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1861.409, "latencies_ms": [1861.409], "images_per_second": 0.537, "prompt_tokens": 1109, "response_tokens_est": 30, "n_tiles": 1, "output_text": " The orange is a vibrant orange color, and the asphalt is a dark gray color. The sky is cloudy, and there are trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.75, "peak": 114.12, "min": 29.39}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.07, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.07, "energy_joules_est": 55.99, "sample_count": 18, "duration_seconds": 1.862}, "timestamp": "2026-01-19T13:59:15.856053"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1999.322, "latencies_ms": [1999.322], "images_per_second": 0.5, "prompt_tokens": 1432, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man in a suit is sitting at a table with a bowl of food and three empty beer bottles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26006.4, "ram_available_mb": 99765.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26007.9, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.6, "peak": 118.54, "min": 28.31}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 16.36, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 31.42, "peak": 41.76, "min": 18.14}}, "power_watts_avg": 31.42, "energy_joules_est": 62.84, "sample_count": 20, "duration_seconds": 2.0}, "timestamp": "2026-01-19T13:59:17.947996"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2198.186, "latencies_ms": [2198.186], "images_per_second": 0.455, "prompt_tokens": 1446, "response_tokens_est": 29, "n_tiles": 1, "output_text": " man: 1, bowl: 1, bottle: 2, keys: 1, remote: 1, table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.9, "ram_available_mb": 99764.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.48, "peak": 121.62, "min": 27.79}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 16.26, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 30.84, "peak": 42.15, "min": 16.95}}, "power_watts_avg": 30.84, "energy_joules_est": 67.81, "sample_count": 22, "duration_seconds": 2.199}, "timestamp": "2026-01-19T13:59:20.249478"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2577.891, "latencies_ms": [2577.891], "images_per_second": 0.388, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The man is seated at a table with the bottles of beer placed to his right. The bowl is located in front of him on the table. The keys are placed on the table to the left of the bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.09, "peak": 120.87, "min": 31.29}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 16.15, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.53, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 29.53, "energy_joules_est": 76.13, "sample_count": 25, "duration_seconds": 2.578}, "timestamp": "2026-01-19T13:59:22.865307"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2034.989, "latencies_ms": [2034.989], "images_per_second": 0.491, "prompt_tokens": 1444, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A man is sitting at a table with a bowl of food in front of him and three bottles of beer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.88, "peak": 126.96, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 16.46, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 31.64, "peak": 41.74, "min": 16.95}}, "power_watts_avg": 31.64, "energy_joules_est": 64.4, "sample_count": 20, "duration_seconds": 2.035}, "timestamp": "2026-01-19T13:59:24.946355"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1906.924, "latencies_ms": [1906.924], "images_per_second": 0.524, "prompt_tokens": 1442, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The man is wearing a grey suit and white shirt, and the bottles are clear glass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.85, "peak": 126.03, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 16.36, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 32.53, "peak": 42.15, "min": 18.53}}, "power_watts_avg": 32.53, "energy_joules_est": 62.05, "sample_count": 19, "duration_seconds": 1.908}, "timestamp": "2026-01-19T13:59:26.922072"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1870.318, "latencies_ms": [1870.318], "images_per_second": 0.535, "prompt_tokens": 1099, "response_tokens_est": 31, "n_tiles": 1, "output_text": " The image depicts a neatly made hotel room with a large bed, two pillows, and a neatly made bed with a white comforter and white sheets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.7, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26008.3, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.76, "peak": 126.64, "min": 32.43}, "VIN_SYS_5V0": {"avg": 14.33, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.46, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 30.46, "energy_joules_est": 57.0, "sample_count": 18, "duration_seconds": 1.871}, "timestamp": "2026-01-19T13:59:28.803162"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2566.628, "latencies_ms": [2566.628], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bed: 2\n2. pillows: 6\n3. chair: 1\n4. desk: 1\n5. lamp: 1\n6. window: 1\n7. carpet: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.21, "peak": 116.17, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.95, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.42, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 27.42, "energy_joules_est": 70.4, "sample_count": 25, "duration_seconds": 2.567}, "timestamp": "2026-01-19T13:59:31.402419"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2152.169, "latencies_ms": [2152.169], "images_per_second": 0.465, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The bed is positioned to the left of the desk, which is situated in the foreground of the image. The window is located in the background, behind the bed, and the wall is directly behind the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.8, "peak": 112.74, "min": 30.4}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.52, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 28.52, "energy_joules_est": 61.4, "sample_count": 21, "duration_seconds": 2.153}, "timestamp": "2026-01-19T13:59:33.587938"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2394.997, "latencies_ms": [2394.997], "images_per_second": 0.418, "prompt_tokens": 1111, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The image depicts a hotel room with a large bed, neatly made with white linens and pillows, and a small desk with a chair. The room has a window with a view of the outside, and there is a luggage bag placed on the floor.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.78, "peak": 113.19, "min": 28.32}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.35, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 27.35, "energy_joules_est": 65.52, "sample_count": 24, "duration_seconds": 2.395}, "timestamp": "2026-01-19T13:59:36.090363"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1732.585, "latencies_ms": [1732.585], "images_per_second": 0.577, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through a window. The walls are painted white and the carpet is grey.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.05, "peak": 111.53, "min": 29.59}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 29.78, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 29.78, "energy_joules_est": 51.62, "sample_count": 17, "duration_seconds": 1.733}, "timestamp": "2026-01-19T13:59:37.860118"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2046.642, "latencies_ms": [2046.642], "images_per_second": 0.489, "prompt_tokens": 1100, "response_tokens_est": 38, "n_tiles": 1, "output_text": " In the image, there are three stuffed animals, a teddy bear, a snowman, and a bear wearing a hat, all sitting on a blue surface with a red and orange background.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 26008.4, "ram_available_mb": 99763.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.52, "peak": 126.65, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.1, "peak": 39.78, "min": 18.14}}, "power_watts_avg": 29.1, "energy_joules_est": 59.58, "sample_count": 20, "duration_seconds": 2.047}, "timestamp": "2026-01-19T13:59:39.954662"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2474.641, "latencies_ms": [2474.641], "images_per_second": 0.404, "prompt_tokens": 1114, "response_tokens_est": 54, "n_tiles": 1, "output_text": " teddy bear: 2, green hat: 1, red scarf: 1, black hat: 1, white bear: 1, orange background: 1, red Coca-Cola logo: 1, green hat with white stripes: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.97, "peak": 116.14, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.35, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.35, "energy_joules_est": 67.7, "sample_count": 24, "duration_seconds": 2.475}, "timestamp": "2026-01-19T13:59:42.448306"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2598.56, "latencies_ms": [2598.56], "images_per_second": 0.385, "prompt_tokens": 1118, "response_tokens_est": 59, "n_tiles": 1, "output_text": " The teddy bear wearing a green hat is positioned to the left of the teddy bear wearing a red hat, which is in front of the teddy bear wearing a black hat. The teddy bear wearing a red hat is in the foreground, while the teddy bear wearing a green hat is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.89, "peak": 130.53, "min": 36.39}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.09, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.09, "energy_joules_est": 70.41, "sample_count": 25, "duration_seconds": 2.599}, "timestamp": "2026-01-19T13:59:45.054704"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3148.125, "latencies_ms": [3148.125], "images_per_second": 0.318, "prompt_tokens": 1112, "response_tokens_est": 80, "n_tiles": 1, "output_text": " Three stuffed animals are sitting on a blue and white striped blanket. The teddy bear on the left is wearing a green hat and a red shirt with the word \"Berlin\" on it. The teddy bear in the middle is wearing a red hat with the word \"Coca-Cola\" on it. The teddy bear on the right is wearing a black hat with a white pom pom on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.2, "ram_available_mb": 99764.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.59, "peak": 127.34, "min": 29.7}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 25.24, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 25.24, "energy_joules_est": 79.47, "sample_count": 31, "duration_seconds": 3.148}, "timestamp": "2026-01-19T13:59:48.276333"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1932.821, "latencies_ms": [1932.821], "images_per_second": 0.517, "prompt_tokens": 1110, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The teddy bears are made of plush material and are colored in shades of brown, white, and red. The background is a vibrant orange with a floral pattern.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.64, "peak": 118.5, "min": 29.96}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.8, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.8, "energy_joules_est": 55.67, "sample_count": 19, "duration_seconds": 1.933}, "timestamp": "2026-01-19T13:59:50.253446"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1711.609, "latencies_ms": [1711.609], "images_per_second": 0.584, "prompt_tokens": 1432, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A glass bowl filled with oranges sits on a table.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.94, "peak": 118.22, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.26, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 32.51, "peak": 41.36, "min": 16.95}}, "power_watts_avg": 32.51, "energy_joules_est": 55.66, "sample_count": 17, "duration_seconds": 1.712}, "timestamp": "2026-01-19T13:59:52.039665"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1704.87, "latencies_ms": [1704.87], "images_per_second": 0.587, "prompt_tokens": 1446, "response_tokens_est": 11, "n_tiles": 1, "output_text": " bowl: 1\noranges: 12", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.79, "peak": 136.02, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 16.36, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 34.09, "peak": 42.54, "min": 20.11}}, "power_watts_avg": 34.09, "energy_joules_est": 58.14, "sample_count": 17, "duration_seconds": 1.705}, "timestamp": "2026-01-19T13:59:53.810543"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2572.29, "latencies_ms": [2572.29], "images_per_second": 0.389, "prompt_tokens": 1450, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The bowl is located in the foreground, and the oranges are placed inside the bowl. The oranges are arranged in a circular pattern, with some oranges placed closer to the center of the bowl and others towards the edges.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.17, "peak": 109.51, "min": 30.6}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 16.36, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.62, "peak": 42.94, "min": 20.1}}, "power_watts_avg": 30.62, "energy_joules_est": 78.78, "sample_count": 25, "duration_seconds": 2.573}, "timestamp": "2026-01-19T13:59:56.410642"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1796.2, "latencies_ms": [1796.2], "images_per_second": 0.557, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A bowl of oranges is placed on a table with a silver tablecloth.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.92, "peak": 136.5, "min": 27.98}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 32.89, "peak": 41.74, "min": 16.95}}, "power_watts_avg": 32.89, "energy_joules_est": 59.1, "sample_count": 18, "duration_seconds": 1.797}, "timestamp": "2026-01-19T13:59:58.291111"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2389.957, "latencies_ms": [2389.957], "images_per_second": 0.418, "prompt_tokens": 1442, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The oranges are a vibrant orange color, and the bowl is made of glass. The lighting is bright and natural, and the oranges are placed on a textured silver tablecloth.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.3, "ram_available_mb": 99763.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.99, "peak": 132.24, "min": 30.11}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 30.48, "peak": 42.53, "min": 18.14}}, "power_watts_avg": 30.48, "energy_joules_est": 72.86, "sample_count": 24, "duration_seconds": 2.39}, "timestamp": "2026-01-19T14:00:00.785035"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1448.937, "latencies_ms": [1448.937], "images_per_second": 0.69, "prompt_tokens": 766, "response_tokens_est": 32, "n_tiles": 1, "output_text": " In the image, a surfer is skillfully riding a large wave on a surfboard, with the ocean's vast expanse and a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5033.1, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.18, "peak": 110.32, "min": 31.51}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 14.81, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.14, "min": 12.61}, "VDD_GPU": {"avg": 27.25, "peak": 39.01, "min": 15.77}}, "power_watts_avg": 27.25, "energy_joules_est": 39.51, "sample_count": 14, "duration_seconds": 1.45}, "timestamp": "2026-01-19T14:00:02.246800"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2263.471, "latencies_ms": [2263.471], "images_per_second": 0.442, "prompt_tokens": 780, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Wave: 1\n4. Ocean: 1\n5. Sky: 1\n6. Clouds: 1\n7. Water: 1\n8. Surfboard leash: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.85, "peak": 114.3, "min": 29.71}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 15.24, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.54, "min": 14.57}, "VDD_GPU": {"avg": 24.75, "peak": 38.21, "min": 18.13}}, "power_watts_avg": 24.75, "energy_joules_est": 56.03, "sample_count": 22, "duration_seconds": 2.264}, "timestamp": "2026-01-19T14:00:04.537194"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1578.122, "latencies_ms": [1578.122], "images_per_second": 0.634, "prompt_tokens": 784, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that dominates the majority of the image. The wave is in the background, with the sky occupying the upper portion of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.35, "peak": 123.73, "min": 27.79}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.14, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 13.79}, "VDD_GPU": {"avg": 26.25, "peak": 37.42, "min": 16.56}}, "power_watts_avg": 26.25, "energy_joules_est": 41.43, "sample_count": 16, "duration_seconds": 1.578}, "timestamp": "2026-01-19T14:00:06.203134"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1401.596, "latencies_ms": [1401.596], "images_per_second": 0.713, "prompt_tokens": 778, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A surfer is riding a large wave in the ocean. The surfer is wearing a black wetsuit and is standing on a surfboard.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.2, "peak": 101.48, "min": 30.0}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.04, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 27.07, "peak": 38.59, "min": 15.77}}, "power_watts_avg": 27.07, "energy_joules_est": 37.96, "sample_count": 14, "duration_seconds": 1.402}, "timestamp": "2026-01-19T14:00:07.660181"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1850.18, "latencies_ms": [1850.18], "images_per_second": 0.54, "prompt_tokens": 776, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a surfer riding a large wave in the ocean, with the wave being a deep blue color and the surfer wearing a black wetsuit. The sky is overcast, with gray clouds covering the entire background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.1, "ram_available_mb": 99764.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5034.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.61, "peak": 111.45, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.04, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 25.69, "peak": 37.82, "min": 17.73}}, "power_watts_avg": 25.69, "energy_joules_est": 47.54, "sample_count": 18, "duration_seconds": 1.85}, "timestamp": "2026-01-19T14:00:09.536969"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1409.374, "latencies_ms": [1409.374], "images_per_second": 0.71, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A cat is sitting on a laptop computer and looking at the screen.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.0, "peak": 126.66, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.66, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 31.66, "energy_joules_est": 44.65, "sample_count": 14, "duration_seconds": 1.41}, "timestamp": "2026-01-19T14:00:10.998542"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2613.712, "latencies_ms": [2613.712], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. cat: 1\n2. laptop: 1\n3. keyboard: 1\n4. screen: 1\n5. mouse: 1\n6. mousepad: 1\n7. laptop screen: 1\n8. cat collar: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.13, "peak": 115.35, "min": 27.17}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.46, "peak": 40.57, "min": 17.34}}, "power_watts_avg": 27.46, "energy_joules_est": 71.79, "sample_count": 26, "duration_seconds": 2.614}, "timestamp": "2026-01-19T14:00:13.701448"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2409.586, "latencies_ms": [2409.586], "images_per_second": 0.415, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The cat is in the foreground, looking at the laptop screen. The laptop is on a table, and the cat is sitting on the table. The laptop is in the middle of the table, and the cat is on the left side of the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.28, "peak": 120.89, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.96, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 26.96, "energy_joules_est": 64.97, "sample_count": 24, "duration_seconds": 2.41}, "timestamp": "2026-01-19T14:00:16.206874"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1439.082, "latencies_ms": [1439.082], "images_per_second": 0.695, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A cat is sitting on a laptop computer and looking at the screen.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.83, "peak": 108.54, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 31.21, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 31.21, "energy_joules_est": 44.94, "sample_count": 14, "duration_seconds": 1.44}, "timestamp": "2026-01-19T14:00:17.674428"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1634.078, "latencies_ms": [1634.078], "images_per_second": 0.612, "prompt_tokens": 1109, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The cat is white and brown, and the laptop is black. The cat is sitting on a brown couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26007.6, "ram_available_mb": 99764.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.88, "peak": 112.68, "min": 30.03}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 32.13, "peak": 40.97, "min": 21.29}}, "power_watts_avg": 32.13, "energy_joules_est": 52.52, "sample_count": 16, "duration_seconds": 1.635}, "timestamp": "2026-01-19T14:00:19.343766"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1958.058, "latencies_ms": [1958.058], "images_per_second": 0.511, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, a group of horses, including a brown horse and a foal, are gathered around a hay feeder, with a house and trees in the background.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.85, "peak": 118.9, "min": 30.05}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 29.81, "peak": 40.18, "min": 18.92}}, "power_watts_avg": 29.81, "energy_joules_est": 58.4, "sample_count": 19, "duration_seconds": 1.959}, "timestamp": "2026-01-19T14:00:21.333060"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1976.845, "latencies_ms": [1976.845], "images_per_second": 0.506, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " horse: 6, hay: 1, fence: 1, house: 1, trees: 1, power lines: 1, grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.56, "peak": 128.93, "min": 30.11}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.55, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 29.55, "energy_joules_est": 58.42, "sample_count": 19, "duration_seconds": 1.977}, "timestamp": "2026-01-19T14:00:23.323862"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2985.255, "latencies_ms": [2985.255], "images_per_second": 0.335, "prompt_tokens": 1117, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The brown horses are standing in a line, with the brown horse on the left being the closest to the camera, and the brown horse on the right being the furthest away. The brown horse on the left is standing in front of the brown horse on the right, and the brown horse on the right is standing in front of the brown horse on the left.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.76, "peak": 126.83, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 26.14, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 26.14, "energy_joules_est": 78.05, "sample_count": 29, "duration_seconds": 2.986}, "timestamp": "2026-01-19T14:00:26.346029"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2352.915, "latencies_ms": [2352.915], "images_per_second": 0.425, "prompt_tokens": 1111, "response_tokens_est": 49, "n_tiles": 1, "output_text": " In a rural setting, a group of horses, including a brown foal, are gathered around a hay feeder, enjoying their meal. The scene is set in a field with a fence and trees in the background, under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.29, "peak": 104.15, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.49, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 27.49, "energy_joules_est": 64.7, "sample_count": 23, "duration_seconds": 2.353}, "timestamp": "2026-01-19T14:00:28.741089"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2566.578, "latencies_ms": [2566.578], "images_per_second": 0.39, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image features a group of horses in a field with a mix of brown and orange colors, with the horses eating hay from a feeder. The lighting is bright and natural, suggesting it is daytime. The weather appears to be clear, as there are no visible clouds in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.6, "ram_available_mb": 99763.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.24, "peak": 119.92, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 27.06, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.06, "energy_joules_est": 69.46, "sample_count": 25, "duration_seconds": 2.567}, "timestamp": "2026-01-19T14:00:31.340059"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1601.474, "latencies_ms": [1601.474], "images_per_second": 0.624, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A man in a black wetsuit is riding a yellow surfboard on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.63, "peak": 135.52, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.58, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.58, "energy_joules_est": 49.0, "sample_count": 16, "duration_seconds": 1.602}, "timestamp": "2026-01-19T14:00:33.016707"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2630.915, "latencies_ms": [2630.915], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. Surfer: 1\n2. Surfboard: 1\n3. Ocean: 1\n4. Water: 1\n5. Waves: 1\n6. Clouds: 1\n7. Sky: 1\n8. Land: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.92, "peak": 119.87, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.02, "peak": 40.18, "min": 17.73}}, "power_watts_avg": 27.02, "energy_joules_est": 71.1, "sample_count": 26, "duration_seconds": 2.631}, "timestamp": "2026-01-19T14:00:35.721909"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2002.479, "latencies_ms": [2002.479], "images_per_second": 0.499, "prompt_tokens": 1117, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, with the ocean and coastline in the background. The surfer is closer to the camera than the coastline, which is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.9, "peak": 122.89, "min": 28.52}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.53, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.53, "energy_joules_est": 57.15, "sample_count": 20, "duration_seconds": 2.003}, "timestamp": "2026-01-19T14:00:37.807365"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1484.543, "latencies_ms": [1484.543], "images_per_second": 0.674, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A surfer is riding a wave on a yellow surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26008.4, "ram_available_mb": 99763.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.73, "peak": 131.32, "min": 27.91}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.02, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.02, "energy_joules_est": 46.06, "sample_count": 15, "duration_seconds": 1.485}, "timestamp": "2026-01-19T14:00:39.378590"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1987.621, "latencies_ms": [1987.621], "images_per_second": 0.503, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The surfer is wearing a black wetsuit and is riding a yellow surfboard. The water is a greenish-blue color, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.47, "peak": 118.65, "min": 27.27}, "VIN_SYS_5V0": {"avg": 14.19, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 29.27, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.27, "energy_joules_est": 58.2, "sample_count": 20, "duration_seconds": 1.988}, "timestamp": "2026-01-19T14:00:41.458942"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4405.912, "latencies_ms": [4405.912], "images_per_second": 0.227, "prompt_tokens": 1099, "response_tokens_est": 128, "n_tiles": 1, "output_text": " The image captures a vibrant display of Halloween decorations, featuring a trio of jack-o'-lanterns, each with its own unique design. The largest jack-o'-lantern, positioned in the center, boasts a face carved into its surface, complete with a wide smile and two prominent teeth. To its left, a smaller jack-o'-lantern showcases a more abstract design, with a skull-like face and a prominent eye. On the right, a third jack-o'-lantern features a classic bat face, complete with pointed ears and a bat-like nose. In the background, a collection of children's drawings", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.09, "peak": 123.24, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 23.56, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 23.56, "energy_joules_est": 103.82, "sample_count": 43, "duration_seconds": 4.407}, "timestamp": "2026-01-19T14:00:45.937645"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1951.992, "latencies_ms": [1951.992], "images_per_second": 0.512, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " pumpkin: 4, flowers: 1, pumpkin: 1, pumpkin: 1, pumpkin: 1, pumpkin: 1, pumpkin: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.25, "peak": 110.69, "min": 29.72}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.09, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.09, "energy_joules_est": 56.79, "sample_count": 19, "duration_seconds": 1.952}, "timestamp": "2026-01-19T14:00:47.918103"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2420.183, "latencies_ms": [2420.183], "images_per_second": 0.413, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The large pumpkin with the face carved into it is positioned in the foreground, with the smaller pumpkin and the vase of flowers placed behind it. The smaller pumpkin is located to the left of the large pumpkin, while the vase of flowers is situated to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26009.2, "ram_available_mb": 99763.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.95, "peak": 112.76, "min": 29.27}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.55, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.55, "energy_joules_est": 66.68, "sample_count": 24, "duration_seconds": 2.42}, "timestamp": "2026-01-19T14:00:50.422895"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2683.952, "latencies_ms": [2683.952], "images_per_second": 0.373, "prompt_tokens": 1111, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image captures a vibrant display of Halloween decorations, featuring a trio of jack-o-lanterns with intricate designs and a bouquet of pink and white flowers in a vase. The setting appears to be a classroom or a community center, as indicated by the presence of a bulletin board in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.42, "peak": 112.2, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.5, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.5, "energy_joules_est": 71.15, "sample_count": 26, "duration_seconds": 2.685}, "timestamp": "2026-01-19T14:00:53.123429"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1532.101, "latencies_ms": [1532.101], "images_per_second": 0.653, "prompt_tokens": 1109, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The pumpkins are orange, the flowers are pink and white, and the lighting is natural.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.2, "peak": 116.29, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.2, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 31.2, "energy_joules_est": 47.82, "sample_count": 15, "duration_seconds": 1.533}, "timestamp": "2026-01-19T14:00:54.686724"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1726.163, "latencies_ms": [1726.163], "images_per_second": 0.579, "prompt_tokens": 1100, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The image depicts a bathroom with a white sink, a mirror, and a door, with a black bag placed on the floor.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.73, "peak": 115.61, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.92, "peak": 40.18, "min": 19.71}}, "power_watts_avg": 30.92, "energy_joules_est": 53.4, "sample_count": 17, "duration_seconds": 1.727}, "timestamp": "2026-01-19T14:00:56.462960"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1921.686, "latencies_ms": [1921.686], "images_per_second": 0.52, "prompt_tokens": 1114, "response_tokens_est": 33, "n_tiles": 1, "output_text": " 1. mirror\n2. sink\n3. door\n4. toilet\n5. shelf\n6. trash bag\n7. wall\n8. floor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.83, "peak": 119.35, "min": 28.78}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.57, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 29.57, "energy_joules_est": 56.84, "sample_count": 19, "duration_seconds": 1.922}, "timestamp": "2026-01-19T14:00:58.441142"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2364.263, "latencies_ms": [2364.263], "images_per_second": 0.423, "prompt_tokens": 1118, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The sink is located to the left of the door, and the black garbage bag is positioned in the foreground, closer to the camera than the door. The mirror is above the sink, and the shelf is to the right of the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.0, "peak": 124.76, "min": 29.37}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.56, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 27.56, "energy_joules_est": 65.17, "sample_count": 23, "duration_seconds": 2.365}, "timestamp": "2026-01-19T14:01:00.842563"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1346.655, "latencies_ms": [1346.655], "images_per_second": 0.743, "prompt_tokens": 1112, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A bathroom with a sink, mirror, and door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.21, "peak": 126.87, "min": 28.84}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.46, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.46, "energy_joules_est": 42.38, "sample_count": 13, "duration_seconds": 1.347}, "timestamp": "2026-01-19T14:01:02.198892"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1486.012, "latencies_ms": [1486.012], "images_per_second": 0.673, "prompt_tokens": 1110, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The bathroom is lit by a warm yellow light, and the walls are painted white.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.16, "peak": 125.0, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.88, "peak": 40.95, "min": 22.07}}, "power_watts_avg": 32.88, "energy_joules_est": 48.88, "sample_count": 15, "duration_seconds": 1.487}, "timestamp": "2026-01-19T14:01:03.764175"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1495.453, "latencies_ms": [1495.453], "images_per_second": 0.669, "prompt_tokens": 1099, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A little girl is sitting on a bed with a laptop in front of her.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.82, "peak": 127.38, "min": 27.22}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.65, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.7, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 31.7, "energy_joules_est": 47.43, "sample_count": 15, "duration_seconds": 1.496}, "timestamp": "2026-01-19T14:01:05.327501"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2633.918, "latencies_ms": [2633.918], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. laptop: 1\n2. child: 1\n3. bed: 1\n4. wall: 1\n5. blanket: 1\n6. laptop screen: 1\n7. keyboard: 1\n8. mouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.36, "peak": 106.23, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.02, "peak": 40.57, "min": 17.34}}, "power_watts_avg": 27.02, "energy_joules_est": 71.18, "sample_count": 26, "duration_seconds": 2.634}, "timestamp": "2026-01-19T14:01:08.035294"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2028.19, "latencies_ms": [2028.19], "images_per_second": 0.493, "prompt_tokens": 1117, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The laptop is on the left side of the bed, and the child is sitting on the right side of the bed. The child is sitting closer to the laptop than the bed.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26005.4, "ram_available_mb": 99766.7, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.49, "peak": 117.23, "min": 29.5}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.36, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.36, "energy_joules_est": 57.54, "sample_count": 20, "duration_seconds": 2.029}, "timestamp": "2026-01-19T14:01:10.115301"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1487.459, "latencies_ms": [1487.459], "images_per_second": 0.672, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A little girl is sitting on a bed with a laptop in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.08, "peak": 130.5, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.93, "min": 13.4}, "VDD_GPU": {"avg": 31.26, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 31.26, "energy_joules_est": 46.51, "sample_count": 15, "duration_seconds": 1.488}, "timestamp": "2026-01-19T14:01:11.680900"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1539.734, "latencies_ms": [1539.734], "images_per_second": 0.649, "prompt_tokens": 1109, "response_tokens_est": 17, "n_tiles": 1, "output_text": " The image is in black and white, with a white background and a white bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.12, "peak": 116.5, "min": 29.58}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 32.02, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 32.02, "energy_joules_est": 49.31, "sample_count": 15, "duration_seconds": 1.54}, "timestamp": "2026-01-19T14:01:13.242278"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1644.637, "latencies_ms": [1644.637], "images_per_second": 0.608, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A skier wearing a brown jacket, blue helmet, and goggles is skiing down a snowy slope surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.16, "peak": 121.75, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.89, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.89, "energy_joules_est": 52.47, "sample_count": 16, "duration_seconds": 1.645}, "timestamp": "2026-01-19T14:01:14.915881"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2611.127, "latencies_ms": [2611.127], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. skier: 1\n2. ski poles: 2\n3. skis: 2\n4. snow: 1\n5. trees: 1\n6. jacket: 1\n7. helmet: 1\n8. goggles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.98, "peak": 126.21, "min": 27.26}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.25, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 27.25, "energy_joules_est": 71.17, "sample_count": 26, "duration_seconds": 2.612}, "timestamp": "2026-01-19T14:01:17.619340"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2075.891, "latencies_ms": [2075.891], "images_per_second": 0.482, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The skier is in the foreground, skiing down a slope with trees in the background. The skier is to the left of the trees, and the trees are behind the skier.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.16, "peak": 120.0, "min": 35.09}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.56, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.56, "energy_joules_est": 59.3, "sample_count": 20, "duration_seconds": 2.076}, "timestamp": "2026-01-19T14:01:19.703508"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1571.609, "latencies_ms": [1571.609], "images_per_second": 0.636, "prompt_tokens": 1111, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A skier in a brown jacket and blue helmet is skiing down a snowy slope surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.46, "peak": 132.56, "min": 27.64}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.07, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 31.07, "energy_joules_est": 48.84, "sample_count": 16, "duration_seconds": 1.572}, "timestamp": "2026-01-19T14:01:21.372113"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1607.666, "latencies_ms": [1607.666], "images_per_second": 0.622, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The skier is wearing a brown jacket and blue pants, and the snow is white and fluffy.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.36, "peak": 119.39, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.65, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.93, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 30.93, "energy_joules_est": 49.74, "sample_count": 16, "duration_seconds": 1.608}, "timestamp": "2026-01-19T14:01:23.043165"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1941.914, "latencies_ms": [1941.914], "images_per_second": 0.515, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image depicts a spacious hotel room with a large bed, a blue armchair, a small table, and two lamps, all arranged in a well-organized manner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.58, "peak": 109.95, "min": 29.54}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.8, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 29.8, "energy_joules_est": 57.88, "sample_count": 19, "duration_seconds": 1.942}, "timestamp": "2026-01-19T14:01:25.022246"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2540.751, "latencies_ms": [2540.751], "images_per_second": 0.394, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bed: 1\n2. lamps: 2\n3. chair: 1\n4. suitcase: 1\n5. window: 1\n6. curtains: 1\n7. wall: 1\n8. carpet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.95, "peak": 128.12, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.33, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.33, "energy_joules_est": 69.45, "sample_count": 25, "duration_seconds": 2.541}, "timestamp": "2026-01-19T14:01:27.624525"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2904.345, "latencies_ms": [2904.345], "images_per_second": 0.344, "prompt_tokens": 1117, "response_tokens_est": 70, "n_tiles": 1, "output_text": " The bed is positioned in the center of the room, with the nightstands placed on either side. The window is located to the left of the bed, while the armchair is situated to the right. The suitcase is placed on the floor near the bed, and the lamp is positioned on the nightstand to the right of the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.54, "peak": 112.11, "min": 29.4}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.19, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 26.19, "energy_joules_est": 76.08, "sample_count": 28, "duration_seconds": 2.905}, "timestamp": "2026-01-19T14:01:30.539688"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1429.721, "latencies_ms": [1429.721], "images_per_second": 0.699, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A hotel room with a large bed, a chair, and a lamp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.57, "peak": 120.92, "min": 29.16}, "VIN_SYS_5V0": {"avg": 14.68, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 32.02, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 32.02, "energy_joules_est": 45.79, "sample_count": 14, "duration_seconds": 1.43}, "timestamp": "2026-01-19T14:01:31.999434"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1680.96, "latencies_ms": [1680.96], "images_per_second": 0.595, "prompt_tokens": 1109, "response_tokens_est": 24, "n_tiles": 1, "output_text": " The room is well-lit with natural light coming through the window, and the walls are painted in a neutral color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.78, "peak": 125.02, "min": 29.47}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.49, "peak": 40.97, "min": 20.5}}, "power_watts_avg": 31.49, "energy_joules_est": 52.95, "sample_count": 17, "duration_seconds": 1.681}, "timestamp": "2026-01-19T14:01:33.772089"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2219.839, "latencies_ms": [2219.839], "images_per_second": 0.45, "prompt_tokens": 1099, "response_tokens_est": 44, "n_tiles": 1, "output_text": " In the image, a skier dressed in white and orange is skillfully performing a jump on a red rail on a snowy mountain, with other skiers and snowboarders in the background, under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.67, "peak": 130.18, "min": 30.01}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 28.1, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.1, "energy_joules_est": 62.39, "sample_count": 22, "duration_seconds": 2.22}, "timestamp": "2026-01-19T14:01:36.066093"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2907.055, "latencies_ms": [2907.055], "images_per_second": 0.344, "prompt_tokens": 1113, "response_tokens_est": 70, "n_tiles": 1, "output_text": " 1. snowboard: 1\n2. skier: 1\n3. ski lift: 1\n4. ski pole: 1\n5. ski: 1\n6. snowboarder: 1\n7. snowboarder's pants: 1\n8. snowboarder's jacket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.25, "peak": 125.73, "min": 32.18}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.22, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.22, "energy_joules_est": 76.23, "sample_count": 28, "duration_seconds": 2.907}, "timestamp": "2026-01-19T14:01:38.981897"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2875.084, "latencies_ms": [2875.084], "images_per_second": 0.348, "prompt_tokens": 1117, "response_tokens_est": 70, "n_tiles": 1, "output_text": " The main object, the snowboarder, is in the foreground, performing a trick on the red rail. The background features other skiers and snowboarders, with the ski lift and ski poles in the distance. The snowboarder is positioned to the left of the red rail, with the ski lift and other skiers to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.63, "peak": 117.8, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.73, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 26.41, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.41, "energy_joules_est": 75.94, "sample_count": 28, "duration_seconds": 2.875}, "timestamp": "2026-01-19T14:01:41.884145"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2954.233, "latencies_ms": [2954.233], "images_per_second": 0.338, "prompt_tokens": 1111, "response_tokens_est": 73, "n_tiles": 1, "output_text": " The image captures a lively scene at a ski resort on a clear day. A skier, dressed in a white jacket and orange pants, is in the midst of performing a daring jump off a red ramp. The ramp is situated on a snowy slope, with other skiers and snowboarders visible in the background, adding to the dynamic atmosphere of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.58, "peak": 126.93, "min": 29.49}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 25.96, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 25.96, "energy_joules_est": 76.7, "sample_count": 29, "duration_seconds": 2.955}, "timestamp": "2026-01-19T14:01:44.906425"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2549.4, "latencies_ms": [2549.4], "images_per_second": 0.392, "prompt_tokens": 1109, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image features a vibrant scene of a snow-covered slope with a clear blue sky overhead. The snow is a pristine white, and the sunlight casts a bright glow on the landscape, highlighting the texture of the snow and the vibrant colors of the skiers' attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.9, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.4, "peak": 114.2, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 12.05}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.63, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.63, "energy_joules_est": 67.9, "sample_count": 25, "duration_seconds": 2.55}, "timestamp": "2026-01-19T14:01:47.514325"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4598.995, "latencies_ms": [4598.995], "images_per_second": 0.217, "prompt_tokens": 1100, "response_tokens_est": 129, "n_tiles": 1, "output_text": " The image captures a scene of urban artistry, where a parking meter stands as a silent observer amidst a vibrant display of graffiti art. The parking meter, a common sight in many cities, is adorned with a colorful mural that adds a touch of whimsy to the otherwise mundane object. The graffiti, a riot of colors and shapes, sprawls across the wall, each piece telling its own story. The words \"THE ONE N' ONLY\" and \"CHAKA\" are prominently displayed, hinting at the identity of the artist or the theme of the artwork. The wall itself is a canvas of urban creativity, with various tags and markings adding", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.51, "peak": 128.85, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 22.83, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 22.83, "energy_joules_est": 105.01, "sample_count": 45, "duration_seconds": 4.6}, "timestamp": "2026-01-19T14:01:52.198053"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2398.435, "latencies_ms": [2398.435], "images_per_second": 0.417, "prompt_tokens": 1114, "response_tokens_est": 51, "n_tiles": 1, "output_text": " 1. Graffiti wall\n2. Graffiti art\n3. Graffiti lettering\n4. Graffiti design\n5. Graffiti paint\n6. Graffiti paintbrush\n7. Graffiti spray\n8. Graffiti spray can", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.97, "peak": 128.68, "min": 27.26}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.91, "peak": 39.78, "min": 14.59}}, "power_watts_avg": 26.91, "energy_joules_est": 64.55, "sample_count": 24, "duration_seconds": 2.399}, "timestamp": "2026-01-19T14:01:54.693436"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2357.945, "latencies_ms": [2357.945], "images_per_second": 0.424, "prompt_tokens": 1118, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The parking meter is located in the foreground of the image, with the graffiti wall extending into the background. The parking meter is positioned to the right of the graffiti wall, and the wall extends from the left to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.72, "peak": 122.21, "min": 29.25}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.34, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 27.34, "energy_joules_est": 64.47, "sample_count": 23, "duration_seconds": 2.358}, "timestamp": "2026-01-19T14:01:57.083490"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2277.951, "latencies_ms": [2277.951], "images_per_second": 0.439, "prompt_tokens": 1112, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image captures a vibrant scene of urban artistry, where a parking meter stands as a silent observer amidst a wall adorned with graffiti. The wall, a canvas of various colors and styles, tells a story of the city's creative spirit.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.07, "peak": 119.0, "min": 29.8}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.63, "peak": 39.39, "min": 16.55}}, "power_watts_avg": 27.63, "energy_joules_est": 62.95, "sample_count": 22, "duration_seconds": 2.278}, "timestamp": "2026-01-19T14:01:59.376753"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2243.564, "latencies_ms": [2243.564], "images_per_second": 0.446, "prompt_tokens": 1110, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image features a parking meter with graffiti on it, and the surrounding area has a mix of colors, including blue, yellow, and black. The lighting appears to be natural, and the weather seems to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.26, "peak": 110.54, "min": 28.8}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.94, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.94, "energy_joules_est": 62.69, "sample_count": 22, "duration_seconds": 2.244}, "timestamp": "2026-01-19T14:02:01.662267"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1444.736, "latencies_ms": [1444.736], "images_per_second": 0.692, "prompt_tokens": 1099, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A surfer is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26005.7, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.6, "peak": 126.33, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.63, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.63, "energy_joules_est": 45.72, "sample_count": 14, "duration_seconds": 1.446}, "timestamp": "2026-01-19T14:02:03.129604"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2565.601, "latencies_ms": [2565.601], "images_per_second": 0.39, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 1\n2. surfboard: 1\n3. wave: 1\n4. ocean: 1\n5. sky: 0\n6. water: 1\n7. surfboard: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.6, "peak": 126.45, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.08, "peak": 40.97, "min": 18.91}}, "power_watts_avg": 28.08, "energy_joules_est": 72.05, "sample_count": 25, "duration_seconds": 2.566}, "timestamp": "2026-01-19T14:02:05.728529"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2181.709, "latencies_ms": [2181.709], "images_per_second": 0.458, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground of the image, riding a wave that is in the middle ground. The surfer is facing towards the right side of the image, with the wave moving towards the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.73, "peak": 112.4, "min": 31.99}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.46, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.46, "energy_joules_est": 62.1, "sample_count": 21, "duration_seconds": 2.182}, "timestamp": "2026-01-19T14:02:07.913880"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1326.449, "latencies_ms": [1326.449], "images_per_second": 0.754, "prompt_tokens": 1111, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A surfer is riding a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.41, "peak": 122.48, "min": 28.86}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 32.67, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 32.67, "energy_joules_est": 43.34, "sample_count": 13, "duration_seconds": 1.327}, "timestamp": "2026-01-19T14:02:09.269480"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2616.566, "latencies_ms": [2616.566], "images_per_second": 0.382, "prompt_tokens": 1109, "response_tokens_est": 57, "n_tiles": 1, "output_text": " The image captures a surfer riding a wave in the ocean, with the surfer wearing a black wetsuit and the wave displaying a deep blue color. The lighting in the image suggests it is either early morning or late afternoon, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.19, "peak": 111.39, "min": 27.25}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.61, "peak": 40.97, "min": 16.95}}, "power_watts_avg": 27.61, "energy_joules_est": 72.25, "sample_count": 26, "duration_seconds": 2.617}, "timestamp": "2026-01-19T14:02:11.972726"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1564.901, "latencies_ms": [1564.901], "images_per_second": 0.639, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A double-decker bus is parked at a bus stop with a man standing next to it.", "error": null, "sys_before": {"cpu_percent": 7.7, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.24, "peak": 124.31, "min": 34.5}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 30.63, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 30.63, "energy_joules_est": 47.96, "sample_count": 15, "duration_seconds": 1.566}, "timestamp": "2026-01-19T14:02:13.548004"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2247.855, "latencies_ms": [2247.855], "images_per_second": 0.445, "prompt_tokens": 1114, "response_tokens_est": 47, "n_tiles": 1, "output_text": " 1. double-decker bus\n2. yellow license plate\n3. black front bumper\n4. white flowers\n5. brick sidewalk\n6. bus stop sign\n7. advertisement board\n8. street lamp", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.0, "peak": 133.1, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.67, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.89, "peak": 40.57, "min": 18.91}}, "power_watts_avg": 28.89, "energy_joules_est": 64.95, "sample_count": 22, "duration_seconds": 2.248}, "timestamp": "2026-01-19T14:02:15.839854"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2331.671, "latencies_ms": [2331.671], "images_per_second": 0.429, "prompt_tokens": 1118, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The bus is parked on the left side of the street, with the bus stop on the right side. The bus is in the foreground, while the bus stop is in the background. The bus is closer to the viewer than the bus stop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.72, "peak": 112.31, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.51, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.59, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.59, "energy_joules_est": 64.34, "sample_count": 23, "duration_seconds": 2.332}, "timestamp": "2026-01-19T14:02:18.233964"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1467.164, "latencies_ms": [1467.164], "images_per_second": 0.682, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A double decker bus is parked at a bus stop on a city street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.11, "peak": 107.88, "min": 28.88}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.33, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.33, "energy_joules_est": 44.51, "sample_count": 15, "duration_seconds": 1.467}, "timestamp": "2026-01-19T14:02:19.800905"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2334.885, "latencies_ms": [2334.885], "images_per_second": 0.428, "prompt_tokens": 1110, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image features a double-decker bus with a yellow and black color scheme, parked at a bus stop with a blue and white advertisement board. The sky is overcast, and the lighting is natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.94, "peak": 122.86, "min": 30.19}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.44, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 27.75, "peak": 39.78, "min": 18.13}}, "power_watts_avg": 27.75, "energy_joules_est": 64.81, "sample_count": 23, "duration_seconds": 2.335}, "timestamp": "2026-01-19T14:02:22.190003"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1638.624, "latencies_ms": [1638.624], "images_per_second": 0.61, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A yellow and red biplane with the letters SP-AWF on the side is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.27, "peak": 116.74, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.56, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 30.56, "energy_joules_est": 50.1, "sample_count": 16, "duration_seconds": 1.639}, "timestamp": "2026-01-19T14:02:23.858861"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2803.131, "latencies_ms": [2803.131], "images_per_second": 0.357, "prompt_tokens": 1113, "response_tokens_est": 64, "n_tiles": 1, "output_text": " 1. propeller: 1\n2. wing: 2\n3. tail: 1\n4. propeller blade: 1\n5. propeller hub: 1\n6. propeller shaft: 1\n7. propeller blade tip: 1\n8. propeller blade tip end: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.49, "peak": 107.76, "min": 30.88}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 26.73, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 26.73, "energy_joules_est": 74.94, "sample_count": 27, "duration_seconds": 2.804}, "timestamp": "2026-01-19T14:02:26.671934"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1699.697, "latencies_ms": [1699.697], "images_per_second": 0.588, "prompt_tokens": 1117, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The yellow biplane is positioned in the foreground, flying from left to right, while the cloudy sky is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.24, "peak": 116.92, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.34, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.34, "energy_joules_est": 51.58, "sample_count": 17, "duration_seconds": 1.7}, "timestamp": "2026-01-19T14:02:28.438710"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1380.885, "latencies_ms": [1380.885], "images_per_second": 0.724, "prompt_tokens": 1111, "response_tokens_est": 12, "n_tiles": 1, "output_text": " A yellow and red biplane is flying in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.53, "peak": 129.29, "min": 28.2}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.94, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 31.94, "energy_joules_est": 44.12, "sample_count": 14, "duration_seconds": 1.381}, "timestamp": "2026-01-19T14:02:29.900947"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1729.28, "latencies_ms": [1729.28], "images_per_second": 0.578, "prompt_tokens": 1109, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The aircraft is painted in vibrant yellow with red accents and has a blue propeller. The sky is overcast with gray clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.36, "peak": 121.73, "min": 28.34}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.15, "peak": 40.97, "min": 20.1}}, "power_watts_avg": 31.15, "energy_joules_est": 53.88, "sample_count": 17, "duration_seconds": 1.73}, "timestamp": "2026-01-19T14:02:31.681570"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2122.908, "latencies_ms": [2122.908], "images_per_second": 0.471, "prompt_tokens": 1099, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image captures an aerial view of a bustling airport, with a white airplane wing visible on the left side, and a large parking lot filled with numerous cars, surrounded by a variety of buildings and greenery.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.48, "peak": 119.9, "min": 30.13}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.76, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 28.76, "energy_joules_est": 61.08, "sample_count": 21, "duration_seconds": 2.124}, "timestamp": "2026-01-19T14:02:33.865808"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2234.386, "latencies_ms": [2234.386], "images_per_second": 0.448, "prompt_tokens": 1113, "response_tokens_est": 43, "n_tiles": 1, "output_text": " airplane wing: 1, car: 100, building: 1, parking lot: 100, trees: 100, clouds: 10, sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.93, "peak": 109.57, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.01, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.01, "energy_joules_est": 62.59, "sample_count": 22, "duration_seconds": 2.235}, "timestamp": "2026-01-19T14:02:36.156363"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2109.601, "latencies_ms": [2109.601], "images_per_second": 0.474, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The airplane wing is on the left side of the image, while the parking lot is on the right side. The parking lot is in the foreground, with the cityscape and fields in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.51, "peak": 135.77, "min": 29.07}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.33, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.33, "energy_joules_est": 59.78, "sample_count": 21, "duration_seconds": 2.11}, "timestamp": "2026-01-19T14:02:38.344069"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2707.37, "latencies_ms": [2707.37], "images_per_second": 0.369, "prompt_tokens": 1111, "response_tokens_est": 62, "n_tiles": 1, "output_text": " The image captures a bustling scene from an airplane window, offering a bird's eye view of a sprawling parking lot nestled amidst a verdant landscape. The parking lot, teeming with cars of various colors, is nestled amidst a mix of residential and commercial buildings, creating a harmonious blend of urban and natural elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.58, "peak": 123.43, "min": 34.8}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.65, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 26.65, "energy_joules_est": 72.16, "sample_count": 26, "duration_seconds": 2.708}, "timestamp": "2026-01-19T14:02:41.056143"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1888.232, "latencies_ms": [1888.232], "images_per_second": 0.53, "prompt_tokens": 1109, "response_tokens_est": 32, "n_tiles": 1, "output_text": " The image displays a clear blue sky with white clouds, and the scene is taken from an airplane window, providing a bird's eye view of the landscape below.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.49, "peak": 124.51, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.86}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.26, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 29.26, "energy_joules_est": 55.26, "sample_count": 19, "duration_seconds": 1.889}, "timestamp": "2026-01-19T14:02:43.045549"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1507.117, "latencies_ms": [1507.117], "images_per_second": 0.664, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A person is holding a pink flip phone with a picture of a girl on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.06, "peak": 113.92, "min": 28.54}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 31.12, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.12, "energy_joules_est": 46.93, "sample_count": 15, "duration_seconds": 1.508}, "timestamp": "2026-01-19T14:02:44.611572"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2580.78, "latencies_ms": [2580.78], "images_per_second": 0.387, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 2\n2. phone: 1\n3. cup: 1\n4. book: 1\n5. bag: 1\n6. chair: 1\n7. blanket: 1\n8. person's hand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.08, "peak": 130.76, "min": 29.22}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.66, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 27.66, "energy_joules_est": 71.39, "sample_count": 25, "duration_seconds": 2.581}, "timestamp": "2026-01-19T14:02:47.218662"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2339.021, "latencies_ms": [2339.021], "images_per_second": 0.428, "prompt_tokens": 1117, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The pink flip phone is held in the foreground by a person's hand, while the person is seated on a couch in the background. The phone is positioned to the left of the person's body, and the couch is located behind the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.42, "peak": 131.06, "min": 29.82}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.67, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 27.67, "energy_joules_est": 64.74, "sample_count": 23, "duration_seconds": 2.34}, "timestamp": "2026-01-19T14:02:49.623504"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1871.329, "latencies_ms": [1871.329], "images_per_second": 0.534, "prompt_tokens": 1111, "response_tokens_est": 31, "n_tiles": 1, "output_text": " A person is sitting on the floor with a pink flip phone in their hand. There is a cup of soda and a bag of chips on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.5, "peak": 127.95, "min": 28.32}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 12.15}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 29.15, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 29.15, "energy_joules_est": 54.56, "sample_count": 19, "duration_seconds": 1.872}, "timestamp": "2026-01-19T14:02:51.601581"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2382.981, "latencies_ms": [2382.981], "images_per_second": 0.42, "prompt_tokens": 1109, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image is taken in a dimly lit room with a pink flip phone in the foreground. The lighting is soft and warm, creating a cozy atmosphere. The phone is made of plastic and has a glossy finish, reflecting the light in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.72, "peak": 114.96, "min": 28.94}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.54, "min": 12.22}, "VDD_GPU": {"avg": 27.59, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.59, "energy_joules_est": 65.75, "sample_count": 23, "duration_seconds": 2.383}, "timestamp": "2026-01-19T14:02:53.999173"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2228.935, "latencies_ms": [2228.935], "images_per_second": 0.449, "prompt_tokens": 1099, "response_tokens_est": 45, "n_tiles": 1, "output_text": " In the image, there are two zebras standing in a field of tall, dry grass, with one zebra looking directly at the camera and the other facing away, both exhibiting the characteristic black and white stripes of their species.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.04, "peak": 123.53, "min": 30.16}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.94, "peak": 39.39, "min": 16.56}}, "power_watts_avg": 27.94, "energy_joules_est": 62.3, "sample_count": 22, "duration_seconds": 2.23}, "timestamp": "2026-01-19T14:02:56.304536"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1183.756, "latencies_ms": [1183.756], "images_per_second": 0.845, "prompt_tokens": 1113, "response_tokens_est": 4, "n_tiles": 1, "output_text": " zebra: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.03, "peak": 108.6, "min": 28.77}, "VIN_SYS_5V0": {"avg": 14.09, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.94, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.94, "energy_joules_est": 37.82, "sample_count": 12, "duration_seconds": 1.184}, "timestamp": "2026-01-19T14:02:57.558500"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2476.03, "latencies_ms": [2476.03], "images_per_second": 0.404, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The zebras are positioned in the foreground of the image, with the one on the left slightly closer to the camera than the one on the right. The background of the image features a grassy field with trees and hills, providing a natural habitat for the zebras.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.64, "peak": 120.71, "min": 30.72}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.86, "peak": 41.76, "min": 19.31}}, "power_watts_avg": 28.86, "energy_joules_est": 71.48, "sample_count": 24, "duration_seconds": 2.477}, "timestamp": "2026-01-19T14:03:00.059256"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1392.731, "latencies_ms": [1392.731], "images_per_second": 0.718, "prompt_tokens": 1111, "response_tokens_est": 13, "n_tiles": 1, "output_text": " Two zebras are standing in a tall, dry grass field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.15, "peak": 125.98, "min": 29.75}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.85, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 31.85, "energy_joules_est": 44.37, "sample_count": 14, "duration_seconds": 1.393}, "timestamp": "2026-01-19T14:03:01.524178"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2547.654, "latencies_ms": [2547.654], "images_per_second": 0.393, "prompt_tokens": 1109, "response_tokens_est": 56, "n_tiles": 1, "output_text": " The image features two zebras standing in a field of tall, dry grass. The zebras are black and white with distinctive stripes, and the grass is a golden brown color. The lighting in the image is bright and natural, suggesting that the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.39, "peak": 120.27, "min": 28.63}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.78, "peak": 41.36, "min": 18.52}}, "power_watts_avg": 27.78, "energy_joules_est": 70.78, "sample_count": 25, "duration_seconds": 2.548}, "timestamp": "2026-01-19T14:03:04.121816"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1402.67, "latencies_ms": [1402.67], "images_per_second": 0.713, "prompt_tokens": 1099, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man is walking into the ocean carrying a yellow surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.18, "peak": 111.01, "min": 29.45}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.46, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.46, "energy_joules_est": 44.15, "sample_count": 14, "duration_seconds": 1.403}, "timestamp": "2026-01-19T14:03:05.590010"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2635.597, "latencies_ms": [2635.597], "images_per_second": 0.379, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. man: 1\n2. surfboard: 1\n3. ocean: 2\n4. waves: 2\n5. sky: 1\n6. water: 1\n7. sand: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.54, "peak": 126.83, "min": 29.81}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.35, "peak": 40.97, "min": 17.35}}, "power_watts_avg": 27.35, "energy_joules_est": 72.1, "sample_count": 26, "duration_seconds": 2.636}, "timestamp": "2026-01-19T14:03:08.297211"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2074.706, "latencies_ms": [2074.706], "images_per_second": 0.482, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The surfer is positioned on the left side of the image, with the ocean waves on the right side. The surfer is in the foreground, with the ocean waves in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.45, "peak": 120.05, "min": 35.2}, "VIN_SYS_5V0": {"avg": 14.36, "peak": 15.75, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.64, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.64, "energy_joules_est": 59.43, "sample_count": 20, "duration_seconds": 2.075}, "timestamp": "2026-01-19T14:03:10.377594"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1659.063, "latencies_ms": [1659.063], "images_per_second": 0.603, "prompt_tokens": 1111, "response_tokens_est": 23, "n_tiles": 1, "output_text": " A man is walking into the ocean with a yellow surfboard. The ocean is blue and the sky is blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.9, "peak": 127.6, "min": 28.83}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.15, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 31.15, "energy_joules_est": 51.69, "sample_count": 16, "duration_seconds": 1.659}, "timestamp": "2026-01-19T14:03:12.052197"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1981.343, "latencies_ms": [1981.343], "images_per_second": 0.505, "prompt_tokens": 1109, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image features a man in the ocean, holding a yellow surfboard, with the ocean waves crashing around him. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.4, "ram_available_mb": 99766.8, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.46, "peak": 115.77, "min": 30.3}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.45, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 29.45, "energy_joules_est": 58.36, "sample_count": 20, "duration_seconds": 1.982}, "timestamp": "2026-01-19T14:03:14.138588"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1439.723, "latencies_ms": [1439.723], "images_per_second": 0.695, "prompt_tokens": 1099, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black and white cow is standing in the water near the shore.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26005.6, "ram_available_mb": 99766.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.23, "peak": 106.75, "min": 30.33}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.65, "min": 11.44}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 31.32, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 31.32, "energy_joules_est": 45.12, "sample_count": 14, "duration_seconds": 1.441}, "timestamp": "2026-01-19T14:03:15.606921"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1579.94, "latencies_ms": [1579.94], "images_per_second": 0.633, "prompt_tokens": 1113, "response_tokens_est": 20, "n_tiles": 1, "output_text": " cow: 1\nwater: 1\nsand: 1\nrocks: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.35, "peak": 123.41, "min": 27.77}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.11, "peak": 40.95, "min": 21.28}}, "power_watts_avg": 32.11, "energy_joules_est": 50.75, "sample_count": 16, "duration_seconds": 1.581}, "timestamp": "2026-01-19T14:03:17.276345"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1815.758, "latencies_ms": [1815.758], "images_per_second": 0.551, "prompt_tokens": 1117, "response_tokens_est": 28, "n_tiles": 1, "output_text": " The cow is standing in the foreground, close to the camera. The cow is standing on the beach, with the water in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 75.43, "peak": 127.0, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.14, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.03, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 30.03, "energy_joules_est": 54.54, "sample_count": 18, "duration_seconds": 1.816}, "timestamp": "2026-01-19T14:03:19.154541"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1439.459, "latencies_ms": [1439.459], "images_per_second": 0.695, "prompt_tokens": 1111, "response_tokens_est": 14, "n_tiles": 1, "output_text": " A black and white cow is standing in the water near the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.17, "peak": 120.49, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 31.99, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 31.99, "energy_joules_est": 46.07, "sample_count": 14, "duration_seconds": 1.44}, "timestamp": "2026-01-19T14:03:20.615893"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1473.448, "latencies_ms": [1473.448], "images_per_second": 0.679, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The cow is black and white, and the photo is in black and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.9, "peak": 116.29, "min": 29.41}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 32.75, "peak": 40.97, "min": 21.28}}, "power_watts_avg": 32.75, "energy_joules_est": 48.27, "sample_count": 15, "duration_seconds": 1.474}, "timestamp": "2026-01-19T14:03:22.180376"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1709.661, "latencies_ms": [1709.661], "images_per_second": 0.585, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A woman in a white blouse and black pants is standing on a snowy hill, holding ski poles and wearing skis.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.58, "peak": 131.51, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.56}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.75, "peak": 40.57, "min": 18.92}}, "power_watts_avg": 30.75, "energy_joules_est": 52.6, "sample_count": 17, "duration_seconds": 1.71}, "timestamp": "2026-01-19T14:03:23.954318"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2590.607, "latencies_ms": [2590.607], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. woman: 1\n2. ski poles: 2\n3. skis: 2\n4. backpack: 1\n5. trees: 2\n6. clouds: 1\n7. snow: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.59, "peak": 115.5, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.2, "peak": 40.57, "min": 17.74}}, "power_watts_avg": 27.2, "energy_joules_est": 70.47, "sample_count": 25, "duration_seconds": 2.591}, "timestamp": "2026-01-19T14:03:26.560662"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2074.697, "latencies_ms": [2074.697], "images_per_second": 0.482, "prompt_tokens": 1118, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The woman is standing in the foreground of the image, with the ski poles and skis positioned to her left. The trees are located in the background, providing a natural backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.53, "peak": 121.14, "min": 29.66}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.86, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.86, "energy_joules_est": 59.88, "sample_count": 20, "duration_seconds": 2.075}, "timestamp": "2026-01-19T14:03:28.644821"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1458.948, "latencies_ms": [1458.948], "images_per_second": 0.685, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A woman is standing on a snowy mountain holding ski poles and wearing ski boots.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.03, "peak": 125.58, "min": 35.85}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.85, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 31.6, "peak": 39.39, "min": 17.35}}, "power_watts_avg": 31.6, "energy_joules_est": 46.11, "sample_count": 14, "duration_seconds": 1.459}, "timestamp": "2026-01-19T14:03:30.108497"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2273.921, "latencies_ms": [2273.921], "images_per_second": 0.44, "prompt_tokens": 1110, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image is in black and white, with a clear contrast between the subject and the background. The subject is wearing a white blouse with black patterns and black pants, and the background features snow-covered trees and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.95, "peak": 129.0, "min": 29.23}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.94, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 28.94, "energy_joules_est": 65.81, "sample_count": 22, "duration_seconds": 2.274}, "timestamp": "2026-01-19T14:03:32.397192"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1783.995, "latencies_ms": [1783.995], "images_per_second": 0.561, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A dog with a yellow frisbee in its mouth is standing on a sandy beach with the ocean and a small island in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.58, "peak": 122.7, "min": 27.91}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.83, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 29.83, "energy_joules_est": 53.23, "sample_count": 18, "duration_seconds": 1.784}, "timestamp": "2026-01-19T14:03:34.274512"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2603.144, "latencies_ms": [2603.144], "images_per_second": 0.384, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. dog: 1\n2. frisbee: 1\n3. sand: 1\n4. ocean: 1\n5. island: 1\n6. sky: 1\n7. beach: 1\n8. paw: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.4, "peak": 123.36, "min": 28.46}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.76, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 26.76, "energy_joules_est": 69.68, "sample_count": 26, "duration_seconds": 2.604}, "timestamp": "2026-01-19T14:03:36.974629"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2433.539, "latencies_ms": [2433.539], "images_per_second": 0.411, "prompt_tokens": 1117, "response_tokens_est": 51, "n_tiles": 1, "output_text": " The dog is in the foreground, holding a yellow frisbee in its mouth. The beach extends into the background, with the ocean and a small island visible. The dog is positioned to the left of the frame, with the ocean to its right.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.63, "peak": 121.47, "min": 29.09}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.0, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.0, "energy_joules_est": 65.73, "sample_count": 24, "duration_seconds": 2.434}, "timestamp": "2026-01-19T14:03:39.470993"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1785.244, "latencies_ms": [1785.244], "images_per_second": 0.56, "prompt_tokens": 1111, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A dog with a yellow frisbee in its mouth is standing on a beach with the ocean and a small island in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.55, "peak": 113.19, "min": 28.66}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 40.18, "min": 15.77}}, "power_watts_avg": 29.53, "energy_joules_est": 52.74, "sample_count": 18, "duration_seconds": 1.786}, "timestamp": "2026-01-19T14:03:41.347903"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1767.247, "latencies_ms": [1767.247], "images_per_second": 0.566, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The dog is black, white, and gray, and the beach is sandy. The sky is cloudy and the ocean is blue.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.89, "peak": 109.64, "min": 33.04}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.2, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.2, "energy_joules_est": 53.38, "sample_count": 17, "duration_seconds": 1.768}, "timestamp": "2026-01-19T14:03:43.118448"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2017.029, "latencies_ms": [2017.029], "images_per_second": 0.496, "prompt_tokens": 1099, "response_tokens_est": 37, "n_tiles": 1, "output_text": " In a kitchen, a group of people, including a woman in a camouflage uniform, are gathered around a large pot on the stove, with a woman in a striped shirt standing nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.84, "peak": 115.53, "min": 29.43}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 29.39, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 29.39, "energy_joules_est": 59.3, "sample_count": 20, "duration_seconds": 2.018}, "timestamp": "2026-01-19T14:03:45.212035"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2544.066, "latencies_ms": [2544.066], "images_per_second": 0.393, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. refrigerator: 2\n2. stove: 1\n3. pot: 1\n4. bowl: 1\n5. cup: 1\n6. spoon: 1\n7. knife: 1\n8. spoon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.99, "peak": 126.0, "min": 30.07}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.04, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 27.04, "energy_joules_est": 68.8, "sample_count": 25, "duration_seconds": 2.545}, "timestamp": "2026-01-19T14:03:47.797354"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1799.449, "latencies_ms": [1799.449], "images_per_second": 0.556, "prompt_tokens": 1117, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The large pot is in the foreground, the refrigerator is in the background, and the women are standing in the middle of the kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.12, "peak": 130.41, "min": 27.36}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 29.62, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.62, "energy_joules_est": 53.31, "sample_count": 18, "duration_seconds": 1.8}, "timestamp": "2026-01-19T14:03:49.678810"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1515.559, "latencies_ms": [1515.559], "images_per_second": 0.66, "prompt_tokens": 1111, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A group of people are in a kitchen with a woman in a military uniform.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 77.55, "peak": 112.18, "min": 29.11}, "VIN_SYS_5V0": {"avg": 14.1, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 31.02, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 31.02, "energy_joules_est": 47.04, "sample_count": 15, "duration_seconds": 1.516}, "timestamp": "2026-01-19T14:03:51.246254"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1777.494, "latencies_ms": [1777.494], "images_per_second": 0.563, "prompt_tokens": 1109, "response_tokens_est": 27, "n_tiles": 1, "output_text": " The image is taken in a kitchen with a grey wall and a stainless steel refrigerator. The lighting is natural, coming from the windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.69, "peak": 130.26, "min": 29.68}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.51, "peak": 40.57, "min": 19.7}}, "power_watts_avg": 30.51, "energy_joules_est": 54.25, "sample_count": 18, "duration_seconds": 1.778}, "timestamp": "2026-01-19T14:03:53.120380"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1581.454, "latencies_ms": [1581.454], "images_per_second": 0.632, "prompt_tokens": 1100, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The image depicts a bathroom with a toilet, a mirror, and a shelf with various toiletries.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.91, "peak": 123.84, "min": 27.56}, "VIN_SYS_5V0": {"avg": 14.08, "peak": 15.65, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 30.51, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 30.51, "energy_joules_est": 48.28, "sample_count": 16, "duration_seconds": 1.582}, "timestamp": "2026-01-19T14:03:54.797019"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2634.145, "latencies_ms": [2634.145], "images_per_second": 0.38, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. mirror: 1\n2. toilet: 1\n3. towel: 2\n4. toilet paper: 1\n5. shelf: 3\n6. bottles: 4\n7. wall: 1\n8. floor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.68, "peak": 118.64, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.65, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.66, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 26.66, "energy_joules_est": 70.24, "sample_count": 26, "duration_seconds": 2.635}, "timestamp": "2026-01-19T14:03:57.500446"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2740.35, "latencies_ms": [2740.35], "images_per_second": 0.365, "prompt_tokens": 1118, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The toilet is located in the foreground of the image, with the sink and mirror above it. The towel rack is positioned to the left of the toilet, while the toilet paper holder is located to the right. The mirror is directly above the sink, and the towel is hanging on the towel rack above the sink.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.08, "peak": 124.13, "min": 28.45}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 26.14, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 26.14, "energy_joules_est": 71.65, "sample_count": 27, "duration_seconds": 2.741}, "timestamp": "2026-01-19T14:04:00.304779"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2503.962, "latencies_ms": [2503.962], "images_per_second": 0.399, "prompt_tokens": 1112, "response_tokens_est": 54, "n_tiles": 1, "output_text": " The image depicts a bathroom with a toilet, a sink, and a mirror. The toilet is located in the lower right corner of the image, while the sink is situated in the upper left corner. The mirror is positioned above the sink, reflecting the bathroom's interior.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.61, "peak": 118.34, "min": 26.97}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.24, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 26.24, "energy_joules_est": 65.72, "sample_count": 25, "duration_seconds": 2.504}, "timestamp": "2026-01-19T14:04:02.902903"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1645.796, "latencies_ms": [1645.796], "images_per_second": 0.608, "prompt_tokens": 1110, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The bathroom is well-lit with a warm yellow light, and the walls are covered in beige tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.45, "peak": 127.6, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 30.11, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 30.11, "energy_joules_est": 49.58, "sample_count": 16, "duration_seconds": 1.647}, "timestamp": "2026-01-19T14:04:04.578343"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1781.218, "latencies_ms": [1781.218], "images_per_second": 0.561, "prompt_tokens": 1099, "response_tokens_est": 28, "n_tiles": 1, "output_text": " A dog wearing a green hat is sitting in the back seat of a car, and the car has a shamrock decoration on the side.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26006.1, "ram_available_mb": 99766.1, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.1, "peak": 114.63, "min": 29.29}, "VIN_SYS_5V0": {"avg": 14.45, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.36, "peak": 40.18, "min": 18.53}}, "power_watts_avg": 30.36, "energy_joules_est": 54.1, "sample_count": 18, "duration_seconds": 1.782}, "timestamp": "2026-01-19T14:04:06.458250"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2572.572, "latencies_ms": [2572.572], "images_per_second": 0.389, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. dog: 1\n2. person: 1\n3. hat: 1\n4. shamrock: 1\n5. window: 1\n6. mirror: 1\n7. light: 1\n8. vehicle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.55, "peak": 109.34, "min": 28.69}, "VIN_SYS_5V0": {"avg": 14.3, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.01, "peak": 40.18, "min": 16.56}}, "power_watts_avg": 27.01, "energy_joules_est": 69.49, "sample_count": 25, "duration_seconds": 2.573}, "timestamp": "2026-01-19T14:04:09.074713"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2457.321, "latencies_ms": [2457.321], "images_per_second": 0.407, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The dog is in the front seat of the vehicle, with the person sitting behind it. The person is wearing a green hat, and the dog is wearing a green hat. The vehicle is parked in a parking lot, and there are other vehicles in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.97, "peak": 129.68, "min": 28.5}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.3, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.3, "energy_joules_est": 67.1, "sample_count": 24, "duration_seconds": 2.458}, "timestamp": "2026-01-19T14:04:11.572045"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1946.213, "latencies_ms": [1946.213], "images_per_second": 0.514, "prompt_tokens": 1111, "response_tokens_est": 34, "n_tiles": 1, "output_text": " A dog wearing a green hat is sitting in the back seat of a car, and a person is driving. The car is decorated with shamrocks and green balloons.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.27, "peak": 114.1, "min": 28.71}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.11, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 29.11, "energy_joules_est": 56.66, "sample_count": 19, "duration_seconds": 1.946}, "timestamp": "2026-01-19T14:04:13.565507"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2712.611, "latencies_ms": [2712.611], "images_per_second": 0.369, "prompt_tokens": 1109, "response_tokens_est": 63, "n_tiles": 1, "output_text": " The image depicts a dog wearing a green hat and a green shamrock on its collar, sitting in the passenger seat of a vehicle. The vehicle is decorated with green shamrocks and has a green shamrock on the side mirror. The lighting in the image is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.06, "peak": 115.72, "min": 29.61}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 13.39}, "VDD_GPU": {"avg": 26.53, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.53, "energy_joules_est": 71.97, "sample_count": 27, "duration_seconds": 2.713}, "timestamp": "2026-01-19T14:04:16.381445"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1952.83, "latencies_ms": [1952.83], "images_per_second": 0.512, "prompt_tokens": 1099, "response_tokens_est": 34, "n_tiles": 1, "output_text": " In the image, a large elephant is seen in a zoo enclosure, standing in a pool of water and drinking from it, with a fence and trees in the background.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.28, "peak": 114.93, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.2, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.72, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 28.72, "energy_joules_est": 56.11, "sample_count": 19, "duration_seconds": 1.954}, "timestamp": "2026-01-19T14:04:18.374516"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1945.607, "latencies_ms": [1945.607], "images_per_second": 0.514, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " elephant: 1, rock: 3, water: 1, tree: 1, fence: 1, person: 2, ground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.04, "peak": 120.6, "min": 31.21}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.61, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 29.61, "energy_joules_est": 57.63, "sample_count": 19, "duration_seconds": 1.946}, "timestamp": "2026-01-19T14:04:20.350491"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2451.275, "latencies_ms": [2451.275], "images_per_second": 0.408, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The elephant is positioned in the foreground, with the water and rocks in the middle ground, and the people and fence in the background. The elephant is facing the water, with its trunk extended towards it, and the rocks are positioned to the left of the elephant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.5, "peak": 125.35, "min": 30.31}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.56, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.56, "energy_joules_est": 67.57, "sample_count": 24, "duration_seconds": 2.452}, "timestamp": "2026-01-19T14:04:22.846748"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2011.002, "latencies_ms": [2011.002], "images_per_second": 0.497, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the image, a large elephant is seen in a zoo enclosure, standing in a pool of water and drinking. The enclosure is surrounded by a fence and has a dirt ground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.62, "peak": 131.14, "min": 27.59}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.68, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.68, "energy_joules_est": 57.69, "sample_count": 20, "duration_seconds": 2.011}, "timestamp": "2026-01-19T14:04:24.936541"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1497.613, "latencies_ms": [1497.613], "images_per_second": 0.668, "prompt_tokens": 1109, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The elephant is gray, the water is blue, and the ground is brown.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.41, "peak": 120.36, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.17, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.93, "min": 12.61}, "VDD_GPU": {"avg": 31.02, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.02, "energy_joules_est": 46.47, "sample_count": 15, "duration_seconds": 1.498}, "timestamp": "2026-01-19T14:04:26.500512"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2043.431, "latencies_ms": [2043.431], "images_per_second": 0.489, "prompt_tokens": 1099, "response_tokens_est": 38, "n_tiles": 1, "output_text": " In the image, there are four people standing on a snowy mountain, each equipped with ski equipment and wearing ski jackets, with a clear blue sky and snow-covered mountains in the background.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.06, "peak": 109.4, "min": 30.64}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.57, "peak": 40.18, "min": 19.32}}, "power_watts_avg": 29.57, "energy_joules_est": 60.45, "sample_count": 20, "duration_seconds": 2.044}, "timestamp": "2026-01-19T14:04:28.593587"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2628.734, "latencies_ms": [2628.734], "images_per_second": 0.38, "prompt_tokens": 1113, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. skis: 4\n2. ski poles: 4\n3. skiers: 4\n4. mountain: 1\n5. snow: 1\n6. sky: 1\n7. trees: 1\n8. mountain range: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.89, "peak": 126.1, "min": 28.72}, "VIN_SYS_5V0": {"avg": 14.48, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 26.88, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.88, "energy_joules_est": 70.67, "sample_count": 26, "duration_seconds": 2.629}, "timestamp": "2026-01-19T14:04:31.304490"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2895.067, "latencies_ms": [2895.067], "images_per_second": 0.345, "prompt_tokens": 1117, "response_tokens_est": 64, "n_tiles": 1, "output_text": " The skiers are positioned in the foreground of the image, with the mountains in the background. The skier on the left is closest to the camera, while the skier on the right is farthest away. The skiers are standing on the snow-covered slope, with the ski lift poles visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 60.68, "peak": 124.38, "min": 29.02}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 25.5, "peak": 39.0, "min": 14.98}}, "power_watts_avg": 25.5, "energy_joules_est": 73.84, "sample_count": 28, "duration_seconds": 2.896}, "timestamp": "2026-01-19T14:04:34.216219"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1503.421, "latencies_ms": [1503.421], "images_per_second": 0.665, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " Four people are standing on a snowy mountain, wearing ski gear and holding ski poles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.35, "peak": 120.58, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.12, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.12, "energy_joules_est": 46.8, "sample_count": 15, "duration_seconds": 1.504}, "timestamp": "2026-01-19T14:04:35.797040"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2096.944, "latencies_ms": [2096.944], "images_per_second": 0.477, "prompt_tokens": 1109, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image features a group of people wearing colorful ski gear, standing on a snowy mountain with a clear blue sky in the background. The sunlight reflects off the snow, creating a bright and vibrant atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.23, "peak": 126.56, "min": 30.2}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.87, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 28.87, "energy_joules_est": 60.55, "sample_count": 21, "duration_seconds": 2.097}, "timestamp": "2026-01-19T14:04:37.995683"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1588.898, "latencies_ms": [1588.898], "images_per_second": 0.629, "prompt_tokens": 1100, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A person is holding a black smartphone in their hand, and the screen displays a photo of a tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.94, "peak": 110.98, "min": 27.09}, "VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.21, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 30.21, "energy_joules_est": 48.02, "sample_count": 16, "duration_seconds": 1.59}, "timestamp": "2026-01-19T14:04:39.672732"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2526.9, "latencies_ms": [2526.9], "images_per_second": 0.396, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. hand: 1\n2. smartphone: 1\n3. keyboard: 1\n4. screen: 1\n5. reflection: 1\n6. light: 1\n7. table: 1\n8. wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.85, "peak": 120.49, "min": 27.5}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.19, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.19, "energy_joules_est": 68.71, "sample_count": 25, "duration_seconds": 2.527}, "timestamp": "2026-01-19T14:04:42.286888"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2134.842, "latencies_ms": [2134.842], "images_per_second": 0.468, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The smartphone is held in the left hand, with the right hand's thumb resting on the back of the device. The smartphone is positioned in the foreground, with the keyboard and wooden table in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.1, "peak": 122.69, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.97, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 27.97, "energy_joules_est": 59.72, "sample_count": 21, "duration_seconds": 2.135}, "timestamp": "2026-01-19T14:04:44.484789"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1628.001, "latencies_ms": [1628.001], "images_per_second": 0.614, "prompt_tokens": 1112, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A person is holding a black smartphone in their hand, and the screen is reflecting a picture of a keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.53, "peak": 130.83, "min": 28.26}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.01, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.01, "energy_joules_est": 48.87, "sample_count": 16, "duration_seconds": 1.628}, "timestamp": "2026-01-19T14:04:46.162563"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1347.24, "latencies_ms": [1347.24], "images_per_second": 0.742, "prompt_tokens": 1110, "response_tokens_est": 12, "n_tiles": 1, "output_text": " The phone is black and the screen is reflecting a keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.33, "peak": 126.49, "min": 29.97}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 32.91, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 32.91, "energy_joules_est": 44.36, "sample_count": 13, "duration_seconds": 1.348}, "timestamp": "2026-01-19T14:04:47.519809"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2602.534, "latencies_ms": [2602.534], "images_per_second": 0.384, "prompt_tokens": 1432, "response_tokens_est": 44, "n_tiles": 1, "output_text": " A red and blue parking meter with a sign that says \"DENVER'S ROAD HOME\" is on a sidewalk next to a sign that says \"CAMPAIGN TO END HOMELESSNESS.\"", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26005.8, "ram_available_mb": 99766.4, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.67, "peak": 119.18, "min": 29.69}, "VIN_SYS_5V0": {"avg": 14.83, "peak": 16.36, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 30.29, "peak": 42.53, "min": 20.1}}, "power_watts_avg": 30.29, "energy_joules_est": 78.85, "sample_count": 25, "duration_seconds": 2.603}, "timestamp": "2026-01-19T14:04:50.137486"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2631.91, "latencies_ms": [2631.91], "images_per_second": 0.38, "prompt_tokens": 1446, "response_tokens_est": 46, "n_tiles": 1, "output_text": " 1. red and white parking meter\n2. white sign with red text\n3. green bushes\n4. white fence\n5. concrete sidewalk\n6. metal light fixture\n7. yellow flowers\n8. trees", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.82, "peak": 120.9, "min": 27.41}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.36, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 29.52, "peak": 41.76, "min": 17.35}}, "power_watts_avg": 29.52, "energy_joules_est": 77.7, "sample_count": 26, "duration_seconds": 2.632}, "timestamp": "2026-01-19T14:04:52.858079"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2546.376, "latencies_ms": [2546.376], "images_per_second": 0.393, "prompt_tokens": 1450, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The parking meter is located on the right side of the image, with a sign placed on the left side of the meter. The sign is positioned in the foreground, while the parking meter is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.01, "peak": 125.23, "min": 28.76}, "VIN_SYS_5V0": {"avg": 14.52, "peak": 16.36, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 29.6, "peak": 42.15, "min": 15.77}}, "power_watts_avg": 29.6, "energy_joules_est": 75.38, "sample_count": 25, "duration_seconds": 2.547}, "timestamp": "2026-01-19T14:04:55.460936"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2214.994, "latencies_ms": [2214.994], "images_per_second": 0.451, "prompt_tokens": 1444, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A red and white parking meter is on the side of the road, with a sign that says \"DENVER'S ROAD HOME\".", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.4, "peak": 106.15, "min": 30.79}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.78, "peak": 42.15, "min": 16.56}}, "power_watts_avg": 30.78, "energy_joules_est": 68.19, "sample_count": 22, "duration_seconds": 2.215}, "timestamp": "2026-01-19T14:04:57.762515"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1841.798, "latencies_ms": [1841.798], "images_per_second": 0.543, "prompt_tokens": 1442, "response_tokens_est": 16, "n_tiles": 1, "output_text": " The parking meter is red and white, and it is located on a sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.29, "peak": 123.86, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.66, "peak": 42.15, "min": 16.95}}, "power_watts_avg": 32.66, "energy_joules_est": 60.17, "sample_count": 18, "duration_seconds": 1.842}, "timestamp": "2026-01-19T14:04:59.640797"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1840.159, "latencies_ms": [1840.159], "images_per_second": 0.543, "prompt_tokens": 1099, "response_tokens_est": 29, "n_tiles": 1, "output_text": " In the image, a herd of zebras is grazing in a grassy field, their distinctive black and white stripes standing out against the natural backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.17, "peak": 113.48, "min": 30.36}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.54, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 30.62, "peak": 40.57, "min": 20.11}}, "power_watts_avg": 30.62, "energy_joules_est": 56.36, "sample_count": 18, "duration_seconds": 1.841}, "timestamp": "2026-01-19T14:05:01.533113"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2084.345, "latencies_ms": [2084.345], "images_per_second": 0.48, "prompt_tokens": 1113, "response_tokens_est": 39, "n_tiles": 1, "output_text": " zebra: 1\ngrass: 1\ntrees: 1\ngrass: 1\ntrees: 1\ngrass: 1\ntrees: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.43, "peak": 123.44, "min": 33.01}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.16, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 29.16, "energy_joules_est": 60.79, "sample_count": 20, "duration_seconds": 2.085}, "timestamp": "2026-01-19T14:05:03.622308"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2412.431, "latencies_ms": [2412.431], "images_per_second": 0.415, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The main zebra is in the foreground, grazing on the grass. The other zebras are in the background, with some standing and others grazing. The zebras are spread out across the field, with some closer to the foreground and others further in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.15, "peak": 123.25, "min": 27.4}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.66, "peak": 39.78, "min": 17.73}}, "power_watts_avg": 27.66, "energy_joules_est": 66.74, "sample_count": 24, "duration_seconds": 2.413}, "timestamp": "2026-01-19T14:05:06.116062"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2020.6, "latencies_ms": [2020.6], "images_per_second": 0.495, "prompt_tokens": 1111, "response_tokens_est": 36, "n_tiles": 1, "output_text": " In the vast savannah, a group of zebras graze peacefully on the golden grass. Their distinctive black and white stripes stand out against the natural backdrop, creating a striking contrast.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.79, "peak": 123.83, "min": 28.53}, "VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.52, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.52, "energy_joules_est": 57.64, "sample_count": 20, "duration_seconds": 2.021}, "timestamp": "2026-01-19T14:05:08.202181"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2190.238, "latencies_ms": [2190.238], "images_per_second": 0.457, "prompt_tokens": 1109, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image features a group of zebras grazing in a grassy field with a clear blue sky in the background. The zebras have black and white stripes, and the grass is a mix of green and brown hues.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.3, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.91, "peak": 130.71, "min": 28.43}, "VIN_SYS_5V0": {"avg": 14.37, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.03, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.03, "energy_joules_est": 61.41, "sample_count": 22, "duration_seconds": 2.191}, "timestamp": "2026-01-19T14:05:10.492225"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1504.624, "latencies_ms": [1504.624], "images_per_second": 0.665, "prompt_tokens": 1099, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A man in a black wetsuit is surfing on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.97, "peak": 123.56, "min": 28.56}, "VIN_SYS_5V0": {"avg": 14.13, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.73, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.73, "energy_joules_est": 46.26, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T14:05:12.073881"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2623.452, "latencies_ms": [2623.452], "images_per_second": 0.381, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. surfer: 1\n2. surfboard: 1\n3. wave: 1\n4. ocean: 1\n5. sky: 0\n6. water: 1\n7. surfboard: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.9, "peak": 117.12, "min": 28.03}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.65, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.28, "peak": 40.97, "min": 17.73}}, "power_watts_avg": 27.28, "energy_joules_est": 71.57, "sample_count": 26, "duration_seconds": 2.624}, "timestamp": "2026-01-19T14:05:14.777975"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2123.866, "latencies_ms": [2123.866], "images_per_second": 0.471, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The surfer is positioned in the foreground, riding a wave that is in the middle ground. The surfer is facing towards the right side of the image, with the wave moving towards the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.31, "peak": 122.96, "min": 29.03}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.93, "min": 13.0}, "VDD_GPU": {"avg": 28.14, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.14, "energy_joules_est": 59.77, "sample_count": 21, "duration_seconds": 2.124}, "timestamp": "2026-01-19T14:05:16.971629"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1618.75, "latencies_ms": [1618.75], "images_per_second": 0.618, "prompt_tokens": 1111, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A surfer in a black wetsuit is riding a wave on a surfboard in the ocean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.92, "peak": 129.66, "min": 29.35}, "VIN_SYS_5V0": {"avg": 14.26, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 30.53, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 30.53, "energy_joules_est": 49.43, "sample_count": 16, "duration_seconds": 1.619}, "timestamp": "2026-01-19T14:05:18.647139"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1991.578, "latencies_ms": [1991.578], "images_per_second": 0.502, "prompt_tokens": 1109, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The surfer is wearing a black wetsuit and is riding a wave on a white surfboard. The ocean is a deep blue color and the sky is clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.47, "peak": 130.23, "min": 27.24}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.26, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 29.26, "energy_joules_est": 58.29, "sample_count": 20, "duration_seconds": 1.992}, "timestamp": "2026-01-19T14:05:20.740892"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1716.012, "latencies_ms": [1716.012], "images_per_second": 0.583, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " Two people dressed in white snow gear are standing on a snowy mountain, one of them is holding a pair of skis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.34, "peak": 127.38, "min": 29.99}, "VIN_SYS_5V0": {"avg": 14.16, "peak": 15.54, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 14.94, "peak": 16.53, "min": 12.21}, "VDD_GPU": {"avg": 29.97, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 29.97, "energy_joules_est": 51.46, "sample_count": 17, "duration_seconds": 1.717}, "timestamp": "2026-01-19T14:05:22.516157"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2591.793, "latencies_ms": [2591.793], "images_per_second": 0.386, "prompt_tokens": 1113, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. person: 2\n2. skis: 2\n3. backpack: 1\n4. snowboard: 1\n5. pole: 1\n6. sun: 1\n7. mountain: 1\n8. sky: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.11, "peak": 109.83, "min": 28.91}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.37, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.37, "energy_joules_est": 70.95, "sample_count": 25, "duration_seconds": 2.592}, "timestamp": "2026-01-19T14:05:25.124765"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2418.684, "latencies_ms": [2418.684], "images_per_second": 0.413, "prompt_tokens": 1117, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The skier on the left is positioned closer to the camera, while the skier on the right is farther away. The skier on the left is also closer to the foreground of the image, while the skier on the right is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.73, "peak": 130.85, "min": 29.08}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.93, "min": 14.18}, "VDD_GPU": {"avg": 27.35, "peak": 39.78, "min": 16.55}}, "power_watts_avg": 27.35, "energy_joules_est": 66.16, "sample_count": 24, "duration_seconds": 2.419}, "timestamp": "2026-01-19T14:05:27.630234"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1524.421, "latencies_ms": [1524.421], "images_per_second": 0.656, "prompt_tokens": 1111, "response_tokens_est": 18, "n_tiles": 1, "output_text": " Two people are standing on a snowy mountain at sunset, preparing to ski down the mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.0, "ram_available_mb": 99766.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.27, "peak": 108.79, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.24, "peak": 15.54, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.83, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.83, "energy_joules_est": 47.01, "sample_count": 15, "duration_seconds": 1.525}, "timestamp": "2026-01-19T14:05:29.204524"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2429.204, "latencies_ms": [2429.204], "images_per_second": 0.412, "prompt_tokens": 1109, "response_tokens_est": 52, "n_tiles": 1, "output_text": " The image features two individuals dressed in white snow gear, with one of them holding a pair of skis. The sun is setting in the background, casting a warm glow over the scene. The sky is a clear blue, and the snow is pristine white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.13, "peak": 111.78, "min": 29.21}, "VIN_SYS_5V0": {"avg": 14.44, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.94, "peak": 40.57, "min": 18.13}}, "power_watts_avg": 27.94, "energy_joules_est": 67.88, "sample_count": 24, "duration_seconds": 2.43}, "timestamp": "2026-01-19T14:05:31.695219"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1596.788, "latencies_ms": [1596.788], "images_per_second": 0.626, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A baseball player in a red shirt and white pants is swinging a bat at a ball during a game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.74, "peak": 130.18, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.34, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.56, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.56, "energy_joules_est": 48.81, "sample_count": 16, "duration_seconds": 1.597}, "timestamp": "2026-01-19T14:05:33.374634"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2704.814, "latencies_ms": [2704.814], "images_per_second": 0.37, "prompt_tokens": 1113, "response_tokens_est": 62, "n_tiles": 1, "output_text": " 1. baseball bat: 1\n2. baseball glove: 1\n3. baseball: 1\n4. baseball player: 1\n5. catcher: 1\n6. umpire: 1\n7. fence: 1\n8. spectators: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.93, "peak": 120.5, "min": 28.07}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 26.69, "peak": 40.57, "min": 16.95}}, "power_watts_avg": 26.69, "energy_joules_est": 72.2, "sample_count": 27, "duration_seconds": 2.705}, "timestamp": "2026-01-19T14:05:36.180324"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2113.96, "latencies_ms": [2113.96], "images_per_second": 0.473, "prompt_tokens": 1117, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The batter is positioned in the foreground, with the catcher and umpire behind him. The field is enclosed by a chain link fence, which separates the players from the spectators in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.56, "peak": 122.31, "min": 29.2}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.44, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 28.05, "peak": 39.0, "min": 14.59}}, "power_watts_avg": 28.05, "energy_joules_est": 59.31, "sample_count": 21, "duration_seconds": 2.114}, "timestamp": "2026-01-19T14:05:38.370132"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2183.01, "latencies_ms": [2183.01], "images_per_second": 0.458, "prompt_tokens": 1111, "response_tokens_est": 43, "n_tiles": 1, "output_text": " A baseball game is taking place on a sunny day in a park. The batter is swinging his bat at the ball, while the catcher and umpire are ready to catch the ball if the batter misses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.63, "peak": 103.99, "min": 29.87}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.42, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 28.42, "energy_joules_est": 62.05, "sample_count": 21, "duration_seconds": 2.183}, "timestamp": "2026-01-19T14:05:40.562605"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2120.731, "latencies_ms": [2120.731], "images_per_second": 0.472, "prompt_tokens": 1109, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image captures a vibrant scene of a baseball game, with the players dressed in crisp white and red uniforms, the sun casting a warm glow on the field, and the lush green trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.66, "peak": 135.63, "min": 28.32}, "VIN_SYS_5V0": {"avg": 14.61, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.57, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 28.57, "energy_joules_est": 60.6, "sample_count": 21, "duration_seconds": 2.121}, "timestamp": "2026-01-19T14:05:42.755650"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1925.77, "latencies_ms": [1925.77], "images_per_second": 0.519, "prompt_tokens": 1432, "response_tokens_est": 19, "n_tiles": 1, "output_text": " A large milkshake with whipped cream sits on a table next to a slice of cake.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.84, "peak": 129.21, "min": 27.31}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.26, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 31.33, "peak": 41.36, "min": 15.77}}, "power_watts_avg": 31.33, "energy_joules_est": 60.34, "sample_count": 19, "duration_seconds": 1.926}, "timestamp": "2026-01-19T14:05:44.756339"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2974.135, "latencies_ms": [2974.135], "images_per_second": 0.336, "prompt_tokens": 1446, "response_tokens_est": 58, "n_tiles": 1, "output_text": " 1. glass: 1\n2. plate: 1\n3. cake: 1\n4. fork: 2\n5. knife: 1\n6. glass of milk: 1\n7. table: 1\n8. person: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.26, "peak": 126.81, "min": 28.89}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 16.26, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 28.64, "peak": 42.13, "min": 18.14}}, "power_watts_avg": 28.64, "energy_joules_est": 85.19, "sample_count": 29, "duration_seconds": 2.975}, "timestamp": "2026-01-19T14:05:47.765902"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2170.838, "latencies_ms": [2170.838], "images_per_second": 0.461, "prompt_tokens": 1450, "response_tokens_est": 29, "n_tiles": 1, "output_text": " The milkshake is to the left of the cake, the fork is in front of the cake, and the person is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.17, "peak": 134.56, "min": 29.57}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 31.44, "peak": 41.36, "min": 16.56}}, "power_watts_avg": 31.44, "energy_joules_est": 68.27, "sample_count": 21, "duration_seconds": 2.171}, "timestamp": "2026-01-19T14:05:49.967078"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1786.399, "latencies_ms": [1786.399], "images_per_second": 0.56, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A table with a milkshake and a slice of cake on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.19, "peak": 134.45, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 16.36, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 33.24, "peak": 41.74, "min": 18.14}}, "power_watts_avg": 33.24, "energy_joules_est": 59.39, "sample_count": 18, "duration_seconds": 1.787}, "timestamp": "2026-01-19T14:05:51.843596"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2066.767, "latencies_ms": [2066.767], "images_per_second": 0.484, "prompt_tokens": 1442, "response_tokens_est": 25, "n_tiles": 1, "output_text": " The image is taken in a restaurant with a blue tablecloth, the lighting is dim and the table is made of wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.32, "peak": 129.69, "min": 29.76}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.15, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.6, "peak": 42.15, "min": 19.32}}, "power_watts_avg": 32.6, "energy_joules_est": 67.39, "sample_count": 20, "duration_seconds": 2.067}, "timestamp": "2026-01-19T14:05:53.927479"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1675.152, "latencies_ms": [1675.152], "images_per_second": 0.597, "prompt_tokens": 1100, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A large, multi-tiered wedding cake with blue and gold accents sits on a table covered with a blue tablecloth.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26006.2, "ram_available_mb": 99765.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.01, "peak": 118.39, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.78}, "VDD_GPU": {"avg": 30.87, "peak": 40.18, "min": 19.31}}, "power_watts_avg": 30.87, "energy_joules_est": 51.73, "sample_count": 17, "duration_seconds": 1.676}, "timestamp": "2026-01-19T14:05:55.703866"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2654.553, "latencies_ms": [2654.553], "images_per_second": 0.377, "prompt_tokens": 1114, "response_tokens_est": 61, "n_tiles": 1, "output_text": " 1. Cake: 1\n2. Tablecloth: 1\n3. Chairs: 1\n4. People: 1\n5. Flowers: 1\n6. Table: 1\n7. Window: 1\n8. Chandelier: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.61, "peak": 115.21, "min": 29.24}, "VIN_SYS_5V0": {"avg": 14.39, "peak": 15.54, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 26.7, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 26.7, "energy_joules_est": 70.89, "sample_count": 26, "duration_seconds": 2.655}, "timestamp": "2026-01-19T14:05:58.411887"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2177.42, "latencies_ms": [2177.42], "images_per_second": 0.459, "prompt_tokens": 1118, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The cake is positioned in the foreground, with the tablecloth and chairs in the background. The cake is located to the left of the tablecloth, and the tablecloth is to the right of the cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.49, "peak": 115.61, "min": 33.77}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.54, "min": 13.4}, "VDD_GPU": {"avg": 28.19, "peak": 39.39, "min": 15.76}}, "power_watts_avg": 28.19, "energy_joules_est": 61.39, "sample_count": 21, "duration_seconds": 2.178}, "timestamp": "2026-01-19T14:06:00.597389"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1447.463, "latencies_ms": [1447.463], "images_per_second": 0.691, "prompt_tokens": 1112, "response_tokens_est": 16, "n_tiles": 1, "output_text": " A large wedding cake sits on a table in a room with tables and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.97, "peak": 116.43, "min": 30.9}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.95, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 31.57, "peak": 39.39, "min": 17.73}}, "power_watts_avg": 31.57, "energy_joules_est": 45.71, "sample_count": 14, "duration_seconds": 1.448}, "timestamp": "2026-01-19T14:06:02.063433"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1550.974, "latencies_ms": [1550.974], "images_per_second": 0.645, "prompt_tokens": 1110, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The cake is white with blue and gold accents, and it is lit by a chandelier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.96, "peak": 126.09, "min": 30.81}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 32.62, "peak": 40.57, "min": 20.89}}, "power_watts_avg": 32.62, "energy_joules_est": 50.6, "sample_count": 15, "duration_seconds": 1.551}, "timestamp": "2026-01-19T14:06:03.627153"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1766.413, "latencies_ms": [1766.413], "images_per_second": 0.566, "prompt_tokens": 1100, "response_tokens_est": 27, "n_tiles": 1, "output_text": " A woman wearing a blue sweater with red and white patterns is standing in a kitchen and holding a plate with a pie on it.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.99, "peak": 109.04, "min": 36.35}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.75, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.1, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.1, "energy_joules_est": 54.95, "sample_count": 17, "duration_seconds": 1.767}, "timestamp": "2026-01-19T14:06:05.406327"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2588.119, "latencies_ms": [2588.119], "images_per_second": 0.386, "prompt_tokens": 1114, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. stove: 1\n3. pot: 1\n4. pan: 1\n5. oven: 1\n6. wall: 1\n7. poster: 1\n8. wall-mounted shelf: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.59, "peak": 128.5, "min": 29.55}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.39, "peak": 40.18, "min": 18.52}}, "power_watts_avg": 27.39, "energy_joules_est": 70.9, "sample_count": 25, "duration_seconds": 2.588}, "timestamp": "2026-01-19T14:06:08.006363"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2115.588, "latencies_ms": [2115.588], "images_per_second": 0.473, "prompt_tokens": 1118, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The person is standing in the foreground of the image, with the stove and pot in the background. The person is holding the plate with the pie in the foreground, while the pot is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.35, "peak": 131.73, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.85, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 28.41, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 28.41, "energy_joules_est": 60.11, "sample_count": 21, "duration_seconds": 2.116}, "timestamp": "2026-01-19T14:06:10.189697"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1523.362, "latencies_ms": [1523.362], "images_per_second": 0.656, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman wearing a sweater is standing in a kitchen and holding a plate of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.91, "peak": 115.16, "min": 29.63}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 30.55, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 30.55, "energy_joules_est": 46.55, "sample_count": 15, "duration_seconds": 1.524}, "timestamp": "2026-01-19T14:06:11.759686"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2422.296, "latencies_ms": [2422.296], "images_per_second": 0.413, "prompt_tokens": 1110, "response_tokens_est": 50, "n_tiles": 1, "output_text": " The image is taken in a kitchen with a warm and cozy atmosphere. The lighting is natural, coming from the window in the background, and the colors are vibrant and inviting. The materials are mostly wood and metal, with a few plastic containers visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.78, "peak": 129.75, "min": 29.05}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.63, "peak": 40.18, "min": 17.74}}, "power_watts_avg": 27.63, "energy_joules_est": 66.94, "sample_count": 24, "duration_seconds": 2.423}, "timestamp": "2026-01-19T14:06:14.255165"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1922.579, "latencies_ms": [1922.579], "images_per_second": 0.52, "prompt_tokens": 1100, "response_tokens_est": 33, "n_tiles": 1, "output_text": " A woman wearing a pink shirt, black pants, and brown boots is walking on a dirt path in a fenced area with a white horse wearing a halter.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.67, "peak": 123.14, "min": 30.63}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 28.74, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.74, "energy_joules_est": 55.28, "sample_count": 19, "duration_seconds": 1.923}, "timestamp": "2026-01-19T14:06:16.251987"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2643.138, "latencies_ms": [2643.138], "images_per_second": 0.378, "prompt_tokens": 1114, "response_tokens_est": 60, "n_tiles": 1, "output_text": " 1. horse: 1\n2. woman: 1\n3. rope: 1\n4. fence: 1\n5. ground: 1\n6. trees: 1\n7. woman's hair: 1\n8. woman's pants: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.93, "peak": 126.86, "min": 29.98}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 26.61, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.61, "energy_joules_est": 70.34, "sample_count": 26, "duration_seconds": 2.643}, "timestamp": "2026-01-19T14:06:18.958148"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2043.032, "latencies_ms": [2043.032], "images_per_second": 0.489, "prompt_tokens": 1118, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The woman is standing in the foreground of the image, with the horse in the background. The horse is positioned to the left of the woman, and the fence is visible behind them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.79, "peak": 121.64, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.47, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 28.47, "energy_joules_est": 58.17, "sample_count": 20, "duration_seconds": 2.043}, "timestamp": "2026-01-19T14:06:21.049134"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1542.259, "latencies_ms": [1542.259], "images_per_second": 0.648, "prompt_tokens": 1112, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A woman wearing a pink shirt and black pants is walking a horse in a dirt field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.46, "peak": 125.47, "min": 29.26}, "VIN_SYS_5V0": {"avg": 14.31, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.62, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.62, "energy_joules_est": 47.24, "sample_count": 15, "duration_seconds": 1.543}, "timestamp": "2026-01-19T14:06:22.620379"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1956.326, "latencies_ms": [1956.326], "images_per_second": 0.511, "prompt_tokens": 1110, "response_tokens_est": 34, "n_tiles": 1, "output_text": " The image is taken in a sunny day with clear sky. The woman is wearing a pink shirt and black pants. The horse is white and has a black mane.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.35, "peak": 122.39, "min": 28.96}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 29.86, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 29.86, "energy_joules_est": 58.43, "sample_count": 19, "duration_seconds": 1.957}, "timestamp": "2026-01-19T14:06:24.603824"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2182.511, "latencies_ms": [2182.511], "images_per_second": 0.458, "prompt_tokens": 1100, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The image captures a bustling city street during the day, with a mix of vehicles and pedestrians navigating the urban landscape, and a yellow sign with a camera icon hanging from a pole on the right side of the frame.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.52, "peak": 108.85, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.65, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 28.48, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 28.48, "energy_joules_est": 62.18, "sample_count": 21, "duration_seconds": 2.183}, "timestamp": "2026-01-19T14:06:26.801132"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2552.089, "latencies_ms": [2552.089], "images_per_second": 0.392, "prompt_tokens": 1114, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. sign: 2\n2. car: 3\n3. person: 1\n4. tree: 2\n5. building: 4\n6. street: 1\n7. traffic light: 1\n8. bus: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.86, "peak": 125.6, "min": 30.02}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.06, "peak": 39.78, "min": 16.95}}, "power_watts_avg": 27.06, "energy_joules_est": 69.07, "sample_count": 25, "duration_seconds": 2.552}, "timestamp": "2026-01-19T14:06:29.408233"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2315.291, "latencies_ms": [2315.291], "images_per_second": 0.432, "prompt_tokens": 1118, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The yellow sign is located in the foreground, to the right of the street. The traffic light is located in the background, to the left of the street. The person is located in the background, to the right of the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.58, "peak": 108.65, "min": 27.87}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.85, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.31, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.31, "energy_joules_est": 63.24, "sample_count": 23, "duration_seconds": 2.316}, "timestamp": "2026-01-19T14:06:31.809197"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2028.028, "latencies_ms": [2028.028], "images_per_second": 0.493, "prompt_tokens": 1112, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The image captures a bustling city street during the day, with cars and buses navigating through the traffic. The skyline of tall buildings looms in the background, suggesting a metropolitan area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.51, "peak": 133.13, "min": 28.85}, "VIN_SYS_5V0": {"avg": 14.25, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 27.95, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 27.95, "energy_joules_est": 56.7, "sample_count": 20, "duration_seconds": 2.029}, "timestamp": "2026-01-19T14:06:33.889849"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2155.147, "latencies_ms": [2155.147], "images_per_second": 0.464, "prompt_tokens": 1110, "response_tokens_est": 41, "n_tiles": 1, "output_text": " The image depicts a city street with a mix of modern and older buildings, with a clear blue sky and a few clouds. The street is lined with trees and has a few cars parked along the side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 64.64, "peak": 119.87, "min": 29.17}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 28.18, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 28.18, "energy_joules_est": 60.74, "sample_count": 21, "duration_seconds": 2.155}, "timestamp": "2026-01-19T14:06:36.083453"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1657.453, "latencies_ms": [1657.453], "images_per_second": 0.603, "prompt_tokens": 1099, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The image depicts a bathroom with a stainless steel toilet, a blue toilet brush, and a metal toilet paper holder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.86, "peak": 126.9, "min": 29.46}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.88, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 30.88, "energy_joules_est": 51.21, "sample_count": 16, "duration_seconds": 1.658}, "timestamp": "2026-01-19T14:06:37.757686"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2440.706, "latencies_ms": [2440.706], "images_per_second": 0.41, "prompt_tokens": 1113, "response_tokens_est": 53, "n_tiles": 1, "output_text": " toilet: 1, toilet brush: 1, tiled floor: 1, metal toilet tank: 1, metal toilet seat: 1, metal toilet bowl: 1, metal toilet lid: 1, metal toilet seat cover: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.53, "peak": 121.31, "min": 30.22}, "VIN_SYS_5V0": {"avg": 14.59, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.91, "peak": 40.18, "min": 18.14}}, "power_watts_avg": 27.91, "energy_joules_est": 68.13, "sample_count": 24, "duration_seconds": 2.441}, "timestamp": "2026-01-19T14:06:40.259910"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2171.446, "latencies_ms": [2171.446], "images_per_second": 0.461, "prompt_tokens": 1117, "response_tokens_est": 42, "n_tiles": 1, "output_text": " The toilet is located in the center of the image, with the person's feet visible in the foreground. The toilet brush is placed to the left of the toilet, while the wall is located in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.09, "peak": 105.1, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.43, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 28.29, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 28.29, "energy_joules_est": 61.44, "sample_count": 21, "duration_seconds": 2.172}, "timestamp": "2026-01-19T14:06:42.457244"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1447.265, "latencies_ms": [1447.265], "images_per_second": 0.691, "prompt_tokens": 1111, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A person is standing in a bathroom with a toilet and a blue brush.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.5, "peak": 119.84, "min": 29.78}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 31.99, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 31.99, "energy_joules_est": 46.31, "sample_count": 14, "duration_seconds": 1.448}, "timestamp": "2026-01-19T14:06:43.923755"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2019.521, "latencies_ms": [2019.521], "images_per_second": 0.495, "prompt_tokens": 1109, "response_tokens_est": 37, "n_tiles": 1, "output_text": " The image is taken in a bathroom with a stainless steel toilet, a blue toilet brush, and a white tiled floor. The lighting is bright and natural, coming from the ceiling.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.39, "peak": 124.98, "min": 28.82}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.02, "peak": 40.97, "min": 18.92}}, "power_watts_avg": 30.02, "energy_joules_est": 60.64, "sample_count": 20, "duration_seconds": 2.02}, "timestamp": "2026-01-19T14:06:46.010815"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1455.035, "latencies_ms": [1455.035], "images_per_second": 0.687, "prompt_tokens": 1100, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A pink bicycle is leaning against a wall in a store with other bicycles.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.42, "peak": 113.07, "min": 30.35}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.54, "min": 11.95}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 31.49, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 31.49, "energy_joules_est": 45.84, "sample_count": 14, "duration_seconds": 1.456}, "timestamp": "2026-01-19T14:06:47.480420"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2513.156, "latencies_ms": [2513.156], "images_per_second": 0.398, "prompt_tokens": 1114, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bicycle: 1\n2. wall: 1\n3. floor: 1\n4. bicycle: 1\n5. bicycle: 1\n6. bicycle: 1\n7. bicycle: 1\n8. bicycle: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.66, "peak": 119.15, "min": 29.91}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.85, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.85, "peak": 40.97, "min": 17.35}}, "power_watts_avg": 27.85, "energy_joules_est": 70.0, "sample_count": 25, "duration_seconds": 2.513}, "timestamp": "2026-01-19T14:06:50.084302"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2018.996, "latencies_ms": [2018.996], "images_per_second": 0.495, "prompt_tokens": 1118, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The pink bicycle is positioned in the foreground, leaning against the wall, while the other bicycles are arranged in the background, with some hanging from the ceiling and others on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26006.2, "ram_available_mb": 99766.0, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26012.0, "ram_available_mb": 99760.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.02, "peak": 127.09, "min": 31.82}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.92, "min": 12.61}, "VDD_GPU": {"avg": 28.31, "peak": 39.78, "min": 14.98}}, "power_watts_avg": 28.31, "energy_joules_est": 57.17, "sample_count": 20, "duration_seconds": 2.019}, "timestamp": "2026-01-19T14:06:52.173952"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2522.304, "latencies_ms": [2522.304], "images_per_second": 0.396, "prompt_tokens": 1112, "response_tokens_est": 55, "n_tiles": 1, "output_text": " The image captures a quaint bicycle shop with a variety of bicycles on display. The shop has a clean and organized appearance, with the bicycles neatly arranged on the floor and on racks. The walls are adorned with various bicycle accessories and tools, creating a welcoming atmosphere for potential customers.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26012.0, "ram_available_mb": 99760.2, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 12.4, "ram_used_mb": 26155.7, "ram_available_mb": 99616.4, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.77, "peak": 135.99, "min": 32.45}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 17.23, "peak": 18.5, "min": 13.39}, "VDD_GPU": {"avg": 26.54, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 26.54, "energy_joules_est": 66.96, "sample_count": 25, "duration_seconds": 2.523}, "timestamp": "2026-01-19T14:06:54.757012"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2114.546, "latencies_ms": [2114.546], "images_per_second": 0.473, "prompt_tokens": 1110, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The image is taken in a well-lit room with natural light coming in from the window. The bicycles are made of metal and have a variety of colors, including pink, black, and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26155.7, "ram_available_mb": 99616.4, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 12.0, "ram_used_mb": 26174.1, "ram_available_mb": 99598.1, "ram_percent": 20.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.78, "peak": 108.83, "min": 34.7}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 17.98, "peak": 19.29, "min": 15.76}, "VDD_GPU": {"avg": 28.07, "peak": 39.39, "min": 15.76}}, "power_watts_avg": 28.07, "energy_joules_est": 59.36, "sample_count": 21, "duration_seconds": 2.115}, "timestamp": "2026-01-19T14:06:56.950575"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1619.504, "latencies_ms": [1619.504], "images_per_second": 0.617, "prompt_tokens": 1099, "response_tokens_est": 21, "n_tiles": 1, "output_text": " A giraffe with a brown and white spotted coat stands in a dry grassy field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26176.0, "ram_available_mb": 99596.1, "ram_percent": 20.8}, "sys_after": {"cpu_percent": 18.4, "ram_used_mb": 26592.3, "ram_available_mb": 99179.9, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.85, "peak": 126.77, "min": 35.5}, "VIN_SYS_5V0": {"avg": 14.57, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 19.14, "peak": 20.08, "min": 16.54}, "VDD_GPU": {"avg": 30.6, "peak": 39.78, "min": 15.76}}, "power_watts_avg": 30.6, "energy_joules_est": 49.58, "sample_count": 16, "duration_seconds": 1.62}, "timestamp": "2026-01-19T14:06:58.616798"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2032.111, "latencies_ms": [2032.111], "images_per_second": 0.492, "prompt_tokens": 1113, "response_tokens_est": 35, "n_tiles": 1, "output_text": " giraffe: 1, tree: 2, bush: 1, sky: 1, cloud: 1, grass: 1, tree trunk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26592.3, "ram_available_mb": 99179.9, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 23.2, "ram_used_mb": 26684.5, "ram_available_mb": 99087.6, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.55, "peak": 122.63, "min": 33.39}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 16.05, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 20.04, "peak": 24.8, "min": 17.32}, "VDD_GPU": {"avg": 29.34, "peak": 40.16, "min": 18.91}}, "power_watts_avg": 29.34, "energy_joules_est": 59.63, "sample_count": 20, "duration_seconds": 2.032}, "timestamp": "2026-01-19T14:07:00.704823"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2243.454, "latencies_ms": [2243.454], "images_per_second": 0.446, "prompt_tokens": 1117, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The giraffe is positioned in the foreground of the image, with the background consisting of a cloudy sky and sparse trees. The giraffe is facing towards the right side of the image, with its body angled slightly towards the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26684.5, "ram_available_mb": 99087.6, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 11.4, "ram_used_mb": 26689.9, "ram_available_mb": 99082.2, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.98, "peak": 122.59, "min": 31.77}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.85, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 18.02, "peak": 19.68, "min": 16.15}, "VDD_GPU": {"avg": 28.06, "peak": 40.16, "min": 16.55}}, "power_watts_avg": 28.06, "energy_joules_est": 62.96, "sample_count": 22, "duration_seconds": 2.244}, "timestamp": "2026-01-19T14:07:02.996636"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1517.245, "latencies_ms": [1517.245], "images_per_second": 0.659, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A giraffe stands in a dry savanna, surrounded by sparse vegetation and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26689.9, "ram_available_mb": 99082.2, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 10.4, "ram_used_mb": 26695.3, "ram_available_mb": 99076.9, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.67, "peak": 127.43, "min": 32.41}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.85, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 17.98, "peak": 19.68, "min": 16.15}, "VDD_GPU": {"avg": 31.36, "peak": 40.18, "min": 16.16}}, "power_watts_avg": 31.36, "energy_joules_est": 47.59, "sample_count": 15, "duration_seconds": 1.518}, "timestamp": "2026-01-19T14:07:04.550053"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1679.931, "latencies_ms": [1679.931], "images_per_second": 0.595, "prompt_tokens": 1109, "response_tokens_est": 23, "n_tiles": 1, "output_text": " The giraffe is brown and white with a long neck and legs. The sky is cloudy and the grass is dry.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26695.3, "ram_available_mb": 99076.9, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 27.0, "ram_used_mb": 26994.1, "ram_available_mb": 98778.1, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 78.8, "peak": 121.07, "min": 35.85}, "VIN_SYS_5V0": {"avg": 14.54, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 19.52, "peak": 23.23, "min": 16.15}, "VDD_GPU": {"avg": 31.17, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 31.17, "energy_joules_est": 52.38, "sample_count": 17, "duration_seconds": 1.68}, "timestamp": "2026-01-19T14:07:06.317583"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1528.032, "latencies_ms": [1528.032], "images_per_second": 0.654, "prompt_tokens": 1099, "response_tokens_est": 18, "n_tiles": 1, "output_text": " A family with two children is on a luggage cart in front of a car dealership.", "error": null, "sys_before": {"cpu_percent": 83.3, "ram_used_mb": 26994.1, "ram_available_mb": 98778.1, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 26.7, "ram_used_mb": 26809.6, "ram_available_mb": 98962.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.79, "peak": 145.91, "min": 29.36}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 21.02, "peak": 26.76, "min": 16.14}, "VDD_GPU": {"avg": 31.59, "peak": 39.77, "min": 17.34}}, "power_watts_avg": 31.59, "energy_joules_est": 48.28, "sample_count": 15, "duration_seconds": 1.528}, "timestamp": "2026-01-19T14:07:07.885261"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2816.994, "latencies_ms": [2816.994], "images_per_second": 0.355, "prompt_tokens": 1113, "response_tokens_est": 65, "n_tiles": 1, "output_text": " 1. suitcase: 2\n2. car: 2\n3. child: 2\n4. suitcase handle: 1\n5. suitcase wheels: 2\n6. car mirror: 1\n7. car door: 1\n8. car license plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26809.6, "ram_available_mb": 98962.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 18.3, "ram_used_mb": 26784.8, "ram_available_mb": 98987.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.01, "peak": 113.31, "min": 30.73}, "VIN_SYS_5V0": {"avg": 14.65, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 17.43, "peak": 22.05, "min": 14.57}, "VDD_GPU": {"avg": 26.69, "peak": 40.57, "min": 17.35}}, "power_watts_avg": 26.69, "energy_joules_est": 75.2, "sample_count": 28, "duration_seconds": 2.817}, "timestamp": "2026-01-19T14:07:10.792522"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2094.689, "latencies_ms": [2094.689], "images_per_second": 0.477, "prompt_tokens": 1117, "response_tokens_est": 38, "n_tiles": 1, "output_text": " The luggage is positioned in the foreground, with the children sitting on it. The luggage is located to the left of the children, and the car is positioned to the right of the luggage.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26784.8, "ram_available_mb": 98987.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 26791.3, "ram_available_mb": 98980.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 65.51, "peak": 116.21, "min": 29.12}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.65, "min": 11.85}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 13.79}, "VDD_GPU": {"avg": 28.03, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.03, "energy_joules_est": 58.72, "sample_count": 21, "duration_seconds": 2.095}, "timestamp": "2026-01-19T14:07:12.975832"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1831.249, "latencies_ms": [1831.249], "images_per_second": 0.546, "prompt_tokens": 1111, "response_tokens_est": 29, "n_tiles": 1, "output_text": " A family of three is traveling to a new destination, with the children sitting on a luggage cart and the parents pushing it through a parking lot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26791.6, "ram_available_mb": 98980.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.59, "peak": 120.8, "min": 29.74}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 29.66, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.66, "energy_joules_est": 54.32, "sample_count": 18, "duration_seconds": 1.832}, "timestamp": "2026-01-19T14:07:14.845536"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2226.157, "latencies_ms": [2226.157], "images_per_second": 0.449, "prompt_tokens": 1109, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image depicts a rainy day with wet pavement and a cloudy sky. The colors in the image are muted due to the rain, with the blue of the sky and the gray of the pavement being the most prominent.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26772.1, "ram_available_mb": 99000.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.64, "peak": 134.54, "min": 30.94}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.75, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.3, "peak": 40.18, "min": 17.35}}, "power_watts_avg": 28.3, "energy_joules_est": 63.01, "sample_count": 22, "duration_seconds": 2.227}, "timestamp": "2026-01-19T14:07:17.142923"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1573.997, "latencies_ms": [1573.997], "images_per_second": 0.635, "prompt_tokens": 1099, "response_tokens_est": 20, "n_tiles": 1, "output_text": " A yellow and blue bus is driving down a street in a town with a mountain in the background.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26772.1, "ram_available_mb": 99000.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26770.6, "ram_available_mb": 99001.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.55, "peak": 120.49, "min": 27.87}, "VIN_SYS_5V0": {"avg": 14.29, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 30.53, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 30.53, "energy_joules_est": 48.07, "sample_count": 16, "duration_seconds": 1.574}, "timestamp": "2026-01-19T14:07:18.813133"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2574.115, "latencies_ms": [2574.115], "images_per_second": 0.388, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. bus: 1\n2. street: 1\n3. building: 2\n4. car: 1\n5. person: 1\n6. flower: 1\n7. street light: 1\n8. mountain: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26770.6, "ram_available_mb": 99001.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 26754.4, "ram_available_mb": 99017.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 62.8, "peak": 119.11, "min": 31.34}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 15.44, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 27.29, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 27.29, "energy_joules_est": 70.26, "sample_count": 25, "duration_seconds": 2.574}, "timestamp": "2026-01-19T14:07:21.424097"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2584.631, "latencies_ms": [2584.631], "images_per_second": 0.387, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The yellow and blue bus is in the foreground, driving on the road. The white building is in the background, and the green hill is in the far background. The bus is to the left of the white building, and the green hill is behind the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.4, "ram_available_mb": 99017.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26754.0, "ram_available_mb": 99018.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 58.61, "peak": 118.03, "min": 30.55}, "VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.93, "min": 13.79}, "VDD_GPU": {"avg": 26.56, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 26.56, "energy_joules_est": 68.66, "sample_count": 25, "duration_seconds": 2.585}, "timestamp": "2026-01-19T14:07:24.020646"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2927.005, "latencies_ms": [2927.005], "images_per_second": 0.342, "prompt_tokens": 1111, "response_tokens_est": 72, "n_tiles": 1, "output_text": " The image captures a quaint town nestled in the mountains, where a vibrant yellow and blue double-decker bus is making its way down the street, passing by a charming white building with a black roof. The lush green grass and colorful flowers add a touch of vibrancy to the scene, creating a picturesque setting that exudes a sense of tranquility and charm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26754.0, "ram_available_mb": 99018.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26834.7, "ram_available_mb": 98937.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.61, "peak": 124.32, "min": 34.12}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 15.85, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 25.98, "peak": 39.78, "min": 16.16}}, "power_watts_avg": 25.98, "energy_joules_est": 76.06, "sample_count": 29, "duration_seconds": 2.928}, "timestamp": "2026-01-19T14:07:27.042087"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2024.45, "latencies_ms": [2024.45], "images_per_second": 0.494, "prompt_tokens": 1109, "response_tokens_est": 36, "n_tiles": 1, "output_text": " The sky is overcast, casting a muted light over the scene. The colors are muted, with the green of the grass and the blue of the sky blending together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26834.7, "ram_available_mb": 98937.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 15.2, "ram_used_mb": 27008.2, "ram_available_mb": 98764.0, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.94, "peak": 121.69, "min": 32.66}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.95, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 17.6, "peak": 19.68, "min": 13.39}, "VDD_GPU": {"avg": 28.44, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 28.44, "energy_joules_est": 57.59, "sample_count": 20, "duration_seconds": 2.025}, "timestamp": "2026-01-19T14:07:29.128121"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1750.989, "latencies_ms": [1750.989], "images_per_second": 0.571, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A pelican stands on a rocky outcropping overlooking a beach, with a cloudy sky and distant mountains in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 27008.2, "ram_available_mb": 98764.0, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 22.7, "ram_used_mb": 27007.5, "ram_available_mb": 98764.7, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.53, "peak": 129.87, "min": 30.36}, "VIN_SYS_5V0": {"avg": 14.5, "peak": 15.75, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 18.48, "peak": 20.48, "min": 15.76}, "VDD_GPU": {"avg": 30.19, "peak": 39.77, "min": 16.55}}, "power_watts_avg": 30.19, "energy_joules_est": 52.88, "sample_count": 17, "duration_seconds": 1.752}, "timestamp": "2026-01-19T14:07:30.901930"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2534.204, "latencies_ms": [2534.204], "images_per_second": 0.395, "prompt_tokens": 1113, "response_tokens_est": 56, "n_tiles": 1, "output_text": " 1. bird: 1\n2. rocks: 1\n3. beach: 1\n4. mountains: 1\n5. sea: 1\n6. clouds: 1\n7. sky: 1\n8. pier: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27007.5, "ram_available_mb": 98764.7, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 27010.5, "ram_available_mb": 98761.7, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.28, "peak": 124.15, "min": 29.99}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.57}, "VDD_GPU": {"avg": 27.44, "peak": 40.18, "min": 18.13}}, "power_watts_avg": 27.44, "energy_joules_est": 69.55, "sample_count": 25, "duration_seconds": 2.535}, "timestamp": "2026-01-19T14:07:33.503767"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2656.378, "latencies_ms": [2656.378], "images_per_second": 0.376, "prompt_tokens": 1117, "response_tokens_est": 60, "n_tiles": 1, "output_text": " The pelican is positioned in the foreground, perched on a rock in the middle of the image. The beach, with its sandy shore and thatched umbrellas, is located in the background. The mountains rise up in the far background, providing a sense of depth and scale to the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 27010.5, "ram_available_mb": 98761.7, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 27010.5, "ram_available_mb": 98761.7, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.31, "peak": 120.56, "min": 28.7}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 26.64, "peak": 40.18, "min": 15.38}}, "power_watts_avg": 26.64, "energy_joules_est": 70.77, "sample_count": 26, "duration_seconds": 2.657}, "timestamp": "2026-01-19T14:07:36.208873"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2229.122, "latencies_ms": [2229.122], "images_per_second": 0.449, "prompt_tokens": 1111, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The image captures a serene beach scene with a pelican perched on a rock, overlooking the calm ocean. The sky is filled with clouds, and the beach is lined with palm trees and thatched-roof huts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27010.5, "ram_available_mb": 98761.7, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 27010.2, "ram_available_mb": 98762.0, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.75, "peak": 134.57, "min": 28.87}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 27.92, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.92, "energy_joules_est": 62.24, "sample_count": 22, "duration_seconds": 2.229}, "timestamp": "2026-01-19T14:07:38.506833"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2377.61, "latencies_ms": [2377.61], "images_per_second": 0.421, "prompt_tokens": 1109, "response_tokens_est": 49, "n_tiles": 1, "output_text": " The image features a beach with a rocky outcropping, a pelican standing on it, and a cloudy sky with a hint of blue. The colors are predominantly blue and brown, with the pelican being the only animal in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 27010.2, "ram_available_mb": 98762.0, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 27006.1, "ram_available_mb": 98766.1, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.48, "peak": 119.05, "min": 28.81}, "VIN_SYS_5V0": {"avg": 14.32, "peak": 15.75, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 27.14, "peak": 39.78, "min": 15.77}}, "power_watts_avg": 27.14, "energy_joules_est": 64.54, "sample_count": 24, "duration_seconds": 2.378}, "timestamp": "2026-01-19T14:07:40.986392"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1708.719, "latencies_ms": [1708.719], "images_per_second": 0.585, "prompt_tokens": 1099, "response_tokens_est": 25, "n_tiles": 1, "output_text": " A man wearing glasses and a plaid shirt is sitting in a chair, holding a remote control and a piece of paper.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 27006.1, "ram_available_mb": 98766.1, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26999.7, "ram_available_mb": 98772.4, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.95, "peak": 112.79, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.22, "peak": 15.54, "min": 11.65}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 29.8, "peak": 39.39, "min": 14.98}}, "power_watts_avg": 29.8, "energy_joules_est": 50.93, "sample_count": 17, "duration_seconds": 1.709}, "timestamp": "2026-01-19T14:07:42.765146"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2609.937, "latencies_ms": [2609.937], "images_per_second": 0.383, "prompt_tokens": 1113, "response_tokens_est": 59, "n_tiles": 1, "output_text": " 1. person: 1\n2. remote control: 2\n3. paper bag: 1\n4. television: 1\n5. couch: 1\n6. glasses: 1\n7. remote control: 2\n8. television: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26999.7, "ram_available_mb": 98772.4, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26995.4, "ram_available_mb": 98776.8, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 63.74, "peak": 124.72, "min": 33.64}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 27.28, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 27.28, "energy_joules_est": 71.21, "sample_count": 25, "duration_seconds": 2.61}, "timestamp": "2026-01-19T14:07:45.379826"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2427.809, "latencies_ms": [2427.809], "images_per_second": 0.412, "prompt_tokens": 1117, "response_tokens_est": 53, "n_tiles": 1, "output_text": " The man is seated on the left side of the image, with the remote control and paper bag placed on the right side of the image. The paper bag is positioned in front of the man, while the remote control is located to the right of the paper bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26995.4, "ram_available_mb": 98776.8, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26995.9, "ram_available_mb": 98776.3, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.83, "peak": 120.21, "min": 29.48}, "VIN_SYS_5V0": {"avg": 14.75, "peak": 15.95, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 27.53, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 27.53, "energy_joules_est": 66.85, "sample_count": 24, "duration_seconds": 2.428}, "timestamp": "2026-01-19T14:07:47.870142"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1842.22, "latencies_ms": [1842.22], "images_per_second": 0.543, "prompt_tokens": 1111, "response_tokens_est": 30, "n_tiles": 1, "output_text": " A man is sitting in a chair, holding a remote control and a bag of chips. He is wearing glasses and has a smile on his face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26995.9, "ram_available_mb": 98776.3, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26995.5, "ram_available_mb": 98776.7, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.44, "peak": 134.2, "min": 30.09}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 13.0}, "VDD_GPU": {"avg": 29.55, "peak": 39.39, "min": 15.77}}, "power_watts_avg": 29.55, "energy_joules_est": 54.45, "sample_count": 18, "duration_seconds": 1.843}, "timestamp": "2026-01-19T14:07:49.757106"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1727.19, "latencies_ms": [1727.19], "images_per_second": 0.579, "prompt_tokens": 1109, "response_tokens_est": 26, "n_tiles": 1, "output_text": " The room is dimly lit with a warm ambiance, and the man is wearing a black and white checkered shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26995.5, "ram_available_mb": 98776.7, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 26995.5, "ram_available_mb": 98776.7, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.83, "peak": 128.86, "min": 30.09}, "VIN_SYS_5V0": {"avg": 14.47, "peak": 15.65, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 30.61, "peak": 39.78, "min": 17.35}}, "power_watts_avg": 30.61, "energy_joules_est": 52.88, "sample_count": 17, "duration_seconds": 1.728}, "timestamp": "2026-01-19T14:07:51.534871"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1668.259, "latencies_ms": [1668.259], "images_per_second": 0.599, "prompt_tokens": 1100, "response_tokens_est": 24, "n_tiles": 1, "output_text": " A man in an orange shirt and black shorts is playing tennis on a court with a blue and white tennis racket.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 26995.5, "ram_available_mb": 98776.7, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26996.7, "ram_available_mb": 98775.4, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.19, "peak": 122.71, "min": 35.14}, "VIN_SYS_5V0": {"avg": 14.42, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 31.0, "peak": 39.78, "min": 17.74}}, "power_watts_avg": 31.0, "energy_joules_est": 51.74, "sample_count": 16, "duration_seconds": 1.669}, "timestamp": "2026-01-19T14:07:53.214821"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2687.17, "latencies_ms": [2687.17], "images_per_second": 0.372, "prompt_tokens": 1114, "response_tokens_est": 63, "n_tiles": 1, "output_text": " 1. man: 1\n2. tennis racket: 1\n3. tennis ball: 1\n4. tennis court: 1\n5. net: 1\n6. orange shirt: 1\n7. black shorts: 1\n8. white shoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26996.7, "ram_available_mb": 98775.4, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26996.7, "ram_available_mb": 98775.4, "ram_percent": 21.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.71, "peak": 122.62, "min": 28.79}, "VIN_SYS_5V0": {"avg": 14.7, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 14.18}, "VDD_GPU": {"avg": 27.21, "peak": 40.18, "min": 18.91}}, "power_watts_avg": 27.21, "energy_joules_est": 73.14, "sample_count": 26, "duration_seconds": 2.688}, "timestamp": "2026-01-19T14:07:55.923557"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2316.954, "latencies_ms": [2316.954], "images_per_second": 0.432, "prompt_tokens": 1118, "response_tokens_est": 48, "n_tiles": 1, "output_text": " The tennis player is positioned on the left side of the image, with the tennis ball in the center and the net in the foreground. The player is closer to the camera than the tennis ball, which is in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26996.7, "ram_available_mb": 98775.4, "ram_percent": 21.5}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 26815.3, "ram_available_mb": 98956.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.69, "peak": 115.45, "min": 28.74}, "VIN_SYS_5V0": {"avg": 14.63, "peak": 15.75, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 27.48, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 27.48, "energy_joules_est": 63.68, "sample_count": 23, "duration_seconds": 2.317}, "timestamp": "2026-01-19T14:07:58.324111"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1408.565, "latencies_ms": [1408.565], "images_per_second": 0.71, "prompt_tokens": 1112, "response_tokens_est": 13, "n_tiles": 1, "output_text": " A man is playing tennis on a court with a green surface.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26815.3, "ram_available_mb": 98956.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.22, "peak": 123.21, "min": 28.97}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.75, "min": 11.97}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.7, "peak": 39.78, "min": 15.38}}, "power_watts_avg": 30.7, "energy_joules_est": 43.26, "sample_count": 14, "duration_seconds": 1.409}, "timestamp": "2026-01-19T14:07:59.781666"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1537.176, "latencies_ms": [1537.176], "images_per_second": 0.651, "prompt_tokens": 1110, "response_tokens_est": 18, "n_tiles": 1, "output_text": " The tennis player is wearing an orange shirt and black shorts, and the court is green.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.16, "peak": 104.44, "min": 27.52}, "VIN_SYS_5V0": {"avg": 14.23, "peak": 15.44, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.2, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 32.2, "energy_joules_est": 49.51, "sample_count": 15, "duration_seconds": 1.537}, "timestamp": "2026-01-19T14:08:01.356739"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2315.946, "latencies_ms": [2315.946], "images_per_second": 0.432, "prompt_tokens": 1432, "response_tokens_est": 33, "n_tiles": 1, "output_text": " The image depicts a compact kitchen area with a variety of appliances and cabinets, including a sink, stove, and refrigerator, all arranged in a functional and efficient manner.", "error": null, "sys_before": {"cpu_percent": 13.3, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.96, "peak": 119.02, "min": 27.64}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.26, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 30.28, "peak": 42.15, "min": 18.92}}, "power_watts_avg": 30.28, "energy_joules_est": 70.15, "sample_count": 23, "duration_seconds": 2.317}, "timestamp": "2026-01-19T14:08:03.763078"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2960.1, "latencies_ms": [2960.1], "images_per_second": 0.338, "prompt_tokens": 1446, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. cupboard: 4\n2. counter: 2\n3. sink: 1\n4. stove: 1\n5. refrigerator: 1\n6. door: 1\n7. chair: 1\n8. rug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26774.8, "ram_available_mb": 98997.4, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26775.0, "ram_available_mb": 98997.2, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.42, "peak": 132.76, "min": 28.51}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.36, "min": 11.87}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.71, "min": 13.0}, "VDD_GPU": {"avg": 28.3, "peak": 42.13, "min": 16.16}}, "power_watts_avg": 28.3, "energy_joules_est": 83.78, "sample_count": 29, "duration_seconds": 2.96}, "timestamp": "2026-01-19T14:08:06.777033"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2498.506, "latencies_ms": [2498.506], "images_per_second": 0.4, "prompt_tokens": 1450, "response_tokens_est": 39, "n_tiles": 1, "output_text": " The sink is located near the stove, and the refrigerator is positioned in the background. The door is situated on the left side of the room, and the countertop is located in the foreground.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26775.0, "ram_available_mb": 98997.2, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26776.5, "ram_available_mb": 98995.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.34, "peak": 126.63, "min": 33.89}, "VIN_SYS_5V0": {"avg": 14.66, "peak": 16.26, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 13.39}, "VDD_GPU": {"avg": 29.97, "peak": 41.74, "min": 16.16}}, "power_watts_avg": 29.97, "energy_joules_est": 74.89, "sample_count": 24, "duration_seconds": 2.499}, "timestamp": "2026-01-19T14:08:09.281462"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3044.516, "latencies_ms": [3044.516], "images_per_second": 0.328, "prompt_tokens": 1444, "response_tokens_est": 61, "n_tiles": 1, "output_text": " The image captures the interior of a vintage camper van, showcasing its compact yet functional kitchen area. The kitchen is equipped with a sink, stove, and refrigerator, all neatly arranged within the confines of the van. The walls and cabinets are painted in a cheerful yellow, providing a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.5, "ram_available_mb": 98995.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26776.5, "ram_available_mb": 98995.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.64, "peak": 129.72, "min": 27.43}, "VIN_SYS_5V0": {"avg": 14.76, "peak": 16.36, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 13.79}, "VDD_GPU": {"avg": 28.25, "peak": 41.74, "min": 17.34}}, "power_watts_avg": 28.25, "energy_joules_est": 86.02, "sample_count": 30, "duration_seconds": 3.045}, "timestamp": "2026-01-19T14:08:12.411380"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2498.944, "latencies_ms": [2498.944], "images_per_second": 0.4, "prompt_tokens": 1442, "response_tokens_est": 40, "n_tiles": 1, "output_text": " The interior of the camper is painted in a bright yellow color, with a black rubber mat on the floor. The lighting is provided by fluorescent lights, and the materials used are wood and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26776.5, "ram_available_mb": 98995.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26777.7, "ram_available_mb": 98994.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.07, "peak": 105.16, "min": 27.13}, "VIN_SYS_5V0": {"avg": 14.46, "peak": 16.36, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 29.38, "peak": 41.74, "min": 15.38}}, "power_watts_avg": 29.38, "energy_joules_est": 73.43, "sample_count": 25, "duration_seconds": 2.499}, "timestamp": "2026-01-19T14:08:15.016610"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1625.882, "latencies_ms": [1625.882], "images_per_second": 0.615, "prompt_tokens": 1099, "response_tokens_est": 22, "n_tiles": 1, "output_text": " A sandwich with lettuce, tomato, and ham is on a paper plate with pickles and mustard on the side.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26777.7, "ram_available_mb": 98994.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26777.7, "ram_available_mb": 98994.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 70.61, "peak": 117.39, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.44, "min": 11.54}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.41, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.41, "energy_joules_est": 49.46, "sample_count": 16, "duration_seconds": 1.627}, "timestamp": "2026-01-19T14:08:16.702809"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2618.667, "latencies_ms": [2618.667], "images_per_second": 0.382, "prompt_tokens": 1113, "response_tokens_est": 57, "n_tiles": 1, "output_text": " 1. sandwich: 1\n2. lettuce: 1\n3. tomato: 1\n4. cheese: 1\n5. ham: 1\n6. pickle: 2\n7. mustard: 1\n8. paper plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26777.7, "ram_available_mb": 98994.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26778.5, "ram_available_mb": 98993.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.3, "peak": 110.33, "min": 27.51}, "VIN_SYS_5V0": {"avg": 14.4, "peak": 15.65, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 26.88, "peak": 40.18, "min": 16.95}}, "power_watts_avg": 26.88, "energy_joules_est": 70.41, "sample_count": 26, "duration_seconds": 2.619}, "timestamp": "2026-01-19T14:08:19.414725"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2199.916, "latencies_ms": [2199.916], "images_per_second": 0.455, "prompt_tokens": 1117, "response_tokens_est": 43, "n_tiles": 1, "output_text": " The sandwich is located in the foreground of the image, with the plate of pickles and mustard placed to the right of the sandwich. The computer monitor is positioned in the background, behind the plate of pickles and mustard.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 26778.5, "ram_available_mb": 98993.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26778.5, "ram_available_mb": 98993.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 66.45, "peak": 123.04, "min": 28.47}, "VIN_SYS_5V0": {"avg": 14.28, "peak": 15.54, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.54, "min": 12.61}, "VDD_GPU": {"avg": 27.7, "peak": 39.39, "min": 14.59}}, "power_watts_avg": 27.7, "energy_joules_est": 60.95, "sample_count": 22, "duration_seconds": 2.2}, "timestamp": "2026-01-19T14:08:21.702850"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1504.299, "latencies_ms": [1504.299], "images_per_second": 0.665, "prompt_tokens": 1111, "response_tokens_est": 17, "n_tiles": 1, "output_text": " A sandwich is on a paper plate on a desk with a computer in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26778.5, "ram_available_mb": 98993.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 68.47, "peak": 128.22, "min": 30.52}, "VIN_SYS_5V0": {"avg": 14.18, "peak": 15.54, "min": 11.77}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.53, "min": 12.61}, "VDD_GPU": {"avg": 30.97, "peak": 39.39, "min": 15.38}}, "power_watts_avg": 30.97, "energy_joules_est": 46.61, "sample_count": 15, "duration_seconds": 1.505}, "timestamp": "2026-01-19T14:08:23.269872"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1547.675, "latencies_ms": [1547.675], "images_per_second": 0.646, "prompt_tokens": 1109, "response_tokens_est": 19, "n_tiles": 1, "output_text": " The sandwich is on a white paper plate with a green and white computer monitor in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 74.54, "peak": 125.9, "min": 29.01}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 15.54, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.53, "min": 13.0}, "VDD_GPU": {"avg": 32.28, "peak": 40.57, "min": 19.32}}, "power_watts_avg": 32.28, "energy_joules_est": 49.97, "sample_count": 15, "duration_seconds": 1.548}, "timestamp": "2026-01-19T14:08:24.839871"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1946.879, "latencies_ms": [1946.879], "images_per_second": 0.514, "prompt_tokens": 1099, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image depicts a desk with two computer monitors, a keyboard, a mouse, and a tablet, all arranged in a way that suggests a workspace or a home office setup.", "error": null, "sys_before": {"cpu_percent": 12.5, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.08, "peak": 126.04, "min": 27.97}, "VIN_SYS_5V0": {"avg": 14.58, "peak": 15.85, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 30.29, "peak": 40.57, "min": 20.1}}, "power_watts_avg": 30.29, "energy_joules_est": 58.99, "sample_count": 19, "duration_seconds": 1.947}, "timestamp": "2026-01-19T14:08:26.822503"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1937.075, "latencies_ms": [1937.075], "images_per_second": 0.516, "prompt_tokens": 1113, "response_tokens_est": 34, "n_tiles": 1, "output_text": " monitor: 2, keyboard: 1, mouse: 1, tablet: 1, phone: 1, camera: 1, computer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.6, "ram_available_mb": 98992.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26780.3, "ram_available_mb": 98991.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.69, "peak": 129.97, "min": 27.38}, "VIN_SYS_5V0": {"avg": 14.56, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 13.79}, "VDD_GPU": {"avg": 29.48, "peak": 39.39, "min": 16.95}}, "power_watts_avg": 29.48, "energy_joules_est": 57.12, "sample_count": 19, "duration_seconds": 1.938}, "timestamp": "2026-01-19T14:08:28.809112"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2575.1, "latencies_ms": [2575.1], "images_per_second": 0.388, "prompt_tokens": 1117, "response_tokens_est": 58, "n_tiles": 1, "output_text": " The keyboard is located in the foreground, to the left of the mouse. The tablet is positioned in the background, to the right of the keyboard. The computer monitors are placed above the keyboard and mouse, with the left monitor positioned to the left and the right monitor positioned to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.3, "ram_available_mb": 98991.8, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.76, "peak": 124.64, "min": 28.57}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.65, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 27.09, "peak": 39.78, "min": 16.56}}, "power_watts_avg": 27.09, "energy_joules_est": 69.77, "sample_count": 25, "duration_seconds": 2.575}, "timestamp": "2026-01-19T14:08:31.423431"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2281.224, "latencies_ms": [2281.224], "images_per_second": 0.438, "prompt_tokens": 1111, "response_tokens_est": 47, "n_tiles": 1, "output_text": " The image depicts a computer desk with multiple monitors, a keyboard, and a mouse. The monitors are displaying various screens, including a web browser and a document. The desk is equipped with a keyboard, a mouse, and a tablet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.6, "ram_available_mb": 98991.6, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26781.1, "ram_available_mb": 98991.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.4, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.72, "peak": 126.96, "min": 29.19}, "VIN_SYS_5V0": {"avg": 14.6, "peak": 15.75, "min": 12.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 13.39}, "VDD_GPU": {"avg": 28.06, "peak": 39.39, "min": 16.16}}, "power_watts_avg": 28.06, "energy_joules_est": 64.02, "sample_count": 22, "duration_seconds": 2.282}, "timestamp": "2026-01-19T14:08:33.716358"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1558.048, "latencies_ms": [1558.048], "images_per_second": 0.642, "prompt_tokens": 1109, "response_tokens_est": 20, "n_tiles": 1, "output_text": " The desk is white and the monitors are black. The lighting is dim and the room is dark.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26781.1, "ram_available_mb": 98991.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26781.3, "ram_available_mb": 98990.9, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5354.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.24, "peak": 117.24, "min": 35.0}, "VIN_SYS_5V0": {"avg": 14.72, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 31.65, "peak": 39.78, "min": 17.34}}, "power_watts_avg": 31.65, "energy_joules_est": 49.33, "sample_count": 15, "duration_seconds": 1.558}, "timestamp": "2026-01-19T14:08:35.281913"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2346.619, "latencies_ms": [2346.619], "images_per_second": 0.426, "prompt_tokens": 1432, "response_tokens_est": 35, "n_tiles": 1, "output_text": " The image shows a bathroom with a tiled floor that has a collage of people's faces on it, and a white toilet bowl with a blue and red sticker on it.", "error": null, "sys_before": {"cpu_percent": 11.1, "ram_used_mb": 26781.3, "ram_available_mb": 98990.9, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26779.2, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 73.33, "peak": 135.66, "min": 30.14}, "VIN_SYS_5V0": {"avg": 14.92, "peak": 16.46, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 30.69, "peak": 42.53, "min": 19.7}}, "power_watts_avg": 30.69, "energy_joules_est": 72.04, "sample_count": 23, "duration_seconds": 2.347}, "timestamp": "2026-01-19T14:08:37.685116"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2314.082, "latencies_ms": [2314.082], "images_per_second": 0.432, "prompt_tokens": 1446, "response_tokens_est": 34, "n_tiles": 1, "output_text": " 1. Toilet\n2. Floor\n3. Tile\n4. People\n5. Wall\n6. Door\n7. Light\n8. Mirror", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26779.2, "ram_available_mb": 98993.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26780.2, "ram_available_mb": 98992.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.56, "peak": 119.25, "min": 28.68}, "VIN_SYS_5V0": {"avg": 14.64, "peak": 16.36, "min": 12.27}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 30.63, "peak": 42.15, "min": 16.95}}, "power_watts_avg": 30.63, "energy_joules_est": 70.89, "sample_count": 23, "duration_seconds": 2.314}, "timestamp": "2026-01-19T14:08:40.072202"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2604.688, "latencies_ms": [2604.688], "images_per_second": 0.384, "prompt_tokens": 1450, "response_tokens_est": 44, "n_tiles": 1, "output_text": " The main objects are the toilet and the floor. The toilet is located in the background, while the floor is in the foreground. The floor is also the closest object to the viewer, while the toilet is the farthest.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26780.2, "ram_available_mb": 98992.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 26780.2, "ram_available_mb": 98992.0, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 67.02, "peak": 117.13, "min": 30.5}, "VIN_SYS_5V0": {"avg": 14.55, "peak": 16.36, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 29.37, "peak": 42.15, "min": 16.56}}, "power_watts_avg": 29.37, "energy_joules_est": 76.51, "sample_count": 26, "duration_seconds": 2.605}, "timestamp": "2026-01-19T14:08:42.768057"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1794.083, "latencies_ms": [1794.083], "images_per_second": 0.557, "prompt_tokens": 1444, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A group of people are standing on a tiled floor in a bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26780.2, "ram_available_mb": 98992.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26781.4, "ram_available_mb": 98990.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.31, "peak": 128.76, "min": 28.41}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.15, "min": 11.75}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 12.61}, "VDD_GPU": {"avg": 32.48, "peak": 41.76, "min": 15.77}}, "power_watts_avg": 32.48, "energy_joules_est": 58.29, "sample_count": 18, "duration_seconds": 1.795}, "timestamp": "2026-01-19T14:08:44.647703"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1679.881, "latencies_ms": [1679.881], "images_per_second": 0.595, "prompt_tokens": 1442, "response_tokens_est": 10, "n_tiles": 1, "output_text": " The bathroom has white tiles and a white toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26781.4, "ram_available_mb": 98990.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 26781.4, "ram_available_mb": 98990.7, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 79.06, "peak": 120.74, "min": 28.6}, "VIN_SYS_5V0": {"avg": 14.38, "peak": 16.36, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 33.81, "peak": 42.54, "min": 18.92}}, "power_watts_avg": 33.81, "energy_joules_est": 56.81, "sample_count": 17, "duration_seconds": 1.68}, "timestamp": "2026-01-19T14:08:46.418681"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1830.003, "latencies_ms": [1830.003], "images_per_second": 0.546, "prompt_tokens": 1432, "response_tokens_est": 15, "n_tiles": 1, "output_text": " A bird with a grey body and black beak is perched on a branch.", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 26781.4, "ram_available_mb": 98990.7, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.2, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 71.51, "peak": 120.9, "min": 29.44}, "VIN_SYS_5V0": {"avg": 14.49, "peak": 16.15, "min": 11.46}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 12.21}, "VDD_GPU": {"avg": 32.98, "peak": 42.15, "min": 20.5}}, "power_watts_avg": 32.98, "energy_joules_est": 60.37, "sample_count": 18, "duration_seconds": 1.83}, "timestamp": "2026-01-19T14:08:48.303060"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1503.176, "latencies_ms": [1503.176], "images_per_second": 0.665, "prompt_tokens": 1446, "response_tokens_est": 4, "n_tiles": 1, "output_text": " bird: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 80.34, "peak": 131.2, "min": 30.56}, "VIN_SYS_5V0": {"avg": 14.62, "peak": 16.46, "min": 12.17}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 13.39}, "VDD_GPU": {"avg": 34.93, "peak": 42.53, "min": 19.71}}, "power_watts_avg": 34.93, "energy_joules_est": 52.53, "sample_count": 15, "duration_seconds": 1.504}, "timestamp": "2026-01-19T14:08:49.858262"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1994.49, "latencies_ms": [1994.49], "images_per_second": 0.501, "prompt_tokens": 1450, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The bird is positioned in the foreground, perched on a branch, while the background is filled with green foliage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 69.44, "peak": 113.51, "min": 28.27}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 16.26, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 33.78, "peak": 43.31, "min": 20.89}}, "power_watts_avg": 33.78, "energy_joules_est": 67.39, "sample_count": 20, "duration_seconds": 1.995}, "timestamp": "2026-01-19T14:08:51.942714"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1728.466, "latencies_ms": [1728.466], "images_per_second": 0.579, "prompt_tokens": 1444, "response_tokens_est": 11, "n_tiles": 1, "output_text": " A bird is perched on a branch in a forest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 76.55, "peak": 116.9, "min": 28.42}, "VIN_SYS_5V0": {"avg": 14.41, "peak": 16.36, "min": 11.66}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.71, "min": 12.61}, "VDD_GPU": {"avg": 33.27, "peak": 42.13, "min": 17.74}}, "power_watts_avg": 33.27, "energy_joules_est": 57.52, "sample_count": 17, "duration_seconds": 1.729}, "timestamp": "2026-01-19T14:08:53.715232"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2008.76, "latencies_ms": [2008.76], "images_per_second": 0.498, "prompt_tokens": 1442, "response_tokens_est": 22, "n_tiles": 1, "output_text": " The bird is gray and black, and the photo was taken in a forest with a lot of green leaves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26781.7, "ram_available_mb": 98990.5, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 26781.4, "ram_available_mb": 98990.8, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4305.9, "gpu_mem_reserved_mb": 6460.0, "gpu_max_mem_alloc_mb": 5677.3, "gpu_max_mem_reserved_mb": 6460.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN": {"avg": 72.49, "peak": 129.66, "min": 29.0}, "VIN_SYS_5V0": {"avg": 14.53, "peak": 16.15, "min": 12.07}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 13.0}, "VDD_GPU": {"avg": 32.85, "peak": 42.15, "min": 20.89}}, "power_watts_avg": 32.85, "energy_joules_est": 66.0, "sample_count": 20, "duration_seconds": 2.009}, "timestamp": "2026-01-19T14:08:55.804568"}
